DISTRIBUTED OPERATING SYSTEMS

IEEE Press
445 Hoes Lane, P.O. Box 1331
Piscataway, NJ 08855-1331
EditorialBoard
John B.Anderson,Editor in Chief'
P.M.Anderson A.H.Haddad P. Laplante
M. Eden R.Herrick R. S. Muller
M. E. El-Hawary G.F.Hoffnagle W.D. Reeve
S. Furui R.F.Hoyt D. J. Wells
S.Kartalopoulos
Dudley R. Kay,Director ofBook Publishing
John Griffin,Senior Editor
LisaDayne, Assistant Editor
Linda Matarazzo,EditorialAssistant
DenisePhillip,Associate Production Editor
IEEECommunicationsSociety,Sponsor
TomRobertazzi,C-S Liaison to IEEE Press
Technical Reviewers
Dr.WalterKohler,Digital Equipment Corporation
Mr.HaroldLorin,The Manticore Consultancy
Ms.Celine Vaolt
Dr.WayneWolf, Princeton University
Also from IEEE Press
Computer Communications and Networks, Second Edition
John Freer
Copublishedwith VeL Press
1996 Hardcover 400 pp IEEEOrder No,PC5654 ISBN 0-7803-1179-5
Engineering Networks for Synchronization, CCS 7 and ISDN: Standards, Protocols,
Planning, and Testing
P. K.Bhatnagar
1997 Hardcover 528 pp IEEEOrder No. PC5628 ISBN 0-7803-1158-2
SONET-SDH: A Sourcebook of Synchronous Networking
editedby CurtisA.Siller,Jr.and Mansoor Shafi
1996 Hardcover 406 pp IEEEOrder No. PC4457 ISBN 0-7803-1168-X

DISTRIBUTED OPERATING SYSTEMS
Concepts and Design
Pradeep K. Sinha
Centrefor Development ofAdvanced Computing
~
IEEE COMPUTER
~
SOCIETY PRESS
.®
+
IEEE
PRESS
®
IEEE Communications Society, Sponsor
The Institute of Electrical and Electronics Engineers, Inc., New York

ANOmTOrasREADER
Thisbookhasbeenelectronicallyreproducedfrom
digitalinformationstoredatJohnWiley& Sons,Inc.
Wearepleasedthattheuseoftbisnewtechnology
willenableustokeepworksofenduringscholarly
valueinprintaslongasthereisareasonabledemand
forthem. Thecontentofthisbookisidenticalto
previousprintings.
This book may be purchased at a discount from the publisher
when ordered in bulk quantities. Contact:
IEEE Press Marketing
Attn: Special Sales
445 Hoes Lane, ~O. Box 1331
Piscataway, NJ 08855-1331
Fax: (732) 981-9334
For more information on the IEEE Press, visit the IEEE home page:
http://www.ieee.org/
© 1997by the InstituteofElectricaland ElectronicsEngineers, Inc.,
3 ParkAvenue, 17thFloor, New York, NY 10016-5997
All rights reserved. No partofthis book may be reproducedin anyform,
nor may it be stored in a retrievalsystem or transmitted in any form,
without written permissionfrom the publisher.
Printed in the United States of America
10 9 8 7 6 5 4 3
ISBN0-7803-1119-1
IEEE Order Number: PC470S
Library of Congress Cataloging-in-Publication Data
Sinha, Pradeep K. (Pradeep Kumar)
Distributed operating systemslPradeepK. Sinha.
p. em.
Includes bibliographical references and index.
ISBN0-7803-1119-1 (cloth)
I. Distributedoperating systems (Computers) I. Title.
QA76.76.063S568 1996 96..26565
005.4'4--dc20 CIP

Contents
Pr.face xi
Acknowledgments xv
Abbreviations Qnd Acronyms xvii
Chapter 1: Fundamentals 1
1.1 What Is a Distributed Computing System?
1.2 Evolution of Distributed Computing Systems 3
1.3 Distributed Computing System Models 5
1.4 Why Are Distributed Computing Systems Gaining Popularity? 12
1.5 What Is a Distributed Operating System? 16
1.6 Issues in Designing a Distributed Operating System 19
1.7 Introduction to Distributed Computing Environment (DCE) 34
1.8 Summary 38
Exercises 39
Bibliography 41
Pointers to Bibliographies on the Internet 44
v

vi Contents
ChapterI: Computar Networks 46
2.1 Introduction 46
2.2 Networks Types 47
2.3 LAN Technologies 48
2.4 WANTechnologies 59
2.5 Communication Protocols 64
2.6 Intemetworking 83
2.7 ATMTechnology 91
2.8 Summary 104
Exercises 105
Bibliography 108
Pointers to Bibliographies on the Internet 112
Chapter3: Messag_ Passing 114
3.1 Introduction 114
3.2 Desirable Features of a Good Message-Passing System 115
3.3 Isses in IPC by Message Passing 118
3.4 Synchronization 120
3.5 Buffering 122
3.6 Multidatagram Messages 125
3.7 Encoding and Decoding of Message Data 126
3.8 Process Addressing 127
3.9 Failure Handling 130
3.10 Group Communication 139
3.11 Case Study: 4.38SD UNIX IPC Mechanism 153
3.12 Summary 157
Exercises 160
Bibliography 163
Pointers to Bibliographies on the Internet 166
Chapter4: RemotaProcedure Calls 167
4.1 Introduction 167
4.2 The RPCModel 168
4.3 Transparency of RPC 170
4.4 Implementing RPC Mechanism 171
4.5 Stub Generation 174
4.6 RPC Messages 174
4.7 Marshaling Arguments and Results 177
4.8 Server Management 178
4.9 Parameter-Passing Semantics 183

Contents vii
4.10 Call Semantics 184
4.11 Communication Protocols for RPCs 187
4.12 Complicated RPCs 191
4.13 Client-Server Binding 193
4.14 Exception Handling 198
4.15 Security 198
4.16 Some Special Types of RPCs 199
4.17 RPCin Heterogeneous Environments 203
4.18 Lightweight RPC 204
4.19 Optimizations for Better Performance 208
4.20 Case Studies: Sun RPC, r)CI~ RPC 212
4.21 Summary 222
Exercises 224
Bibliography 227
Pointers to Bibliographies on the Internet 230
Chapter 5: Distributed Shared Memory 231
5.1 Introduction 231
5.2 General Architecture of I)SM Systerns 233
5.3 Design and Implementation Issues of DSM 234
5.4 Granularity 235
5.5 Structure ofShared Memory Space 237
5.6 Consistency Models 238
5.7 Replacement Strategy 262
5.8 Thrashing 264
5.9 OtherApproaches to DSM 266
5.10 Heterogeneous DSM 267
5.11 Advantages ofDSM 270
5.12 Summary 272
Exercises 274
Bibliography 275
Pointers to Bibliographies on the Internet 281
Chapter 6: Synchronization 282
6.1 Introduction 282
6.2 Clock Synchronization 283
6.3 Event Ordering 292
6.4 Mutual Exclusion 299
6.5 Deadlock 305
6.6 EJection Algorithms 332
6.7 Summary 336

viii Contents
Exercises 337
Bibliography 341
Pointers to Bibliographies on the Internet 345
Chapter 7: "-sourceMQnQgcament 347
7.1 Introduction 347
7.2 Desirable Features of a Good Global Scheduling Algorithm 348
7.3 Task AssignmentApproach 351
7.4 Load-BalancingApproach 355
7.5 Load-SharingApproach 367
7.6 Summary 371
Exercises 372
Bibliography 374
Pointers to Bibliographies on the Internet 380
Chapter 8: Process Management 381
8.1 Introduction 381
8.2 Process Migration 382
8.3 Threads 398
8.4 Summary 414
Exercises 415
Bibliography 418
Pointers to Bibliographies on the Internet 420
Chapter 9: Distributed File Systems 421
9.1 Introduction 421
9.2 Desirable Features of a Good Distributed File System 423
9.3 File Models 426
9.4 File-Accessing Models 427
9.5 File-Sharing Semantics 430
9.6 File-Caching Schemes 433
9.7 File Replication 440
9.8 FaultTolerance 447
9.9 Atomic Transactions 453
9.10 Design Principles 474
9.11 Case Study: DeE Distributed File Service 475
9.12 Summary 484
Exercises 486
Bibliography 489
Pointers to Bibliographies on the Internet 495

Contents ix
Chapter 10: Naming 496
10.1 Introduction 496
10.2 Desirable Features of a Good Naming System 497
10.3 Fundamental Terminologies and Concepts 499
10.4 System-Oriented Names 509
10.5 Object-Locating Mechanisms 512
10.6 Human-Oriented Names 515
10.7 Name Caches 541
10.8 Naming and Security 544
10.9 Case Study: DCE Directory Service 546
10.10 Summary 556
Exercises 558
Bibliography 560
Pointers to Bibliographies on the Internet 564
Chapter 11: Security 565
11.1 Introduction 565
11.2 Potential Attacks to Computer Systems 567
11.3 Cryptography 575
11.4 Authentication 586
11.5 Access Control 607
11.6 Digital Signatures 623
11.7 Design Principles 626
11.8 Case Study: DeE Security Service 627
11.9 Summary 630
Exercises 631
Bibliography 634
Pointers to Bibliographies onthe Internet 640
Chapter 12: Case Studies 642
12.1 Introduction 642
12.2 Amoeba 643
12.3 V-System 659
12.4 Mach 674
12.5 Chorus 696
12.6 A Comparison of Amoeba, V-System, Mach, and Chorus 714
12.7 Summary 718
Exercises 722
Bibliography 725
Pointers to Bibliographies on the Internet 730
Index 731

Preface
Motivation
The excellentprice/performance ratio offered by microprocessor-based workstationsover
traditional mainframe systems and the steady improvements in networking technologies
have made distributed computing systems very attractive. While the hardware issues of
building such systems have been fairly well understood for quite some time, the major
stumbling block until now has beentheavailabilityofgood distributedoperatingsystems.
Fortunately, recent researchanddevelopmentwork inacademic institutionsand industries
have helped us better understand the basic concepts and design issues in developing
distributedoperating systems. Distributedoperatingsystems are no moreonly inresearch
laboratories but are now commercially available.
With the proliferation ofdistributed computing systems, it has become increasingly
important for computer science and computer engineering students to learn about
distributedoperatingsystems. Asa result, anumberof universities have institutedregular
courses on distributed operating systems at the graduate level. Even in various
undergraduate-level operating systems courses, the fundamental concepts and design
principles of distributed operating systems have been incorporated.
However, there isstillalackofgood textbooksthatcan provide acomprehensiveand
solid introduction todistributedoperatingsystems inan orderly manner. Exceptforafew
xi

xii Preface
recently published books, almost all books in this area are research monographs.
Therefore, for both an educator and a student, the creation of an overall image of
distributed operating systems is currently a complicated and time-consuming task.
Furthermore, computerprofessionals and starting researchers who want to get an overall
pictureofdistributed operating systems so as to identify the various research and design
issues have difficulty in finding a good text for their purpose.
Motivated by these factors, I decided to do research toward the preparation of a
textbookon distributedoperatingsystems.My primaryobjectivewas toconciselypresent
a clearexplanation ofthe current state ofthe art in distributed operating systems so that
readers can gain sufficient background to appreciate more advanced materials of this
field.
The book is designed to provide a clear description of the fundamental concepts and
design principles that underlie distributed operating systems. It does not concentrate on
any particulardistributedoperatingsystemor hardware. Instead, itdiscusses, in ageneral
setting, the fundamental concepts and design principles that are applicable to a varietyof
distributedoperating systems. However, case studies are included in the text to relate the
discussed concepts with real distributed operating systems.
The material in the book has been drawn largely from the research literature in the
field. Ofthe vast amount ofresearch literature available in this field, effort was made to
selectand givemore emphasistothose conceptsthat areofpracticalvalue inreal systems,
rather than those that are only oftheoretical interest.
Each chapter contains carefully designed exercises that are meant to test the
understanding of the materials in the text and to stimulate investigation.
Anextensivesetofreferencesand alistofselectedpointerstoon-linebibliographies
on the Internet have been provided at the end ofeach chapterto allow interested readers
to explore more advanced materials dealing with finer details about each chapter's
topics.
Throughoutthe book, the style ofpresentation used is motivational and explanatory
in nature.
Chapter1providesanintroductiontodistributedcomputingsystems,distributedoperating
systems, and the issues involved in designing distributed operating systems. It also
provides a brief introduction to Distributed Computing Environment (DCE), whose
components are described as case studies ofkey technologies in several chapters ofthe
book.
Chapter 2 presents a brief introduction to computer networks and describes the
current state ofthe art in networking technology.

Preface xiii
Chapters 3, 4, and 5 describe the various communication techniques used for
exchange of information among the processes of a distributed computing system. In
particular, these three chapters deal with the issues involved inthe design ofinterprocess
communicationmechanismsand thecommonlyused practicalapproachesto handlethese
issues. Chapter 3 deals with the message-passing mechanism. Chapter 4 deals with the
remote procedure call mechanism, and Chapter 5 deals with the distributed shared
memory mechanism for interprocess communication.
Synchronization issues to be dealt with in a distributed system, such as clock
synchronization, mutual exclusion, deadlock, and election algorithms, are discussed in
Chapter 6.
Chapter 7 presents a discussion of the commonly used approaches for resource
management in distributed systems.
Chapter 8 deals with the process management issues. In particular, it presents a
discussion of process migration mechanisms and mechanisms to support threads
facility.
A discussion of the issues and the approaches for designing a file system for a
distributed system is given in Chapter 9.
Chapter 10deals with the issues and mechanisms for naming and locating objects in
distributed systems.
The security issues and security mechanisms fordistributed systems arediscussed in
Chapter 11.
Finally, Chapter 12 contains case studies of four existing distributed operating
systems to relate the concepts discussed in the preceding chapters with real distributed
operating systems.
Audience
The book is suitable for anyone who needs a concise and informal introduction to
distributed operating systems.
Itcan serve as an ideal textbook foracourse on distributedoperatingsystems. Itcan
also be used for advanced undergraduate and postgraduatecourses on operating systems,
which often need to cover the fundamental concepts and design issues of distributed
operating systems in addition to those of centralized operating systems.
The book can also be used as a self-study text by system managers, professional
software engineers, computer scientists, and researchers, who either need to learn about
distributedoperatingsystems orareinvolved inthedesign anddevelopmentofdistributed
operating systems or distributed application systems.
Advanced researchers will also find therich setofreferences and the pointers toon
line bibliographies on the Internet provided at the end of each chapter very helpful in
probing further on any particular topic.
Although full care has been taken to make the subject matter simple and easy to
understand by a wide range of readers, I have assumed that the reader has a knowledge
of elementary computer architecture and is familar with basic centralized operating
systems concepts discussed in standard operating systems textbooks.

xiv Preface
About Pointers toBibliographies ontheInternet
In addition to agood number of references provided intheend-of-chapterbibliographies,
I have also provided lists of selected pointers to the on-line bibliographies ofinterest on
the Internet. The purpose of these pointers is twofold:
1. The end-of-chapterbibliographiescontainonly selected references. A large number of
references on the topics covered in a chapter are not included in the chapter's
bibliography due to space limitations. The pointers may be used by interested readers
to locate such references.
2. The end-of-chapter bibliographies contain references to only already published
documents. Distributed operating systems is currently an active area of research, and
alarge volume of newdocuments are publishedalmost every month. Since theon-line
bibliographiesontheInternetareupdated fromtimetotime,interested readers mayuse
the pointers to locate those documents that are published after the publication of this
book.Thus, inaddition totheinformationcontainedinit,thebookalsoprovides away
for its readers to keep track of on-going research activities on the topics covered and
related topics.
Note that the end-of-chapter lists of pointers to on-line bibliographies are by no
means exhaustive. I have provided pointers for only those on-line bibliographies that I
knew about and Ifelt would be useful ineasily locating references ofinterest. Moreover,
it is often the case that there are many mirrors for an on-line bibliography (the same
bilbiographyexistsonmultiple sites). Forsuchcases,I haveprovided onlyonepointer for
a bibliography.
Alsonotethatmostoftheon-line bibliographiesarenotabouton-linedocuments,but
about on-line references todocuments. A vastmajority ofdocuments referenced intheon
line bibliographies only exist in hard-copy form. However, a few of the referenced
documents do have an on-line version on the Internet. For such documents, the
bibliographies normally contain URLs (Uniform Resource Locators) pointing to the on
line version ofthe document. If you find a reference containing such a URL, just follow
the URLto access the on-line version of the corresponding document.

Acknowledgments
Many people have contributed to this book, either directly or indirectly. To start with, I
must thank allthe researchers who havecontributedtothe fieldsofdistributed computing
systems and distributed operating systems because the book is based on their research
results.
Mamoru Maekawa, my Ph.D. supervisor, provided me with the opportunity to do
research in the area of distributed operating systems. Without this opportunity, this book
would not have been possible.
Lively discussions with the members of the Galaxy distributed operating system
project, including Kentaro Shimizu, Xiaohua Jia, Hyo Ashishara, Naoki Utsunomiya,
Hirohiko Nakano, Kyu Sung Park, and Jun Hamano, helped a lot in broadening my
knowledge of distributed operating systems.
Without the efforts of all the people who collected references and made them
available on the Internet, it would not have been possible for me to provide the end-of
chapter pointers to the bibliographies on the Internet.
Several anonymous reviewers of mydraft manuscript provided invaluable feedback
concerning organization, topic coverage, t.ypographicalerrors, and parts of the text that
were not as clear as they are now.
Myproductioneditor atIEEEPress, DenisePhillip, didanexcellentjob innumerous
ways to present the book in its current form. IEEE Press Director Dudley Kay,Senior
xv

xvi Acknowledgements
Acquisitions Editor John Griffin, and review coordinators Lisa Mizrahi andLisa Dayne
were ofgreat help in improving the overall quality ofthe book and in bringingit out in
a timely manner.
Finally,Ithank mywife, Priti,forpreparingtheelectronicversionoftheentirehand
writtendraft manuscript. Ialso thankherforhercontinuouspatienceandsacrificesduring
the entire period of this long project. Without her loving support and understanding, I
would never have succeeded in completing this project.
Pradeep K.Sinha

Abbreviations Qnd
Acronyms
AAL ATMAdaptation Layer CDS Cell Directory Service/Server
ACL Access Control List CERN European Centre forNuclear
AFS Andrew File System Research
ANSI American National Standards CFS Cedar File System
Institute CICS CustomerInformation Control
API Application Programming System
Interface CLP Cell Loss Priority
APPN Advanced Peer-to-Peer eMH Chandy-Misra-Hass
Networking CMIP Common Management
ARP Address Resolution Protocol Information Protocol
ARPANET Advanced Research Projects COOL Chorus Object-Oriented Layer
Agency NETwork CSMAlCD Carrier Sense Multiple Access
ASN.I Abstract Syntax Notation with Collision Detection
ATM AsynchronousTransfer Mode CSRG ComputerSystems Research
BIOS Basic Input OutputSystem Group
B-ISDN Broadband IntegratedServices DCE Distributed Computing
Digital Network Environment
CBR Constant Bit Rate DDLeN Distributed Double-Loop
CCITT InternationalTelegraphand Computer Network
Telephone Consultative DEC Digital Equipment Corporation
Committee DES Data Encryption Standard
xvii

xviii Abbreviations andAcronyms
DFS Distributed FileService IP InternetProtocol
DI Directory Identifier IPC Inter-Process Communication
DIB Directory Information Base ISDN IntegratedServicesDigital
OIT Directory Information Tree Network
DME Distributed Management ISO International Standards
Environment Organization
ON Distinguished Name lTV International
ONS Domain NamelNaming Telecommunications Union
Service/System KDBMS Kerberos DatabaseManagement
DoD Department of Defense Server
DSM Distributed SharedMemory KDC KeyDistributionCenter
DSVM Distributed SharedVirtual LAN LocalAreaNetwork
Memory LEC LANEmulationClient
DTS Distributed TimeService LES LANEmulationServer
Email Electronic Mail LRPC Lightweight RemoteProcedure
ERB Expanding RingBroadcast Call
FOOl FiberDistributed Data MAN Metropolitan AreaNetwork
Interface MBone MulticastBackbone
FIFO First-In First-Out Mbps Megabitspersecond
FLIP FastLocal InternetProtocol MIG MachInterfaceGenerator
FTP FileTransfer Protocol MMU MemoryManagementUnit
Gbps Gigabitspersecond MTBF MeanTimeBetweenFailures
GDA Global Directory Agent MTU Maximum TransferUnit
GDS Global Directory Service/Server NCA NetworkComputing
GEOS Geostationary Operational Architecture
Environmental Satellites NCS NetworkComputingSystem
GFC Generic FlowControl NFS NetworkFileSystem
GNS Global Name Service NIC NetworkInformationCenter
GNU Gnu'sNot Unix NIST National InstituteforStandards
GPS Global Positioning System andTechnology
HEe HeaderErrorControl NRMB Non-Replicated Migrating
HRPC Heterogeneous Remote Block
Procedure Call NRNMB Non-Replicated Non-Migrating
IBM International Business Machines Block
ICMP InternetControl Message NSAP NetworkServiceAccessPoint
Protocol NTP NetworkTimeProtocol
IOL Interface Definition Language NVMA Non-Uniform MemoryAccess
IEEE Institute ofElectrical and OC-n OpticalCarrierleveln
Electronics Engineers OLTP On LineTransaction
IETF InternetEngineering TaskForce Processing
IMS Information Management OPC OutputPortController
System OSF OpenSoftwareFoundation
INRIA Institute National deRecherche OSI OpenSystemInternational
enInformatiqueet PCB ProcessControlBlock
Automatique PEM PrivacyEnhancedMail

Abbreviations andAcronyms xix
PKM Public Key Manager STS-n SynchronousTransportSignal
PMD Physical Medium Dependent level n
POSIX PortableOperating System TC TransmissionConvergence
InterfaceforComputer TCF TransparentComputing Facility
Environments TCP TransportControl Protocol
PRAM PipelinedRandomAccess TFTP Trivial FileTransferProtocol
Memory TI-RPC Transport Independent-Remote
PSE PacketSwitching Exchange ProcedureCall
PTI PayloadTypeIdentifier TP Transport Protocol
RARP ReverseAddressResolution TWFG TransactionWait-For-Graph
Protocol VCP UnilateralCommit Protocol
RON RelativeDistinguishedName UDP User DatagramProtocol
RFS RemoteFile Server UDS UniversalDirectoryService
RFT RequestForTechnology UNI User NetworkInterface
RMB ReplicatedMigrating Block UTC Coordinated UniversalTime
RNMB ReplicatedNon-Migrating VUID UniversallyUniqueIdentifier
Block VBR VariableBit Rate
RPC RemoteProcedureCall vel VirtualChannel Identifier
RPCL RemoteProcedureCall VM VirtualMemory
Language VMTP Versatile MessageTransfer
RR Round-Robin Protocol
RSA Rivest-Shamir-Adleman VPI VirtualPath Identifier
RSS ResearchStorage System WAIS WideArea InformationServers
SDH SynchronousDigital Hierarchy WAN WideArea Network
SEAL Simpleand EfficientAdaptation WFG WaitFor Graph
Layer WWW \\lorld WideWeb
SLIP Serial Line InternetProtocol X-IPe eXtcnted Inter-Process
SMTP Simple MailTransferProtocol Communication
SNA System NetworkArchitecture XDR eXternal DataRepresentation
SNMP Simple NetworkManagement XDS X/Open DirectoryServer
Protocol XNS Xerox NetworkingSystem
SONET SynchronousOptical NETwork XOM X/OpenObjectManagement

1
CHAPTER
Fundamentals
1.1 WHAT ISADISTRIBUTED COMPUTING SYSTEM?
Over the past two decades, advancements in microelectronic technology have resulted in
the availability of fast, inexpensive processors, and advancements in communication
technology haveresulted intheavailability ofcost-effectiveandhighlyefficientcomputer
networks.The net result of the advancements in these two technologies isthat the price
performance ratiohasnowchanged tofavortheuseofinterconnected, multipleprocessors
in place of a single, high-speed processor.
Computer architectures consisting of interconnected, multiple processors are
basically of two types:
I. Tightly coupled systems. In these systems, there is a single systemwide primary
memory (address space) thatisshared byall theprocessors [Fig. l.1(a)].Ifanyprocessor
writes, for example, the value 100 to the memory location x, any other processor
subsequently reading from location x will get the value 100.Therefore,in these systems,
any communication between the processors usually takes place through the shared
memory.
2. Loosely coupled systems. In these systems, the processors do not share memory,
andeachprocessor hasitsownlocal memory [Fig. l.1(b)].Ifaprocessor writes the value
1

2 Chap.1 • Fundamentals
Systemwide
CPU CPU sharedmemory CPU CPU
I
Interconnectionhardware
(a)
Localmemory Localmemory Localmemory Localmemory
CPU CPU CPU CPU
Communicationnetwork
(b)
Fig_1.1 Differencebetween tightlycoupled andlooselycoupled multiprocessor
systems:(a) atightlycoupled multiprocessor system;(b) alooselycoupled
multiprocessor system.
100 to the memory location x, this write operation will only change the contents of its
local memory and will not affect the contents of the memory of any other processor.
Hence, ifanother processor reads the memory location x, it will get whatever value was
there before in that location of its own local memory. In these systems, all physical
communication between the processors is done by passing messages across the network
that interconnects the processors.
Usually, tightly coupled systems are referred to asparallel processing systems, and
loosely coupled systems are referred to as distributed computing systems, or simply
distributed systems.Inthisbook,however,theterm"distributedsystem"willbeusedonly
fortruedistributed systems-distributedcomputing systemsthatusedistributed operating
systems (see Section 1.5).Therefore, before the term "true distributed system" isdefined
in Section 1.5,the term "distributed computing system" will beused to refer to loosely
coupled systems. Incontrast to the tightly coupled systems, the processors of distributed
computing systems canbelocated farfromeachothertocover awider geographical area.
Furthermore, in tightly coupled systems, the number of processors that can be usefully
deployed isusually small andlimited by thebandwidthof theshared memory.This isnot
thecase withdistributed computing systemsthataremorefreelyexpandableandcanhave
an almost unlimited number of processors.

Sec.1.2• Evolution ofDistributedComputing Systems 3
In short, a distributed computing system is basically a collection of processors
interconnected bya communication network in which eachprocessor hasitsownlocal
memoryandotherperipherals,andthecommunicationbetweenanytwoprocessorsofthe
system takes place bymessage passing over thecommunicationnetwork. For aparticular
processor, itsownresources arelocal, whereastheotherprocessorsandtheirresources are
remote. Together, aprocessorand itsresources are usually referred toasanode or site or
machine of the distributed computing system.
1.2 EVOLUTION OF DISTRIBUTED COMPUTING SYSTEMS
Early computerswere veryexpensive (theycost millions ofdollars) and verylargeinsize
(they occupied a big room). There were very few computers and were available only in
research laboratories of universities and industries. These computers were run from a
console byanoperatorandwerenotaccessible toordinary users.Theprogrammerswould
write their programs and submit them to the computer center on some media, such as
punched cards, for processing. Before processing a job, the operator would set up the
necessary environment (mounting tapes, loading punched cards in acard reader, etc.) for
processing thejob. Thejob wasthenexecutedandtheresult, intheformofprintedoutput,
was later returned to the programmer.
The job setup time was a real problem in early computers and wasted most ofthe
valuable central processingunit(CPU)time. Several newconcepts were introduced inthe
1950sand 1960stoincrease CPU utilization ofthese computers. Notable among these are
batching together of jobs with similar needs before processing them, automatic
sequencing of jobs, off-line processing by using the concepts of buffering and spooling,
and" multiprogramming. Batching similar jobs improved CPU utilization quite a bit
because nowtheoperatorhadtochange theexecution environmentonlywhenanewbatch
ofjobs had to be executed and not before starting the execution of every job. Automatic
job sequencing with the use of control cards to define the beginning and end of a job
improved CPU utilization by eliminating the need for human job sequencing. Off-line
processing improved CPU utilization by allowing overlap of CPU and input/output (I/O)
operations byexecuting those twoactions on two independentmachines (110devices are
normally several orders of magnitude slower than the CPU). Finally, multiprogramming
improved CPU utilization by organizing jobs so that the CPU always had something to
execute.
However, none of these ideas allowed multiple users to directly interact with a
computer system and to share its resources simultaneously. Therefore, execution of
interactive jobsthatarecomposedofmany short actions inwhichthenextaction depends
ontheresultofapreviousactionwasatedious andtime-consumingactivity.Development
and debugging of programs are examples of interactive jobs. It was not until the early
1970s that computers started to use the concept of time sharing to overcome this hurdle.
Early time-sharing systems had several dumb terminals attached to the main computer.
These terminals were placed in a room different from the main computer room. Using
these terminals, multiple users could now simultaneously execute interactive jobs and
share the resources of the computersystem. In a time-sharing system, each user is given

4 Chap. 1 • Fundamentals
the impression that he or she has his or her own computer because the system switches
rapidly fromoneuser'sjob tothenextuser'sjob, executingonlyaverysmallpartofeach
jobat atime.Although theideaof timesharing wasdemonstrated asearly as 1960,time
sharing computer systems were not common until the early 1970s because they were
difficult and expensive to build.
Parallel advancements in hardware technology allowed reduction in the size and
increase in the processing speed of computers, causing large-sized computers to be
gradually replaced by smaller and cheaper ones that had moreprocessingcapability than
their predecessors. These systems were called minicomputers.
The advent of time-sharing systems was thefirst step toward distributed computing
systems because itprovided uswithtwoimportantconcepts usedindistributed computing
systems-the sharing of computer resources simultaneously by many users and the
accessing ofcomputers fromaplacedifferent fromthemaincomputerroom. Initially,the
terminals of atime-sharing system were dumb terminals, andall processing was done by
the main computer system. Advancements in microprocessor technology in the 1970s
allowed thedumb terminals tobereplaced byintelligent terminals sothat theconcepts of
off-line processing and time sharing could becombined to have the advantages of both
concepts in a single system. Microprocessor technology continued to advance rapidly,
making available in the early 1980s single-user computers called workstations that had
computing power almost equal to that of minicomputers but were available for only a
smallfractionofthepriceofaminicomputer.Forexample,thefirstworkstation developed
at Xerox PARC (called Alto) had a high-resolution monochrome display, a mouse, 128
kilobytes of main memory,a 2.5-megabyte hard disk, and amicroprogrammed CPU that
executed machine-level instructions at speeds of 2-6 f.Ls. These workstations were then
used as terminals inthe time-sharing systems. In these time-sharing systems, most of the
processing of a user'sjob could be done at the user's own computer, allowing the main
computerto besimultaneously shared byalargernumberofusers. Shared resources such
as files, databases, and software libraries were placed on the main computer.
Centralized time-sharing systems described above had a limitation in that the
terminals could notbeplacedveryfarfromthemaincomputerroomsinceordinary cables
were usedtoconnect theterminals tothemaincomputer.However,inparallel, there were
advancements incomputernetworking technology in the late 1960sand early 1970sthat
emergedastwokeynetworking technologies-LAN(localareanetwork)and WAN(wide
area network).The LAN technology allowed severalcomputers located within abuilding
or a campus to be interconnected in such a way that these machines could exchange
information witheachother atdataratesofabout 10megabits persecond (Mbps). Onthe
other hand, WANtechnology allowed computers located far from each other (may be in
different cities or countries or continents) to be interconnected in such a way that these
machines couldexchange information witheachotheratdataratesofabout56kilobitsper
second (Kbps). The first high-speed LAN was theEthernet developed at Xerox PARe in
1973, and the first WAN was the ARPAnet (Advanced Research Projects Agency
Network) developed by the U.S. Department of Defense in 1969. The data rates of
networks continued to improve gradually in the 1980s,providing data rates of upto 100
Mbps for LANs and data rates of up to64 KbpsforWANs.Recently (early 1990s)there
hasbeen another majoradvancementinnetworking technology-theATM(asynchronous

Sec.1.3• DistributedComputing System Models 5
transfer mode) technology. The Al'Mtechnology is an emerging technology that is still
not very well established. It will make very high speed networking possible, providing
data transmission rates up to 1.2gigabits per second (Gbps) in both LANand WAN
environments. The availability of such high-bandwidth networks will allow future
distributed computing systems to support a completely new class of distributed
applications, called multimedia applications, that deal with the handling of a mixture of
information,includingvoice, video,andordinary data.Theseapplicationswerepreviously
unthinkable with conventional LANs and WANs.
The merging of computer and networking technologies gave birth to distributed
computing systems in the late 1970s. Although the hardware issues of building such
systems were fairly well understood, the major stumbling block at that time was the
availability of adequate software for making these systems easy to use and for fully
exploiting their power. Therefore, starting from the late 1970s, a significant amount of
research work wascarriedout in both universities and industries inthe area ofdistributed
operating systems. These research activities have provided us with the basic ideas of
designingdistributedoperatingsystems.Althoughthefield isstillimmature, withongoing
active research activities, commercial distributed operating systems have already started
toemerge. These systemsare basedonalready establishedbasicconcepts. This bookdeals
with these basic concepts and their use in the design and implementation of distributed
operating systems. Several of these concepts are equally applicable to the design of
applications for distributedcomputing systems, making this book also suitable for use by
the designers of distributed applications.
1.3 DISTRI8UTED COMPUTING SYSTEM MODELS
Variousmodels are usedfor building distributedcomputingsystems. These models can be
broadly classified into five categories-minicomputer, workstation, workstation-server,
processor-pool, and hybrid. They are briefly described below.
1.3.1 MinicomputerModel
The minicomputermodel isasimple extensionof thecentralizedtime-sharingsystem. As
shown inFigure 1.2,adistributedcomputingsystem based onthismodelconsistsofafew
minicomputers (they may be large supercomputers as well) interconnected by a
communication network. Each minicomputer usually has multiple users simultaneously
logged ontoit.For this, several interactiveterminalsareconnectedtoeach minicomputer.
Each user is logged on to one specific minicomputer, with remote access to other
minicomputers.The network allows ausertoaccess remote resources thatareavailableon
some machine other than the one on to which the user is currently logged.
The minicomputer model may be used when resource sharing (such as sharing of
informationdatabasesof different types, with each type ofdatabase located on adifferent
machine) with remote users is desired.
The early ARPAnet is an example of a distributed computing system based on the
minicomputer model.

6 Chap.1 • Fundamentals
Mini /TerminalS
compute
Mini·
compute
Mini
computer
Fig_1.2 Adistributed computing systembasedontheminicomputer model.
1.3.1 Workstation Mod.1
As shown in Figure 1.3,a distributed computing system based on the workstation model
consists of several workstations interconnected by a communication network. A
company's office or a university department may have several workstations scattered
throughout abuildingorcampus, eachworkstationequipped withitsowndiskandserving
asasingle-user computer.Ithasbeenoften foundthatinsuch anenvironment, atanyone
time (especially at night), a significant proportion of the workstations are idle (not being
used), resulting in the waste of large amounts of CPU time. Therefore, the idea of the
workstation model is to interconnectall these workstations by ahigh-speed LAN so that
idle workstations may be used to process jobs of users who are logged onto other
workstations anddo nothavesufficient processing powerattheirown workstations toget
theirjobs processed efficiently.
In this model, a user logs onto one of the workstations called his or her "home"
workstation and submits jobs for execution. When the system finds that the user's
workstation does not have sufficient processing power for executing the processes of
the submitted jobs efficiently, it transfers one or more of the processes from the user's
workstation to some other workstation that is currently idle and gets the process
executed there, and finally the result of execution is returned to the user's
workstation.
This model is not so simple to implement as it might appear at first sight because
several issues must be resolved. These issues are [Tanenbaum 1995] as follows:

Sec. 1.3 • Distributed Computing System Models 7
li'ig.1.3 Adistributedcomputingsystem based on the workstation model.
1. How does the system find an idle workstation?
2. How is a process transferred from one workstation to get itexecuted on another
workstation?
3. What happens to a remote process if a user logs onto a workstation that was
idle until now and was being used to execute a process of another
workstation?
Ways to handle the first two issues are described in Chapters 7 and 8, respectively.
Three commonly used approaches for handling the third issue are as follows:
1. The first approach is to allow the remote process share the resources of the
workstation along with its own logged-on user's processes. This method is easy to
implement, but it defeats the main idea of workstations serving as personal computers,
because if remote processes are allowed to execute simultaneously with the logged-on
user's own processes, the logged-on user does not get his or her guaranteed
response.
2. The second approach is to kill the remote process. The main drawbacks of this
method are that all processing done for the remote process gets lost and the file system
may be left in an inconsistent state, making this method unattractive.
3. The third approach is to migrate the remote process back to its home
workstation, so that its execution can be continued there. This method is difficult to
implement because it requires the system to support preemptive process migration
facility. The definition of preemptive process migration and the issues involved in
preemptive process migration are given in Chapter 8.

8 Chap. I • Fundamentals
The Sprite system [Ousterhoutetal. 1988]and an experimental system developedat
Xerox PARe [Shoch and Hupp 1982]aretwo examplesofdistributedcomputingsystems
basedon the workstation model.
1.3.3 Workstatlon-S.rver Model
The workstationmodel isanetworkofpersonal workstations,each with its own disk and
a local file system. A workstation with its own local disk is usually called a diskful
workstation and a workstation withouta local disk is calleda diskless workstation. With
theproliferationofhigh-speednetworks,diskless workstationshave becomemore popular
in networkenvironmentsthan diskful workstations, making the workstation-servermodel
more popularthan the workstation model for building distributed computing systems.
As shown in Figure 1.4, a distributed computing system based on the workstation
servermodelconsistsofafew minicomputersandseveral workstations(mostofwhichare
diskless, but a few of which may be diskful) interconnected by a communication
network.
Mini Mini Mini
com.puter compute computer
usedas usedas usedas
file database print
server server server
Fig. 1.4 Adistributedcomputing systembasedonthe
workstation-servermodel.
Notethatwhendiskless workstationsareusedonanetwork, thefilesystem tobeused
by these workstations must be implemented either by a diskful workstation or by a
minicomputerequipped with a disk for file storage. The minicomputers are used for this
purpose.Oneormoreoftheminicomputersareusedforimplementingthefilesystem.Other
minicomputersmaybeusedforprovidingothertypesofservices, suchasdatabaseservice
andprint service. Therefore,each minicomputerisusedasaservermachinetoprovideone
or more types of services. Hence in the workstation-server model, in addition to the
workstations,therearespecializedmachines(maybespecializedworkstations)forrunning
serverprocesses(called servers)formanagingandprovidingaccess toshared resources.

Sec.1.3• DistributedComputing System Models 9
For a number of reasons, such as higher reliability and better scalability, multiple
servers are often used for managing the resources of a particular type in a distributed
computing system. For example, there may be multiple file servers, each running on a
separate minicomputerand cooperating viathe network, for managing thefiles of all the
users in the system. Due to this reason, a distinction isoften made between the services
that are provided to clients and the servers that provide them. That is, a service is an
abstract entity that is provided by one or more servers. For example, one or more file
servers may be used in a distributed computing system to provide file service to the
users.
In this model, a user logs onto a workstation called his or her home workstation.
Normalcomputation activities required bytheuser'sprocesses areperformedattheuser's
home workstation, but requests for services provided by special servers (such as a file
serveroradatabase server)aresenttoaserverproviding thattypeofservicethatperforms
the user's requested activity and returns the result of request processing to the user's
workstation. Therefore, in this model, the user's processes need not be migrated to the
server machines for getting the work done by those machines.
For better overall system performance, the local disk of a diskful workstation is
normally used for such purposes as storage of temporary files, storage of unshared files,
storage of shared files that are rarely changed, paging activity in virtual-memory
management, and caching of remotely accessed data.
As compared to the workstation model, the workstation-server model has several
advantages:
1. In general, it is much cheaper to use a few minicomputers equipped with large,
fastdisks that areaccessed overthenetwork than a largenumber ofdiskful workstations,
with each workstation having a small, slow disk.
2. Diskless workstations are also preferred to diskful workstations from a system
maintenance point of view.Backup and hardware maintenance areeasier toperform with
a few large disks than with many small disks scattered all over a building or campus.
Furthermore, installing new releases of software (such as a file server with new
functionalities) iseasier whenthesoftware istobeinstalled onafewfileservermachines
than on every workstation.
3. In the workstation-server model, since all files are managed by the file servers,
users have the flexibility to use any workstation and access the files in thesame manner
irrespective ofwhich workstation theuseriscurrently loggedon.Notethatthisisnottrue
with the workstation model, in which each workstation has its local file system, because
different mechanisms are needed to access local and remote files.
4. Intheworkstation-servermodel, therequest-responseprotocol described above is
mainly used to access the services of the server machines. Therefore, unlike the
workstationmodel,thismodeldoesnotneedaprocess migrationfacility,whichisdifficult
to implement.
The request-response protocol is known as the client-server model of communica
tion. Inthis model, aclient process (which in this case resides on a workstation) sends a

10 Chap.1 • Fundamentals
requesttoaserverprocess(which inthiscase resides onaminicomputer)forgetting some
service such as reading a block of a file. The server executes the request and sends back
a reply to the client that contains the result of request processing.
The client-server model provides an effective general-purpose approach to the
sharing of information and resources in distributed computing systems. It is not only
meantfor use with the workstation-servermodel butalso can beimplementedina variety
ofhardware and software environments. The computers used to run the clientand server
processes need not necessarilybe workstationsand minicomputers.They can be ofmany
types and there is no need to distinguish between them. It is even possible for both the
clientandserverprocessestoberun on thesamecomputer. Moreover, some processesare
both client and server processes.That is, aserver process may use the servicesofanother
server, appearing as a client to the latter.
5. A user has guaranteed response time because workstations are not used for
executingremote processes. However, the model does notutilize theprocessingcapability
ofidle workstations.
The V-System [Cheriton 1988] is anexampleofadistributedcomputing system that
is based on the workstation-server model.
1.3.4 Proc.ssor-Pool Mod.1
The processor-poolmodelis based on the observation that most of the time a user does
not need any computingpowerbutonceinawhileheorshemay need avery large amount
ofcomputing power for a short time (e.g., when recompiling a program consisting ofa
large number of files after changing a basic shared declaration). Therefore, unlike the
workstation-servermodel in which aprocessorisallocatedtoeach user, in the processor
pool model the processors are pooled together to be shared by the users as needed. The
pool of processors consists of a large number of microcomputers and minicomputers
attached to the network. Each processorin the pool has its own memory to load and run
a system program or an application program ofthe distributed computing system.
As shown inFigure 1.5,in the pure processor-pool model, the processorsinthe pool
have no terminals attached directly to them, and users access the system from terminals
that are attached to the network via special devices. These terminals are either small
diskless workstations or graphic terminals, such as X terminals. A special server (called
a run server) manages and allocates the processors in the pool to different users on a
demand basis. When a user submits a job for computation, an appropriate number of
processorsare temporarilyassignedtohisorherjobbytherun server.For example,ifthe
user'scomputationjobisthecompilationofaprogramhaving nsegments, in whicheach
of the segments can be compiled independently to produce separate relocatable object
files, nprocessorsfrom thepool can beallocatedtothisjobtocompileallthe nsegments
inparallel.When thecomputationiscompleted,theprocessorsarereturnedtothepool for
use by other users.
In the processor-pool model there is no concept of a home machine. That is, a user
does not log onto a particular machine but to the system as a whole. This is in contrast

Sec.1.3• DistributedComputing System Models 11
Terminals
File
serve
~
Fig. 1.5 Adistributed computing system
basedon theprocessor-pool model. Poolofprocessors
to other models in which each user has a home machine (e.g., a workstation or
minicomputer) onto which he or she logs and runs most of his or her programs there
by default.
As compared to the workstation-server model, the processor-pool model allows
better utilization of the available processing power of a distributed computing system.
This is because in the processor-pool model, the entireprocessingpowerofthe system is
available for use by the currently logged-on users, whereas this is not true for the
workstation-server model in which several workstations may be idle at a particular time
buttheycannot beused forprocessingthejobsofotherusers. Furthermore,theprocessor
poolmodel provides greaterflexibility than the workstation-servermodelinthe sense that
the system's services can be easily expanded without the need to install any more
computers; the processors inthe pool can be allocated to act asextra servers tocarry any
additional load arising from an increased user population or to provide new services.
However, the processor-pool model is usually considered to be unsuitable for high
performance interactiveapplications, especially those using graphics or window systems.
This is mainly because of the slow speed of communication between the computer on
which theapplication program of a user isbeing executed and the terminal via which the
user is interacting with the system. The workstation-server model isgenerally considered
to be more suitable for such applications.
Amoeba [Mullender et a1. 1990], Plan 9 [Pike et at 1990], and the Cambridge
DistributedComputing System [Needham and l-lerbert 1982] are examples of distributed
computing systems based on the processor-pool model.

12 Chap. 1 • Fundamentals
1.3.5 Hybrid Mod.'
Outofthefourmodelsdescribedabove, theworkstation-server model,isthemostwidely
usedmodelforbuildingdistributedcomputing systems.Thisisbecausealargenumberof
computer users only perform simple interactive tasks such as editing jobs, sending
electronic mails,andexecutingsmallprograms.Theworkstation-server modelisidealfor
suchsimpleusage.However,inaworkingenvironmentthathasgroupsofuserswhooften
perform jobs needing massive computation, the processor-pool model is more attractive
and suitable.
To combine the advantages of both the workstation-server and processor-pool
models, a hybrid model may be used to build a distributed computing system. The
hybrid model is based on the workstation-server model but with the addition of a pool
of processors. The processors in the pool can beallocated dynamically for computations
that are too largefor workstations or that require several computers concurrently for
efficient execution. In addition to efficient execution of computation-intensive jobs, the
hybrid model gives guaranteed response to interactive jobs by allowing them to be
processed on local workstations of the users. However, the hybrid model is more
expensive to implement than the workstation-server model or the processor-pool
model.
1.4 WHY ARE DISTRIBUTED COMPUTING SYSTEMS
GAINING POPULARnv?
From the models of distributed computing systems presented above, it is obvious that
distributed computing systems are much more complex and difficult to build than
traditional centralizedsystems(thoseconsistingofasingleCPU,itsmemory,peripherals,
and one or more terminals). The increased complexity is mainly due to the fact that in
addition to being capable of effectively using and managing a very large number of
distributed resources, the system software of adistributed computing system should also
be capable of handling the communication and security problems that are very different
from those of centralized systems. For example, the performance and reliability of a
distributed computingsystemdepends toagreatextentontheperformance andreliability
of the underlying communication network. Special software is usually needed to handle
lossof messagesduringtransmission across the networkor topreventoverloading ofthe
network, which degrades the performance and responsiveness to the users. Similarly,
special software security measures are needed to protect the widely distributed shared
resources and services against intentional or accidental violation of access control and
privacy constraints.
Despitetheincreasedcomplexityandthedifficultyofbuildingdistributedcomputing
systems, theinstallationanduseofdistributed computing systemsarerapidly increasing.
This is mainly because the advantages of distributed computing systems outweigh their
disadvantages.Thetechnicalneeds,theeconomicpressures,andthemajoradvantagesthat
haveled totheemergenceandpopularity ofdistributedcomputing systems aredescribed
next.

Sec.1.4• WhyAreDistributedComputing Systems Gaining Popularity? 13
1.4.1 Inherently Distributed Applications
Distributed computing systems come into existence in some very natural ways. For
example, severalapplications areinherently distributed innatureandrequire adistributed
computing system for their realization. For instance, in an employee database of a
nationwide organization, thedata pertaining toaparticularemployee aregenerated atthe
employee's branch office, and in addition to the global need to view the entire database,
there isa local need for frequent and immediate access to locally generated data ateach
branchoffice.Applications suchas theserequire thatsomeprocessing powerbeavailable
at the many distributed locations for collecting, preprocessing, and accessing data,
resulting in the need for distributed computing systems. Some other examples of
inherently distributed applications are a computerized worldwide airline reservation
system,acomputerized banking systeminwhichacustomercandeposit/withdraw money
from his or her account from any branch of the bank, and a factory automation system
controlling robots and machines all along an assembly line.
1.4.2 Information Sharing among Distributed Users
Another reason for the emergence of distributed computing systems was a desire for
efficient person-to-person communication facility by sharing information over great
distances. In a distributed computing system, information generated by one of the users
can be easily and efficiently shared by the users working at other nodes of the system.
This facility may be useful in many ways. For example, a project can be performed by
two or more users who are geographically far off from each other but whose computers
are a part of the same distributed computing system. In this case, although the users
are geographically separated from each other, they can work in cooperation, for
example, by transferring the files of the project, logging onto each other's remote
computers to run programs, and exchanging messages by electronic mail to coordinate
the work.
The useofdistributed computing systems byagroup of users toworkcooperatively
isknownascomputer-supportedcooperativeworking (CSCW),orgroupware. Groupware
applications depend heavily on the sharing of data objects between programs running on
different nodesof adistributed computing system. Groupware isanemerging technology
that holds major promise for software developers.
1.4.3 Resource Sharing
Information is not the only thing that can be shared in a distributed computing system.
Sharingofsoftwareresources suchassoftware librariesanddatabases aswellashardware
resources suchasprinters,harddisks,andplotterscanalsobedoneinaveryeffectiveway
among all the computers and the users of a single distributed computing system. For
example, wesaw that in adistributed computing system based on the workstation-server
model the workstations may have no disk or only a small disk (10-20 megabytes) for
temporary storage,andaccess topermanent filesonalargediskcanbeprovided toallthe
workstations by a single file server.

14 Chap. 1 • Fundamentals
This is one of the most important reasons for the growing popularity of distributed
computing systems. With the rapidly increasing power and reduction in the price of
microprocessors, combined with the increasing speed of communication networks,
distributedcomputingsystems potentiallyhaveamuchbetterprice-performanceratiothan
asingle large centralizedsystem. For example, wesawhow asmall numberofCPUs ina
distributedcomputingsystembasedontheprocessor-poolmodelcanbeeffectivelyusedby
alargenumberofusersfrominexpensiveterminals, giving afairly highprice-performance
ratio as compared to either a centralized time-sharing system or a personal computer.
Anotherreasonfordistributedcomputingsystemstobemorecost-effectivethancentralized
systemsisthat they facilitate resourcesharingamong multiple computers. For example, a
single unit ofexpensiveperipheral devicessuch ascolorlaserprinters,high-speedstorage
devices, and plotters can be shared among all the computers of the same distributed
computing system. If these computers are not linked together with a communication
network,each computermusthaveitsown peripherals,resultinginhigher cost.
1.4.5 ShorterResponsenmesandHigherThroughput
Duetomultiplicityofprocessors,distributedcomputingsystemsareexpectedtohavebetter
performance than single-processor centralized systems. The two most commonly used
performance metrics are response time and throughput of user processes. That is, the
multipleprocessorsofadistributedcomputingsystemcanbeutilizedproperly forproviding
shorter response times and higher throughput than a single-processorcentralized system.
For example, if there are two different programs to be run, two processors are evidently
more powerful than one because the programs can be simultaneously run on different
processors. Furthermore, if a particularcomputation can be partitioned into a number of
subcomputations that can run concurrently, in a distributed computing system all the
subcomputations can be simultaneously run with each one on a different processor.
Distributed computing systems with very fast communication networks are increasingly
beingusedasparallelcomputerstosolvesinglecomplexproblemsrapidly.Anothermethod
often used indistributedcomputingsystems forachieving better overall performanceisto
distribute the load more evenly among the multiple processors by moving jobs from
currently overloaded processors to lightly loaded ones. For example, in a distributed
computingsystem basedontheworkstationmodel, ifausercurrentlyhastwoprocessesto
run,outofwhichoneisaninteractiveprocessandtheotherisaprocess thatcanberuninthe
background,itmaybeadvantageoustoruntheinteractiveprocessonthehome nodeofthe
userandtheotheroneonaremote idlenode(ifanynodeisidle).
1.4.6 HigherReliability
Reliability refers to the degree of tolerance against errors and component failures in a
system [Stankovic 1984].Areliable system prevents loss ofinformationeven intheevent
ofcomponentfailures. The multiplicity ofstorage devicesand processorsin adistributed
computingsystem allows themaintenanceofmultiple copies ofcritical informationwithin

Sec.1.4• WhyAreDistributed Computing SystemsGainingPopularity? 15
the system and the execution of important computations redundantly to protect them
against catastrophic failures. With this approach, if one of the processors fails, the
computationcan besuccessfullycompletedattheotherprocessor, andifoneofthestorage
devices fails, the informationcanstillbeused from theother storage device. Furthermore,
the geographical distribution of the processors and other resources in a distributed
computing system limits the scope offailures caused by natural disasters.
Animportantaspectofreliability isavailability,which referstothefractionoftimefor
which a system is available for usc. In comparison to a centralized system, a distributed
computing system also enjoys theadvantage of increasedavailability. For example, ifthe
processor of a centralized system fails (assuming that it is a single-processorcentralized
system), theentire system breaksdown andnouseful workcan beperformed. However,in
thecaseofadistributedcomputingsystem, afewparts ofthesystem can bedown without
interruptingthejobs oftheuserswhoareusingtheotherpartsofthesystem,Forexample, if
a workstation of a distributed computing system that is based on the workstation-server
model fails,only the userofthatworkstationisaffected. Otherusers ofthesystem arenot
affectedbythisfailure. Similarly,inadistributedcomputingsystem basedontheprocessor
pool mode),ifsome of theprocessors inthepool aredown atany moment, thesystem can
continue tofunction normally,simply withsomelossinperformancethatisproportionalto
the number ofprocessorsthat aredown. Inthis case, none of the users isaffected and the
userscannot even know thatsomeoftheprocessorsaredown.
The advantage of higher reliability is an important reason for the use ofdistributed
computing systems for critical applications whose failure may be disastrous. However,
often reliability comes at the cost of performance. Therefore, it is necessary to maintain
a balance between the two.
1.4.7 Extensibility andIncremental Growth
Another major advantage of distributed computing systems is that they are capable of
incremental growth. That is,itispossibletograduallyextend thepower and functionality
of a distributed computing system by simply adding additional resources (both hardware
and software) to the system as and when the need arises. For example, additional
ha~dle
processors can be easily added to the system to the increased workload of an
organization that might have resulted from its expansion. Incremental growth is a very
attractive feature because for most existing and proposed applications it is practically
impossible to predict future demands of the system. Extensibility is also easier in a
distributedcomputingsystem because addition ofnew resources toanexisting system can
be performed without significant disruption of the normal functioning of the system.
Properly designed distributed computing systems that have the property of extensibility
and incremental growth are called open distributed systems.
1.4.8 SetterFlexibility inM••tlng Users' Needs
Different types of computers are usually more suitable for performing different types of
computations. For example, computers with ordinarypowerare suitable forordinary data
processing jobs, whereas high-performance computers are more suitable for complex

16 Chap. 1 • Fundamentals
mathematicalcomputations.Inacentralizedsystem,theusershavetoperformalltypesof
computationson theonlyavailablecomputer.However,a distributedcomputingsystem
mayhaveapoolofdifferenttypesofcomputers,inwhichcasethemostappropriateone
can be selected for processing a user's job depending on the nature of the job. For
instance,wesawthatinadistributedcomputingsystemthatisbasedonthehybridmodel,
interactivejobs can be processedat a user's.own workstationand the processorsin the
pool maybeusedtoprocessnoninteractive, computation-intensivejobs.
Note that the advantages of distributed computing systems mentioned above are not
achieved automatically but depend on the careful design of a distributed computing
system. This book deals with the various design methodologies that may be used to
achievethese advantages.
1.5 WHAT ISADISTRI8UTED OPERATING SYSTEM?
TanenbaumandVanRenesse[1985]defineanoperatingsystemasaprogramthatcontrols
the resources of a computer systemand provides its users with an interface or virtual
machine that is more convenient to use than the bare machine. According to this
definition,the twoprimarytasksof an operatingsystemareasfollows:
1. To present users with a virtual machine that is easier to program than the
underlyinghardware.
2. To managethe variousresourcesof the system.This involvesperformingsuch
tasksaskeepingtrackofwhoisusingwhichresource,grantingresourcerequests,
accountingforresourceusage,andmediatingconflictingrequestsfromdifferent
programsandusers.
Therefore, the users' viewof a computer system, the manner in which the users
accessthevariousresourcesofthecomputersystem,andthewaysinwhichtheresource
requests are granted depend to a great extent on the operating systemof the computer
system.Theoperatingsystemscommonlyusedfordistributedcomputingsystemscanbe
broadlyclassifiedintotwotypes-network operating systems anddistributedoperating
systems.Thethreemostimportantfeaturescommonlyusedtodifferentiatebetweenthese
two types of operating systems are system image, autonomy, and fault tolerance
capability.These featuresareexplainedbelow.
1. System image.The mostimportantfeatureusedtodifferentiatebetweenthetwo
typesofoperatingsystemsistheimageofthedistributedcomputingsystemfromthepoint
ofviewofitsusers.Incaseofanetworkoperatingsystem,theusersviewthedistributed
computing system as a collectionof distinct machines connected by a communication
subsystem.Thatis,theusersareawareofthefactthatmultiplecomputersarebeingused.
On the other hand, a distributed operating system hides the existence of multiple
computersandprovidesa single-systemimagetoitsusers.Thatis,itmakesacollection

Sec. 1.5• What IsaDistributedOperatingSystem? 17
ofnetworked machines actasavirtualuniprocessor.Thedifference between thetwotypes
of operating systems based on this feature can be best illustrated with the help of
examples. Two such examples are presented below.
In the case of a network operating system, although a user can run a job on any
machine of the distributed computing system, he or she is fully aware of the machine
on which his or her job is executed. This is because, by default, a user's job is
executed on the machine on which the user is currently logged. If the user wants to
execute a job on a different machine, he or she should either log on to that machine
by using some kind of "remote login" command or use a special command for remote
execution to specify the machine on which the job is to be executed. In either case,
the user knows the machine on which the job is executed. On the other hand, a
distributed operating system dynamically and automatically allocates jobs to the
various machines of the system for processing. Therefore, a user of a distributed
operating system generally has no knowledge of the machine on which a job is
executed. That is, the selection of a machine for executing a job is entirely manual in
the case of network operating systems but is automatic in the case of distributed
operating systems.
With a network operating system, a user is generally required to know the location
of a resource toaccess it, and different setsof system calls have tobeused foraccessing
local and remote resources. On the other hand, users of a distributed operating system
neednotkeeptrackofthelocationsofvariousresources foraccessing them,andthesame
set of system calls is used for accessing both local and remote resources. For instance,
usersofanetworkoperating systemareusuallyawareofwhereeachoftheirfilesisstored
and must use explicit file transfer commands for moving a file from one machine to
another, buttheusers ofadistributed operating systemhave noknowledgeofthelocation
of their files within the system and usethesamecommandtoaccess a fileirrespective of
whether it is on the local machine or on a remote machine. That is, control over file
placementisdone manually bytheusersinanetwork operating system butautomatically
by the system in a distributed operating system.
Notice thatthekeyconcept behindthisfeature is"transparency."Wewillseelaterin
thischapterthatadistributed operatingsystemhastosupportseveralfOnDSoftransparency
toachievethegoalofprovidingasingle-system imagetoitsusers.Moreover,itisimportant
tonoteherethatwiththecurrentstateoftheartindistributed operating systems,thisgoalis
notfullyachievable. Researchers arestillworkinghardtoachieve thisgoal.
2. Autonomy. A network operating system is built on a set of existing centralized
operating systems and handles the interfacing and coordination of remote operations and
communications between these operating systems. That is, in the case of a network
operating system, each computer of the distributed computing system has its own local
operating system (the operating systems of different computers may be the same or
different), and there isessentially no coordination at aJIamong the computers except for
the rule that when two processes of different computers communicate with each other,
they must use a mutually agreed on communication protocol. Each computer functions
independently ofother computers inthesensethateach one makes independent decisions
about the creation and termination of their own processes and management of local

18 Chap. 1 • Fundamentals
resources. Notice that due to thepossibility of difference in local operating systems, the
system calls for different computers of the same distributed computing system may be
different in this case.
On theother hand, withadistributed operating system, there isa single systemwide
operating system and each computer of the distributed computing system runs a part of
this global operating system.Thedistributed operating system tightly interweaves all the
computers of the distributed computing system in the sense that they work in close
cooperation with each other for the efficient and effective utilization of the various
resources ofthe system. That is, processes and several resources are managed globally
(some resources are managed locally). Moreover, there is a single set of globally valid
system calls available on all computers of the distributed computing system.
The set of system calls thatan operating system supports are implemented by a set
of programs called the kernel of the operating system. The kernel manages and controls
the hardware of the computer system to provide the facilities and resources that are
accessed by other programs through system calls, Tomake the same set of system calls
globally valid, with a distributed operating system identical kernels are run on all the
computers of a distributed computing system. The kernels of different computers often
cooperate with each other in making global decisions, such as finding the most suitable
machine for executing a newly created process in the system.
In short, itcan besaidthatthedegree ofautonomy ofeach machine of adistributed
computingsystemthatusesanetworkoperating systemisconsiderably highascompared
to that of machines of a distributed computing system that uses a distributed operating
system.
3. Fault tolerance capability. A network operating system provides little or no
fault tolerance capability in the sense that if 10% of the machines of the entire
distributed computing system are down at any moment, at least 10% of the users are
unable to continue with their work. On the other hand, with a distributed operating
system, most of the users are normally unaffected by the failed machines and can
continue to perform their work normally, with only a 10% loss in performance of the
entire' distributed computing system. Therefore, the fault tolerance capability of a
distributed operating system is usually very high as compared to that of a network
operating system.
The following definition ofadistributed operating system given byTanenbaum and
VanRenesse [1985] covers mostof its features mentioned above:
A distributed operating systemis one that looks to its users like an ordinary centralized
operating system butrunson multiple,independentcentral processing units (CPUs).The
keyconcept hereistransparency.Inotherwords,theuseofmultipleprocessors shouldbe
invisible (transparent) tothe user.Another wayofexpressing the same idea isto say that
the user views the system as a "virtual uniprocessor," not as a collection of distinct
machines. [P.419].
A distributed computing system that uses a network operating system is usually
referred to as a network system, whereas one that uses a distributed operating system is

Sec.1.6• IssuesinDesigning aDistributed Operating System 19
usually referred to as a true distributed system (or simply a distributed system). In this
book, the term distributed system will be used to mean a true distributed system.
Note that with the current state ofthe art in distributed operating systems, it is not
possible to design a completely true distributed system. Completely true distributed
systems are the ultimate goal ofresearchers working in the area ofdistributed operating
systems.
1.6 ISSUES INDESIGNING ADISTRIBUTED OPERATING
SYSTEM
In general, designing a distributed operating system is more difficult than designing a
centralized operating system for several reasons. In the design ofa centralizedoperating
system, it is assumed that the operating system has access to complete and accurate
information about the environment in which itis functioning. Forexample, acentralized
operating system can request status information, being assured that the interrogated
component will not change state while awaiting a decision based on that status
information, since only the single operating system asking the question may give
commands. However, a distributed operating system must be designed with the
assumption that complete information about the system environment will never be
available. In a distributed system, the resources are physically separated, there is no
common clock among the multiple processors, delivery of messages is delayed, and
messagescouldevenbe lost. Due toall these reasons, adistributedoperatingsystemdoes
not have up-to-date,consistentknowledgeaboutthestate ofthe variouscomponentsofthe
underlying distributed system. Obviously, lack of up-to-date and consistent information
makesmany things(such asmanagementofresourcesand synchronizationofcooperating
activities) much harderin the design ofa distributed'operating system. For example, it is
hard to schedulethe processorsoptimally ifthe operatingsystem isnot sure how many of
them are up at the moment.
Despite these complexities and difficulties, a distributed operating system must be
designedtoprovidean the advantagesofadistributedsystemtoitsusers. Thatis,the users
should be able to view adistributed system as a virtual centralizedsystem that isflexible,
efficient, reliable, secure, and easy to use. To meet this challenge, the designers of a
distributedoperatingsystemmust deal with severaldesign issues. Someofthe key design
issues are describedbelow. The rest of the chaptersof this book basicallycontaindetailed
descriptions of these design issues and the commonly used techniques to deal with
them.
1.6.1 Transparency
We saw that one of the main goals of a distributed operating system is to make the
existence of multiple computers invisible (transparent) and provide a single system
image to its users. That is, a distributed operating system must be designed in such a
way that a collection of distinct machines connected by a communication subsystem

20 Chap.1 • Fundamentals
appears to its users as a virtual uniprocessor. Achieving complete transparency is a
difficult task and requires that several different aspects of transparency besupported by
the distributed operating system. The eight forms of transparency identified by the
International Standards Organization's Reference Model for Open Distributed Process
ing [ISO 1992] are access transparency, location transparency, replication transparency,
failure transparency, migration transparency, concurrency transparency, performance
transparency, and scaling transparency. These transparency aspects are described
below.
AccessTransparency
Access transparency means thatusers should not need or be able to recognize whether a
resource (hardware or software) is remote or local. This implies that the distributed
operating system should allow users toaccess remote resources in the same way as local
resources. That is,the userinterface, which takes theformofasetof system calls, should
not distinguishbetween local and remote resources, and itshould be the responsibility of
the distributed operating system tolocate the resources and to arrange for servicing user
requests in a user-transparent manner.
This requirementcalls for a well-designed setof system calls that are meaningful in
both centralized and distributed environments and a global resource naming facility. We
will see in Chapters 3 and 4 that due to the need to handle communication failures in
distributedsystems, itis notpossible todesign system calls that provide complete access
transparency. However, the area of designing a global resource naming facility has been
well researchedwithconsiderablesuccess. Chapter 10deals withtheconcepts anddesign
ofaglobal resource naming facility.Thedistributedsharedmemory mechanismdescribed
inChapter5isalsomeanttoprovideauniform setofsystemcallsforaccessing bothlocal
and remote memory objects.Although this mechanism isquite useful inproviding access
transparency, it is suitable only for limited types of distributed applications due to its
performance limitation.
Location Transparency
The two main aspects of location transparency are as follows:
1. Nametransparency. This refers to the fact thatthe name of a resource (hardware
or software)should notreveal anyhintastothephysical location ofthe resource. That is,
the name ofaresource should be independent ofthephysical connectivityor topology of
the system orthecurrent location oftheresource. Furthermore, such resources, which are
capableofbeing moved from one nodetoanother inadistributed system (such asafile),
must beallowedtomove withouthavingtheir nameschanged. Therefore, resource names
must be unique systemwide.
2. Usermobility.Thisreferstothefactthatnomatter whichmachine auserislogged
onto, he or she should be able to access aresource with the same name. That is, the user
should not be required to use different names to access the same resource from.two

Sec.1.6• IssuesinDesigning aDistributed Operating System 21
differentnodesofthesystem.Inadistributedsystemthatsupportsusermobility,userscan
freelylogontoanymachine inthesystemandaccessanyresource without making any
extraeffort.
Bothnametransparency andusermobilityrequirementscallforasystemwide, global
resource naming facility.
Replication Transparency
For better performance and reliability, almost all distributed operating systems have
the provision to create replicas (additional copies) of files and other resources on
different nodes of the distributed system. In these systems, both the existence of
multiple copies of a replicated resource and the replication activity should be
transparent to the users. That is, two 'important issues related to replication transpar
ency are naming of replicas and replication control. It is the responsibility of the
system to name the various copies of a resource andto map a user-supplied name of
the resource to an appropriate replica of the resource. Furthermore, replication control
decisions such as how many copies of the resource should be created, where should
each copy be placed, and when should a copy be created/deleted should be made
entirely automatically by the system in a user-transparent manner. Replica manage
ment issues are described in Chapter 9.
Failure Transparency
Failure transparency deals with masking from the users' partial failures in the system,
such as a communication link failure, a machine failure, or a storage device crash. A
distributed operating system having failure transparency property will continue to
function, perhaps in a degraded form, in the face of partial failures. For example,
suppose the file service of a distributed operating system is to be made failure
transparent. This can be done by implementing it as agroup of file servers that closely
cooperate with each other to manage the files of the system and that function in such
a manner that the users can utilize the file service even if only one of the file servers
is up and working. In this case, the users cannot notice the failure of one or more file
servers, except for slower performance of file access operations. Any type of service
can be implemented in this way for failure transparency. However, in this type of
design, care should be taken to ensure that the cooperation among multiple servers does
notadd too much overhead to the system.
Complete failure transparency is not achievable with the current state of the art in
distributed operating systems because all types of failures cannot be handled in a user
transparent manner. For example, failure of the communication network of a distributed
system normally disrupts the work of its users and is noticeable by the users. Moreover,
an attempt to design a completely failure-transparent distributed system will result in a
very slow and highly expensive system due to the largeamount ofredundancy required
for tolerating all types of failures. The design of such a distributed system, although
theoretically possible, is not practically justified.

22 Chap. 1 • Fundamentals
Migration Transparency
Forbetter performance,reliability,andsecurityreasons,anobjectthatiscapableofbeing
moved (such as a process or a file) is often migrated from one node to another in a
distributed system. The aimof migrationtransparency is toensure that the movement of
the object is handled automatically by the system in a user-transparent manner. Three
important issues in achieving this goalare as follows:
1. Migration decisions such aswhich object is to be moved from where to where
should bemade automaticallyby the system.
2. Migrationofanobjectfromonenodetoanothershouldnotrequire anychange in
its name.
3. When the migrating object is a process, the interprocess communication
mechanism shouldensure thatamessage senttothe migrating process reaches it
withoutthe needforthesenderprocesstoresend itifthereceiver process moves
to another node before the messageis received.
Chapter 7 deals with the first issue. The second issue calls for a global resource
naming facility, which is described in Chapter 10. Ways to handle the third issue are
described in Chapter 8.
Concurrency Transparency
In a distributed system, multiple users who are spatially separated use the system
concurrently. In such a situation, it is economical to share the system resources
(hardware or software) among the concurrently executing user processes. However,
since the number of available resources in a computing system is restricted, one user
process must necessarily influence the action of other concurrently executing user
processes, as it competes for resources. For example, concurrent update to the same
file by two different processes should beprevented. Concurrency transparency means
that each user has a feeling that he or she is the sole user of the system and other
users do not exist in the system. For providing concurrency transparency, the resource
sharing mechanisms of the distributed operating system must have the following four
properties:
1. An event-ordering property ensures that all access requests to various system
resources are properly ordered to provide a consistent view to all users of the
system.
2. A mutual-exclusion property ensures that at any time at most one process
accesses a shared resource, which must not be used simultaneously by multiple
processes if program operation is to be correct.
3. A no-starvation property ensuresthat ifevery process that is granted a resource,
which must not be used simultaneously by multiple processes, eventually releases it,
every request for that resource is eventually granted.

Sec.1.6• IssuesinDesigning aDistributed Operating System 23
4. A no-deadlock property ensures that a situation will never occur in which
competing processes prevent their mutual progress even though no single one requests
more resources than available in the system.
Chapter 6 deals with the above-mentioned issues of concurrency transparency.
Performance Transparency
Theaimofperformancetransparency istoallowthesystemtobeautomaticallyreconfigured
to improve performance, as loads varydynamically inthe system. As far as practicable, a
situation in which one processor of the system is overloaded with jobs while another
processor is idle should not be allowed tooccur. That is, the processing capability ofthe
system should beuniformlydistributedamongthecurrently availablejobs inthesystem.
This requirement calls for the support of intelligent resource allocation and process
migration facilities indistributedoperating systems. Chapters 7and 8deal with these two
issues.
Scaling Transparency
The aim of scaling transparency is to allow the system to expand in scale without
disrupting the activities of the users.This requirement calls for open-system architecture
and the use of scalable algorithms for designing the distributed operating system
components. Section 1.6.3ofthis chapterand Section 2.6ofChapter2focusonthe issues
of designing an open distributed system. On the other hand, since every component of a
distributed operating system must use scalable algorithms, this issue has been dealt with
in almost all chapters of the book.
1.6.2 Rallabillty
In general, distributed systems are expected to be more reliable than centralized systems
due totheexistenceofmultiple instancesofresources. However, theexistenceofmultiple
instances of the resources alone cannot increase the system's reliability. Rather, the
distributed operating system, which manages these resources, must be designed properly
toincreasethesystem'sreliability bytakingfull advantage ofthischaracteristicfeature of
a distributed system.
Afault isa mechanical or algorithmic defect that may generate an error.Afault in a
system causes system failure. Depending onthemanner inwhichafailed system behaves,
systemfailuresareoftwotypes-fail-stop[SchlichtingandSchneider1983)andByzantine
[Lamport et a1. 1982]. In the case offail-stop failure, the system stops functioning after
changing to a state in which its failure canbe detected. On theother hand, in thecase of
Byzantinefailure, thesystemcontinuestofunction butproduces wrongresults. Undetected
software bugsoftencause Byzantine failureofasystem. Obviously, Byzantine failures are
much moredifficult todeal withthanfail-stopfailures.
For higher reliability, the fault-handling mechanisms of a distributed operating
system must be designed properly to avoid faults, to tolerate faults, and to detect and

24 Chap. 1 • Fundamentals
recover from faults. Commonly used methods for dealing with these issues are briefly
described next.
FaultAvoidance
Faultavoidancedeals withdesigning thecomponents ofthesystem insucha waythatthe
occurrence of faults is minimized. Conservative design practices such as using high
reliabilitycomponents-areoften employed forimproving thesystem'sreliabilitybasedon
the idea offault avoidance. Although adistributed operating system often has little or no
role to play in improving the fault avoidance capability of a hardware component, the
designersofthevarioussoftwarecomponents ofthedistributed operatingsystemmusttest
them thoroughly to make these components highly reliable.
Fault Tolerance
Fault tolerance is the ability of a system to continue functioning in the event ofpartial
system failure. The performance of the system might be degraded due to partial failure,
butotherwisethe system functions properly.Some of theimportant concepts that may be
used to improve the fault tolerance ability of a distributed operating system are as
follows:
1. Redundancytechniques. The basicidea behind redundancy techniquesistoavoid
single points offailure byreplicating critical hardware and software components, so that
if one ofthem fails, the others can be used to continue. Obviously, having two or more
copies of a critical component makes it possible, at least in principle, to continue
operations in spite of occasional partial failures. For example, a critical process can be
simultaneouslyexecuted ontwo nodes sothat ifone ofthe two nodes fails, the execution
of the process can be completed at the other node. Similarly, a critical file may be
replicated on two or more storage devices for better reliability.
Notice that with redundancy techniques additional system overhead is needed to
maintain twoormorecopiesofareplicatedresourceandtokeepallthecopiesofaresource
consistent. Forexample, ifafileisreplicatedontwoormorenodesofadistributedsystem,
additional disk storage space isrequired, andforcorrect functioning, it isoften necessary
thatallthecopies ofthefilearemutuallyconsistent. Ingeneral,thelargeristhenumberof
copies kept, the better is the reliability but the larger is the system overhead involved.
Therefore, a distributed operating system must be designed to maintain a properbalance
between the degree of reliability and the incurred overhead. This raises an important
question: How much replication is enough? For an answer to this question, note that a
system is said to be k-fault tolerant if itcancontinue tofunction even in the event of the
failure of k components [Cristian 1991, Nelson 1990]. Therefore, if the system is to be
designed totolerate kfail-stopfailures,k+1replicasareneeded.Ifkreplicas arelostdueto
failures,theremainingonereplicacanbeusedforcontinuedfunctioning ofthesystem.On
theotherhand,ifthesystemistobedesignedtotoleratekByzantinefailures,aminimumof
2k+1replicas are needed. This is because a votingmechanism can be used to believe the
majorityk+1ofthereplicas whenkreplicasbehaveabnormally.

Sec.1.6• Issues inDesigning aDistributedOperating System 2S
Replicationandconsistency control mechanisms formemoryobjects aredescribed in
Chapter 5 and for file objects are described in Chapter 9.
Another application of redundancy technique is in the design of a stable storage
device, which is a virtual storage device that can even withstand transient110faults and
decay of the storage media. The reliability of a critical file may be improved by storing
it on a stable storage device. Stable storage devices are described in Chapter9.
2. Distributed control. For better reliability, many of the particular algorithms or
protocols used in a distributed operating system must employ a distributed control
mechanism to avoid single points of failure. For example, a highly available distributed
file system should have multiple and independent file servers controlling multiple and
independent storage devices. In addition to file servers, a distributed control technique
could also be used for name servers, scheduling algorithms, and other executive control
functions. Itis important to note here that whenmultiple distributed servers are used ina
distributed system toprovide aparticular typeofservice,theserversmustbeindependent.
Thatis,thedesign mustnotrequire simultaneous functioning oftheservers;otherwise, the
reliability willbecome worseinstead ofgettingbetter.Distributedcontrolmechanisms are
described throughout this book.
Fault Detection and Recovery
The fault detection and recovery method of improving reliability deals with the use of
hardware and software mechanisms to determine the occurrence of a failure and then to
correctthesystemtoastateacceptableforcontinuedoperation.Someofthecommonlyused
techniques forimplementingthismethodinadistributed operatingsystemareasfollows:
1. Atomic transactions. An atomic transaction (or just transaction for short) is a
computation consisting of a collection of operations that take place indivisibly in the
presence of failures andconcurrent computations. That is,eitherall of the operations are
performed successfully or none of their effects prevails, and other processes executing
concurrently cannot modify or observe intermediate states of the computation.
Transactions help topreserve theconsistency ofasetofshareddata objects (e.g.,files) in
thefaceoffailures.andconcurrent access.They makecrashrecoverymucheasier,because
a transaction can only end in two states: Either all the operations of the transaction are
performed or none of the operations of the transaction is performed.
In a system with transaction facility, if a process halts unexpectedly due to a
hardware fault or a software error before a transaction is completed, the system
subsequently restores anydata objects that wereundergoing modification totheiroriginal
states. Notice that if a system does not support a transaction mechanism, unexpected
failure of a process during the processing ofan operation mayleave thedata objects that
were undergoing modification in an inconsistent state. Therefore, without transaction
facility,itmaybedifficultoreven impossible insomecases torollback(recover)thedata
objects from their current inconsistent states to their original states. Atomic transaction
mechanisms are described in Chapter9.
2. Statelessservers.Theclient-servermodelisfrequently usedindistributed systems
toservice user requests. Inthis model, a server may beimplemented by using anyoneof

26 Chap. 1 • Fundamentals
the following two service paradigms-stateful or stateless. The two paradigms are
distinguished byone aspect oftheclient-server relationship, whether or notthehistory of
theservicedrequests betweenaclient andaserveraffectstheexecution ofthenextservice
request.The stateful approach doesdependonthehistory oftheserviced requests, butthe
stateless approach does not depend on it. Stateless servers have adistinctadvantage over
statefulservers intheeventofafailure.Thatis,thestateless serviceparadigmmakescrash
recovery veryeasy because noclient stateinformation ismaintained bythe server.Onthe
other hand, the stateful service paradigm requires complex crash recovery procedures.
Boththeclientandserver need toreliably detectcrashes. The server needs todetectclient
crashessothatitcandiscard anystateitisholdingfortheclient,andtheclientmustdetect
servercrashes sothatitcanperform necessaryerror-handlingactivities. Although stateful
service becomes necessary in some cases, to simplify the failure detection and recovery
actions, the stateless service paradigm must be used wherever possible. Stateless and
stateful servers are described in Chapters 4 and 9.
3. Acknowledgments and timeout-based retransmissions of messages. In a dis
tributedsystem, events suchasanodecrashoracommunicationlinkfailure mayinterrupt
a communication that was in progress between two processes, resulting in the loss of a
message.Therefore,areliable interprocesscommunicationmechanism musthavewaysto
detect lost messages sothat they can beretransmitted. Handling of lost messages usually
involves return of acknowledgment messages and retransmissions on the basis of
timeouts.That is,thereceiver mustreturnanacknowledgmentmessage forevery message
received, and if the sender does not receive any acknowledgment for a message within a
fixedtimeout period, itassumes thatthemessage waslostandretransmits themessage.A
problem associated with this approach is thatof duplicate messages. Duplicate messages
may be sent in the event of failures or because of timeouts. Therefore, a reliable
interprocess communicationmechanism shouldalso becapable ofdetecting andhandling
duplicate messages. Handling of duplicate messages usually involves a mechanism for
automatically generatingandassigning appropriatesequencenumbers tomessages. Useof
acknowledgment messages, timeout-based retransmissions of messages, and handling of
duplicate request messages for reliable communication are described in Chapter 3.
The mechanisms described above may be employed to create a very reliable
distributed system. However, the main drawback of increased system reliability is
potential loss ofexecution time efficiency due to the extra overhead involved in these
techniques. For many systems it is just too costly to incorporate a large number of
reliability mechanisms. Therefore, the major challenge for distributed operating system
designers is to integrate these mechanisms in a cost-effective manner for producing a
reliable system.
1.6.3 Flexibility
Another important issue in the design of distributed operating systems is flexibility.
Flexibility is the most important feature for open distributed systems. The design of a
distributed operating system should be flexible due to the following reasons:

Sec.1.6• IssuesinDesigning aDistributedOperating System 27
1. Ease ofmodification. From theexperience of system designers, ithas been found
that some parts of the design often need tobe replaced/modifiedeitherbecausesome bug
isdetected inthedesign orbecause thedesign isnolonger suitable forthechangedsystem
environment or new-user requirements. Therefore, it should be easy to incorporate
changes in the system in a user-transparent manner or with minimum interruption caused
to the users.
2. Ease ofenhancement. Inevery system, new functionalities have tobeadded from
time totime tomake itmore powerful andeasy touse.Therefore, itshould beeasy toadd
new services tothe system. Furthermore, ifagroup ofusersdo not likethestyle inwhich
a particular service is provided by the operating system, they should have the flexibility
toaddand usetheir own service that works inthe style withwhichthe usersofthatgroup
are more familiar and feel more comfortable.
The most important design factor that influences the flexibility of a distributed
operating system is the model used for designing its kernel. The kernel of an operating
system is its central controlling part that provides basic system facilities. It operates in a
separate address space that is inaccessible to user processes. It is the only part of an
operating system that a user cannot replace or modify. We saw that in the case of a
distributed operating system identical kernels are run on all the nodes of the distributed
system.
The two commonly used models for kernel design in distributed operating systems
are the monolithic kernel and the microkernel (Fig. 1.6). Inthe monolithic kernel model,
most operating system services such as process management, memory management,
device management, file management, name management, and interprocess communica
tion are provided by the kernel. As a result, the kernelhas a large, monolithic structure.
Many distributed operating systems that are extensions or imitations of the UNIX
operating system usethemonolithic kernel model.This ismainlybecause UNIXitselfhas
a large, monolithic kernel.
On the other hand, inthe microkernel model, the main goal is to keep the kernel as
small as possible. Therefore, inthis model, the kernel isavery small nucleus of software
that provides only the minimal facilities necessary for implementingadditional operating
system services. The only services provided by the kernel in this model are interprocess
communication, low-level device management, a limited amount of low-level process
management, and some memory management. All other operating system services, such
as file management, name management, additional process, and memory management
activities, and much system call handling are implementedas user-level server processes.
Each server process has its own address space and can be programmed separately.
As compared to the monolithic kernel model, the microkernel model has several
advantages. Inthemonolithickernel model, thelarge sizeofthekernel reduces theoverall
flexibility and configurability of the resulting operating system. On the other hand, the
resulting operating system of the microkernel model is highly modular in nature. Due to
thischaracteristicfeature, theoperatingsystem ofthemicrokernelmodeliseasytodesign,
implement, andinstall. Moreover, since mostoftheservices areimplementedasuser-level
server processes, itisalsoeasy tomodify thedesign oraddnewservices.This also allows

28 Chap.1 • Fundamentals
Node1 Node2 Noden
User User User
applications applications applications
...
Monolithickemel Monolithickernel Monolithickernel
(includesmost (Includesmost (includesmost
as
services) OSservices) OSservices)
I I
Networkhardware
(a)
Node1 Node2 Noden
User User User
applications applications applications
...
Server/manager Server/manager server/manager
modules modules modules
Mlcrokemel Microkemel MicrokerneJ
(hasonlyminimal (hasonlyminimal (hasonlyminimal
facilities) facilities) facilities)
I I
Networkhardware
(b)
Fig.l.6 Models of kernel design indistributed operatingsystems: (a)The monolithic
kernel model. The levelabove thekernel levelcan bepartitionedbythe
users into whateverhierarchical levelsare appropriate. (b)The microkernel
model. Although the figure showsatwo-level heirarchy above the kernel
level, users canextend thehierarchy to whatever levels areappropriate.
those users who do not like a particular service provided by the operating system to
implementandusetheirownservice.Furthermore,foraddingorchanging aservice,there
isnoneed tostopthesystem andboot anewkernel,asinthecaseofamonolithic kernel.
Therefore, changes in the system can be incorporated without interrupting the users.
The modular design of a system based on the microkemel model, however, is
potentially subject to a performance penalty.This is because in the microkernel model
eachserverisanindependent process havingitsownaddressspace.Therefore, theservers
have to use some form of message-based interprocess communication mechanism to
communicate with each other while performing somejob. Furthermore, message passing
between server processes and the microkernel requires context switches, resulting in
additional performanceoverhead. In the monolithic kernel model, however, since allthe
services are provided by the kernel, the same address space is shared by all of them.
Therefore, no message passing and no context switching are required while the kernel is
performingthejob. Hencearequestmaybeservicedfasterinthemonolithickernelmodel
than in the microkemel model.

Sec. 1.6• Issues inDesigning aDistributedOperating System 29
In spite of its potential performance cost, the microkemel model is being preferred
forthe design of modemdistributed operating systems. The two mainreasons forthis are
as follows:
1. The advantages of the microkemel model more than compensate for the
performancecost. Notice thatthesituation hereisverysimilar totheonethatcausedhigh
level programminglanguages tobepreferred toassembly languages.Inspiteofthebetter
performance of programs written in assembly languages, most programs are written in
high-level languages duetotheadvantages ofeaseofdesign, maintenance,andportability.
Similarly, the flexibility advantages of the microkernel model previouslydescribed more
than outweigh its small performance penalty.
2. Some experimental results have shown that although in theory the microkernel
model seemstohavepoorerperformancethanthemonolithic kernelmodel,thisisnottrue
in practice. This is because other factors tend to dominate, and the small overhead
involved in exchanging messages is usually negligible [Douglis eta1. 1991].
Details of several distributed operating systems whose design is based on the
microkernel model are presented in Chapter 12.
1.6.4 Performance
If a distributed system is to be used, its performance must be at least as good as a
centralized system. That is, when a particular application is run ona distributed system,
itsoverall performance shouldbe better than or at least equal tothatofrunning the same
application on a single-processor system. However, to achieve thisgoal, it is important
that the various components of the operating system of a distributed system be designed
properly; otherwise, the overall performanceof thedistributed systemmay turnout to be
worse than a centralized system. Some design principles considered useful for better
performance are as follows:
1. Batch ifpossible. Batching often helps in improving performance greatly. For
example, transfer of data across the network in large chunks rather than as individual
pages is much more efficient. Similarly, piggybacking of acknowledgment of previous
messages with the next message during a series of messages exchanged between two
communicating entities also improves performance,
2. Cache whenever possible. Caching of data at clients' sites frequently improves
overall system performance because itmakesdata available whereveritisbeingcurrently
used, thus saving a large amount of computing time and network bandwidth.Inaddition,
caching reduces contention on centralized resources.
3. Minimize copying ofdata. Datacopying overhead (e.g., movingdatainandoutof
buffers) involves a substantial CPU cost of many operations. Forexample, while being
transferred from its sender toitsreceiver, a message data may takethefollowing path on
the sending side:

30 Chap. 1 • Fundamentals
(a) From sender's stack to its message buffer
(b) From the message buffer in the sender's address space to the message buffer in
the kernel's address space
(c) Finally, from the kernel to the network interface board
On thereceiving side, thedata probably takesasimilar path in the reverse direction.
Therefore, inthiscase, atotal of sixcopy operationsare involved inthe message transfer
operation. Similarly, in several systems, the datacopying overhead is also large for read
and write operations on block 1/0 devices. Therefore, for better performance, it is
desirable toavoid copying of data, although thisisnotalways simple to achieve. Making
optimal use of memory management often helps in eliminating much data movement
between the kernel, block 1/0 devices, clients, andservers.
4. Minimizenetworktraffic. System performance mayalso beimproved byreducing
internode communication costs. For example, accesses to remote resources require
communication,possiblythroughintermediatenodes.Therefore,migratingaprocesscloser
tothe resources itisusing mostheavily may behelpfulinreducing network traffic inthe
systemifthedecreasedcostofaccessing itsfavoriteresourceoffsetsthepossible increased
cost ofaccessing itslessfavored ones.Another waytoreduce network traffic istousethe
process migration facility to cluster two or moreprocesses that frequently communicate
with each other on the same node of the system.Avoiding the collection of global state
information formaking somedecision alsohelpsinreducing network traffic.
5. Takeadvantageoffine-grainparallelismformultiprocessing.Performancecanalso
be improved by taking advantage of fine-grain parallelism for multiprocessing. For
example, threads (described inChapter8) are oftenused forstructuring server processes.
Servers structured as a group of threads can operate efficiently because they can
simultaneously service requests from several clients.Fine-grainedconcurrencycontrol of
simultaneous accesses by multiple processes to ashared resource is another example of
applicationofthisprinciple forbetterperformance.
Throughout the book we will come across the use of these design principles in the
designofthevariousdistributed operating systemcomponents.
1.6.5 SCQlablllty
Scalability refers to the capability of a system to adapt to increased service load. It is
inevitablethatadistributed systemwillgrowwithtimesinceitisverycommontoaddnew
machines or an entire subnetwork to the system to take care of increased.workload or
organizational changes inacompany.Therefore, adistributed operatingsystem should be
designed to easily cope with the growth of nodesand users in the system. That is, such
growth shouldnotcause seriousdisruption ofserviceorsignificantlossofperformanceto
users.Someguidingprinciples fordesigningscalabledistributedsystemsareasfollows:
1. Avoid centralizedentities. In the design ofa distributed operating system, use of
centralized entities such as a single central file serveror a single database for the entire
system makes the distributed system nonscalable due to the following reasons:

Sec.1.6 • IssuesinDesigning aDistributed Operating System 31
(a) The failure of the centralized entity often brings the entire system down. Hence,
the system cannot tolerate faults in a graceful manner,
(b) The performance of the centralized entity often becomes a system bottleneck
when contention for itincreases with the growing number of users.
(c) Even if the centralized entity has enough processing and storage capacity, the
capacity of the network that connects the centralized entity with other nodes of
the system often gets saturated when the contention for the entity increases
beyond a certain level.
(d) Ina wide-area networkconsistingof several interconnectedlocal-areanetworks,
it isobviously inefficient to always get aparticulartype ofrequest serviced at a
server node that is several gateways away. This also increases network traffic.
Local area and wide-area networking concepts are described in Chapter 2.
Therefore, the useofcentralizedentities should beavoided inthedesign. Replication
of resources and distributed controlalgorithms are frequently used techniques to achieve
this goal. In fact, for better scalability, as far as practicable, a functionally symmetric
configuration should be usedinwhich all nodes ofthe system have anearly equal role to
play in the operation of the system.
2. Avoid centralized algorithms. A centralized algorithm is one that operates by
collecting information from allnodes, processing this information on a single node and
then distributing the results toother nodes. The useof such algorithms inthedesign of a
distributed operating system isalso not acceptable from a scalability point of view.The
reasons for this are very similarto those mentioned in the use of centralizedentities. For
example, a scheduling algorithm that makes scheduling decisions by first inquiring from
all the nodes and then selectingthe most lightly loaded node as acandidate forreceiving
jobs has poor scalability factor.Such an algorithm may workfine for small networks but
gets crippled when applied tolargenetworks. This isbecause the inquirer.receivesavery
large number of replies almostsimultaneously and the time required toprocess the reply
messages for making a host selection is normally too long. Moreover, since the
complexity of the algorithm is O(n2 ), it creates heavy network traffic and quickly
consumes network bandwidth.Therefore, in the design of adistributed operating system,
onlydecentralizedalgorithmsshouldbeused. Inthesealgorithms, globalstateinformation
of the system is not collected or used, decision at a node is usually based on locally
available information,anditisassumed that asystemwideglobalclock does notexist (the
clocks of all nodes are not synchronized).
3. Perform most operations on client workstations. If possible, an operation should
be performed on the client's own workstation rather than on a server machine. This is
because a server is a common resource for several clients, and hence server cycles are
more precious than the cycles of client workstations. This principle enhances the
scalability of the system, sinceit allows graceful degradation of system performance as
the system grows in size, by reducing contention for shared resources. Caching is a
frequently used technique for the realization of this principle.
Throughout the book, wewill come across the use of these design principles in the
design of the various distributed operating system components.

32 Chap. I • Fundamentals
A heterogeneous distributed system consists of interconnected sets of dissimilar
hardware or software systems. Because of the diversity, designing heterogenous
distributed systems is far more difficult than designing homogeneous distributed
systems in which each system is based on the same, or closely related, hardware
and software. However, as a consequence of large scale, heterogeneity is often
inevitable in distributed systems. Furthermore, often heterogeneity is preferred by
many users because heterogeneous distributed systems provide the flexibility to their
users of different computer platforms for different applications. For example, a user
may have the flexibility of a supercomputer for simulations, a Macintosh for
document processing, and a UNIX workstation for program development.
Incompatibilities in a heterogeneous distributed system may be of different
types. For example, the internal formatting schemes of different communication
and host processors may be different; or when several networks are interconnected
via gateways, the communication protocols and topologies of different networks
may be different; or the servers operating at different nodes of the system may
be different. For instance, some hosts use 32-bit word lengths while others use
word lengths of 16 or 64 bits. Byte ordering within these data constructs can
vary as well, requiring special converters to enable data sharing between incom
patible hosts.
In a heterogeneous distributed system, some form of data translation is neces
sary for interaction between two incompatible nodes. Some earlier systems left this
translation to the users, but this is no longer acceptable. The data translation job
may be performed either at the sender's node or at the receiver's node. Suppose
this job is performed at the receiver's node. With this approach, at every node
there must be a translator to convert each format in the system to the format used
on the receiving node. Therefore, if there are n different formats, n - 1 pieces of
translation software must be supported at each node, resulting in a total of n(n - 1)
pieces of translation software in the system. This is undesirable, as adding a new
type of format becomes a more difficult task over time. Performing the translation
job at the sender's node instead of the receiver's node also suffers from the same
drawback.
The software complexity of this translation process can be greatly reduced by
using an intermediate standard data format. In this method, an intermediate standard
data format is declared, and each node only requires a translation software for
converting from its own format to the standard format and from the standard format
to its own format. In this case, when two incompatible nodes interact at the sender
node, the data to be sent is first converted to the standard format, the data is moved
in the format of the standard, and finally, at the receiver node, the data is converted
from the standard format to the receiver's format. By choosing the standard format to
be the most common format in the system, the number of conversions can be
reduced.
Varioustechniques to deal with heterogeneity in distributed systems are described
in Chapters 2, 4, 5, and 8.

Sec.1.6• IssuesinDesigning aDistributedOperating System 33
1.6.7 Security
In order that the users can trust the system and rely on it, the various resources of a
computer system must be protected against destruction and unauthorized access.
Enforcing security in a distributed system is more difficult than in a centralized
system because of the lack of a single point of control and the use of insecure
networks for data communication. In a centralized system, all users are authenticated
by the system at login time, and the system can easily check whether a user is
authorized to perform the requested operation on an accessed resource. In a dis
tributed system, however, since the client-server model is often used for requesting
and providing services, when a client sends a request message to a server, the server
must have some way·of knowing who is the client. This is not so simple as it might
appear because any client identification field in the message cannot be trusted. This
is because an intruder (a person or program trying to obtain unauthorized access to
system resources) may pretend to be an authorized client or may change the message
contents during transmission. Therefore, as compared to a centralized system,
enforcement of security in a distributed system has the following additional
requirements:
1. It should be possible for the sender of a message to know that the message was
received by the intended receiver.
2. Itshould be possible for the receiverof a message to know that the message was
sent by the genuine sender.
3. It should be possible for both the sender and receiver of a message to be
guaranteed that the contents of the message were not changed while it was in
transfer.
Cryptography (described in Chapter 11) is the only known practical method for
dealing withthese security aspects ofadistributedsystem. Inthis method, comprehension
of private information is prevented by encrypting the information, which can then be
decrypted only by authorized users.
Another guiding principle for security is that a system whose security depends on
the integrity of the fewest possible entities is more likely to remain secure as it grows.
For example, it is much simpler to ensure security based on the integrity of the much
smaller number of servers rather than trusting thousands of clients. In this case, it is
sufficient to only ensure the physical security of these servers and the software they
run. Chapter 11 deals with the commonly used techniques for designing secure
distributed systems.
1.6.8 Emulation of Existing Opcsrating Systems
Forcommercialsuccess, itisimportantthatanewly designeddistributedoperatingsystem
be able to emulate existing popularoperating systems such as UNIX. With this property,
new software can be written using the system call interface ofthe new operating system

34 Chap. I • Fundamentals
to take full advantage of its special features of distribution, but a vast amount of already
existingoldsoftwarecanalsoberunonthesamesystemwithouttheneedtorewritethem.
Therefore, moving to the new distributed operating system will allow both types of
software to berun side by side.
We will see in Chapter 12how some of the existing distributed operating systems
have been designed to support UNIX emulation facility.
1.7 INTRODUmON TODISTRIBUTED COMPunNG
ENVIRONMENT (DeE)
Chapter 12 of the book presents case studies of four distributed operating systems:
Amoeba, V-System, Mach, and Chorus. In addition, examples of key technologies of
individual distributed operating systemcomponents thathaveeither become orarepoised
to become de factointernational standards are presented in individual chapters wherever
such industry examples are available. In particular, the following technologies are
presented as case studies:
• Ethernet, IEEE Token Ring, the Internet Protocol suite, and the Internet are
presented as case studies of networking technologies in Chapter 2.
• The4.3BSDUNIXinterprocess communication mechanism ispresented asacase
study of message-passing technology in Chapter 3.
• SUN RPCand DeERPCare presented ascase studies of Remote Procedure Call
(RPC) technology in Chapter 4.
• IVY and Munin are presented as case studies of Distributed Shared Memory
(DSM) technology in Chapter 5.
• DCE Distributed Time Service (DTS) is presented as a case study of clock
synchronization technology in Chapter 6.
• The DCE threads package is presented as a case study of threads technology in
Chapter 8.
• DeEDistributedFileService(DFS)ispresented asacase studyofdistributed file
system technology in Chapter 9.
• The variouscomponents of DeE naming facility are presented as case studies of
naming technology in Chapter 10.
• The Kerberos authentication system and DeE Security Service are presented as
case studies of security technology in Chapter 11.
Notice from theabove listthatalmost halfofthekey technologies presented ascase
studiesinthevariouschaptersofthisbookaretoolsandservicesthatbelongtoDCE.This
isbecause of thewayinwhich DCE wascreated (described next).Therefore, fora better
understanding ofthesekey technologies, itwillbe useful to know something about DCE
before going into the details of its key components. This section presents a brief
introduction to DCE.

Sec.1.7• Introduction toDistributedComputing Environment (DeE) 35
1.7.1 WhatIsDeE?
Avendor-independentdistributedcomputingenvironment, DCE wasdefined bytheOpen
Software Foundation (OSF), a consortium of computer manufacturers, including IBM,
DEC, and Hewlett-Packard. Itis not anoperating system, nor is itan application. Rather,
itisan integratedsetofservices and tools thatcan be installed as acoherentenvironment
on top of existing operating systems and serve as a platform for building and running
distributed applications.
A primary goal of DeE is vendor independence. It runs on many different kinds of
computers, operatingsystems, and networks produced bydifferent vendors. For example,
some operating systems to which DeE can be easily ported include OSF/I, AIX,
DOMAIN OS, ULTRIX, HP-UX, SINIX, SunOS, UNIX System V,VMS, WINDOWS,
and OS/2. On the other hand, it can be used with any network hardware and transport
software, including TCPIIP,X.25, as welJas other similarproducts.
As shown in Figure 1.7,DeE is a middleware software layered between the DCE
applicationslayerandtheoperating systemand networkinglayer.Thebasicideaistotake
acollectionof existing machines (possibly from different vendors), interconnect them by
acommunication network, add the DCE software platform on top of thenative operating
systems of the machines, and then beable tobuild and run distributed applications. Each
machine has its own local operating system, which may be different from that of other
machines. The DeE software layer on top of the operating system and networking layer
hides the differences between machines by automatically performing data-type conver
sions when necessary.Therefore, theheterogeneous nature of the system istransparent to
the applications programmers, making their job of writing distributed applications much
simpler.
DCEapplications
DCEsoftware
Fig. J.7 Positionof DeE software ina Operatingsystemsandnetworking
DeE-baseddistributed system.
1.7.2 How WasDCE Created?
The aSF did not create DeE from scratch. Instead, it created DCE by taking
advantage of work already done at universities and industries in the area of
distributed computing. For this, OSF issued a request for technology (RFT), asking
for tools and services needed to build a coherent distributed computing environment.
To be a contender, a primary requirement was that actual working code must
ultimately be provided. The submitted bids were carefully evaluated by OSF employ
ees and a team of outside experts. Finally, those tools and services were selected that
the members of the evaluation committee believed provided the best solutions. The
code comprising the selected tools and services, almost entirely written in C, was

36 Chap.1 • Fundamentals
then further developed by OSF to produce a single integrated package that was made
available to the world as DCE. Version 1.0 of DCE was released by OSF in January
1992.
1.7.3 DeE COmpo.Ats
As mentioned above, DeE is a blend ofvarious technologies developed independently
and nicelyintegratedbyOSF.Each ofthese technologiesforms acomponentofDCE. The
main components ofDeE are as follows:
1. Threads package. It provides a simple programming model for building
concurrent applications. It includes operations to create and control multiple threads of
execution in a single process and to synchronize access to global data within an
application. Details are given in Chapter 8.
2. Remote ProcedureCall(RPC)facility. Itprovidesprogrammerswith anumberof
powerfultools necessarytobuild client-serverapplications. In fact, the DCERPC facility
isthe basisfor allcommunicationinDCE becausethe programmingmodelunderlyingall
ofDCEisthe client-servermodel. Itiseasy touse, isnetwork-and protocol-independent,
provides secure communication between a client and a server, and hides differences in
data requirements by automatically converting data to the appropriate forms needed by
clients and servers. Details are given in Chapter4.
3. Distributed lime Service (DTS). It closely synchronizes the clocks of all the
computersinthe system. Italso permitsthe use oftime valuesfrom externaltime sources,
u.s.
such as those of the National Institute for Standards and Technology (NIST), to
synchronizetheclocksofthecomputersinthe systemwith externaltime. This facility can
also be used to synchronize the clocks of the computers ofone distributed environment
with the clocksofthe computers of anotherdistributed environment. Details are given in
Chapter 6.
4. Name services. The name services of DCE include the Cell Directory Service
(CDS), the Global Directory Service (GDS), and the Global Directory Agent (GDA).
These services allow resources such as servers, files, devices, and so on, to be uniquely
named and accessed in a location-transparent manner. Details are given in Chapter 10.
5. SecurityService.Itprovidesthe tools needed for authenticationand authorization
to protect system resources against illegitimate access. Details are given in Chapter 11.
6. DistributedFileService(DFS).Itprovidesasystemwidefile systemthat has such
characteristicsaslocationtransparency,high performance,and high availability.Aunique
featureofDeEDFS isthat itcan also providefile servicestoclientsofotherfile systems.
Details are given in Chapter9.
The DCE components listed above are tightly integrated. It is difficult to give a
pictorial representation of their interdependencies because they are recursive. For
example, the name services use RPC facility for internal communication among its

Sec.1.7• Introduction toDistributedComputing Environment (DeE) 37
various servers, but the RPC facility uses the name services to locate the destination.
Therefore, the interdependenciesof thevarious DeEcomponents can be best depicted in
tabular form, as shown in Figure 1.8.
Component OthercomponentsusedbyIt
name
Threads None
RPe Threads,name,security
DTS Threads, RPC,name,security
Name Threads, RPe,DTS,security
Security Threads, RPC,DTS,name
Fig. 1.8 Interdependenciesof DCE DFS Threads, RPC, DTS,name,security
components.
1.7.4 DeE (.lIs
The DeE system is highly scalable in the sense that a system running DeE can have
thousands of computers and millions of users spread over a worldwide geographic area.
Toaccommodate such large systems, DCE uses the concept of cells. This concept helps
break down a large system into smaller, manageable units called cells.
In a DeE system, a cell is a group of users, machines, or other resources that
typically have a common purpose and share common DCE services. The minimum cell
configuration requires a cell directory server, a security server, a distributed time server,
and one or more client machines. Each DeE client machine has client processes for
security service, cell directory service, distributed time service, RPC facility,and threads
facility.ADCEclient machine mayalsohaveaprocess fordistributed fileserviceifacell
configuration has a DeE distributed file server. Due to the use of the method of
intersection for clock synchronization (described in Chapter 6), it is recommended that
each cell in a DeE system should have at least three distributed time servers.
Animportantdecision tobemadewhilesetting upaDCEsystemistodecidethecell
boundaries. The following fourfactors should betaken intoconsiderationformaking this
decision [Tanenbaum 1995, Rosenberry et al. 1992, aSF 1992]:
1. Purpose. The machines of usersworking on acommon goa)should beput in the
same cell, as they need easy access to a common set of system resources. That is, users
of machines in the same cell have closer interaction with each other than with users of
machines in different cells. For example, if a company manufactures and sells various

38 Chap. 1 • Fundamentals
types of products, depending on the manner in which the company functions, either a
product-orientedor afunction-orientedapproachmaybetakentodecidecell boundaries
[Tanenbaum1995].Inthe product-orientedapproach,separatecells are formed foreach
product, withtheusersof the machinesbelongingtothe samecell beingresponsiblefor
all typesofactivities(design,manufacturing,marketing,andsupportservices)relatedto
oneparticularproduct.Ontheotherhand,inthefunction-orientedapproach,separatecells
are formed for each type of activity,with the users belonging to the same cell being
responsible fora particularactivity,suchas design,of all typesof products.
2. Administration. Each systemneedsan administratorto register new users inthe
systemandtodecidetheiraccessrightstothesystem'sresources.Toperformhisorherjob
properly,anadministratormustknowtheusersandtheresourcesofthesystem.Therefore,
to simplify administrationjobs, all the machines and their users that are known to and
manageablebyanadministratorshouldbeputinasinglecell.Forexample,allmachines
belongingtothesamedepartmentofacompanyorauniversitycanbelongtoasinglecell.
Fromanadministrationpointofview,eachcellhasadifferentadministrator.
3. Security.Machinesof thoseuserswhohavegreatertrustineach other shouldbe
put in the samecell.That is, usersof machinesofacell trusteach other morethanthey
trust the users of machines of other cells. In such a design, cell boundaries act like
firewallsinthesensethataccessingaresourcethatbelongstoanothercell requiresmore
sophisticatedauthenticationthanaccessinga resourcethat belongsto a user's owncell.
4. Overhead.SeveralDeE operations,suchasnameresolutionanduserauthentica
tion, incur more overhead when they are performed betweencells than when they are
performedwithinthesamecell.Therefore,machinesofuserswhofrequentlyinteractwith
each other and the resourcesfrequentlyaccessed by them shouldbe placed in the same
cell. The need to access a resource of another cell should arise infrequently for better
overall systemperformance.
Noticefromtheabovediscussionthatindeterminingcellboundariestheemphasisis
on purpose, administration,security,andperformance.Geographicalconsiderations can,
butdonothaveto,playapartincelldesign.Forbetterperformance,itisdesirabletohave
as few cells as possible to minimize the number of operations that need to cross cell
boundaries.However,subjecttosecurityandadministrationconstraints,itisdesirableto
have smallercells withfewermachinesand users.Therefore,it is importantto properly
balancetherequirementsimposedbythefourfactorsmentionedabovewhiledecidingcell
boundaries ina DeE system.
1.8 SUMMARY
A distributed computing system is a collection of processors interconnected by a
communication network in which each processor has its own local memory and other
peripheralsandcommunicationbetweenanytwoprocessorsofthesystemtakesplaceby
messagepassingoverthecommunicationnetwork.

Chap. I • Exercises 39
The existingmodels fordistributedcomputing systems can bebroadly classifiedinto
five categories: minicomputer, workstation, workstation-server, processor-pool, and
hybrid.
Distributedcomputingsystems aremuchmorecomplex anddifficulttobuildthanthe
traditional centralized systems. Despite the increased complexity and the difficulty of
building, the installation and use of distributed computingsystems are rapidly increasing.
This is mainly because the advantages of distributed computing systems outweigh its
disadvantages. The main advantages of distributed computing systems are (a) suitability
forinherentlydistributedapplications, (b)sharing ofinformationamong distributedusers,
(c)sharing ofresources, (d) better price-performanceratio, (e)shorterresponse times and
higher throughput, (f) higher reliability, (g)extensibility and incremental growth, and (h)
better flexibility in meeting users' needs.
The operating systems commonly used for distributed computing systems can be
broadly classified into two types: network operating systems and distributed operating
systems. As compared to a networkoperating system, adistributed operating system has
better transparency and fault tolerance capability and provides the image of a virtual
uniprocessor to the users.
The main issues involved in the design of a distributed operating system are
transparency, reliability, flexibility, performance, scalability, heterogeneity, security, and
emulation of existing operating systems.
EXERCISES
1.1. Differentiate among the following typesof operating systems by defining their essential
properties:
(a) Timesharing
(b) Parallelprocessing
(c) Network
(d) Distributed
1.2. In what respect are distributedcomputingsystems betterthan parallelprocessingsystems?
Giveexamplesof threeapplicationsforwhichdistributedcomputingsystemswillbemore
suitablethanparallelprocessingsystems.
1.3. What were the major technological, economical, and social factors that motivated the
developmentofdistributedcomputingsystems?Whataresomeofthemainadvantagesand
disadvantagesofdistributedcomputingsystemsovercentralizedones?
1.4. Discusstherelativeadvantagesanddisadvantagesofthevariouscommonlyusedmodelsfor
configuringdistributedcomputingsystems.Whichmodeldoyouthinkisgoingtobecomethe
mostpopularmodelinfuture?Givereasonsforyouranswer.
1.5. Considerthecaseofadistributedcomputingsystembasedontheprocessor-poolmodelthat
has P processorsin the pool. In this system,suppose a user starts a computationjob that
involvescompilationofaprogramconsistingofF sourcefiles(F <P). Assumethatatthis
timethisuseristheonlyuserusingthesystem.Whatmaximumgaininspeedcanbehoped
forthisjob inthissystemascomparedtoitsexecutiononasingle-processorsystem(assume
thatalltheprocessorswearetalkingaboutareofequalcapability)?Whatfactorsmightcause
thegaininspeedtobelessthanthismaximum?

40 Chap.1 • Fundamentals
1.6. Explainthedifference between thetermsservice andserver. Inthedesignof adistributed
operatingsystem,discusstherelativeadvantagesanddisadvantagesofusingasingleserver
andmultiple servers forimplementing aservice.
1.7. Whyaredistributed operating systemsmoredifficulttodesignthanoperatingsystemsfor
centralizedtime-sharing systems?
1.8. What is groupware? Why is it considered to be a promising technology for software
development?
1.9. Whatarethemaindifferencesbetweenanetworkoperatingsystemandadistributedoperating
system?
1.10. Whatarethemajorissuesindesigning adistributed operating system?
1.11. Adistributedoperatingsystemmakesacollectionofnetworkedmachinestoactlikeavirtual
uniprocessor. Whatarethemainadvantages ofthisvirtual-machine architecture forauser?
What issues are important for a distributed operating systemdesigner in achieving this
goal?
1.12. Concurrency transparency is an important issue in the designof a distributed operating
system.Isitalsoanimportant issueinthedesignofanoperating systemforacentralized
system?Ifno,explainwhy.Ifyes,listsomemechanismsthatarecommonlyusedinoperating
systemsforcentralized systemstosupportthisfeature.
1.13. Discusssomeoftheimportantconceptsthatadistributedoperatingsystemdesignermightuse
toimprovethereliabilityofhisorhersystem.Whatisthemainprobleminmakingasystem
highlyreliable?
1.14. Differentiate between the monolithic kernel and microkernel approaches for designing a
distributedoperating system. Discusstheirrelative advantages anddisadvantages.
1.15. In the microkernel approach for designing a distributed operating system, what are the
primarytasksthatthekernelmustperform?
1.16. Figure1.6indicatesthatalayeredapproach isusedtodesignadistributedsystem. Whatare
themainadvantages ofusingthisapproach?
1.17. Discussthemainguidingprinciplesthatadistributedoperatingsystemdesignermustkeepin
mindforthegoodperformance ofhisorhersystem.
1.18. Whyisscalabilityanimportantfeatureinthedesignofadistributed system?Discusssome
oftheguidingprinciples fordesigning ascalabledistributed system.
1.19. Why is heterogeneity unavoidable in manydistributed systems? What are some of the
commontypesofincompatibilitiesencountered inheterogeneousdistributedsystems?What
arethecommonissueswithwhichthedesignerofaheterogeneous distributed systemmust
deal?
1.20. Suppose a component of a distributed system suddenly crashes. How will this event
inconvenience theuserswhen:
(a) Thesystemusestheprocessor-pool modelandthecrashedcomponent isaprocessor
inthepool.
(b) The systemuses the processor-pool model and the crashedcomponent is a user
terminal.
(c) Thesystemusestheworkstation-servermodelandthecrashedcomponentisaserver
machine.
(d) Thesystemusestheworkstation-server modelandthecrashedcomponent isa user
workstation.

Chap. 1 • Bibliography 41
1.21. Compare the followingtypesof systemsin termsofcost, hardwarecomplexity,operating
systemcomplexity,potentialparallelism,andprogrammability (howeasily userscan write
efficientprograms):
(a) Amultiprocessorsystemhavingasinglesharedmemory.
(b) A multiprocessorsysteminwhicheachprocessorhasitsownlocalmemoryinaddition
toa sharedmemoryusedbyall processorsin thesystem.
(c) Amultiprocessorsysteminwhicheachprocessorhasitsownmemory.Allprocessors
are keptin a bighallandareinterconnectedbya high-capacitycommunicationline
forminga network.Eachprocessorcancommunicatewithotherprocessorsonlyby
exchangingmessages.
(d) A multiprocessor system in which each processor has its own memory. The
processorsare locatedfar fromeach other(may be in differentcitiesof a country)
and are interconnectedby a low-capacity communicationline forminga network.
Each processor can communicate with other processors only by exchanging
messages.
Forcomparingthesystems,considerthreecases-(a) numberofprocessorsissmall(2-8);
(b)numberofprocessorsislarge(16-32); and(c)numberofprocessorsisverylarge(more
than 100).
BIBLIOGRAPHY
[Accettaet al. 1986]Accetta,M.,Baron,R.,Golub,D.,Rashid,R.,Tevanian,A.,andYoung, M.,
"Mach:ANewKernelFoundationforUNIXDevelopment,"In:ProceedingsoftheSummer1986
USENIX Technical Conference, pp.93-112 (July1986).
[Avresky and Pradhan 1996]Avresky, D., andPradhan,D. (Eds.),Fault-Tolerant Parallel and
DistributedSystems, IEEE ComputerSocietyPress,LosAlamitos,CA (1996).
[Black et al. 1992] Black, D.L.,Golub, D.B.,Julin, D.P.,Rashid, R.F.,Draves, R. P.,Dean, R.
W.,Forin, A., Barrera, L, Tokuda,H., Malan,G., and Bohman,D., "MicrokernelOperating
SystemArchitectureandMach,"In:Proceedingsofthe USENIX WorkshoponMicrokernels and
Other Kernel Architectures, USENIX,pp. 11-30 (1992).
[Boykin and LoVerso 1990] Boykin, J., and LoVerso, l, "Recent Developmentsin Operating
Systems,"IEEE Computer, pp.5-6 (May 1990).
[BrazierandJohansen1993]Brazier,F.,andJohansen,D.(Eds.),DistributedOpenSystems, IEEE
ComputerSocietyPress,LosAlamitos,CA(1993).
[Butler 1993] Butler,M., Client Server, Prentice-Hall, London,UK(1993).
[Casavant and Singhal 1994]Casavant,T. L., and Singhal,M. (Eds.), Readings in Distributed
Computing Systems, IEEE ComputerSocietyPress,LosAlamitos,CA (1994).
[Cheriton 1984]Cheriton,D.R.,"TheV Kernel:ASoftwareBaseforDistributedSystems,"IEEE
Software, Vol. 1,No.2, pp. 19-42 (1984).
[Cheriton 1988]Cheriton,D. R.,"TheV DistributedSystem,"CommunicationsoftheACM, Vol.
31, No.3, pp.314-333 (1988).
[Coulouris et al, 1994] Coulouris,G. F.,Dollimore,1., and Kindberg,T., Distributed Systems
Concepts and Design, 2nded.,Addison-Wesley, Reading,MA(1994).
[Cristian-1991]Cristian,F.,"UnderstandingFault-TolerantDistributedSystems,"Communications
oftheACM, Vol. 34, pp.56-78 (February1991).

42 Chap. 1 • Fundamentals
[Critchleyand Batty 1993]Critchley,T.,andBatty,K.,OpenSystems: TheReality,Prentice-Hall,
London,UK(1993).
[Deitel 1990] Deitel, H. M., An Introduction to Operating Systems, 2nd ed., Addison-Wesley,
Reading,MA(1990).
(Dougliset al, 1991]Douglis,F., Ousterhout,1.K.,Kaashoek, M.F.,andTanenbaum,A.S.,"A
ComparisonofTwoDistributedSystems:AmoebaandSprite,"Computing Systems, Vol. 4,pp.
353-384 (1991).
[Ghafoor and Yang 1993J Ghafoor,A., and Yang, J., "A Distributed Heterogeneous Super
computingManagementSystem,"IEEE Computer, Vol. 26,No.6, pp.78-86 (1993).
[Gien 1990]Gien,M.,"Micro-Kernel Architecture: KeytoModernOperatingSystemsDesign,"
UNIX Review, p. 10(November1990).
[Gien and Grob 1992]Gien,M.,andGrob,L.,"Microkernel BasedOperatingSystems:Moving
UNIXon to ModernSystemArchitectures,"In:Proceedings ofthe UniForum'92 Conference,
USENIX,pp.43-55 (1992).
[Golub et al, 1990]Golub,D., Dean, R., Forin,A., and Rashid,R., "UNIX as an Application
Program,"In:ProceedingsoftheSummer 1990USENIXConference,USENIX,pp.87-95 (June
1990).
(Goscinski 1991] Goscinski,A., Distributed Operating Systems, The Logical Design, Addison
Wesley, Reading,MA(1991).
[Hariri et al. 1992] Hariri, S., Choudhary, A., and Sarikaya, B, "Architectural Support for
DesigningFault-TolerantOpenDistributedSystems,"IEEEComputer,Vol.25,No.6,pp.50-62
(1992).
[Hunter1995]Hunter,P.,NetworkOperating Systems:MakingtheRightChoice,Addison-Wesley,
Reading,MA(1995).
[Islam 1996]Islam,N.,Distributed Objects: Methodologies for Customizing Operating Systems,
IEEEComputerSocietyPress,LosAlamitos,CA(1996).
[ISO1992]BasicReferenceModelofOpenDistributedProcessing,Part1:OverviewandGuideto
Use, ISO/IECJTCI/SC212IWG7CDI0746-1, International StandardsOrganization(1992).
[Jalote 1994JJalote,P.,Fault Tolerancein Distributed Systems, Prentice-Hall, EnglewoodCliffs,
NJ(1994).
[Khanna 1994] Khanna. R. (Ed.), Distributed Computing: Implementation and Management
Strategies, Prentice-Hall, EnglewoodCliffs,NJ(1994).
[Khokhar et al. 1993] Khokhar, A., Prasanna, V. K., Shaaban, M. E., and Wang, C. L.,
"HeterogeneousComputing:ChallengesandOpportunities,"IEEEComputer,Vol. 26,No.6, pp.
18-27 (1993).
[Lampson1983]Lampson,B.W.,"HintsforComputerSystemDesign,"In:Proceedingsofthe9th
Symposium on Operating Systems Principles (October1983).
[Lamportetal.1982]Lamport,L.,Shostak,R.,andPease,M.,"TheByzantineGeneralsProblem,"
ACM Transactions on Programming Languages and Systems, Vol. 4, No.3, pp. 382-401
(1982).
[Lelann1981]Lelann,G.,"Motivations,Objectives,andCharacterizationofDistributedSystems,"
DistributedSystems-ArchitectureandImplementation,LectureNotesinComputerScience, Vol.
105,Springer-Verlag, NewYork, NY(1981).
[LockhartJr.1994]LockhartJr.,H.W.,OSFDCE:GuidetoDevelopingDistributedApplications,
IEEEComputerSocietyPress,LosAlamitos,CA(1994).

Chap. I • Bibliography 43
[Marca and Bock 1992] Marca, D., and Bock, G. (Eds.), Groupware: Software for Computer
Supported Cooperative Work, IEEEComputerSociety Press, Los Alamitos, CA(1992).
[Martin et al. 1991] Martin, B. E., Pedersen, C. H., and Roberts, 1. B., "An Object-Based
Taxonomy for DistributedComputing Systems," IEEE Computer, Vol.24, No.8 (1991).
[Milenkovic 1992] Milenkovic, M., Operating Systems: Concepts and Design, 2nd ed., McGraw
Hill, New York (1992).
[Mullender 1987] Mullender, S. J.,HDistributed Operating Systems," Computer Standards and
Interfaces, Vol.6, pp. 37-44 (1987).
[Mullender1993]Mullender, S. J. (Ed.), DistributedSystems, 2nd ed., Addison-Wesley, Reading,
MA(1993).
[Mullender and Tanenbaum 1984] Mullender, S. 1., and Tanenbaum, A. S., "Protection and
Resource Control in DistributedOperating Systems," ComputerNetworks, Vol.8, pp. 421-432
(1984).
[Mullenderetal, 1990)Mullender,S.1.,VanRossum,G.,Tanenbaum,A.S.,VanRenesse,R.,and
VanStaverene, H.,"Amoeba: A Distributed Operating System for the 1990s,"IEEE Computer,
Vol.23, No.5, pp. 44-53 (1990).
[Needham and Herbert 1982] Needham, R. M., and Herbert, A. 1., The Cambridge Distributed
Computing System, Addison-Wesley, Reading, MA(1982).
[Nelson1990]Nelson,V.P.,"Fault-TolerantComputing:FundamentalConcepts,"IEEE Computer,
Vol.23, No.7, pp. 19-25 (1990).
[Nicoletal.1993]Nicol,J.R.,Wilkes,C.T.,andManola,F.A.,"ObjectOrientationinHeterogeneous
DistributedComputingSystems,"IEEEComputer,Vol.26, No.6,pp.57-·67 (1993).
[Notkin et al, 1987]Notkin, D., Hutchinson, N., Sanislo, 1., and Schwartz, M., "Heterogeneous
Computing Environments: Report on the ACM SIGOPS Workshop on Accommodating
Heterogeneity," Communications oftheACM, Vol.30, No.2, pp.132-140(1987).
[Nutl1991] Nutt,G.J., Centralizedand DistributedOperatingSystems, Prentice-Hall,Englewood
Cliffs, NJ (1991).
[Nutt 1992] Nutt, G. 1.,Open Systems, Prentice-Hall, Englewood Cliffs, NJ (1992).
[OSF 1992] Introduction to OSF DeE, Prentice-Hall, Englewood Cliffs, NJ (1992).
[Ousterhout et al, 1988] Ousterhout, J.K., Cherenson, A. R., Douglis, F., Nelson, M. N., and
Welch, B. B., "The Sprite Network Operating System," IEEE Computer, Vol. 21, No.2, pp.
23-36 (1988).
[Pikeetal, 1990]Pike, R., Presotto, D., Thompson, K., and Trickey, H., "Plan9 from Bell Labs,"
In:ProceedingsoftheSummer1990UKUUG (UK UnixUsersGroup) Conference, pp. 1-9 (July
1990).
[PopekandWalker1985]Popek,G., and Walker, B.,TheLOCliSDistributedSystemArchitecture,
MIT Press, Cambridge, MA (1985).
[Rashid1985] Rashid,R.F.,"NetworkOperatingSystems,"In:LocalAreaNetworks:AnAdvanced
Course, Lecture Notes inComputerScience, Vol.184,pp. 314-340,Springer-Verlag,New York,
NY (1985).
[Ritchie andThompson1974]Ritchie,D., andThompson,K.,"TheUNIX Time-SharingSystem,"
Communications ofthe ACM, Vol. 17, No.7, pp. 365-375 (1974).
[Rosenberry et ale 1992] Rosenberry, W., Kenney, D., and Fisher, G., OSF DISTRIBUTED
COMPUTING ENVIRONMENT, Understanding DeE, O'Reilly, Sebastopol, CA (1992).

44 Chap. 1 • Fundamentals
[SchlichtingandSchneider1983]Schlichting,R.D.,andSchneider,F.B.,"Fail-StopProcessors:
AnApproachtoDesigningFault-TolerantComputingSystems,"ACMTransactionsonComputer
Systems, Vol. 1,No.3, pp.222-238 (1983).
(Shoch and Hupp 1982]Shoch,J.F.,andHupp,1.A.,"TheWormPrograms:EarlyExperiences
witha DistributedComputation,"Communications ofthe ACM, Vol. 25, No.3, pp. 172-180
(1982).
[Silberschatzand Galvin 1994]Silberschatz,A.,andGalvin,P.B.,Operating Systems Concepts,
4thed.,Addison-Wesley, Reading,MA(1994).
[SinghalandShivaratri1994]Singhal,M.,andShivaratri,N.G.,AdvancedConceptsinOperating
Systems, McGraw-Hili,NewYork, NY(1994).
[Stalling 1995] Stalling, W., Operating Systems, 2nd ed., Prentice-Hall,Englewood Cliffs, NJ
(1995).
[Stankovic 1984] Stankovic, J. A., "A Perspective on DistributedComputer Systems," IEEE
Transactions on Computers, Vol. C-33,No. 12,pp. 1102-1115(1984).
[Suriet al, 1994]Suri,N.,Walter,C.J.,andHugue,M.M.(Eds.),Advances in Ultra-Dependable
DistributedSystems, IEEEComputerSocietyPress,LosAlamitos,CA(1994).
[Tanenbaum 1995]Tanenbaum,A. S., Distributed Operating Systems, Prentice-Hall,Englewood
Cliffs,NJ(1995).
[Tanenbaum and Van Renesse 1985] Tanenbaum,A. S., and VanRenesse, R., "Distributed
OperatingSystems,"ACM Computing Surveys, Vol. 17,No.4, pp.419-470 (1985).© ACM,
Inc., 1985.
[Umar1993]Umar,A.,DistributedComputing:A PracticalApproach, Prentice-Hall,Englewood
Cliffs,NJ(1993).
[Vaughn 1994]Vaughn, L.T.,Client/Server System Design and Implementation, IEEEComputer
SocietyPress,LosAlamitos,CA(1994).
[Wittie 1991]Wittie,L. D.,"ComputerNetworksandDistributedSystems,"IEEE Computer, pp.
67-75 (1991).
POINTERS TO818UOGRAPHIES ONTHE INTERNET
Bibliography containing references onOperating Systems canbefoundat:
ftp:ftp.cs.umanitoba.calpublbibliographies/Os/os.html
Bibliography containing references on Taxonomiesfor Parallel and DistributedSystems
canbefoundat:
ftp:ftp.cs.umanitoba.calpublbibliographieslParalleVtaxonomy.html
Bibliography containing references onDistributed Computing canbefoundat:
ftp:ftp.cs.umanitoba.ca/publbibliographieslDistributedlOsser.html
Bibliographies containing references onDistributedSystems canbefoundat:
ftp:ftp.cs.umanitoba.calpublbibliographieslDistributedlDcs-l.O.html
ftp:ftp.cs.umanitoba.calpublbibliographieslDistributedldist.sys.l.html

Chap. 1 • Pointers toBibliographies ontheInternet 4S
Bibliography containing references onOpen Systems andOpen Computing canbefound
at:
ftp:ftp.cs.umanitoba.calpublbibliographies/Os/opencomp.html
Bibliography containing references on Fault Tolerant Distributed Systems can be found
at:
ftp:ftp.cs.umanitoba.calpub/bibliographieslDistributed/fauIt.tolerant.html
Bibliographies containing references on Computer SupportedCooperative Work(CSCW)
can be found at:
ftp:ftp.cs.umanitoba.ca/pub/bibliographies/Distributed/CSCWBiblio.html
ftp:ftp.cs.umanitoba.calpub/bibliographieslDistributed/CSCW92.htmI
List of publications of the MIT Parallel & Distributed Operating Systems (PDOS) group
can be found at:
http:www.pdos.lcs.mit.eduIPDOS-papers.html
List of publications of the Stanford Distributed Systems Group (DSG) can be found at:
http:www-dsg.stanford.edulPublications.html
List of publications of the Distributed Systems Research Group (DSRG) at Oregon
Graduate Institute can be found at:
http:www.cse.ogi.edu/DSRG/osrg/osrg.html#Current Paper
List of publications of the Distributed Systems Group atTrinity College, Dublin, can be
found at:
http:www.dsg.cs.tcd.ie/dsgpublications/bibs

2
CHAPTER
Computer Networks
2.1 INTRODUmON
Acomputernetwork isacommunicationsystem that linksendsystems bycommunication
linesand software protocols toexchangedata between twoprocessesrunning on different
end systems of the network.The end systems are often referred to as nodes, sites, hosts,
computers, machines, and so on. The nodes may vary in size and function. Sizewise, a
node may be a small microprocessor, a workstation, a minicomputer, or a large
supercomputer. Functionwise,anode may beadedicated system (suchas aprint serveror
afile server) without anycapabilityforinteractiveusers, asingle-userpersonal computer,
or a general-purpose time-sharing system.
As already mentioned in Chapter 1, a distributed system is basically a computer
network whose nodeshavetheirown localmemory andmayalsohaveother hardware and
software resources. A distributed system, therefore, relies entirely on the underlying
computer network for the communication of data and control information between the
nodes of which they are composed. Furthermore, the performance and reliability of a
distributed system depend to a great extent on the performance and reliability of the
underlyingcomputernetwork.Hence abasic knowledge ofcomputernetworks isrequired
for the study ofdistributed operating systems. A comprehensive treatment ofcomputer
networks will require acompletebook in itself, and there are many good books available
on this subject [Tanenbaum 1988, Black 1993, Stallings 1992b, Ramos et at. 1996].
46

Sec.2.2 • Networks Types 47
Therefore,thischapterdealsonlywiththemostimportantaspectsofnetworkingconcepts
anddesigns,withspecialemphasistothoseaspectsthatareneededasabasisfordesigning
distributed operating systems.
2.2 NETWORKS TYPES
Networksarcbroadlyclassifiedintotwotypes:local area networks(LANs) andwide-area
networks (WANs). The WANs are also referred to as long-haul networks. The key
characteristics thatareoftenusedtodifferentiatebetweenthesetwotypesofnetworksare
as follows [Abeysundara and Kamal 1991]:
1. Geographicdistribution. The maindifference betweenthetwotypesofnetworks
isthe wayin which they are geographicallydistributed.A LAN isrestrictedto a limited
geographic coverage of a few kilometers, but aWAN spans greater distances and may
extendoverseveralthousand kilometers.Therefore, LANstypicallyprovidecommunica
tion facilities within a buildingor acampus, whereasWANsmay operate nationwideor
even worldwide.
2. Data rate. Data transmission rates are usually much higher in LANs than in
WANs.TransmissionratesinLANsusuallyrangefrom0.2megabitpersecond(Mbps)to
1gigabitpersecond(Gbps).Ontheotherhand,transmissionratesinWANsusuallyrange
from 1200 bits per second to slightly over 1Mbps.
3. Error rate. Local area networks generally experience fewer data transmission
errors thanWANsdo.Typically, biterror ratesare intherangeof 10--8- 10-12 withLANs
as opposed to 10-5--10-7 with WANs.
4. Communication link. The most common communication Jinksusedin LANs are
twistedpair,coaxial cable, and fiberoptics. On theother hand, sincethe sitesinaWAN
arephysicallydistributed overa largegeographic area,thecommunicationlinks usedare
bydefaultrelativelyslowandunreliable.Typicalcommunication linksusedinWANsare
telephone lines, microwave links, and satellite channels.
5. Ownership. A LAN is typically owned by a single organization because of its
limited geographic coverage. A WAN, however, is usually formed by interconnecting
multiple LANs each of which may belong to a different organization. Therefore,
administrativeandmaintenancecomplexitiesandcostsfor LANsareusuallymuchlower
than for WANs.
6. Communication cost. TheoverallcommunicationcostsofaLANisusuallymuch
lower than that of a WAN.The main reasons for this are lower error rates, simple (or
absence of) routing algorithms, and lower administrative and maintenance costs.
Moreover,thecosttotransmitdatainalJANisnegligiblesincethetransmissionmedium
isusuallyowned by theuserorganization. However,with aWAN,this cost may be very
high because the transmission media used are leased lines or public communication
systems, such as telephone lines, microwave links, and satellitechannels.

48 Chap.2 • ComputerNetworks
Networks that share some of the characteristics of both LANs and WANs are
sometimes referred to as metropolitan area networks (MANs) [Stallings 1993a]. The
MANs usually covera widergeographic area (up toabout 50km indiameter) thanLANs
and frequently operate at speeds very close to LAN speeds. A main objective of MANs
is to interconnect LANs located in an entire city or metropolitan area. Communication
links commonly used for MANs are coaxial cable and microwave links.
WesawinChapter 1that theperformanceofadistributed system must beatleastas
good asacentralized system.That is,when aparticular application isrunon adistributed
system, its overall performance should not be appreciably worse than running the same
application onasingle-processorsystem.Thedata transmission ratesofLANs andMANs
are usually 'considered to be adequate to meet this requirement for many applications.
However, with thecurrent technology, the transmission rates of WANscannot fully meet
this requirement of distributed systems. Therefore, WAN-based distributed systems are
used mainly for those applications for which performance is not important. Several
inherently distributed applications that require information sharing among widely
distributed users/computers belong to this category.
Although current WANs cannot fully meet the performance requirements of
distributed systems, with the emergence of Broadband Integrated Services Digital
Network (B-ISDN) [Kawarasaki and Jabbari 1991] and Asynchronous Transfer Mode
(ATM) technologies [Vetter 1995], future WANsare expected to have data transmission
rates that willbeadequate for theconstructionofWAN-baseddistributed systems andthe
implementation of a wide range of applications on these distributed systems. ISDN
[Helgert 1991] refers to telecommunication networks that transfer many types of data,
such as voice, fax, and computer data, in digital form at data transmission rates that are
multiples of a basic channel speed of 64 kilobits per second (Kbps). B-ISDN are ISDN
networks that provide point-to-point data transmission speeds of 150 Mbps and above.
B-ISDN networks are considered to be suitable for high-bandwidth applications, such as
applications involving high-quality video and bulk data transmissions. ATM technology
canprovide datatransmission ratesofupto622Mbps. (ATMtechnology isdescribed later
in this chapter.)
2.3 LAN TECHNOLOGIES
This section presents adescription of topologies, principles ofoperation, andcase studies
of popular LANs.
2.3.1 LAN Topologl8s
The two commonly used network topologies for constructing LANs are multiaccess
bus and ring. In a simple multiaccess bus network, all sites are directly connected to
a single transmission medium (called the bus) that spans the whole length of the
network (Fig. 2.1). The bus is passive and is shared by all the sites for any message
transmission in the network. Each site is connected to the bus by a drop cable using
a T-connection or tap. Broadcast communication is used for message transmission.

Sec.2.3 • LANTechnologies 49
~Sites
Sharedbus
Fig.2.1 Simple multiaccess bus network topology.
That is, a message is transmitted from one site to another by placing it on the shared
bus. An address designator is associated with the message. As the message travels on
the bus, each site checks whether it is addressed to it and the addressee site picks up
the message.
A variant of the simple multiaccess bus network topology is the multiaccess
branching bus network topology. In such a network, two or more simple multiaccess bus
networksare interconnectedby using repeaters(Fig. 2.2). Repeatersare hardware devices
used to connectcable segments. They simply amplify and copy electric signals from one
segment of a network to its next segment.
Sharedbus
/
~Siles
Fig. 2.2 Multiaccess branching bus network topology.

so Chap. 2 • Computer Networks
The connectioncostofamultiaccessbusnetwork islowandgrows onlylinearly with
the increase in number of sites. The communicationcost isalso quite low,unless there is
heavy contentionfor theshared bus and the bus becomes abottleneck. Adisadvantageof
multiaccess bus topology is that message signals, transmitted over the single shared
medium, suffer more attenuation and distortion compared to the shorter point-to-point
links of other topologies. Therefore, especially in the case of fiber-optic bus networks,
only a few sites can usually be supported.
In a ring network, each site is connected to exactly two other sites so that a loop is
formed (Fig.2.3).Aseparate linkisusedtoconnect twosites.The linksareinterconnected
by using repeaters. Data is transmitted in one direction around the ring by signaling
between sites. That is, to send a message from one site to another, the source site writes
thedestinationsite'saddress inthemessage header andpasses ittoitsneighbor.Asitethat
receives the message checks the message header to see if the message is addressed to it.
If not, the site passes on the message to its own neighbor. In this manner, the message
circulatesaround thering until some siteremoves itfrom thering. Insome ring networks,
the destination site (to which the message is addressed) removes the message from the
ring, while inothers itisremovedbythesource site(which sentthemessage). Inthelatter
case, the message always circulatesforone completeround onthe ring. Generally, inring
networks, one ofthesitesacts asamonitorsite toensure thatamessage does notcirculate
indefinitely (that is, in case the source site or the destination site fails). The monitor site
also perform other jobs, such as housekeeping functions, ring utilization, and handling
other error conditions.
I Monitor
site
Fig.2.3 Ring network topology.

Sec.2.3 • LANTechnologies 51
The connection cost of a ring network is low and grows only linearly with the
increase in number of sites. The average communication cost is directly proportional to
the number of sites in the network. If there are n sites, at most(n-l) links have to be
traversed by a message to reach its destination. An often-quoted disadvantage of ring
topology is the vulnerability ofring networks due to site or link failures; the network is
partitioned by a single link failure. Variations ofthe basic ring topology, such as using
bidirectional links, providingdouble links betweentwo neighbors, and site skippinglinks
with each sitejoinedto its two immediatepredecessors, have beenconsideredto improve
network reliability.
2.3.2 Medium-Access Control Protocols
In case of both multiaccess bus and ring networks, we saw that a single channel is
shared by all the sites ofa network, resulting in a multiaccess environment. In such an
environment, it is possible that several sites will want to transmit information over the
shared channel simultaneously. In this case, the transmitted information may become
scrambled and must be discarded. The concerned sites must be notified about the
discarded information, so that they can retransmit their information. If no special
provisions are made, this situation may be repeated, resulting in degraded performance.
Therefore, special schemes are needed in a multiaccess environment to control the
access to a shared channel. These schemes are known as medium-access control
protocols.
Obviously, in a multiaccess environment, the use of a medium having high
raw data rate alone is not sufficient. The medium-access control protocol used must
also provide for efficient bandwidth use of the medium. Therefore, the medium
access control protocol has a significant effect on the overall performance of a
computer network, and often it is by such protocols that the networks differ the
most. The three most important performance objectives of a medium-access control
protocol are high throughput, high channel utilization, and low message delay. In
addition to meeting the performance objectives, some other desirable characteristics
of a medium-access control protocol are as follows [Abeysundara and Kamal
1991J:
1. For fairness, unless a priority scheme is intentionally implemented, the protocol
should provide equal opportunity to all sites in allowing them to transmit their
information over the shared medium.
2. For better scalability, sites should require a minimum knowledge of the
network structure (topology, size, or relative location of other sites), and
addition, removal, or movement of a site from one place to another in the
network should be possible without the need to change the protocol. Fur
thermore, it should not be necessary to have a knowledge of the exact value
of the end-to-end propagation delay of the network for the protocol to
function correctly.
3. For higher reliability, centralized control should be avoided and the operation of
the protocol should be completely distributed.

52 Chap. 2 • Computer Networks
4. For supporting real-time applications, the protocol should exhibit bounded-delay
properties. That is,the maximum message transfer delay fromone siteto another
in the network must be known and fixed.
It is difficult to achieve all of the previously mentioned characteristics at the same
time while achieving the performance objectives of high throughput, high channel
utilization, and low message delay.
Several protocols have been developed for medium-access control in a multiaccess
environment. Of these, the Carrier Sense Multiple Access with Collision Detection
(CSMNCD) protocol is theone most commonly used for multiaccess bus networks, and
the token ring and slotted ring are the two commonly used protocols for ring networks.
These medium-access control protocols are described next.
Note that for efficient and fair use of network resources, a message isoften divided
into packets prior to transmission. In this case, a packet is the smallest unit of
communication. It usually contains a header and a body.The body contains a part of the
actual message data, and the header contains addressing information (identifiers of the
sender and receiversites) and sequencing information (position of the packet data within
the entire message data).
The CSMAlCD Protocol
The CSMAlCD scheme [IEEE 1985a] employs decentralized control of the shared
medium, In this scheme, each site has equal status in the sense that there is no central
controllersite.Thesitescontend witheachotherforuseoftheshared medium andthesite
that first gains access during an idle period of the medium uses the medium for the
transmission ofitsown message. Obviously,occasional collisions ofmessages mayoccur
when more than one site senses the medium to be idle and transmits messages at
approximately the same time. The scheme uses collision detection, recovery, and
controlled retransmission mechanisms todeal withthisproblem.Therefore, thescheme is
comprised of the following three mechanisms and works as described next
1. Carrier sense and defer mechanism. Whenever a site wishes to transmit a
packet, it first listens for the presence of a signal (known as a carrier by analogy with
radio broadcasting) on the shared medium. If the medium is found to be free (no carrier
is present on the medium), the site starts transmitting its packet. Otherwise, the site
defers its packet transmission and waits (continues to listen) until the medium becomes
free. The site initiates its packet transmission as soon as it senses that the medium is
free.
2. Collisiondetectionmechanism. Unfortunately,carrier sensingdoes notpreventall
collisions because of the nonzero propagation delay of the shared medium. Obviously,
collisions occur only withinashort timeinterval following thestartoftransmission, since
after this interval all sites will detect that the medium is not free and defer transmission.
This time interval is called the collision window or collision interval and is equal to the
amount of time required for a signal to propagate from one end of the shared medium to
theother andbackagain.Ifasiteattempts totransmit apacket, itmustlisten totheshared

Sec.2.3 • LANTechnologies 53
medium foratime period thatisatleastequal tothecollisioninterval inordertoguarantee
that the packet will not experience a collision.
Collision avoidance by listening to the shared medium for at least the collision
interval time before initiation of packettransmission leads to inefficientutilizationof the
medium when collisions are rare. Therefore, instead of trying to avoid collisions, the
CSMAlCD scheme allows collisions to occur, detects them, and then takes necessary
recovery actions.
A decentralized collision detection mechanism is used when a site transmits its
packet through itsoutputport,italsolistens onitsinputportandcomparesthetwosignals.
A collision is detected when a difference is found in the two signals. On detection of a
collision, the site immediately stops transmitting the remaining data in the packet and
sends a special signal, called ajamming signal, on the shared medium to notify all sites
that a collision has occurred. On seeing thejamming signal, all sites discard the current
packet. The sites whose packets collided retransmit their packets at some later time.
Notethat,forensuringthatallcollisionsaredetected,alowerboundonpacketlengthis
needed. Toillustratethis, let usassume that the maximumpropagationdelay between two
sitesofanetwork ist.Ifthetwositesstarttransmittingtheirpacketsalmostatthesametime,
it will take at least time t for the sites to start receiving the other site's packetdata and to
detect thecollision.Hence ifthepacket sizeissosmall thatittakeslessthantimetforasite
topumpallitsdataonthenetwork, thesiteswillnotdetectthecollisionbecausethetwosites
complete their packet transmission before they see the other site's packet data. However,
anyothersiteonthesamenetwork forwhichthepropagationtimefromthetwositesisless
thantwillreceivescrambleddata ofthepackets ofboththesites.
3. Controlled retransmission mechanism. After a collision, the packets that became
corrupted due tothe collision must beretransmitted. Ifall the transmittingstations whose
packets were corrupted by the collision attempt to retransmit their packets immediately
after the jamming signal, collision will probably occur again. To minimize repeated
collisions and to achieve channel stability under overload conditions, a controlled
retransmissionstrategy isused inwhich thecompetitionfortheshared medium isresolved
using a decentralized algorithm.
Retransmission policies have two conflicting goals: (a) scheduling a retransmission
quickly to get the packet out and maintain use ofthe shared medium and (b) voluntarily
backing offtoreduce thesite'sloadonabusy medium. Aretransmissionalgorithmisused
to calculate the delay before a site should retransmit its packet. After a collision takes
place, the objective is to obtain delay periods that will reschedule each site at times
quantized in steps at least as large as a collision interval. This time quantization is called
the retransmission slot time. Toguarantee quick use of the medium, this slot time should
beshort; yet toavoid collisionsitshould be larger than acollisioninterval. Therefore, the
slot time is usually set to be a little longer than the round-trip time of the medium. The
real-timedelay isthe productof some retransmission delay D(apositive integer) and the
retransmission slot time (St).
A good example of the controlled retransmission mechanism is the binary
exponential back-offalgorithm used in Ethernet. This algorithm is described later in this
chapter during the description of Ethernet (a case study ofLAN technology).

54 Chap. 2 • Computer Networks
The CSMNCD scheme works best on networks having a bus topology with
bursty asynchronous transmissions. It has gained favor for transmission media that
have relatively low speeds (around 10 Mbps) mainly because of its ease of
implementation and its channel utilization efficiency. Notice that the performance of
the CSMAlCD scheme depends on the ratio of packet length to propagation delay. The
higher the ratio, the better the performance because the propagation delay is the
interval during which a packet is vulnerable to collision. After that interval, all sites
will have "heard" the transmission and deferred. As this ratio diminishes, the collision
frequency increases, causing significant performance degradation. Because this per
formance becomes more pronounced when the ratio of packet length to propagation
delay is too low, CSMAlCD is unsuitable for high data rates and/or long distances.
For instance, for a transmission medium having photonic speeds (hundreds of
megabits per second or gigabits per second) the efficiency of CSMA/CD is often
unacceptable.
In addition to being unsuitable for systems having high data rates, small-size
packets, and long cable lengths, the CSMNCD scheme has the following drawbacks:
(a) It does not possess a bounded-delay property. Because the loading.of the shared
medium is variable, it is impossible to guarantee the delivery of a given message within
any fixed time, since the network might befully loaded when the message is ready for
transmission. (b) It is not possible to provide priorities for the use of the shared
transmission medium. Since all sites are equal, none have priority over others, even
though some sites may require greater use of the facilities due to the nature of a
particular application.
The Token Ring Protocol
This scheme also employs decentralized control of the shared medium. In this scheme,
access to the shared medium is controlled by using a single token that is circulated
among the sites in the system. A token is a special type of message (having a unique
bit pattern) that entitles its holder to use the shared medium for transmitting its
messages. A special field in the token indicates whether it is free or busy. The token
is passed from one site to the adjacent site around the ring in one direction. A site that
has a message ready for transmission must wait until the token reaches it and it is free.
When it receives the free token, it sets it to busy, attaches its message to the token,
and transmits it to the next site in the ring. A receiving site checks the status of the
token. If it is free, it uses it to transmit its own message. Otherwise, it checks to see
if the message attached to the busy token is addressed to it. If it is, it retrieves the
message attached to the token and forwards the token without the attached message to
the next site in the ring. When the busy token returns to the sending site after one
complete round, the site removes it from the ring, generates a new free token, and
passes it to the next site, allowing the next site to transmit its message (if it has any).
The free token circulates from one site to another until it reaches a site that has some
message to transmit. To prevent a site from holding the token for a very long time, a
token-holding timer is used to control the length of time for which a site may occupy
the token.

Sec.2.3 • LANTechnologies 55
To guarantee reliable operation, the token has to be protected against loss or
duplication. That is, if the token gets lost due to a site failure, the system must detect
the loss and generate a new token. This is usually done by the monitor site. Moreover,
if a site i crashes, the ring must be reconfigured so that site i-I will send the token
directly to site i+1.
An advantage of the token ring protocol is that the message delay can be bounded
becauseoftheabsenceofcollisions.Anotheradvantageisthat itcan work withbothlarge
and small packet size as well as variable-sizepackets. In principle, amessage attached to
the token may be of almost any length. A major disadvantage, however, is the initial
waiting time toreceive afree token even atverylight loads.This initial waitingtimecould
be appreciable, especially in large rings.
A variant of the token ring protocol described above is the IEEE 802.5 standard
Token Ring protocol [IEEE 1985c].This protocol isdescribed later inthischapterduring
the description ofIEEE Token Ring (a case study of LAN technology).
The Slotted-Ring Protocol
In this scheme, a constant number of fixed-length message slots continuously circulate
around the ring. Each slot has two parts-control and data. The control part usually has
fields to specify whether the slot is full or empty, the source and destination addresses of
the message contained in a full slot, and whether the message in it was successfully
received at the destination. On the other hand, the data part can contain a fixed-length
message data.
Asitethat wants tosendamessage firstbreaks down themessage intopackets ofsize
equal tothe size ofthe data part of the slots. It then waits forthe arrival of anempty slot.
As soon as an empty slot arrives, it grabs it, places one packet ofits message in its data
part, sets the source and destination addresses properly, sets the full/empty field to full,
and puts it back on the ring. This slot then circulates on the ring and the site again waits
for the arrival ofanotherempty slot. The site continues doing this until ithas transmitted
all the packets ofthe message.
Eachsiteinspects every fullslottocheck ifit containsapacket addressedtoit.Ifnot,
it simply forwards the slot to the next site on the ring. Otherwise, it removes thepacket
from the slot, properly alters the field in the slot showing that the message inthe slot was
successfully received at the destination, and then forwards the slot to the next site in the
ring. When the slot returns back to its senderafter completing itsjourney round the ring,
the sender changes its full/empty field to empty, making it available for transmission of
any other message in the system.
This scheme prevents hogging of the ring and guarantees fair sharing of the
bandwidth of the shared medium among all sites. It also allows messages from multiple
sites tobesimultaneouslytransmitted(inthe token ringscheme, onlyone sitecan transmit
at a time). However, itrequires a long message to be broken into smaller packets (this is
not required in the token ring scheme).
The slotted-ring scheme is used in the Cambridge Ring [Wilkes and Wheeler 1979],
which wasdevelopedatCambridgeUniversity inthe 1970sand is widely used inBritain.
Further details ofthis protocol can be found in [King and Mitrani 1987].

56 Chap. 2 • Computer Networks
2.3.3 LAN T.chnology (ase Studl.s: Ethernet and IEEE
Tok.n Ring
Ethernet
Ethernet is the most widely used LAN technology for building distributed systems
because itisrelatively fastandeconomical. Itwasintroduced byDEC(Digital Equipment
Corporation), Intel, and Xerox in 1980, and subsequently, a slightly modified version of
it was adopted by the IEEE as a standard LAN technology, known as the IEEE 802.3
standard [IEEE 1985a].
The network topology usedforEthernet isasimple multiaccess busoramultiaccess
branching bus topology. The communication medium used is low-loss coaxial cable
having a data transfer rate of 10Mbps.
A message is transmitted from one site to another by breaking it up into packets
(called/rames in Ethernet) and then by broadcasting the packets to the bus. An address
designatorisassociated witheachpacket.Asapacket travelson the bus,each sitechecks
whether the packet is addressed to it and the addressee site picks up the message.
For medium-access control, Ethernet uses·the CSMNCD protocol with a binary
exponentialback-offalgorithmas thecontrolled retransmission mechanism. In the binary
exponential back-off algorithm of Ethernet, the value of retransmission delay (D) is
selected as a random number from a particularretransmission interval between zero and
some upper limit (L). That is, ifthe packets of two sites collide, one will retransmit after
aninterval ofX X SI(SIistheretransmission slottime) andtheother willretransmit after
an interval of YX St, where Xand Yare likely to be different since they were chosen
randomly. Tocontrol the shared medium and keep it stable under high load, the value of
L is doubled with each successive collision, thus extending the range for the random
selection of the value of D. In Ethernet, on first collision, the value of D is randomly
chosen to be0 or 1;on the secondcollision, itisrandomly chosen tobe0, 1,2,or 3°;and
on the ith successive collision, it is randomly chosen to be an integer in the range and
2;- 1(bothinclusive).Thus,themoreoftenthatasenderfails(duetorepeated collisions),
the longer potential period of time it will defer before attempting to retransmit. This
algorithm has very short retransmission delays at the beginning but will backoffquickly,
preventing the medium from becoming overloaded.
Notice that after some number of back-offs, the retransmission interval becomes
large.Toavoid undue delays and slow response to improved medium characteristics, the
doubling of the retransmission interval is usually stopped at some point, with additional
retransmissions still being drawn from this interval, before the transmission is finally
aborted. This is referred to as the truncatedbinary exponentialback-offalgorithm.
The structure of a packet in Ethernet is shown in Figure 2.4. The destination
and source addresses occupy 6 bytes each. The destination address may be a single
site address (specifies a single site), a multicast address (specifies a group of sites),
or a broadcast address (specifies all sites). The address consisting of all l's is the
broadcast address. The sites that belong to a multicast address have their network
interfaces configured to receive all packets addressed to the multicast address.
Moreover, to distinguish multicast addresses from single-site addresses, the higher

Sec.2.3 • LAN Technologies 57
6bytes 6bytes 2bytes 46bytess lengths1500bytes 4bytes
Destination Source
Type Messagedata Checksum
address address
Fig.2.4 Structure ofapacketinEthernet.
order bit of a multicast address is always 1, whereas this bit of a single-site address
is always O.
The type field is used to distinguish among multiple types ofpackets. This field is
used by higher layer communication protocols (communication protocol layers are
described later in this chapter).
The message data field is the only field in the packet structure that may have
variable length. It contains the actual message data to be transmitted. The length of
this field may vary from 46 to 1500 bytes. The minimum length of 46 bytes for this
field is necessary to ensure that collisions are always detected in the CSMAlCD
scheme used for medium-access control. On the other hand, the maximum length of
1500 bytes for this field is used to allow each site in the network to allocate buffer
space for the largest possible incoming packet and to avoid a sender site waiting for
a long time for the communication channel to become free. Since there is no field to
indicate the length of the message data field, an interval of 9.6J..ls is used between
the transmission of two packets to allow receiving sites to detect the end of
transmission of a packet.
The last 4bytes ofapacket always contain achecksum generated by the senderand
used by the receiver(s) to check the validity of the received packet. A receiver simply
drops a packet that contains an incorrect checksum. Due to this, message delivery is not
guaranteed in Ethernet. This is the reason why simple datagram protocols used in local
networks are potentially unreliable. If guaranteed message delivery is needed, the upper
layer ofthe communication protocol (communication protocol layers are described later
in this chapter) must use acknowledgments for the receipt ofeach packet and timeout
based retransmissions for unacknowledged packets.
Every Ethernethardware interface is assigned aunique address by the manufacturer.
This allows all the sites of a set of interconnected Ethernets to have unique addresses.
IEEE acts as an allocation authority for Ethernet addresses. A separate range of 48-bit
addresses is allocated to each manufacturer of Ethernet hardware interfaces.
IEEE Token Ring
Another commonly used LAN technology for building distributed systems is the IEEE
Token Ring technology, known as the IEEE 802.5 standard [IEEE 1985b]. IBM has
adopted this technology as a basis for its distributed system products.
The networktopology used inthe IEEETokenRing technology isring topology, and
the medium-accesscontrol protocol used is the tokenring protocol. Initially itoperatedat

58 Chap. 2 • Computer Networks
a speed of 4Mbps but was later upgraded to 16Mbps. It can use a cheap twisted pair or
optical fiberasthecommunication mediumandhasalmost nowastedbandwidth when all
sites are trying to send.
A single token of 3bytes keeps circulatingcontinuously around the ring.The token
may either be busy or free.A site willing to send a message attaches its message to the
token andchanges itsstatustobusy,whenthecirculating token arrives atthesender'ssite
with free status. Therefore, a busy token has a message packet attached to it whose
structure is shown in Figure 2.5.
3bytes 6bytes 6bytes lengths5000bytes 4bytes
Token
Destination Source Messagedata Checksum
I I
address address
! 1
Packetcontrol information(1byte) Packet
status
Priorityandstatusinformation(1byte) (1byte)
,
Startingdelimeter(1byte) End
delimiter
(1byte)
Fig.2.5 StructureofapacketinIEEEToken Ring.
The first byte of thetoken contains afixed bitpattern that enables sites torecognize
the start of a packet and synchronize to the data transmission rate. The second byte
contains priority and token status (free/busy) information. The third byte contains packet
control information. The priority fieldcan be used toimplement a variety of methods for
sharing the channel capacity among the sites on the network.
In addition to the token, a packet in the IEEE Token Ring has fields for source
and destination addresses (each 6 bytes long), message data (length ~ 5000 bytes),
checksum (4 bytes), end delimiter (1 byte), and packet status (1 byte). The source
address and destination address fields respectively contain the addresses of the
sending and receiving sites. The message data field contains the data to be
transmitted. This field is of variable length and allows packets to be of almost any
length. The upper bound of 5000 bytes for the length of this field is a default value
for a parameter that can be configured on a per-installation basis. The checksum
field contains a checksum generated by the sender and used by the receiver to check
the validity of the received packet. The end-delimiter field contains a fixed bit
pattern that enables sites to recognize the end of a packet. Finally, the packet status
field specifies whether the packet was successfully received by the receiving site.
This field helps the sending site in knowing whether its packet was received by the
receiver. The sending site is responsible for removing its packet from the ring when
it returns after one rotation.

Sec.2.4 • WAN Technologies S9
2.4 WAN TECHNOLOGIES
A WANof computers is constructed by interconnecting computers that are separated by
large distances; they may be located in different cities or even in different countries. In
general, no fixed regular network topology is used for interconnecting the computers of
a WAN. Moreover, different communication media may be used for different links of a
WAN. For example, in a WAN, computers located in the same country may be
interconnected by coaxial cables (telephone lines), but communications satellites may be
used to interconnect two computers that are located in different countries.
The computers of aWANare notconnected directly to thecommunication channels
but are connected to hardware devices canedpacket-switching exchanges (PSEs), which
arespecial-purposecomputersdedicatedtothetaskofdata communication.Therefore, the
communication channels of the network interconnect the PSEs, which actually perform
the task ofdata communication across thenetwork (Fig. 2.6). AcomputerofaWANonly
interacts with the PSE of the WAN to which it is connected for sending and receiving
messages from other computers on the WAN.
•"ig.2.6 AWANofcomputers.
Tosend amessagepacketto another computeron the network, acomputersends the
packet to the PSE to which it is connected. The packet is transmitted from the sending
computer's PSE to the receiving computer's PSE, possibly via other PSEs. The actual
mode of packet transmission and the route used for forwarding apacket from its sending
computer's PSE to its receiving computer's PSE depend on the switching and routing

60 Chap.2 • ComputerNetworks
techniques usedbythePSEsofthenetwork. Variouspossible options forthesetechniques
aredescribed next.When the packet reaches itsreceiving computer'sPSE, it isdelivered
to the receiving computer.
1.4.1 Switching T.chnlqu8s
We saw that in a WAN communication is achieved by transmitting a packet from its
sourcecomputertoitsdestination computerthroughtwoormorePSEs.The PSEs provide
switching facility to move a packet from one PSE to another until the packet reaches its
destination. That is, a PSE removes a packet from an input channel and places it on an
output channel. Network latency is highly dependent on the switching technique used by
the PSEs of the WAN.The two most commonly used schemes are circuit switching and
packet switching. They are described next.
Circuit Switching
In this method, before data transmission starts,a physical circuit is constructed between
the sender and receiver computers during the circuit establishment phase. During this
phase, the channels constituting the circuit are reserved exclusively for the circuit; hence
there is no need for buffers at the intermediate PSEs. Once the circuit is established, all
packets ofthe data are transferred in the data transfer phase. Since all the packets of a
message dataaretransmitted oneafteranother through thededicated circuit withoutbeing
buffered at intermediate sites, the packets appear to form a continuous data stream.
Finally, in thecircuit termination phase, thecircuit is tom down as the last packet of the
dataistransmitted. Assoonasthecircuit istorndown, thechannels thatwerereserved for
the circuit become available for use by others. If a circuit cannot be established because
adesired channel isbusy (being used),thecircuit issaid tobeblocked. Depending onthe
wayblocked circuits arehandled, thepartialcircuit maybetom down, withestablishment
to be attempted later.
This scheme is similar to that·used in the public telephone system. In this system,
when a telephone call is made, a dedicated circuit is established by the telephone
switching office from the caller's telephone to the callee's telephone. Once this circuit is
established, the only delay involved in the communication is the time required for the
propagation of the electromagnetic signal through all the wires and switches. While it
might be hard to obtain a circuit sometimes (such as calling long distance on Christmas
Day), once the circuit is established, exclusive access to it is guaranteed until the call is
terminated.
The main advantage of a circuit-switching technique is that once the circuit is
established, data is transmitted with no delay other than the propagation delay, which is
negligible. Furthermore,sincethefullcapacity ofthecircuit isavailable forexclusive use
bytheconnectedpairofcomputers, thetransmission timerequired tosend amessage can
beknown andguaranteed afterthecircuit hasbeensuccessfully established. However,the
method requires additional overhead during circuit establishment and circuit disconnec
tion phases, and channel bandwidths may be wasted if the channel capacities of the path
forming the circuit are not utilized efficiently by the connected pair of computers.

Sec.2.4 • WAN Technologies 61
Therefore, the method isconsideredsuitable onlyforlongcontinuoustransmissionsorfor
transmissions that require guaranteed maximum transmission delay. It is the preferred
method for transmission of voice and real-time data in distributed applications.
Circuit switching is used in the Public Switched Telephone Network (PSTN).
Packet Switching
Inthis method, instead ofestablishing adedicated communication path between a sender
and receiver pair (of computers), the channels are shared for transmitting packets of
different sender-receiver pairs. That is, a channel is occupied by a sender-receiver pair
only while transmitting a single packet of the message of that pair; thechannel may then
beusedfortransmittingeither another packet ofthesame sender-receiverpairorapacket
of some other sender-receiver pair.
In this method, each packet of a message contains the address of the destination
computer, sothatitcan besenttoitsdestination independentlyofallotherpackets. Notice
that different packets of the same message may take adifferent path through the network
and, at the destination computer, the receiver may get the packets in an order different
fromtheorder inwhichtheyweresent.Therefore, atthedestinationcomputer, thepackets
have tobeproperly reassembledintoamessage. When apacket reaches aPSE, thepacket
is temporarily stored there in a packet buffer.The packet is then forwarded to a selected
neighboring PSE when the next channel becomes available and the neighboring PSE has
an available packet buffer. Hence the actual path taken by a packet to its destination is
dynamic because the path is established as the packet travels along. Packet-switching
technique is also known as store-and-forward communication because every packet is
temporarily stored by each PSE along its route before it is forwarded to another PSE.
Ascomparedto circuit switching, packet switching issuitable for transmitting small
amounts of data that are bursty in nature. The method allows efficient usage of channels
because the communication bandwidth of a channel is shared for transmitting several
messages. Furthermore, the dynamic selection of the actual path to be taken by a packet
gives the network considerable reliability because failed PSEs orchannels can beignored
andalternate paths maybeused.Forexample, intheWANofFigure 2.6,ifchannel 2fails,
a message from computerA to D can still be sent by using the path 1-3. However, due
to the need to buffer each packet at every PSE and to reassemble the packets at the
destination computer, the overhead incurred per packet is large. Therefore, the method is
inefficient for transmitting large messages. Anotherdrawback of the method is that there
is no guarantee of how long it takes a message to go from its source computer to its
destinationcomputerbecause the time taken for each packet depends onthe route chosen
for that packet, along with the volume of data being transferred along that route.
Packet switching is used in the X.25 public packet network and the Internet.
2.4.2 Routing Techniques
In a WAN, when multiple paths exist between the source and destination computers
of a packet, anyone of the paths may be used to transfer the packet. For example,
in the WAN of Figure 2.6, there arc two paths between computers E and F: 3-4 and

62 Chao. 2 • Computer Networks
1-2-4-and anyone of the two may be used to transmit a packet from computer E
to F. The selection of the actual path to be used for transmitting a packet is
determined by the routing technique used. An efficient routing technique is crucial to
the overall performance of the network. This requires that the routing decision
process must be as fast as possible to reduce the network latency. A good routing
algorithm should be easily implementable in hardware. Furthermore, the decision
process usually should" not require global state information of the network because
such information gathering is a difficult task and creates additional traffic in the
network. Routing algorithms are usually classified based on the following three
attributes:
• Place where routing decisions are made
• Time constant ofthe information upon which the routing decisions are based
• Control mechanism used for dynamic routing
Note that routing techniques are not needed in LANs because the sender of a message
simply puts the messageonthecommunication channel andthereceiver takes itofffrom
the channel. There is no need to decide the path to be used for transmitting the message
from the sender to the receiver.
Place Where Routing DecisionsAre Made
Based on this attribute, routing algorithms may be classified into the following three
types:
1. Source routing. In this method, the source computer's PSE selects the entire
path before sending the packet. That is, all intermediate PSEs via which the packet
will be transferred to its destination are decided at the source computer's PSE of
the packet, and this routing information is included along with the packet. The
method requires that the source computer's PSE must have fairly comprehensive
information about the network environment. However, the routing decision process
is efficient because the intermediate PSEs need not make any routing decision. A
drawback of the method is that the path cannot be changed after the packet has
left the source computer's PSE, rendering the method susceptible to component
failures.
2. Hop-by-hop routing. In this method, each PSE along the path decides only the
next PSE for the path.That is,each PSE maintains information about the status of all its
outgoing channels and theadjacent PSEs and thenselects asuitable adjacent PSE forthe
packet and transmits it to that PSE. The routing decisions are typically based on the
channelavailability andthereadinessoftheadjacentPSEstoreceiveandrelaythepacket.
The method requires thateach PSE mustmaintainarouting tableof somesort. However,
ascomparedtothestaticroutingmethod,thismethodmakesmoreefficientuseofnetwork
bandwidth and provides resilience to failures because alternative paths can be used for
packet transmissions.

Sec.2.4 • WAN Technologies 63
3. Hybridrouting. This methodcombines the first two methods in the sense that the
source computer's PSE specifies only certain major intermediate PSEs of the complete
path, and the subpaths between any two of the specifiedPSEs are decided by the method
of hop-by-hop routing.
Static and Dynamic Routing
Depending on when the information used for making routing decisions is specified and
how frequently it is modified, routing algorithms are classified into the following two
types:
1. Static routing. In this method, routing tables (stored at PSEs) are set once and do
not change for very long periods of time. They are changed only when the network
undergoes major modifications. Static routing is also known as fixed or deterministic
routing. Static routing is simple and easy to implement. However, it makes poor use of
network bandwidth and causes blocking of a packet even when alternative paths are
available for its transmission. Hence, static routing schemes are susceptibletocomponent
failures.
2. Dynamic routing. In this method, routing tables are updated relatively frequently,
reflecting shorter term changes in the network environment. Dynamic routing strategy is
also known as adaptive routing because it has a tendency to adapt to the dynamically
changing state of the network, such as the presence of faulty or congested channels.
Dynamicroutingschemescan usealternativepaths forpacket transmissions, making more
efficientuseofnetworkbandwidthand providingresiliencetofailures. The latter property
is particularly important for large-scale architectures, since expanding network size can
increase theprobabilityofencounteringa faulty networkcomponent. Indynamicrouting,
however, packets ofa message may arrive out of order at the destination computer. This
problem can be solved by appending a sequence number to each packet and properly
reassembling the packets at the destination computer.
The path selection policy for dynamic routing may either be minimal or
nonminimal. In the minimal policy, the selected path isone of the shortestpaths between
the source and destination pair ofcomputers. Therefore, every channel visited will bring
the packet closerto the destination. On the otherhand, inthe nonminimal policy, apacket
may follow a longer path, usually in response to current network conditions. If the
nonminirnalpolicyisused, care must betaken toavoid asituationinwhich thepacketwill
continue to be routed through the network but never reach the destination.
Control Mechanisms for Dynamic Routing
In the dynamic routing strategy, routing tables are constantly updated. One of the
following three approaches may be used for controlling the update action:
1. Isolatedmanner. Inthis approach, individual PSEs update theinformation intheir
local routing table inanisolatedmanner, perhapsbyperiodicallytrying various routes and
observing performance.

64 Chap. 2 • Computer Networks
2. Centralized manner. In this method, changes in the network environment,
connectivity,orperformanceareconstantlyreported toonecentralizedPSE. Based onthe
information received, the global routing table maintained at the centralized PSE is
constantly. updated. The updated routing table information is periodically sent from the
centralized PSE to the source PSEs (in the source-routing strategy) or to all PSEs (in the
hop-by-hop routing strategy).
The main advantages of the centralized approach are that the routes are globally
optimal and other PSEs are not involved in the information gathering of the global
network status. However, the centralized approach suffers from poor performance in
situationswhere thesystemislargeorwheretrafficchanges arefrequent. Also, ithaspoor
reliability since the table construction is performed at a single PSE.
3. Decentralized manner. To overcome the shortcomings of the centralized
approach, several systems use the distributed control mechanism. In this method, each
PSE maintains a routing table and the routing table updates are performed by mutual
interactionamong thePSEs. Often aPSE piggybackstherouting table update information
along with some other message being sent to another PSE.
2.5 COMMUNICATION PROTOCOLS
In the last several sections of this chapter we saw that several types of agreements are
needed between the communicating parties in a computer network. For example, it is
importantthatthe sender andreceiver ofapacket agree uponthepositions andsizesofthe
various fields in the packet header, the position and size of actual data in the packet, the
position and size of the checksum field, the method to calculate the checksum for error
detectionand soon. For transmission of message data comprisedof multiple packets, the
sender and receivermust also agree upon the method used for identifyingthe first packet
and the last packet of the message, and since the last packet may only be partially filled,
a method is needed to identify the last bit of the message in this packet. Moreover,
agreementisalso needed forhandling duplicatemessages, avoiding bufferoverflows, and
assuring proper message sequencing. All such agreements, needed for communication
between the communicating parties, are defined in terms of rules and conventions by
network designers. The term protocol is used to refer to a set of such rules and
conventions.
Computer networks are implemented using the concept of layered protocols.
Accordingtothisconcept, theprotocolsofanetworkareorganized intoaseriesoflayersin
suchawaythateachlayercontainsprotocolsforexchangingdataandprovidingfunctionsin
a logical sense with peer entities at other sites in the network. Entities in adjacent layers
interactinaphysicalsensethroughthecommoninterfacedefinedbetweenthetwolayersby
passing parameters such as headers, trailers, and data parameters. The main reasons for
usingtheconceptoflayeredprotocols innetwork design areasfollows:
• The protocols of a network are fairly complex. Designing them in layers makes
their implementation more manageable.

Sec.2.5 • Communication Protocols 6S
• Layeringofprotocols provides well-defined interfaces betweenthe layers, so that
a change in one layer does not affect an adjacent layer. That is, the various
functionalities canbe partitionedand implemented independentlyso that each one
can bechangedas technology improves withoutthe otheronesbeingaffected. For
example,achangeto arouting algorithmin anetworkcontrolprogramshouldnot
affectthe functionsofmessagesequencing,whichislocatedinanotherlayerofthe
network architecture.
• Layeringofprotocolsalso allowsinteractionbetweenfunctionally pairedlayersin
different locations. This concept aids in permitting the distributionoffunctions to
remote sites.
The terms protocol suite, protocolfamily, or protocol stack are used to refer to the
collection of protocols (ofall layers) ofa particular network system.
2.5.1 Protocols for Network Systems
InChapter 1wesaw thatdistributedsystemsare basicallydifferentfrom networksystems.
Therefore,the requirementsofcommunicationprotocolsofthese two types ofsystemsare
also different. The basic goal ofcommunicationprotocols for network systems isto allow
remote computers to communicate with each other and to allow users to access remote
resources. On the other hand, the basic goal ofcommunication protocols for distributed
systems isnot only to allow users to access remote resources but to do so in a transparent
manner.
Several standardsand protocols for network systemsare already available. However,
protocols for distributed systems are still in their infancy and no standards are yet
available. Some standard network protocol models are described next. The protocols for
distributed systems arc presented in the next section.
The ISO/OSI Reference Model
The number of layers, the name of each layer, and the functions of each layer may be
different for different networks. However, to make thejobofthe networkcommunication
protocol designers easier, the International Standardization Organization (ISO) has
developed areference model that identifies seven standard layers and defines thejobs to
beperformedateach layer. This model iscalledthe Open System InternationalReference
Model(abbreviatedOSlmodel) [DePryckeretaJ. 1993, Larmouth 1993, Stallings 1993c].
It is a guide, not a specification. It provides a framework in which standards can be
developed for the services and protocols at each layer. Note that adherence to standard
protocols is important for designing open distributed systems. This is because ifstandard
protocolsare used, separatesoftwarecomponentsofdistributedsystemscan bedeveloped
independently on computers having different architectures (different code ordering and
data representations). To provide an understanding of the structure and functioning of
layered network protocols, a briefdescription ofthe OSI model is presented next. There
are many sources for more detail on this model [Tanenbaum 1988, Stallings 1993c,
Larmouth 1993J.

66 Chap. 2 • Computer Networks
The architecture of the OSI model is shown in Figure 2.7. It is a seven-layer
architecture inwhichaseparatesetofprotocols isdefinedforeachlayer.Thuseach layer
has an independent function and deals with one or more specific aspects of the
communication. The roles of the seven layers are briefly described below.
Site1 Site2
-_
Applicationprotocol ..........
.._--~.-_.~-~_
t--__P_re_s_e~~,protocol
Data-linkprotocol
Physicalprotocol ._.--- - .I'"
14-+-~'---'---'
Network
Fig.2.7 Layers.interfaces,andprotocolsintheOSImodel.
PhysicalLayer: Thephysicallayerisresponsible fortransmittingrawbitstreams
between two sites. That is, it may convert the sequence of binary digits into electric
signals,lightsignals,orelectromagnetic signalsdependingonwhetherthetwositesareon
a cable circuit, fiber-optic circuit, or microwave/radio circuit, respectively. Electrical
details suchashow manyvolts tousefor0 and 1,how manybitscan besentpersecond,
and whether transmission can take place only in one direction or in both directions

Sec.2.5 • Communication Protocols 67
simultaneously are also decided by the physical layer protocols. In addition, the physical
layer protocols also deal with the mechanical details such as the size and shape of the
connecting plugs, the numberof pins in the plugs, and the function ofeach pin. In short,
the physical layer protocols deal with the mechanical, electrical, procedural, and
functional characteristics of transmissionof raw bit streams between two sites. RS232-C
is a popular physical layer standard for serial communication lines.
Data-LinkLayer. The physical layer simply transmits thedata from the sender's
site to the receiver's site as raw bits. It is the responsibility of the data..link layer to
detect and correct any errors in the transmitted data. Since the physical layer is only
concerned with a raw bit stream, the data-link layer partitions it into frames so that
error detection and correction can be performed independently for each frame. The data
link layer also performs flow control of frames between two sites to ensure that a
sender does not overwhelm a receiver by sending frames at a rate faster than the
receiver can process. Therefore, the error control and flow control mechanisms of a
network form the data-link layer protocols in the OSI model. Notice that the data-link
layer and physical layer protocols establish an error-free communication of raw bits
between two sites.
Network Layer. The network layer is responsible for setting up a logical path
between two sites for communication to take place. It encapsulates frames into packets
that can betransmitted from one site toanother using a high-level addressingand routing
scheme. That is, routing isthe primary jobof the network layer and therouting algorithm
forms the main part of the network layer protocols of the network.
Twopopularnetwork layer protocols aretheX.25 Protocoland theInternet Protocol
(called IP). The X.25 is a connection-orientedprotocol that is based on the concept of
establishing a virtual circuit between the sender and receiver before the actual
communication starts between them. In thisprotocol, arequest forconnectionisfirst sent
to the destination, which can eitherbe accepted or rejected. Iftheconnectionisaccepted,
therequesting party isgiven aconnection identifierto use insubsequent requests. During
the connection establishment phase, a route between the two parties is also decided that
is used for the transmission of subsequent traffic.
On the other hand, IP is a connectionless protocol in which no connection is
established between the sender and receiver before sending a message. Therefore, each
packet of the message is transmitted independently and may take a different route. IP is
part ofthe DoD (U.S. Department ofDefense) protocol suite.
Notice that the functions performed at the network layer are primarily required in
WANs. In a single LAN, the network layer is largely redundant because packets can be
transmitteddirectly from any site on the networkto any other site.Thereforethe network
layer, if present, has little work to do.
Transport Layer. The job of the transport layer is to provide site-to-site
communication and to hide all the details ofthe communication subnet from the session
layer by providing a network-independent transport service. Using this service, all the
details of the communication subnet are sealed and one subnet can be replaced with
another without disturbing the layers above the transport layer.

68 Chap. 2 • Computer Networks
Inparticular,the transportlayeracceptsmessagesofarbitrarylength from thesession
layer, segmentsthem intopackets,submitsthem tothenetworklayer fortransmission,and
finally reassembles the packets at the destination. Some packets may be lost on the way
from the sender to the receiver, and depending on the routing algorithms used in the
networklayer, packetsmay arriveatthedestinationinasequencethat isdifferentfrom the
order in which they are sent. .The transport layer protocols include mechanisms for
handlinglost andout-of-sequencepackets. Forthis, the transportlayerrecordsasequence
numberin each packet and uses the sequence numbers for detecting lost packets and for
ensuring that messages are reconstructed in the correct sequence.
The ISO model provides five classes oftransport protocols (known as TPO through
TP4) whichbasicallydifferin theirability to handle errors. TPOis the least powerful one
and TP4 is the most powerful one. The choice of which one to use depends on the
properties ofthe.underlying network layer.
The two most popular transport layer protocols are the Transport Control Protocol
(TCP) and the User Datagram Protocol (UDP). Both are implemented in the DARPA
protocol suite of DARPA Internet. TCP is a connection-oriented transport protocol that
provides the same services as TP4 ofthe ISO model. It uses end-to-end mechanisms to
ensure reliable, ordered delivery of data over a logical connection. These goals are
basicallyachievedbyusing packetsequencenumbersand positiveacknowledgmentswith
timeout and retransmission.
The UDPis aconnectionless transportprotocol. It is an unreliableprotocolbecause,
when itis used, messagepacketscan be lost, duplicated,or arrive out oforder.Therefore,
only those applications that do not need reliable communication should use UDP.
Session Layer. The purposeofthe sessionlayer istoprovidethe means by which
presentation entities can organize and synchronize their dialog and manage their data
exchange. It allows the two partiesto authenticateeach otherbeforeestablishing adialog
session betweenthem. It also specifies dialog type-one way, two way alternate, or two
way simultaneous-and initiates a dialog session if the message is a connection request
message.Theotherservicesofthesessionlayer includequarantineservice, dialogcontrol,
and priority management. The quarantine service buffers a group of messages on the
receiving side until the session layeron the sending side explicitly releases them. This is
useful in database applications where a transaction (consisting ofa group of messages)
needs to be an atomic unit. The dialog control is useful for dialog sessions in which the
user primitives used for sending and receiving messages are of the nonblocking type. In
thiscase,the user may have multiplerequestsoutstandingonthesame session,andreplies
may comeback inanorderdifferentfrom that in which therequestswere sent. The dialog
control reorders replies according to the order of requests. The priority management
service is useful for giving priority to important and time-bound messages over normal,
less-important messages. The session layer is not required for connectionless
communication.
Presentation Layer. The purpose of this layer is to represent message
informationto communicating application layerentities in a way that preserves meaning
while resolving syntax differences. For this, the presentation layer may perform one or
more ofthe following types oftransformations on message data:

Sec. 2.5 • Communication Protocols 69
• A message usually contains structured information that may include any of the
data types used in programming languages-integers, characters, arrays, records,
and so on, including user-defined data types. Translation is therefore required
where language systems or application programs in t.he source and destination
computers use different representations for these data types.
• Data format conversionsare also needed totransfer data between computers when
the hardware of the sending and receiving computers uses different data
representations. In this case, the presentation layer software in the sending
computer transforms message data from the formats used in its own computerto
a set of standard network representations called eXternal Data Representation
(XDR) before transmission. The presentation layer software in the receiving
computer transforms the message data from the network representations to the
formats used in its own computer.
• For applications dealing with confidential or secret data, the presentation layer
software in the sending computer encrypts message data before passing it to the
session layer.On thereceiverside, theencryptedmessage data isdecryptedbythe
presentation layer before being passed on to the application layer.
• In a similar manner, when message data is large in volume (such as multimedia
data) or with networks that are slow or heavily loaded, message data may be
compressed and decompressed by the presentation layer software in the sending
and receiving computers, respectively.
Application Layer. The application layer provides services that directly support
theend users ofthe network. Obviously, the functionality implementedatthis layer ofthe
architecture is application-specific. Since each application has different communication
needs, no fixed or standard set of application layer protocols can meet the needs of all
applications. Therefore, the application layer is basically a collection of miscellaneous
protocols for various commonly used applications such as electronic mail, file transfer,
remote login, remote job entry, and schemas for distributed databases. Some popular
application layer protocols are X.400 (Electronic Mail Protocol), X.SOD(Directory Server
Protocol), FTP (File Transfer Protocol), and rlogin (Remote Login Protocol).
In actual implementation, of the seven layers, the first three layers are likely to be in
hardware, the next two layers in the operating system, the presentation layer in library
subroutines in the user's address space, and the application layer in the user's program.
Example ofMessage Transfer in the OSI Model. To illustrate the functions
of the various layers of the OSI model, let us consider a simple example of message
transmission. With reference to Figure 2.8, let us assume that a process at the sending
site wants to send a message M to a process at the receiving site. The sending site's
process builds the message M and passes it to the application layer (layer 7) on its
machine. The application layer software adds a header (H 7) to M and passes the
resulting message to the presentation layer (6) via the interface between layers 7 and
6. The presentation layer software performs text compression, code conversion, security
encryption, and so 011, on the received message, and after adding a header (H 6) to it,

70 Chap. 2 • ComputerNetworks
Sendingsite Receivingsite
Network
Fig.2.8 Anexampleillustrating transferofmessage Mfromsendingsitetoreceiving
siteintheOSImodel:H",headeradded bylayern; T",traileradded by
layern.
it passes the resulting message on to the session layer (5). Depending on the type of
dialog, the session layer software establishes a dialog between the sender and the
receiver processes. It also regulates the direction of message flow. A header (H5) is
added to the message at this layer, and the resulting message is passed on to the
transport layer (4). The transport layer software now splits the message into smaller
units (M. and M 2) called packets and adds a header (H 4) to each packet. These headers
contain the sequence numbers of the message packets. The packets are then passed on
to the network layer (3). The network layer software makes routing decisions for the
received packets and sets up a logical path between the sending and receiving sites for
transmission of the packets. It then adds a header (H 3) to each packet and passes them
on to the data-link layer (2). The data-link layer software adds a header (Hz) and a
trailer (T 2) to each of these packets. The trailers contain the checksum of the data in
the corresponding packets. The resulting message units are called frames, which are
passed on to the physical layer (1). The physical layer software simply transmits the
raw bits from the sender's machine to the receiver's machine using the physical
connection between the two machines.

Sec.2.5 • Communication Protocols 71
On the receiver's machine, the message data traverses up from the physical layer to
the application layer. As the message data traverses to higher level layers, each layer
performsthe funct.ionsassignedto itand strips offthe headersor trailersaddedbyitspeer
layer at the sending site. For example, the data-link layer at the receiving machine
performs error detection by recalculating the checksum for each frame and comparing it
with the checksum in the trailer of the frame. It strips off the header(H 2) and the trailer
(T 2) from the frames before passing them on to the network layer. The application layer
of the receiver's machine finally passes on the message in its original form to the
communicatingprocesson the receiver'ssite. Noticethat the softwareofaparticularlayer
on the sending machine conceptually communicates only with its peer layer on the
receiving machine, although physically itcommunicates only with the adjacent layers on
the sending machine. This abstraction is crucial to network design.
The IEEE 802 LAN Reference Model
The ISO model is oriented toward WANs rather than LANs because it was conceived as
a model for computer networking primarily in the point-to-point packet-switching
environment. InaLAN, the host computersare connecteddirectlyto anetworkcircuitby
relatively simple interface hardware. The interface hardware and networkdriversoftware
in each site can send and receive data at high speeds with low error rates and without
switching delays. These important characteristics ofLANs give considerable advantages
in cost, speed, and reliability in comparison to WANs. Due to these differences in
characteristics between LANs and WANs, and also because ofthe following differences
between the OSI model and LAN concepts, the OSI modelis generally considered to be
unsuitable for LAN environments:
• In the OSI model, information is exchanged between two communicatingentities
only after they have entered into an agreementabout exchanginginformation. But
inaLAN, nosuch prior agreementisneededforcommunicationtotake place, and
information may be delivered to a destination from a numberofdifferent sources
within a short time interval.
• IntheOSI model, the model ofcommunicationisgenerallyone toone and/orone
to many. But in a LAN, the model of communication is generally many to
many.
• The OSI model is often said to be connection oriented. But communications in a
LAN is mostly connectionless.
This implies that a modified reference model particularly suited to LANs is needed.
This problem was realized long ago, and a reference model suitable for LANs was built
by IEEE in 1983, the IEEE 802 LAN Reference Model (abbreviated IEEE 802 LAN)
[IEEE 1990].The IEEE802 I../AN model was builtwith an aim touse asmuch aspossible
of the OSI model while providing compatibility between LAN equipments made by
different manufacturers such that data communication can take place between these
equipments with the minimum effort on the part of the users or builders of LANs.
Therefore, the IEEE 802 LAN model modifies only the lowest two layers of the OSI

72 Chap. 2 • Computer Networks
model anddoesnotinclude specifications forhigher layers, suggesting theuseoftheOSI
model protocols at higher layers. The modifications of the lowest two layers are mainly
concerned with the most important features of LANs that result from the fact that the
physical mediumisaresource sharedbyallsitesconnected toit.Therelationship between
the IEEE 802 LAN model and the OSI model is shown in Figure 2.9.
TheOSImodel
Layer7
(application)
Layer6
(presentation)
Layer5
(session)
Layer4
(transport)
Layer3 Thelowestthreelayers
(network) oftheIEEE802LANmodel
Layer3
Layer2
(logical-linkcontrol)
(datalink)
Layer2
(medium-accesscontrol)
Layer 1
(physical) Layer 1
(physical)
Fig. 2.9 Relationshipbetween the IEEE 802 LAN model and theOSI model.
As shown inthe figure, the lowest three layers of the IEEE 802 LAN model are the
physical layer, the medium-access-control layer, and the logical-link-control layer. The
physical layer defines interface protocols for the following four types of media that are
commonly usedinLANs:baseband,broadband,fiberoptics,andtwistedpair.Asthename
implies, the medium-access-control layerdeals withthe medium-access-controlprotocols
for LANs. This layer includes functions associated with both the physical and data-link
layers of the OSI model. It includes the following four standards:
1. TheIEEE802.3standard,whichdefines protocols foraLANhavingbustopology
that uses the CSMNCD method for medium-access control.
2. TheIEEE 802.4standard, whichdefines protocols foraLANhavingbustopology
that uses the token-passing method for medium-access control. The sites
connected to the bus are arranged in a logical ring to use the token-passing
method.

Sec.2.5 • Communication Protocols 73
3. The IEEE 802.5 standard, which defines protocols for a LAN having ring
topology that uses the token-passing method for medium-access control.
4. The IEEE 802.6 standard, which defines protocols for a MAN.
Thethirdlayer,thatis,thelogical-link-controllayer,containstheIEEE802.2standard.
This standard basically defines acommon logical-link-controlprotocol thatcanbeusedin
conjunctionwitheachofthefourstandards defined atthemedia-access-controllayer.
Finally, the relationship between the protocols defined in the OSI model and the
standards defined in the IEEE 802 LAN model is described in the IEEE 802.1 standard.
Further details on the IEEE 802 LAN model can be found in [IEEE 1990, 1985a,b,c].
Network Communication Protocol Case Study:
The Internet Protocol Suite
Several protocol suites for network systems, such as the IP suite of the U.S. Department
of Defense, Xerox Networking System (XNS) of Xerox, System Network Architecture
(SNA) of IBM, Advanced Peer-to-Peer Networking (APPN) of IBM, and NetBIOS of
IBM, are available today. Of the available protocol suites for network systems, IP is the
most popular and widely used one because it has several attractive features. Forinstance,
it issuitable for both LANs and WANs;itcan beimplemented onall types ofcomputers,
from personal computers tothe larger supercomputers; and itis not vendor specific. It is
an open standard governed by the nonaligned (vendor-independent) Internet Engineering
Task Force (IETF). Every major vendor supports IP, making it the lingua franca on
networking. Moreover, IP is such a dominant networking standard that companies in a
position to use it as their backbone protocol will be able to move quickly to high-speed
internetworking technologies like ATM, FDDI, switched Ethernet and Token Ring, or
IOO-Mbps Ethernet. Owing to itsimportance and wide popularity, IP is described below
as a case study of protocol suites for network systems.
Figure 2.10 shows the structure of the IP suite. Itconsists of five layers and several
protocols at each layer. The existence of multiple protocols in one layer allows greater
flexibility totheusersdue todifferent protocols fordifferent applications having different
communication requirements. The protocols of each layer are briefly described next.
Layers Protocols ateachlayer
I
FTP,TFTP, TELNET,
Application
SMTP, DNS,others
I
Transport TCP,UDP
I
Network IP,ICMP
Datalink ARP, RARP,others
Fig.2.10 TheInternetProtocol suite Physical SLIP,Ethernet,Token Ring,
others
structure.

74 Chap. 2 • ComputerNetworks
Physical Layer Protocols. Most systems using the IP suite for a LAN use the
Ethernetprotocol atthephysical layer.However, LANproductsusing theIPsuite withthe
Token Ring protocol used at the physical layer are also available. Moreover,
implementations using the SLIP (Serial Line Internet Protocol), which uses an RS-232
serial line protocol (having speeds from 1200 bits per second to 19.2Kbps), also exist.
Networksusing theIPsuitewithphysical layerprotocolsforsatelliteand microwavelinks
also exist.
Data-link Layer Protocols. Every protocol suite defines some type of
addressing for uniquely identifying each computer on a network. In the IP suite, a
computer's address (called Internet address or IP address) consists of the following
information:
1. Net number. Each network using the Internet protocol suite has a unique net
numberthat is assigned by a central authority-the Network Information Center
(NIC) located at SRI International.
2. Subnet number.This is an optional number representing the subnet to which the
computer being addressed is attached. This number is assigned by a user to the
subnet when the subnet is being set up. The subnet number is stated in the IP
address ofa computer only when the computer is on a subnet.
3. Host number. This number uniquely identifies the computer being addressed on
the network identified by a net number.
All this information isrepresented by a 32-bit address divided into four 8-bit fields.
Each field iscalled an octet.Each octet is separated from the next octet by aperiod. The
decimal number represents 1byte ofthe IP address, which can have a value of 0-255.
Therefore, a typical IP address is 190.40.232.12.
There are three classes of Internet address-A, B, and C-plus a provision for
Internet multicast communication that is useful for applications that need multicast
facility. The format and the permissible values for each octet for each class of Internet
address are shown in Figure 2.11. Due to the increasing importance of multicast-based
applications, a group of user organizations has established a multicast network called
MBone (Multicast Backbone) thatisavirtual networkintheInternetthatmulticastsaudio,
video, white-board, and other streams that need to be distributed to large groups
simultaneously [Macedonia and Brutzman 1994]. MBone addresses such network issues
as bandwidth constraints, routing protocols, data compression, and network topology.
Many application tools are available free of cost for a wide variety of host platforms.
Today, hundreds of researchers use MBone to develop protocols and applications for
group communication.
The three classes of Internet addresses are designed to meet the requirements of
different types oforganizations. Thus class A addresses are used for those networks that
need to accommodate a large number of computers (hosts) on a single network, while
class C addresses allow for more networks but fewer hosts per network, and class B
addressesprovideamedian distributionbetweennumberofnetworks andnumberofhosts
per network. Notice from Figure 2.11thatthere can beonly 127classAnetworks, and the

Sec.2.5 • Communication Protocols 75
G1f--7bits--+II4 24bits ~I
Netnumber Hostnumber
t+--Octet 1---+t+--Octet 2---+t+--Octet3--.t+--Octet 4----.'
(0- 127) (0- 255) (0- 255) (0- 255)
AnexampleIPaddressis78.3.50.8
(a)
tB_ _---:---
1
~Octet1 ~" Octet2--+t4---0ctet3-- ..'.4.1----0ctet4--t-t
(128-191) (0- 255) (0- 255) (0- 255)
AnexampleIPaddressis130.150.10.28
(b)
8TJ.... I
Net number --=- Hostnumber_
t+--Octet 1 ~14 Octet2 _'4 Octet3 -14 Octet4~
(192- 233) (0--255) (0- 255) (1- 254)
AnexampleIPaddressis221.198.141.233
(c)
rnE_o _
Multicastaddress
t+--Octet1 -14 Octet2 -It Octet3----+~Octet4~
(234- 255) (0_.255) (0·-255) (1- 254)
AnexampleIPaddress is236.8.26.54
(cf)
Fig. 2.11 Internet address formats:Internet address for(0) classA,(b) class B~
(c) class C networks, and(d) multicastcommunicationfacility.
highest host address'in a class A network can be 255.255.254, thus accommodating a
possible 16,777,214 hosts. On the other hand, a class C network can accommodate only
254 hosts.
Values from 0 to 255 can be assigned to each octet for the host number part of
an address, with the restriction that the host number part cannot be all O's or all 1'so
That is, for example, on a network having a net address of 78, a host could have the

76 Chap.2 • Computer Networks
address 78.15.0.105 or 78.1.1.255 but it could not have the address 78.0.0.0 or
78.255.255.255. This is because Internet addresses with a host number part that is all
O's or all 1's are used for special purposes. Addresses with host number part set to all
O'sare used to refer to "this host," and a host number set to all 1's is used to address
a broadcast message to all of the hosts connected to the network specified in the net
number part of the address.
A multihomedhost is one that is connected to two or more networks. This implies
that it must have two or more Internet addresses, one for each network to which it is
connected. This means that every Internet address specifies a unique host but each host
does not have a unique address.
Each packet received by the data-link layer from the network layer contains the
IP addresses of the sender and receiver hosts. These IP addresses must be converted
to physical network addresses for transmission across the network. Similarly, physical
network addresses embedded in packets received from other networks must be
converted to IP addresses before being passed on to the network layer. The job of
translating IP addresses to physical network addresses and physical network addres
ses to IP addresses is performed by the data-link layer. Two important protocols that
belong to this layer are ARP and RARP. ARP (Address Resolution Protocol)
[Plummer 1982] is the Ethernet address resolution protocol that maps known IP
addresses (32 bits long) to Ethernet addresses (48 bits long). On the other hand,
RARP (Reverse ARP) [Finlayson et al. 1984] is the IP address resolution protocol
that maps known Ethernet addresses (48 bits) to IP addresses (32 bits), the reverse
of AR~
Network Layer Protocols. The two network layer protocols are IP [Postel
1981a] and ICMP (Internet Control Message Protocol) [Postel 1981b]. IP performs the
transmissionofdatagramsfromone host toanother.Adatagram isagroup ofinformation
transmittedasaunit toandfrom theupper layer protocolsonsendingand receivinghosts.
ItcontainstheIPaddressesofsending andreceivinghosts. Fordatagramtransmission,the
two main jobs ofIP are fragmentation and reassembly of datagrams into IP packets and
routing ofIP packets by determining the actual path to be taken by each packet from its
source to the destination. For packet routing, a dynamic routing algorithm that uses a
minimal path selection policy is used.
On the sending host, when a datagram is received at the network layer from the
transport layer, the IP attaches a 20-byte header. This header contains a number of
parameters, most significantly the IP addresses ofthe sending and receiving hosts. Other
parameters include datagram length and identify information if the datagram exceeds the
allowable byte size for network packets [called MTU (maximum transfer unit) of the
network] and must be fragmented. If adatagram is found to be larger than the allowable
byte size for network packets, the IP breaks up the datagram into fragments and sends
each fragment as an IP packet. When fragmentation does occur, the IP duplicates the
source addressanddestinationaddress intoeach IPpacket, sothat theresultingIPpackets
can be delivered independently of each other. The fragments are reassembled into the
originaldatagrambytheIPonthereceivinghost andthenpassedontothehigher protocol
layers.

Sec.2.5 • Communication Protocols 77
The data transmission service ofthe IPhas best-effortdelivery semantics. That is, a
best effortismade to deliverthe packets but deliveryisnot guaranteed. The IPcomputes
and verifies a checksum that covers its own header, which is inexpensive to calculate.
Thereis no data checksum, which avoids overheads whencrossingrouters, but leavesthe
higher level protocols to provide their own checksums. On the receiving host, if the
checksum of the header is found to be in error, the packet is discarded, with the
assumption that a higher layer protocol will retransmit the packet. Hence, the data
transmission service ofthe IP is unreliable.
The ICMPofthe network layerprovides an elementaryform offlow control. When
IP packets arrive at a host or router so fast that they are discarded, the ICMP sends a
message to the original source informing it that the data is arriving too fast, which then
decreases the amount ofdata being sent on that connection link.
Transport Layer Protocols. Transport layer protocols enable communications
between processes running on separate computers ofa network. The two main protocols
of this layer are TCP (Transport Control Protocol) [Postel 1981c] and UDP (User
DatagramProtocol)[Postel 1980].Thesetwo protocolsare sometimesreferredtoas TCPI
II)and UDPIIP, respectively, to indicate that they use the IP at the network layer.
The TCP provides a connection-oriented, reliable, byte-stream service to an
applicationprogram. Itisareliableprotocolbecauseany data writtentoaTCPconnection
will bereceivedbyitspeeroranerrorindicationwill begiven. SincetheIPofthe network
layer provides an unreliable, connectionless delivery service, it is the TCP that contains
the logic necessary to provide areliable, virtual circuitfor auser process.Therefore,TCP
handles the establishment and termination of connections between processes, the
sequencing of data that might be received out of order, the end-to-end reliability
(checksum, positive acknowledgments, timeouts), and the end-to-end flow control. Note
that the ICMP of the network layer does not provide end-to-end flow control service
between two communicating processes. TCP is used in most ofthe well-known Internet
services that are defined at the application layer.
On the otherhand, UDPprovides aconnectionless, unreliabledatagram service. Itis
an unreliableprotocol becauseno attemptis made to recoverfrom failure or loss; packets
may belost, with no errorindication given. Therefore, UDP is very similarto IP with two
additional features that are not provided by IP: process addressing and an optional
checksum to verify the contents ofthe UDP datagram. These two additional features are
enough reason for a user process to use UDP instead oftrying to use IP directly when a
connectionless datagram protocol is required. UDP is often used for experimental or
small-scale distributed applications in which either reliability of communication is not
important or reliability of communication is taken care of at the application level. For
example, rwho service, network monitoring, time service, Trivial File Transfer Protocol
(TFTP),and so on, use UDP.
The IP layer provides host-to-host communication service, but the transport layer
provides process-to-process communication service. For process addressing, both TCP
and UDP use 16-bit integerportnumbers. That is, both TCP andUDPattach a headerto
the data to be transmitted. Among otherparameters, this headercontains port numbersto
specify sending and receiving processes. A port number uniquely identifies a process

78 Chap.2 • ComputerNetworks
within aparticularcomputerandisvalidonly within thatcomputer. Once anIPpackethas
been deliveredto the destinationcomputer, the transport layer protocolsdispatch itto the
specified port number at that computer. The process identified by the port number picks
up the packet from the port. Further details of the port-based process-addressing
mechanism is given in Chapter 3.
Application Layer Protocols. A variety of protocols exist at the application
layer in the IP suite. These standard application protocols are available at almost all
implementationsofthe IP suite. Some ofthe most widely used ones are briefly described
next. Additional details of these protocols can be found in [Stallings 1992b]:
1. FileTransferProtocol(FTP).This protocol is used to transfer files to and from
a remote host in a network. A file transfer takes place in the following manner:
• A user executes theftp command on its local host, specifying the remote host as
a parameter.
• The FrPclient process oftheuser'smachineestablishesaconnectionwithanFrP
server process on the remote host using TC~
• The user is then prompted for login name and password to ensure that the user is
allowed to access the remote host.
• Aftersuccessful login, thedesired file(s)may be transferred in eitherdirection by
using get (for transfer from remote to local machine) and put (for transfer from
local to remote machine) commands. Both binary and text files can betransferred.
The user can also list directories or move between directories of the remote
machine.
2. TrivialFileTransferProtocol(TFTP).This protocol alsoenablesusers totransfer
filestoand from aremote hostinanetwork. However, unlike FfP,itusesUDP (notTCP),
itcannotcheckthe authenticityof the user,and itdoes not provide the facilities oflisting
directories and moving between directories.
3. TELNET. This protocol enables terminals and terminal-oriented processes to
communicate with another host on the network. That is, a user can execute the telnet
command on its local host to start a login session on a remote host. Once a login
session is established, telnet enters the input mode. In this mode, anything typed on the
keyboard by the user is sent to the remote host. The input mode entered will be either
character or line mode depending on what the remote system supports. In the character
mode, every character typed on the keyboard is immediately sent to the remote host
for processing. On the other hand, in the line mode, all typed material is echoed locally,
and (normally) only completed Jines are sent to the remote host for processing. Like
FTP, TELNET uses TCP.
4. SimpleMailTransferProtocol(SMTP).This protocol enables two user processes
on a network to exchange electronic mail using aTCP connection between them.
5. DomainNameService(DNS).The clientprocesses of application protocols, such
as FT~ TFT~ TELNET, SMTP, can be designed to accept Internet addresses (in their

Sec.2.5 • Communication Protocols 79
decimal form) from a user to identify the remote host with which the user wants to
interact. However, as compared to numeric identifiers, symbolic names are easier for
human beings to remember and usc. Therefore, the Internet supports a scheme for the
allocation and use of symbolic names for hosts and networks, such asasuvax.eas.asu.edu
or eas.asu.edu. The named entities are called domains and the symbolic names arecalled
domain names. The domain name space has a hierarchical structure that is entirely
independent of the physical structure of the networks that constitute the Internet. A
hierarchical naming scheme provides greater flexibility of name space management
(described in detail in Chapter 10).
When domain names are accepted as parameters by application protocols such as
FfP, "fFfP, TELNET, SMTP, and so on, they must be translated to Internet addresses
before making communication operation requests to lower level protocols. This job of
mapping domain names to Internet addresses is performed by DNS. Further details of
DNS are given in Chapter 10.
2.5.2 Protocols forDistributad Systams
Although the protocols mentioned previously provide adequate support for traditional net
workapplications suchasfiletransferandremotelogin,theyarenotsuitablefordistributed
systems and applications. This is mainly because of the following special requirements of
distributed systemsascomparedtonetworksystems[Kaashoek eta1. 1993]:
• Transparency. Communication protocols for network systems use location
dependent process identifiers (such as port addresses that are unique only within
a node). However, for efficient utilization of resources, distributed systems
normally support process migration facility. With communication protocols for
network systems, supportingprocess migration facility isdifficult because when a
process migrates, its identifier changes. Therefore, communication protocols for
distributed systems must use location-independent process identifiers that do not
change even when a process migrates from one node to another.
• Client-server-based communication. The communication protocols for network
systems treat communication as an input/output device that is used to transport
data between two nodes of a network. However, most communications in
distributed systems are based on theclient-servermodel inwhich aclient requests
a server to perform some work for it by sending the server a message and then
waiting until the server sends back a reply. Therefore, communication protocols
for distributed systems must have a simple, connectionless protocol having
features to support request/response behavior.
• Group communication. Several distributed applications benefit from group
communication facility that allows a message to be sent reliably from one sender
to n receivers. Although many network systems provide mechanisms to do
broadcast or multicast at the data-link layer, their communication protocols often
hide these useful capabilities from the applications. Furthermore, although
broadcast can be done by sending n point-to-point messages and waiting for n
acknowledgments, this algorithm is inefficient and wastes bandwidth. Therefore,

80 Chap. 2 • ComputerNetworks
communication protocols for distributed systems must support more flexible and
efficientgroupcommunication facilityinwhichagroupaddresscanbemappedon
one or more data-link addresses and the routing protocol can use a data-link
multicast address to send a message to all the receivers belonging to the group
defined by the multicast address.
• Security. Security isacritical issue in networks, and encryption is the commonly
used method to ensure security of message data transmitted across a network.
However, encryption is expensive to use, and all nodes and all communication
channels of a network are not untrustworthy for a particular user. Therefore,
encryption should beusedonly whenthere isapossibility ofacritical message to
travel via an untrustworthy node/channel from its source node to the destination
node. Hence a communication protocol is needed that can support a flexible and
efficient encryption mechanism in which a message is encrypted only ifthe path
it takes across the network cannot be trusted. Existing communication protocols
for network systems do not provide such flexibility.
• Network management. Network management activities, such as adding/removing
a node from a network, usually require manual intervention by a system
administratortoupdatetheconfiguration filesthatreflectthecurrentconfiguration
of the network. For example, the allocation of new addresses is often done
manually. Ideally, network protocols should automatically handle network
management activities to reflect dynamic changes in network configuration.
• Scalability.Acommunication protocol fordistributed systems mustscale welland
allowefficientcommunication totakeplaceinbothLANandWANenvironments.
A single communication protocol must be usable on both types of networks.
Twocommunicationprotocols thathave beendesigned toachieve higher throughput
and/or fast response indistributed systemsandtoaddress oneormoreof theissuesstated
above are VMTP (Versatile Message Transport Protocol) and FLIP (Fast Local Internet
Protocol). VMPT provides group communication facility and implements a secure and
efficientclient-server-basedcommunicationprotocol [CheritonandWilliamson 1989].On
the other hand, FLIP is designed to support transparency, efficient client-server-based
communication, group communication, secure communication, and easy network
management [Kaashoek et al. 1993].These protocols are briefly described next.
The Versatile MessageTransport Protocol
This is a transport protocol that has been especially designed for distributed operating
systems and has been used in the V-System [Cheriton 1988]. It is a connectionless
protocol that has special features to support request/response behavior between a client
and one or more server processes. It is based on the concept of a message transaction
that consists of a request message sent by a client to one or more servers followed by
zero or more response messages sent back to the client by the servers, at most one per
server. Most message transactions involve a single request message and a single
response message.

Sec.2.5 • Communication Protocols 81
For better performance. a response is used to serve as an acknowledgment for the
corresponding request, and a response is usually acknowledgedby the next request from
the same client. Using special facilities, a client can request for an immediate
acknowledgment for its request or aservercan explicitly request an acknowledgmentfor
its response.
To support transparency and to provide group communication facility, entities in
VMTP are identified by 64-bit identifiers that are unique, stable, and independentofthe
host address. The latter property allows entities to be migrated and handled independent
of network layer addressing. A portion of the entity identifierspace is reserved for entity
group identifiers that identify a group of zero or more entities. For example, each file
server, as a separateentity, may belong to the group offile servers, identified by a single
entity group identifier. To find out the location of a particular file directory, a client can
sendarequest forthis informationtotheentity group offileserversandreceive aresponse
from the server containingthedirectory. Agroup managementprotocolhasbeen provided
for creating new groups, adding new members or deleting members from an existing
group, and querying information about existing groups.
Againforbetterperformance,VMPTprovidesaselectiveretransmissionmechanism.
The packets ofa message are divided into packet groups that contain up to a maximum
of 16 kilobytes ofsegment data. The data segment is viewed as a sequence ofsegment
blocks, each of 512 bytes (exceptfor the last, which may beonly partly full), allowingthe
portions of the segment in a packet group to be specified by a 32-bit mask. Each packet
contains a delivery mask field that indicates the portions ofthe data segment the packet
contains. The maximum number of blocks per packet is determined by the network
maximum packet size. When a packet group is received, the delivery masks for the
individual packetsare ORed togethertoobtain abitmapindicating which segmentblocks
are still outstanding. An acknowledgment packet contains this bitmap, and the sender
selectively retransmits only the missing segment blocks.
The VMTPuses arate-basedflow control mechanism. Inthis mechanism, packetsin
a packet group are spaced out with interpacket gaps to reduce the arrival rate at the
receiver. The mechanism allows clients and servers to explicitly communicate their
desired interpacket gap times and to make adjustments based on selective retransmission
requests described previously. For example, if the bitmap returned by the receiver
indicatesthatevery otherpacketneeds toberetransmitted,thesenderreasonablyincreases
the interpacketgap. Ifthe next acknowledgmentbitmap indicatesthat every fourth packet
ismissing, thesenderagain increasesthe interpacketgap. When nopacketlossoccurs,the
sender periodically reduces the interpacket gap to ensure that it is transmitting at the
maximum rate the receivercan handle. Thus, selective retransmission provides feedback
to indicate that the rate of transmission is too high and also minimizes the performance
penalty arising from overflooding of packets from a fast sender to a slow receiver.
An optimization used in VMTP is to differentiate between idempotent and
nonidempotent operations. An idempotent operation is one whose execution can be
repeated any number of times without there being any side effects. For example,
requesting the time of day is a typical idempotent operation, but transferring money
from one bank account to another is a nonidempotent operation. In VMTP, a server can
label a response to indicate that a message transaction was idempotent. By doing so,

82 Chap.2 • ComputerNetworks
arrangements need not be made for retransmitting the response when it is lost because
the servercan reproduce the response when the request is retransmitted. However, when
a response is nonidempotent, VMPT prevents the server from executing a request more
than once.
In addition to the aforementioned features, VMTP provides a rich collection of
optional facilities that expand its functionality and efficiency in various situations. One
such facility that is particularly useful for real-time communication is the facility of
conditional message delivery. With this facility, a client can specify that its message
should only be delivered if the server is able to process it immediately. The optional
facilitiesarecarefullydesignedtoprovide critical extensionstothebasic facilities without
imposing a significant performance penalty, especially on common-case processing.
Furtherdetails ofthe VMTP protocol can be found in [Cheriton 1986, Cheriton and
Williamson 1989].
The Fast Local Internet Protocol
This is a connectionless protocol for distributed operating systems. It has been used in
the Amoeba distributed system [Mullender et a1. 1990]. Its main features include
transparency, security, easy network management, group communication facility, and
efficientclient-server-basedcommunicationfacility.Thefollowing descriptionisbased on
the material presented in (Kaashoek et al. 1993].
For transparency, FLIP identifies entities, called network service access points
(NSAPs),with location-independent 64-bit identifiers. Sites on an internetwork can have
more than one NSAP, typically one or more for each entity (e.g., process). Each site is
connected to the internetwork by a FLIP box that either can be a software layer in the
operating system ofthe corresponding site or can be run on a separate communications
processor. Each FLIP box maintainsarouting table that is basicallyadynamic hint cache
mapping NSAP addresses on data-link addresses. Special primitives are provided to
dynamically registerand unregisterNSAPaddresses into the routing table ofaFLIPbox.
An entity can register more than one address in a FLIP box (e.g., its own address to
receive messages directed to the entity itself and the null address to receive broadcast
messages). FLIPuses aone-way mapping betweenthe private address used to registeran
entity and the public address used to advertise the entity.A one-way encryption function
is used to ensure that one cannot deduce the private address from the public address.
Therefore, entities that know the (public) address of an NSAP (because they have
communicated with it) are not able to receive messages on that address, because they do
not know the corresponding private address.
The FLIP messagesaretransmittedunreliablybetweenNSAPs.AFLIP messagemay
be ofany size less than 232 - 1bytesIf a message is too large for a particular network,
itisfragmentedinto smallerchunks, calledfragments.Afragmenttypicallyfitsinasingle
network packet. The basic function ofFLIP is to route an arbitrary-length message from
the source NSAP to the destination NSAP. The path selection policy is based on the
information stored in the routing tables ofeach FLIP box about the networks to which it
is connected. The two main parameters used for this purpose are the network weight and
a security bit. A low network weight means that the network is desirable on which to

Sec.2.6 • Internetworking 83
forward amessage.The networkweightcan bebased, forexample,on physicalproperties
ofthe network, such as bandwidth and delay. On the other hand, the secure bit indicates
whether sensitive data can be sent unencrypted over the network or not.
The three types ofcalls provided in FlJP for sending a message to a public address
are flip_unicast, flip_multicast, and flip_broadcast. These calls provide both point-to
point and group communication facilities. The group communication protocols make
heavy use of flip_multicast. This has the advantage that a group of n processes can be
addressed using one FLIP address, even if they are located on multiple networks.
Although FLIP does not encrypt messages itself, it provides the following two
mechanisms for secure delivery ofmessages. In the first mechanism, a sender can mark
itsmessagesensitiveby using the security bit. Such messagesare routed only overtrusted
networks. Inthe secondmechanism,messagesroutedoveranuntrustcdnetworkbyaFLIP
are marked unsafe by setting the unsafe bit. When the receiver receives the message, by
checkingthe unsafe bit, itcan tell the sender whetheror not there is a safe route between
them. Ifasafe route exists,thesendertriestosend sensitivemessagesinunencryptedform
but with thesecurity bitset. Ifatsome stage duringroutingnofurther trustedpath isfound
for the message (which can only happen due to network configuration changes), it is
returnedtothe senderwith the unreachable bitset.Ifthis happens,the senderencryptsthe
message and retransmits it with the security bitcleared. Therefore, messageencryption is
done only when required.
The FLIP supports easy network management because dynamic changes in network
configuration are automatically handled. The only network managementjobthat requires
human intervention is the specification of trusted and untrusted networks. That is, FLIP
relies on the system administrator to mark a network interface as trusted or untrusted,
because FLIP itselfcannot determine if a network can be considered trusted.
One requirement for which FLIP does not provide full support is wide-area
networking. Although FIJP has been used successfully in small WANs, it does not scale
well enough to be used as the WAN communication protocol in a large WAN. FLIP
designers traded scalability for functionality becausethey felt that wide-areacommunica
tion should not be done at the network layer, but in higher layers.
Further details of the FLIP protocol can be found in (Kaashoek et a1. 1993].
2.6 INTERNETWORKING
We saw in Chapter 1that two desirable features ofdistributed systems are extensibility
and openness. Both these features call for a need to integrate two or more networks
(possibly supplied by different vendors and based on different networking standards) to
form a single network so that computers that could not cornmunicate because they were
on different networks before interconnection can now communicate with each other.
Interconnecting of two or more networks to form a single network is called
internetworking,and theresulting networkiscalledan internetwork. Therefore, aWANof
multiple LANs is an internetwork.
Internetworks are often heterogeneous networks composed of several network
segments that may differ in topology and protocol. For example, an internetwork may

84 Chap.2 • ComputerNetworks
havemultipleLANs,someofwhichmayhavemultiaccessbustopologywhileothersmay
have ring topology; someof these LANs maybe usingEthernet technology whileothers
may beusing TokenRing technology; and some segments of the network may be using
the IP suite while others may be using IBM's SNA (System Network Architecture)
protocol suite. Internetworking allows these relatively unrelated networks to evolve into
a single working system. That is, the goal of internetworking is to hide the details of
different physical networks, so that the resulting internetwork functions as a single
coordinated unit.
The three important internetworking issues are how to interconnect multiple
(possibly heterogeneous) networks into a single network, which communication
medium to use for connecting two networks, and how to manage the resulting
internetwork. Some commonly used technologies to handle these issues are described
below.An important point to remember here is that handling of intemetworking issues
becomes much easier if the network segments of the resulting internetwork were
designed using widely accepted standards instead of proprietary topologies and
protocols. If an organization has designed its networks using nonstandard technologies,
interconnection to global networks may require tearing of the existing networks and
starting over again. Therefore, adherence to standards is very important from an
internetworking point of view.
1.6.1 Interconnection Technologies
Interconnection technologies enable interconnection of networks that may possibly
have different topologies and protocols. Interconnecting two networks having the
same topology and protocol is simple because the two networks can easily commu
nicate with each other. However, interconnecting two dissimilar networks that have
different topologies and protocols requires an internetworking scheme that provides
some common point of reference for the two networks to communicate with each
other. That point of reference might be a high-level protocol common to the two
networks, a device that allows interconnection of different topologies with different
physical and electrical characteristics, or a protocol that allows operating environment
differences to be ignored. The most commonly used approach is to make use of
common high-level protocols for moving data between common layers on a commu
nications model such as the OSI or the IP suites. Internetworking tools, such as
bridges, routers, brouters, and gateways, make extensive use of this approach. As
described next, each of these tools has strengths, weaknesses, and specific applica
tions in internetworking. These tools are "blackbox" intemetworking technologies that
enable interconnection of similar or dissimilar networks to form a single network
system.
Bridges
Bridges operate at the bottom two layers of the OSI model (physical and data link).
Therefore, they are used to connect networks that use the same communication
protocols above the data-link layer but mayor may not use the same protocols at the

Sec.2.6 • Internetworking 85
physical and data-link layers. For example, bridges may be used to connect two
networks, one of which uses fiber-optic communication medium and the other uses
coaxial cable; or one of which uses Ethernet technology and the other uses Token Ring
technology. But both networks must use the same high-level protocols (e.g., TCP/IP or
XNS) to communicate.
The similarityofhigherlevel protocolsimplies that bridgesdo not modify eitherthe
format or the contents ofthe frames when they transferthem from one network segment
to another (they simply copy the frames). Hence bridges feature high-level protocol
transparency. They can transferdata betweentwo network segmentsover athird segment
in the middle that cannot understand the data passing through it. As far as the bridge is
concerned, the intermediate segment exists for routing purposes only.
Bridges are intelligent devices in the sense that they use a process of learning
and filtering in data forwarding to keep network traffic within the segment of the
network to which it belongs. Therefore, bridges are also useful in network partitioning.
When the performance of a network segment degrades due to excessive network
traffic, it can be broken into two network segments with a bridge interconnecting the
two segments.
Routers
Routersoperateatthe networklayerofthe OSI model. Therefore,routersdo notcarewhat
topologies or access-level protocols the interconnected network segments use. Since
routersusethe bottomthree layersofthe OSI model,they are usually usedtointerconnect
those networks that use the same high-level protocols above the network layer. Note that
the protocols ofdata-link and physical layers are transparent to routers. Therefore, iftwo
network segments use different protocols at these two layers, a bridge must be used to
connect them.
Unlike bridges, routers do not view an internetwork from end to end. That is,
bridges know the ultimate destination of a data, but routers only know which is the
next router for the data being transferred across the network. However, routers are
smarter than bridges in the sense that they not only copy a" data from one network
segment to another, but they also choose the best route for the data by using
information in a routing table to make this decision. That is, managing traffic
congestion is a big plus of routers; they employ a flow control mechanism to direct
traffic on to alternative, less congested paths.
Routers are commonly used to interconnect those network segments of large
intemetworks that use the same communication protocol. They are particularly useful in
controlling traffic flow by making intelligent routing decisions.
An internetwork often uses both bridges and routers to handle both routing and
multiprotocol issues. This requirement has resulted in the design of devices called
brouters, which are a kind of hybrid of bridges and routers. They provide many of the
advantages of both bridges and routers. They are complex, expensive, and difficult to
install, but for very complexheterogeneousintemetworksin which the networksegments
use the same high-level communication protocols, they often provide the best
internetworking solution.

86 Chap. 2 • Computer Networks
Gateways
Gateways operate at the top three layers of the OSI model (session, presentation, and
application). They are the most sophisticated internetworking tools and are used for
interconnecting dissimilar networks that use different communication protocols. That is,
gateways are used to interconnect networks that are built on totally different
communications architectures. For instance, a gateway may be used to interconnect two
networks, one ofwhich uses the IP suite and the other uses the SNA protocol suite.
Since networks interconnected by a gateway use dissimilar protocols, protocol
conversion is the majorjob performed by gateways. Additionally, gateways sometimes
also perform routing functions.
2.6.2 Which Communication M.dlum toUs.?
Another important issue in internetworking is to decide the communication medium that
should be used to connecttwo networks. This largely dependson the locationsofthe two
networks and the throughput desired. For example, FDDI (Fiber Distributed Data
Interface), specified by the American National Standards Institute (ANSI), operates at a
speed of l00Mbps and is an ideal high-bandwidth backbone for interconnecting LANs
that arelocatedwithinamultistorybuildingorhousedinseveralbuildingsinacampusJike
environment.
Ifthe two networksare locatedalittle far from each other(such as on oppositesides
ofatown or in nearbycities), then they may beconnectedby leasedtelephonelines if the
data traffic between the two networks is not heavy. If the data traffic between the two
networks is heavy, dedicated lines may be used to interconnectthe two networks. Use of
dedicated lines is also suggested for security reasons when the data exchanged between
the two networks often contains sensitive information.
Finally, ifthe two networksare located very far from each other(such as indifferent
countries or in two distantly locatedcitiesofacountry), then communicationchannels of
public data networks, such as telephonelines or communicationsatellites, may be used to
interconnect them. Long-haul communication channels are expensive. Moreover, if
communication channels of a public data network are used to interconnect the two
networks, inconsistencies oftraffic and system reliability influence the data throughput
between the two networks. Security is also a problem in this case. The data throughput
problem may be solvedto some extentby using one's own methodoftraffic routing. For
example, ifthe two networks are located in New York and Los Angeles, the total traffic
between the two networks may be routed through both Denver and Dallas for better
throughput. Methods to handle the security problem are described in Chapter 11.
1.6.3 NetworkManagement Technologies
Network management deals with the monitoring and analysis of network status and
activities. Networkmonitoringtools watch network segmentsand provideinformationon
data throughput, node and link failures, and otherglobal occurrenceson the network that
may be useful in some manner to network managers. Simple network monitoring tools

Sec.2.6 • Internetworking 87
reporttheexistence, ifnotthecause,ofaprobleminany part ofthe network. On theother
hand, network analysis tools analyze network activities from a wide variety ofangles at
adepth that includes packet..level protocol analysis. They add quantitativeinformation to
the monitor'squalitativedatabyprovidingawide array ofcompleteinformationaboutthe
operation of a network.
Management of an internetwork is more complex than management of a simple
independent LAN because local problems become global problems when several LANs
are interconnected to form an internetwork. Pinpointing the location ofa problem in an
internetwork is tricky because the problem may beon a local or remote LAN. Therefore,
the tools available for managing a single independent LAN eitherwork poorly or do not
work at all when applied to intemetworks. The heterogeneous nature ofinternetworks is
also a major obstacle in the design of widely acceptable management tools for
internetworks. Every manager of an internetwork dreams of a single tool that has the
following features:
1. It can enfold all the protocols and devices found on a typical heterogeneous
internetwork.
2. It should be easy to usc. Highly graphical user interface that allows rapid user
interactionand reducesthe need for highlyskillednetworkanalystsatmost levels
is desirable.
3. It should be intelligent in the sense that it can learn and reason as it isolates
network faults.
4. Itshould have no preferences regarding a device's vendor or protocol.
Unfortunately, no such tool existsat present. However, the future looks encouraging,
as several networkmanagementframeworks and networkmanagementprofilesare on the
way.To provide for management of future interoperable multivendor networks, the ISO,
the IETF, the OSf~ and other organizations are currently developing management
standards for communications networks based on several reference models and network
management frameworks. Ofthe various emerging standards, three standards seem to be
promising for future network management tools. These are Simple Network Management
Protocol (SNMP), Common Management Information Protocol (CMIP), and Distributed
Management Environment (DME).
The SNMP is a client-server protocol suitable for applications that operate in the
client-servermode and do not require real-timenotification of faults. Itwas introduced in
the late 1980s to control and monitor networks that use the IP suite. Because of its
simplicityfor implementationand lowercosts, SNMP-basedtools are being implemented
by most of the network management element vendors. 10 address speed, security, and
manager-to-manager communication capability, IETF is working on version 2 ofSNMP
protocols. FurtherdetailsofSNMPmay befound in [DataproI990,Shevenell 1994,Janet
1993].
The eMIPis a network management standarddeveloped byOSI (ISO/CCITT). Itis
designed to facilitate interoperability and true integration between large numbers of
separate, isolated network management products and services in a multivendor

88 Chap. 2 • Computer Networks
environment. It is based on a manager-agent model. The managing system invokes the
management operations, and the managed system forwards the notifications to the
manager. Communications between managing systems is also facilitated by the agent
manager role. Further details of CMIP may befound in [IT 1990,Janet 1993].
There are alot more SNMP-based products available as network management tools
ascompared toCMIP-based products. Inspiteofproviding richerfunctionality andmore
sophisticatedfeaturesthanSNMP-based products,CMIP-based productsarenotenjoying
rapid growth because they are more expensive, more complex, and require more
processing power to implement.
The OSF's DME is a set of specifications for distributed network management
products. Its goal is to provide a framework to enable a consistent system and network
management scheme across a global, multivendor distributed environment. To achieve
this goal, the design of DME has been based on SNMP, CMIP, and other de facto
standards. DME-based products are not yet available in the market. Further details of
DME may be found in [Datapro 1993].
1.6.4 Internetwork (ase Study: TheInternet
The Internet is the best example of an internetwork. It is a single worldwide collection
of interconnected heterogeneous networks that share a uniform scheme for addressing
host computers and a suite of agreed protocols. Hosts and other resources on the
Internet are named by using the DNS (Domain Name System) naming scheme
described in Chapter 10.
The Internet has its roots in the ARPANET system of the Advanced Research
ProjectsAgency of the U.S. Department of Defense. ARPANETwas the first WANand
hadonly four sites in 1969.The Internet evolved from the basicideas ofARPANETand
was initially used by research organizations and universities to share and exchange
information. Since restrictions for commercial use were lifted in 1989, the Internet has
rapidly grown into an internetwork that now interconnects more than 10,000 networks,
allowingmorethan3millioncomputersandmorethan40millioncomputer usersinmore
than 150 countries around the world to communicate with each other. The Internet
continues to grow at a rapid pace.
The Internet is a vast ocean of information that is of interest to a wide variety of
users.Severaluser-friendlytoolsareavailablethatallowuserstosuccessfullynavigatethe
Internet and find useful information for one's own purposes.A few of the most popular
of these tools are Gopher, Archie, WAIS (Wide-Area Information Servers), WWW
(World-WideWeb),and Mosaic.
Gopher [Martin 1993] is a text-based tool that provides hierarchical collections of
information of all sorts across the Internet.It is a seemingly endless maze of directory
menus. Developed at the University of Minnesota in 1991,Gopher is currently the most
commonly used tool to locate information on the Internet. For further details, ftp to
boombox.micro.umn.edu and look in the directory /pub/gopher/docs.
Archie is a collection of tools that allows searching of indexes of files available on
public servers by anonymous IIp on the Internet. For further details, gopher to
gopher.gmu.edu.

Sec. 2.6 • Internetworking 89
Wide-Area Information Servers (WAfS) is a group of freeware, shareware, and
commercial software programs that help users locate information on the Internet. For
further details, gopher to gopher.gmu.edu.
The World-Wide Web (WWW) is a hypermedia distribution system that uses
hypertext links to other textual documents or files. With this facility, users can click
on a highlighted word or words in a document to provide additional information about
the selected word(s). With WWW, users can also access graphic pictures, images,
audio clips, or even full-motion video that is set up at sites all over the world to
provide a wealth of useful information. WWW was invented by the European Centre
for Nuclear Research (CERN) in 1992 in an attempt to build a distributed hypermedia
system. WWW traffic is the fastest growing part of the Internet and it is today's
preferred vehicle for the Internet commerce. For further details, refer to [Vetter et al.
1994] or gopher to info.cern.ch.
Mosaic is a hypermedia-based browsing tool for finding and retrieving infor
mation from the Internet. Mosaic browsers are currently available for UNIX work
stations running X Windows, pes running Microsoft Windows, and the Apple
Macintosh computers. Mosaic can access data in WWW servers, WAIS, Gopher
servers, Archie servers, and several others. Its popularity is rapidly increasing
because of its many useful features and capabilities. For further details, refer to
[Vetter et al. 1994] or anonymous ftp to ftp.NCSA.uiuc.edu and look in the
directory IPC/Mosaic.
The worldwide scope of the Internet makes it perhaps the single most valuable tool
for useinmany significantways by both non-profit andcommercial organizations. Some
of the important current strategic uses of the Internet are listed here. The following
description is based on the material presented in [Nejrneh 1994]:
1. On-line communication. The electronic mail service (known as e-mail) on the
Internet is extensively used today by computer users around the world to communicate
with each other. With this facility, the Internet has proved to be a rapid and productive
communication tool for millions of users. As compared to paper mail, telephone, and
fax, e-mail is preferred by many because (a) it is faster than paper mail; (b) unlike the
telephone, the persons communicating with each other need not be available at the
same time; and (c) unlike fax documents, e-mail documents can be stored in a
computer and be easily manipulated using editing programs.
2. Softwaresharing.The Internet provides access to a large number of shareware
software development tools and utilities. A few examples of such available shareware
tools areC++compilers,code libraries, mail servers, andoperating systems (allavailable
viajip from sunsite.unc.edu). The Free Software Foundation also provides a wealth of
GNU software (for details anonymous ftp to prep.ai.mit.eduand look in the directory
IpubIGNU).
3. Exchangeofviewson topics ofcommon interest.The Internet has a number of
news groups. Each news group allows a group of users to exchange their views on some
topic of common interest. For example, the news group comp.os.os'l.advocacy contains
candid dialog about the OS/2 operating system.

90 Chap. 2 • Computer Networks
4. Posting ofinformation ofgeneral interest. The Internet is also being extensively
used as a large electronic bulletin board on which information of general interest can be
posted to bring it to the attention of interested users around the world. Some commonly
posted information include career opportunities, conference and event announcements,
and calls for papers for conferences and journals.
5. Product promotion. Several commercial organizations are effectively using the
Internet services forpromotingtheir products. These organizationsmake useofcorporate
ftp, Gopher, or WWW server sites focused on disseminating timely information about
corporate happenings, product announcements, recent strategic alliances, press releases,
and other information of potential interest to existing and prospective customers. For
example, comp.sys.sun.announce news group contains information about Sun Micro
system's latest product announcements.
6. Feedback about products. In addition to product promotion, commercial
organizations are also using the Internet to gather information about user satisfaction of
existing products, market opportunities of new products, and ideas for potential new
products. This is usually accomplished by putting up an interactive survey application by
the organization on a WWW or Gopher site on the Internet.
7. Customersupportservice.Many software organizationsarealsousingtheInternet
to provide unprecedented levels of timely customer support. The combined electronic
mail,ftp, and otherservices on the Internet provide all of the enabling tools necessary to
provide such first-rate customer support. For example, bugs in fielded software products
canbereportedtoanorganizationviaelectronicmail,andbugfixes,minorreleases, work
arounds, known problems andlimitations, andgeneral advice aboutaproductcanbemade
available by an organization to its customers via anftp server.
8. On-line journals and magazines. The Internet now has literally hundreds of
electronic subscriptions that can be found both for free and low cost. There are many
Gopher andWWW sitesontheInternet thatdealwithelectronic versionsofmanyjournals
andmagazines. Forexample, DowJones NewslRetrievalprovides fee-based access tothe
electronic version of the WallStreet Journal on the Internet. Researchers are working in
the direction to extend this idea to support full-fledged electronic libraries on the
Internet.
9. On-line shopping. The Internet has also facilitated the introduction of a new
market concept that consists of virtual shops. These shops remain open 24 hours all the
year round and are accessible to purchasers all around the world. They provide
information about products or services for sale through ftp, Gopher, or WWW servers.
UsingtheInternet services,customerssubmit specific product queries andrequestspecific
salesquotes. Through awell-defined authorizationandauthenticationscheme, theInternet
services are thenused toaccept orders placed bythecustomers,tohandle order payments,
and to track orders to fulfillment. For example, the Internet Mall isacollection of shops,
each providingseveral products or services forsale. For alistof theavailable products or
services at the Internet Mall, ftp to ftp.netcom.com and look in the directory /pubs/
Guides.

Sec.2.7 • ATM Technology 91
10. Worldwide videoconferencing. Worldwide video conferencing is an emerging
service on the Internet that allows a group of users located around the globe to talk and
interact witheach other as if they were sitting and discussing ina single room.The·CU
SeeMe system developed atCornelllJniversity isanexample ofanInternet-based video
conferencing system.For information onCU-SeeMe, ftptogated.comell.edu andlook in
the directory /pub/videoICU-SeeMe.FAQ.7-6.txt.
2.7 ATM TECHNOLOGY
AsynchronousTransfer Mode (ATM) isoftendescribed asthefuturecomputernetworking
paradigm. Itisa high-speed, connection-oriented switching and multiplexing technology
that uses short, fixed-length packets (called cells) to transmit different types of traffic
simultaneously, including voice, video, and data. It is asynchronous in that information
streamscan besent independently without acommon clock.This emerging technology is
briefly described next. For a more complete treatment of the state of the art in ATM
technology,see[DePrycker 1993,Newman 1994,Fischereta1. 1994,Haendeletal. 1994,
Vetter 1995, Kim and Wang 1995].
2.7.1 MainFeatures of ATM Technology
ATMtechnology is expected to have an enormous impact on future distributed systems
because of its following attractivefeatures:
1. Itenables high-bandwidth distributed applications byprovidingdatatransmission
speedsof 155Mbps, 622Mbps, and potentially 2.5Gbps.This feature willmakepossible
several new distributed applications, such as applications based on video-on-demand
technique, video-conferencing applications, and applications that need to access remote
databases of multimedia data.
2. It provides high transmission speeds for both local and wide-area networks and
services,enabling high-powered distributed applications thatpreviouslyhadlittlehopeof
extending beyond LAN environments to be used in WANenvironments as well.
3. It supports both the fundamental approaches to switching (circuit switching and
packet switching) withinasingle integrated switching mechanism (calledcell switching).
This feature makes it suitable both fordistributed applications thatgenerate constant-bit
rate (eBR) traffic and distributed applications that generate variable-bit-rate (VBR)
traffic. For instance, applications dealing with video and digitized voice generate CBR
traffic. Constant-bit-rate traffic requires guaranteed throughput rates and service levels.
On the other hand, most data applications generate VBR traffic. Variable-bit-rate traffic
can tolerate delays and fluctuating throughput rates.
4. It uses the concept of virtual networking to pass traffic between two locations.
This concept allows the available bandwidth of a physical channel to be shared by
multiple applications, enabling multiple applications to simultaneously communicate at

92 Chap. 2 • ComputerNetworks
different rates over the same path between two end points. That is, it allows the total
bandwidth available to be dynamically distributed among a variety of user applications.
5. In addition topoint-to-pointcommunicationin which there isa single sender and
asinglereceiver,itcaneasily support multicastingfacility inwhichthere isasinglesender
but multiple receivers. Such a facility is needed for transmitting broadcast television to
many houses at the same time. Many collaborative distributed applications also require
frequent use of this kind of facility.
6. It enables the use of a single network to efficiently transport a wide range of
multimediadata such astext, voice, video, broadcasttelevision, and soon.Therefore, the
useofseparate networks suchasatelephone network for voice,anX.25 network fordata,
and a cable television network for video can now bereplaced by a single ATMnetwork
that provides a means for integrating voice, video, data, and other information. This
integration will intum lead to substantial costsavings and simplification inthedesign of
communication networks.
7. It is flexible in the way it grants access to bandwidth. That is, it enables supply
of bandwidth on demand and allows billing network users on a per-cell basis (more
probably on a.giga-cellbasis, given the speed and transfer capabilities ofATM).A user
can grabasbigorassmallachunk ofnetwork bandwidthasheorsheneeds andpayonly
for as much as he or she uses.
8. It is a scalable technology. It enables increase or decrease of such things as
bandwidths and data rates and still maintains the architecture of the signaling process.
Moreover,thesameswitching technology (cellswitching) andthesamecellformatisused
forthetransport ofalltypesofinformation(voice, video,anddata) inbothLANandWAN
environments.
9. It has a fairly solid base of standards. It has been adopted by the International
Telecommunications Union (lTV) (formerly CCITT) and internationally standardized as
the basis for the Broadband Integrated Services Digital Network (B-ISDN).
2.7.2 laslc Cone.pts ofATM Tachnology
There are two fundamental types of network traffic-CBR and VBR. The CBR traffic
(comprising of video and digitized voice information) is smooth, whereas VBR traffic
(comprising of data information) is bursty. The CBR traffic requires a low but constant
bandwidth and guaranteed throughput rates and service levels. On the other hand, VBR
trafficrequires largebandwidth for very short periods oftime atrandom intervals andcan
tolerate delays and fluctuating throughput rates. Due to this basic difference in
characteristicsofthetwotypesoftraffic,circuit switching isthemostsuitable networking
technology for handling CBR traffic, whereas packet switching is the most suitable
networking technology for handling VBR traffic. However, neither circuit switching nor
packet switching is suitable for handling both classes of network traffic. Therefore, when
the standards bodies of the lTV were working on a universal multiplexing and switching
mechanism that could support integrated transport of multiple-bit-rate traffic in an

Sec.2.7 • ATM Technology 93
efficientandcost-effective way, theycameupwiththeideaofahybridformofswitching
technique called cell switching. ATM technology is based on this cell-switching
technique.
Cell-switchingtechnologyisbasedonthedigitalpacket-switchingtechnology,which
relays and routes traffic over virtual paths by means ofan address contained within the
packet (this is different from circuit-switching technology, which routes traffic not by
address but over dedicated physical paths established before communication starts).
However, unlike more familiar packet-switching technologies, such as X.25 or frame
relay, cell-switching technology uses very short, fixed-length packets, called cells. In
ATM,cells are 53 byteslong. Theyconsistofa5-byteheader(containingtheaddress)and
a 48-byte information field.
The cell size of53bytes was chosen to makeATM useful for data as well as voice,
video, and other real-time traffic that cannot tolerate randomly varying transmission
intervals and delays. Pure data sources can produce very long messages-up to 64
kilobytes in many cases. By segmentingsuch messagesinto shortcells,ATMensuresthat
CBRtraffic such as voiceand videocan be given priority andneed neverwait more than
one 53-byte cell transmission time (3f.Ls at a 155-Mbps transmission rate) before it can
gain access to acommunicationchannel. With frame-based packet-switchingtechnology,
the waiting time would be random, possibly several milliseconds in length. The use of
short, fixed-size cells also eliminates the dangerofa smaJIpacket being delayed because
abig one ishogginganeededline. Incaseofcell switching, aftereach cell is transmitted,
a new one (even one belonging to a different message) can be sent.
The proper size ofacell inATM was the subjectofmuch debate withthe standards
committees.This isbecausetelephonypeoplewereinfavorofasmall cell toreducedelay
for voice packets, whereas data communications people were in favor ofa large cell to
minimize the overhead involved in the segmentation and reassembly of message data.
Aftermuch discussion, the cell size debate was narrowedinto two choices-32-bytecells
or 64-byte cells. As a compromise, the lTV set the cell size at 48 bytes plus the
header.
Notice that with a fixed cell size of53 bytes, ATM technology is not an ideal choice
eitherforapplicationsdealingwith CBRtrafficsuch as voice and video orfor applications
dealing with VBR traffic such as file transfers. It is, however, the best technology on the
horizon for handling both types oftraffic on a single integrated network. Because ofits
fast, hardware-based switching, it can emulate the dedicatedcircuits usually required for
handlingCBRtraffic. And becauseitispacketbased, itcan efficientlyhandleVBRtraffic
as well.
The ATM is a connection-oriented technology because a sender first establishes a
connection with the receiver. However, unlike circuit switching, in which a physical
circuit is established between the sender and the receiver and reserved for them for the
entire duration of their communication session, in ATM technology, a virtual circuit is
established between the sender and the receiver. That is, ATM does not reserve the path
for one user exclusively. Any time a given user is not occupying achannel, another user
is free to use it. Connectionestablishment inATM means that a route is determinedfrom
the senderto thereceiverand routing informationis storedinthe switchesalongthe route
during connection establishment. All cells of messages from the sender to the receiver

94 Chap. 2 • Computer Networks
follow this virtual path stored in the switches. When the connection is no longer needed,
it is released and the routing information for this connection is purged from the
switches.
The address information in the headerofeach cell is used by the routing protocol to
determine the virtual path that the cell will traverse. Addresses inATM are only oflocal
significance, in that they matter only between two adjacent pieces ofATM equipment.
When avirtual path isestablished,each switch isprovidedwith asetoflookuptables that
identify an incoming cell by header address, route it through the switch to the proper
outputport, andoverwritetheincomingaddresswith anewone thatthe next switch along
the route will recognize as an entry in its routing table. A message is thus passed from
switch to switch over aprescribedroute, but the route is virtual since the facility carrying
the message is dedicated to it only while a cell ofthe message traverses it.
In ATM, a virtual path is essentially a bundle of virtual channels that can be
multiplexed together. Therefore, overa single virtual path, two hosts may multiplex cells
of many individual applications. Cells are statistically multiplexed, allowing the total
available bandwidth to be dynamically distributed among a variety of distributed
applications. This is achieved by selecting virtual channel paths according to the
anticipatedtraffic and allocatingthenetworkresourcesneeded.Forguaranteedbandwidth
applications, users are required to specify the amount of bandwidth required. It is the
virtual nature ofATM services that provides greater network efficiencies.
1.7.3 ArM Protocol "-f.rence Mod.1
The protocol reference model inATM is divided into three layers-physical layer,ATM
layer, and ATM adaptation layer (AAL) (Fig. 2.12). Applications involving data, voice,
and video are built on top ofthese three layers. The functionalities ofthe three layers are
described next.
I 1
-
OtherIayersno speelIed'In ,..
ATMprotocolreferencemodel
ATMAdaptationlayer(AAL)
ATMlayer
Physicallayer
Fig.2.12 ATMprotocolreferencemodel.
Physical Layer
Thephysicallayeristhebottom-mostlayer oftheATMprotocolsuite.Itisconcernedwith
putting bits on the wire and taking them off again. It has two sublayers: the physical
mediumdependent(PMD)sublayerand the transmissionconvergence(TC) sublayer. The

Sec.2.7 • ATM Technology 95
PMD sublayer defines the actual speed for traffic transmission on the physical
communication medium (electrical/optical) used. On the other hand, the TC sublayer
definesaprotocolfortheencodinganddecodingofcelldata intosuitableelectrical/optical
waveforms for transmission and reception on the physical communication medium
defined by the PMD sublayer. The protocol of the TC sublayer differs according to the
physical communication medium used.
The physicallayer can transfercells from one user toanotherinone ofthefollowing
two ways:
1. By carrying cells as a synchronous data stream. In this case, the user-network
interface(UNI), which takes the form of anATMadaptorboard plugged into acomputer,
puts out a stream of cells on to a wire or fiber. The transmission stream must be
continuous, and when there is no data to be sent, empty cells are transmitted.
2. By carrying cells in the payload portion ofan externally framed transmission
structure. Inthis case, theUNI uses some standardtransmissionstructurefor framing and
synchronization at the physical layer. SONET(Synchronous Optical NETwork) [Omidyar
and Aldridge 1993], the most commonly used standard for this purpose, is briefly
described next. The SONET format is currently supported by single-mode fiber,
multimode fiber, and twisted pair.
SONETisaninternationalsuite ofstandardsfor transmittingdigital informationover
optical interfaces. In SONET, the basic unit of data transmission is a frame whose
structure is shown in Figure 2.13. As shown in the figure, a SONET frame consists ofa
total of 810bytes (9 X 90),outof which 27bytes (9 X 3)are overheadand theremaining
783 bytes (9 X 87)are payload. The overheadbytes are used forerror monitoring, system
maintenance functions, synchronization, and identification ofpayload type. The payload
area can carry a variety of signals, such as several T1signals, or a TJsignal, or several
ATM virtual circuits. Tl is a digital transmission service with a basic data rate of
1.544Mbps, and T3isadigital transmission service with abasic data rate of44.736Mbps
for transport of 28 Tl circuits.
The order of transmission of bytes is row by row, from left to right, with one
entire frame transmitted every 125us. The basic time unit of one frame every 125J.1s
matches with the telephone system's standard sampling rate of 800 samples per
second. Therefore, for the SONET frame format, the gross data rate is 51.840Mbps
(with the overhead bytes included), and the net data rate is 49.920Mbps (with the
overhead bytes excluded).
The basic unit of SONET with a bit rate of 51.840 Mbps is called STS-l
(Synchronous TransportSignal Levell). Higherrate SONETsignals areobtainedbybyte
interleaving n frame-aligned STS-l's to form an STS-n signal [Vetter 1995].
STS uses an electrical rather than an optical signal. Optical Carrier (OC) levels
are obtained from STS levels after scrambling (to avoid long strings of 1's and O's and
allow clock recovery at the receivers) and performing electrical-to-optical conversion
[Vetter 1995]. Thus, OC-n level signals are obtained by scrambling and converting STS
n level signals. The most commonly used values of n are 1, 3, and 12, giving OC-I,

Chap. 2 • ComputerNetworks
f4--3bytes-..~...,. ~-----87bytes------.
...
Byte Byte Byte Byte Byte Byte
9bytes
~I
~I
Fig.2.13 SONETframe format.
OC-3, and OC-12 signal levels having data rates of 51.84, 155.52, and 622.08Mbps,
respectively.
In Europe, another standard for frame formatting called SDH (Synchronous Digital
Hierarchy) [OmidyarandAldridge 1993]isalsoavailable. FortheSOH frameformat, the
gross data rate for the basic unit is 155.52Mbps, instead of the 51.84 Mbps for
SONET.
ATM Layer
The ATM layer handles most of the cell processing and routing activities. These
include building the cell header, cell multiplexing of individual connections into
composite flows of cells, cell demultiplexing of composite flows into individual
connections, cell routing, cell payload type marking and differentiation, cell loss
priority marking and reduction, cell reception and header validation, and generic flow
control of cells. These functions are designed to be carried out in hardware at very
high data rates. The ATM layer is independent of the physical medium used to
transport the cells.
The functionality of the ATM layer is defined by the fields present in the ATM
cell header (Fig. 2.14). These fields and their functions are as follows:

Sec.2.7 • ATMTechnology 97
4bits 8bits 16bits 3bits 1bit 8bits 48bits
~»~
GFC VPI VCI PTI CLP HEC Payload
{{
~ --+I
---------1.WlI4-----Payloadarea
Fig.2.14 ATMcell format:GFC,GenericFlowControl; VPI,VirtualPathIdentifier;
VCI,VirtualChannel Identifier; PTI,PayloadTypeIdentifier;CLP,Cell
LossPriority;HEC,HeaderErrorControl.
• Genericjlowcontrol (GFC)field.This fieldoccupies4bitsinanATMcell header.
ThedefaultsettingoffourD'sindicatesthatthecellisuncontrolled.Anuncontrolled
celldoesnottakeprecedenceoveranothercellwhencontendingforavirtualcircuit.
The bitsintheGFC field canbesuitably settoimplementsome formofprioritized
congestioncontrol. Forexample,thebitsintheGFCfieldcouldbeusedtoprioritize
voiceovervideoortoindicatethatboth voiceandvideo takeprecedenceoverother
typesofdata.TheGFCfield isalsousedbytheUNItocontrol theamount oftraffic
entering the network, allowing the UNI to limit the amount ofdata entering the
network during periodsofcongestion[Vetter 1995].
• Virtualpath identifier (VPJ) and virtual channel identifier(VCI) fields. The VPI
field occupies 8 bits and theVCI field occupies 16 bits in an ATM cell header.
These two fields are used by the routing protocol to determine the path(s) and
channel(s) the cell will traverse. These fields are modified at each hop along the
path. That is, when acell arrives at an ATM switch, its VPI and vel values are
used todetermine the new virtual identifierto be placed inthecell headerand the
outgoinglinkover which totransmitthecell.Asimpliedbyitsname, theVPIfield
is used to establish virtual paths between network end-points. Recall that inATM
twohosts may multiplexcells ofmany individual applicationsover asingle virtual
path connection. This is achieved by the vel fields of the cells that distinguish
among cells ofdifferent applications and thusestablish virtual links overagiven
virtual path.
• Payload-type identifier (PTJ) field. This field occupies 3 bits in an ATM cell
header. It is used to distinguish data cells from control cells so that user data and
control data can be transmitted on different subchannels.
• Cell loss priority (CLP) field. This field occupies 1 bit in an ATM cell header.
When set, it indicates that the cell can be discarded, if necessary, during periods
of network congestion. For example, voice data may be able to suffer lost cells
without the need for retransmission, whereas text data cannot. In this case, an
application may set the CLP field of the cells for voice traffic.
• Headererrorcontrol (HEC)field.This field occupies8bitsinanATMcell header.
It is used to protect the header field from transmission errors. It contains a
checksum of only the header (not the payload).

98 Chap. 2 • ComputerNetworks
ATMAdaptation Layer
The functionality of the physical and the ATMlayers of the ATM protocol suite is not
tailored to any application. We saw that ATM can support various types of traffic,
including voice, video, and data. TheAAL isresponsiblefor providingdifferent types of
services to different types of traffic according to their specific requirements. It packages
various kinds of usertraffic into48-byte cells, togetherwiththe overheadneeded tomeet
specific quality-of-service requirements of different types of traffic. To reflect the
spectrum of applications, four service classes were defined by the lTV (Fig. 2.15):
1. ClassA.Applications having delay-sensitiveCBR traffic that require connection..
oriented service belong to this class. Video and voice applications normally
belong to this class.
2. ClassB.Applications having delay-sensitiveVBRtraffic that require connection
oriented service belongtothisclass.Some videoand audio applicationsbelongto
this class.
3. Class C. Applications having VBR traffic that are not delay-sensitive but that
require connection-oriented service belong to thisclass. Connection-oriented file
transfer is a typical example of an application that belongs to this class.
4. ClassD.Applications havingVBRtrafficthatarenotdelay-sensitiveanddoesnot
requireconnection-orientedservicebelong tothisclass. LAN interconnectionand
electronic mail are typical examples of applications that belong to this class.
Service ClassA ClassB ClassC Class0
class
Bitratetype CBR VBR VBR VBR
Delay
Yes Yes No No
sensitive?
Connection
Yes Yes Yes No
oriented?
AALprotocol AAL3/4 AAL3/4
AAL1 AAL2
tobeused orAAL5 orAAL5
Fig.2.15 Service classes fortheATMAdaptation Layer (AAL).
Tosupportthefourserviceclasses, initiallythelTV recommendedfourtypesofAAL
protocols, called AALI, AAL2, AAL3, and AAL4. It was soon discovered that the
differences between AAL3 and AAL4 protocols are minor. Therefore, they were later
merged into a single protocol, called AAL3/4.
It was.laterdiscovered that the mechanisms of the AAL3/4 protocol were fairly
complex forcomputerdatatraffic.Therefore, anewprotocol calledAAL5 waslateradded

Sec. 2.7 • ATM Technology 99
totheAAL layer for handling computerdata traffic [Suzuki 1994].TheAAL5 protocol is
also called SEAL (Simple and EfficientAdaptation Layer).
As shown in Figure 2.15, class A traffic will use theAALI protocol, class B traffic
theAAL2 protocol, andclass Cand0 trafficeitherAAL3/4 orAAL5. Since bothAAL3/4
and AAL5 protocols are meant for use by class C and 0 traffic, it is important to know
thebasicdifferencesbetween thetwo protocols. AAL3/4 performs errordetection oneach
cell and uses a sophisticated error-checking mechanism that consumes 4 bytes of each
48-byte payload. AAL3/4 allows ATMcells to be multiplexed. On theother hand, AAL5
uses a convent.ional5-byte header (no extra byte from the payload of a cell) and it does
not support cell multiplexing.
2.7.4 ATMNetworks
Inits.simplestform, anATMnetwork hasamesh-stararchitecturewithtwoormoreATM
switches interconnected with copper or optical cables and the host computers connected
to theATMswitches. Figure 2.16 shows such anATMnetwork with three ATMswitches
and nine host computers. Cells originating at any of the nine host computers can be
switched to any of the other host computers attached to the system by traversing through
one or more ATM switches. This simple form is normally suitable for local area ATM
networks. In addition to ATM switches and host computers, a wide-area ATM network
also contains intemetworking devices, such as routers, gateways, and interfaces, to the
public network.
Hostcomputers
/\
Fig.2.16 AnATMnetwork.

100 Chap.2 • Computer Networks
AnATMswitch has several input and output ports. Each inputport hasaninput port
controller, and each output port has an output port controller. Each input port controller
consists of a table, referred to as the VCI table, which maps the VPI and VCI of an
incoming cell to an output VCI and an output port address. Before an incoming cell is
released by an input port controller to the switching fabric of the switch, the VCI of the
cellisreplaced bytheoutputVCI,andtheoutput portaddressisappended forself-routing.
EachATMswitch also hasaswitchcontrollerthat performs different switch management
functions, including updating thetablesoftheinputportcontrollers. Moreover, eachATM
switch usually also has buffers to temporarily store data when cells arriving at different
input ports contend for the same output port. Separate buffers may be associated either
with theinput portsor withtheoutput ports, or theswitch mayhaveapool ofbuffers that
can be used for both input and output buffering.
TheATMswitches thatcontain only a fewports arecheaperandeasiertobuild than
switches containing many ports. Local area ATM networks normally contain a small
number of ATM switches with only a few ports per switch, whereas wide-area ATM
networks normally contain a large number of ATM switches with many ports per
switch.
Each host computer in anATMnetwork is assigned an ATMaddress that could be
based either on a hierarchical 8-byte-long ISDN telephone number scheme E.l64 or a
20-byte address proposed by theATMForum [ATMForum 1993].The latter is modeled
after the address format of an OSI network service access point.
TheATMnetworks having protocol support foramixture of high-level communica
tion services (e.g., TCPII~ UDPIIP, Berkeley Software Distributor [BSD] sockets, and
RPC) may also be used as backbone networks to interconnect existing networks.
As a networking technology, ATM possesses many attractive features, including
enormously high bandwidth, scalability, traffic integration, statistical multiplexing, and
network simplicity. With these features, ATM technology is certainly going to have a
significantimpactonthedesignoffuturedistributedsystems. However,forthesuccessof
ATMasanetworking technology forfuturedistributed systems, severalproblems haveyet
tobesolved.Theseproblems offeranewsetofchallengestonetworkdesigners andusers.
Some of the most important of these problems that are currently under investigation by
researchers working in the area of ATMtechnology are briefly described next.
Ioteroperability
IfATMistosucceed asanetworking technology, itmust interwork withexisting installed
bases. This property is important to the acceptance of ATM since it will allow a huge
number ofexisting distributed applications toberunoverATMnetworks.Tworemarkable
efforts being made inthisdirection areLANemulationoverATMbytheATMForum (the
primary organizationdeveloping anddefiningATMstandards) [ATMForum 1994]andIP
over ATM by the IETF (Internet Engineering Task Force) [Chao et al. 1994, Laubach
1994, Brazdziunas 1994].They are described next.

Sec.2.7 • ATM Technology 101
LAN Emulation over ATM. 'The LAN emulation over ATM (called IAN
emulation) dealswithenablingexistingLAN-baseddistributedapplicationstoberunover
ATM networks. It also enables interconnection of ATM networks with traditional
LANs.
Mostexisting LANs are based on shared-mediainterconnects and employ the IEEE
802 family ofLAN protocols, whichincludesthe Ethernet(802.3),theTokenBus (802.4),
and the Token Ring (802.5) (see Section 2.5.1). Recall from Figure 2.9 that in the IEEE
802 model the data-link layer protocol of the ISO reference model is divided into two
layers-the medium-access-control (MAC) layer, which defines the mechanisms that are
used to access, share, and manage the communication medium, and the logical-link
control (IJLC) layer, which defines a common interface for different network layer
protocols to interwork with different MAC protocols. Each host attached to a LAN has a
globally unique MAC address. MAC addresses are 48 bits long and form a flat address
space. A host can send data to another host only if it knows the MAC address of the
receivinghost (ifboth the hosts are inthe same LAN)or the MAC addressofthe next hop
router (ifboth the hosts are in different LANs).
The key idea behind LAN emulation is to design a separate protocol layer, referred
to as the ATM-MAC layer, above the AAL and below the LLC layer. Two key functions
that must be supported by the ATM-MAC layer are as follows:
1. Emulation of the physical, broadcast, shared medium of I.lANs for supporting
broadcast communication facility
2. Resolution of MAC addresses to ATM addresses for supporting point-to-point
communication facility
The ATM emulates the physical, broadcast, shared medium ofa conventional LAN
by establishing an ATM multicast virtual connection between all of the hosts that are
directlyconnectedto the ATMnetwork, referred to as the LAN emulationclients(LECs).
This multicast connection is the broadcast channel ofthe ATM LAN segment. Any LEC
may broadcast to all others on the ATML.ANsegment by transmitting on the multicast
virtual connection.
For point-to-point communication, an address resolution protocol is required to
translatethe 48-bitMACaddressto anA'TMaddress.TranslatingMACaddressestoATM
addressescan bedone using eitherabroadcast-basedapproachoraserver-basedapproach.
The broadcast-basedapproachrelies onthe switchbroadcastcapability.Ontheotherhand,
in the server-based approach, a LAN emulation server (LES) is placed on a globally
known virtual channel, and a set of query/response messages is implemented for
interactionbetweenthe I.JES and the LEes.AllMAC-to-ATM address resolutionrequests
are sent to the LES, which responds to the requester using a predetermined virtual
channel.
OncetheATMaddressofthe receivinghost has been obtained,apoint-to-pointATM
virtualconnectionmaybeestablishedbetweenthesendingandreceivinghostsbyusingthe
ATMsignalingprotocol.The resultoftheaddressresolutionand theVeloftheestablished
connection are cached in a table in the sending host on the assumption that further
communication with the receiving host is likely. This mechanism operatesentirely within

102 Chap.2 • ComputerNetworks
theATM-MAClayerandistotallytransparenttotheLLCandhigherlayerprotocols inthe
hosts. FurtherdetailsofLANemulation canbefoundin[Newman 1994].
IP over ATM. The IP over ATM deals with enabling existing distributed
applications that have been designed for the IP suite to be runover ATMnetworks. Two
cases that need to be considered for supporting IP over ATMare as follows:
1. Supporting IP over an ATM network that has LAN emulation functionality
implemented over it
2. Supporting IP over a pure ATMnetwork that consists of only ATMswitches
Since LAN emulation defines the ATM-MAC layer above the AAL and below the
LLC~ supporting IP over an emulated LAN is the same as supporting IP over any IEEE
802 LAN. However, when IP is to be supported over a pure ATMnetwork, it ispossible
tosimplify theprotocol stackandrunIPdirectly overATM.ForimplementingIPdirectly
overATM,anaddress resolution protocol isrequired totranslate anIPaddress toanATM
address. Once again the server-based approach (as described for LAN emulation) can be
used for this purpose. WithIPoverATM,the address resolution server, referred to as the
IP-ATM-ARP server, maintains tables that contain mapping information to map IP
addresses to ATMaddresses.
When a new host is added to the ATMnetwork, it first goes through a registration
process and obtains an ATMaddress for itself. It then sends a message to the IP-ATM
ARP server, requesting it to add its address resolution information in the mapping table.
The message contains the IP and ATM addresses of the newly added host. The server
updates the mapping table, allocates anew reserved VCIto the host, and returns the VCI
to the host. The host uses this VCI to discriminate between messages received from the
server and other hosts. The above process can also be adopted to allow port mobility of
hosts. Further details of IP over ATMcan be found in [Chao et al. 1994].
Bandwidth Management
The ATM networks are meant for carrying a mix of synchronous, asynchronous, and
isochronous traffic. Toachieve the desired quality of service, users are often required to
specify the bandwidth requirement for their applications. Twoimportant issues that need
to be resolved in this connection are how users estimate and indicate the bandwidth
requirements of their applications to the network and how the network allocates the
bandwidth totheapplications tomake bestuseoftheavailable bandwidth whilesatisfying
therequests of theapplications. Some proposals made byresearchers working inthis area
are as follows [Turner 1992, Vetter 1995]:
1. Peak-rate allocation method. In this method, users only specify the maximum
traffic rate for their applications. Based on user's requests, the network assigns virtual
channels to the applications in such a manner that on every link of the network the sum
of the rates on the virtual channels is no more than the link's maximum cell rate. If an
application'8 traffic exceeds its specified peak rate, its cells are simply discarded.

Sec.2.7 • ATM Technology 103
2. Minimum-throughputallocationmethod. In this method, usersspecifythe desired
minimum throughputs for their applications, and the network guarantees the specified
throughputs to the applications on a best-effort basis.
3. Bursty-traffic specification method. In this method, users specify the maximum
and average traffic rates plus the maximum burst size for their applications. These
parametersare used to properlyconfigurethe networkto allocatethe availablebandwidth
to the applications to meet their specified requirements.
Noneofthesemethods has been found to be satisfactory. Forinstance, Vetter [1995]
points out that the first method offers a strong performance guarantee and is easy to
implement, but itmay makepooruse ofthe availablenetworkbandwidthincaseofbursty
traffic; the second method can provide high efficiency, but its performance guarantee is
weak; and the third method involves large computation overhead in computing when a
new virtual channel can be safely multiplexed with other virtual channels. Moreover, all
three methods suffer from two additional drawbacks. First, since end-to-end protocols
normallyoperateon dataunits comprisingseveralcells,for an applicationneedingreliable
transport service, the loss or discard of a single cell forces retransmission ofthe entire
data, resulting in lower protocol throughput [Vetter 1995]. The problem becomes more
acute in a wide-area ATM network because the retransmission may also result in
discarding a large amount ofdata already in the connectionpipe. This canhave a serious
performance implication. The second drawback is that none of the three methods
adequately handles multicast virtual circuits [Vetter 1995]. Therefore, new bandwidth
management mechanisms that can overcome the problems mentioned previously are
needed.
LatencylBandwidth Trade-off
Kleinrock [1992]pointedout thatthe latency/bandwidthtrade-offinhigh-speedwide-area
ATM networks will pose new challenges to the designers of these networks. This is
primarily due to the fact that the propagation delay or speed oflight over a wide area is
several magnitudes greaterthan the time it takes to transmit an ATM cell. Forinstance, it
takes roughly 15ms for a bit to travel across the United States one way. At 1Gbps, this
timeismorethan35,000timesgreaterthan the timerequiredtotransmitasingleATMcell
intothe link [Vetter 1995]. Thislatencylbandwidthtrade-offposesthe followingtwomain
problems. These are the same problems described in [Tanenbaum 1995]:
1. Consider a WAN spanning across the United States. The round-trip propagation
delay between two end sites ofthe WAN may be approximately 30ms. Suppose the two
sites are on a 622-Mbps ATM network. At 622Mbps, it takes only about 1.6ms (1/622
second)to pumpall bits ofafile ofsize 1megabitson to the network. Sincethe round-trip
propagationdelay between the two sites is 30ms, the receiver's reply can be received by
the senderonly after31.6ms. Thereforein this example,out oftotal 31.6ms, the link was
idle for 30ms, or 95%ofthe total. Thesituationwill be worsewith higherspeednetworks
and shorter messages. At higher speeds, the fraction of the available virtual circuit

104 Chap.2 • ComputerNetworks
bandwidth thatcan beeffectively usedwilltendtozero.Hence,newprotocols andsystem
architectures are needed to deal with the latency problem in high-speed wide-area ATM
networks.
2. Since thepropagation delayisseveral magnitudes greater than thetime ittakes to
transmit an ATMcell, a sender can send thousands of cells over the network before the
first biteven arrives at the receiver's site. If thereceiver does not possess a large amount
ofbufferingcapacity,mostofthecellswillbelostduetoinadequate bufferspace,andthey
will have to be retransmitted. This can have a serious performance implication. The use
ofthe conventional sliding-window protocol for flow control to solve this problem does
not work well inthiscase.This isbecause ifitisdecided thatthe sender must wait foran
acknowledgmentafter sending every megabit ofdata, due to the latency problem already
described above, the virtual circuit will be 95% idle. To solve this problem, areas that
require particularattention include flow control, buffering, and congestion control. Some
work performed in these areas may be found in [Hong and Suda 1991, Trajkovic and
Golestani 1992,Eckberg 1992,Yazidand Mouftah 1992].
1.8 SUMMARY
A distributed system relies entirely on the underlying computer network for the
communication of data and control information between the nodes of which they are
composed. A computer network is a communication system that links the nodes by
communication lines and software protocols to exchange data between two processes
running on different nodes of the network.
Based on characteristics such as geographic distribution of nodes, data rate, error
rate, and communication cost, networks are broadly classified into two types: LAN and
WAN. Networks that share some of the characteristics of both LANs and WANs are
sometimes referred to as MANs.
The two commonly used network topologies for constructing LANs are the
multiaccess bus andring.For both multiaccess busand ring networks, asingle channel is
shared by all the sites of a network. Therefore, medium-access control protocols are
needed to provide controJledaccess to the shared channel by all sites. Of many medium
access control protocols, theCSMAlCD protocol ismost commonly usedfor multiaccess
bus networks, and the token ring and slotted ring protocols are used for ring networks.
A wide-area network ofcomputers isconstructed by interconnectingcomputers that
are separated by large distances. Special hardware devices called packet-switching
exchanges(PSEs) are usedtoconnect thecomputers tothecommunicationchannels. The
PSEs perform switching and routing activities to transmit data across the network. The
two commonly used switching techniques are circuit switching and packet switching.
The selection of the actual path to be used to transmit a packet in a WAN is
determinedby the routing strategy used.The path used totransmit apacket from one site
to another eithermay befixed or may dynamically change based on network conditions.
This depends on whether a static or a dynamic routing strategy is being used. Moreover,
either the whole path may bedecided at the source siteor thesubpath foreach hop ofthe

Chap. 2 • Exercises 105
path may be decided by each site along the path. Furthermore, with a dynamic routing
strategy, routing tables may be updated in an isolated, centralized, or decentralized
manner.
Computernetworksareimplementedusingtheconceptoflayeredprotocols.The OSI
model providesastandardforlayeredprotocolsforWANs,and theIEEE802 LAN model
defines a standard for LANs. The seven layers of the OSI model are physical, data link,
network, transport, session, presentation, and application. On the other hand, the lowest
three layers of the IEEE 802 lJAN model are physical, medium-access control, and
logical-linkcontrol. The IEEE802 LAN model does not includespecificationsfor higher
layers. Of the available protocol suites for network systems, the Internet Protocol (IP)
suite is the most popular and widely used.
The communication protocols designed for network systems are usually unsuitable
for distributed systems because of the special requirements of distributed systems as
compared to network systems. Several communication protocols have been designed to
achieve higher throughput and/orfast response indistributed systems and to address one
or more of the special requirements of distributed systems. Two such protocols whose
characteristic features were described in this chapter are VMTP and FLIP.
Interconnecting two or more networks to form a single network is called
internetworking, and the resulting network is called an internetwork. The goal of
internetworking is to hide the details of different physical networks so that the resulting
internetwork functions as a single coordinated unit. Tools such as bridges, routers,
brouters, and gateways are used for internetworking. The Internet is the best example of
an internetwork.
The ATM is a high-speed, connection-oriented switching and multiplexing
technologythatuses short, fixed-length packets (called cells) totransmit differenttypes of
traffic sirnultaneously, including voice, video, and data. Due to its many attractive
features, ATM technology is often described as the future computer networking
paradigm.
EXERCISES
2.1. What are the main differences between a LAN and a\VAN?
2.2. Why are medium-access control protocols needed? What properties must a good medium
access-control protocol have?
2.3. The CSMA/CD scheme for medium-access control allows random access to the shared
medium,detects collisions, andtakes necessary steps toretransmit ifacollision isdetected. Is
it possible to devise a random-access scheme for medium-access control in which collisions
never occur (collisions are totally avoided)? If no, explain why. If yes, describe a possible
scheme of this type along with its advantages and disadvantages.
2.4. Answer the following questions for Ethernet:
(a) Howdoes a site acquire the shared channel for transmitting its packets?
(b) How is acollision detected?
(c) How is acollision resolved after detection?
(d) How is the possibility of repeated collisions minimized?

106 Chap. 2 • ComputerNetworks
2.5. In Ethernet, collisionintervalis the time required for asignal topropagatefrom one end of
themedium totheotherandbackagain,andretransmissionslottimeissettobealittlelonger
thanthecollision interval. Prove thatnocollisionswilloccur ifevery sitetransmits itspacket
onlyafteritlistenstothesharedmediumandfindsitfreeforatimeperiodthatisatleastequal
to the retransmission slot time.
2.6. The token ringand theslotted-ringprotocolsfor medium-accesscontrolorganizethe sitesof
anetwork inalogical ring structure. Present analgorithm fordetectingthefailure ofasitein
the ring and reconstructing the logical ring when the failure of a site is detected.
2.7. Presentanalgorithm todetect the lossofatoken inthetokenringscheme formedium-access
control.
2.8. Suggesta priority-based token ring scheme for medium-accesscontrol that does not lead to
the starvation of a low-priority site when higher priority sites always have something to
transmit.
2.9. In a high-speed LAN, suppose the term highspeed means a data transfer rate that is high
enoughtomakethemeanpacket transmissiontimebecome lessthanthemediumpropagation
delay. Considera busLAN that spans 6000 meters, uses amean packet length of 1400bits,
and has adata transmission rate of 50 X 106 bits per second. Ifthecommunication medium
used for this LAN has apropagation speed of 2 X 108mis, would the LAN beconsidereda
high-speed LAN?
2.10. IthasbeenshownthattheperformanceoftheCSMA/CDschemedegradessignificantlyasthe
ratio a:::(TW)/Bincreases, where 'Tis the end-to-end propagationdelay ofthe signal across
the network, Wisthechannel bandwidth, and Bisthe number of bits per packet. One finds
thatagoodestimateofr is 10f.LSforeach kilometerofcable (assumingone repeaterforeach
500 meters ofcable). Experimentalresults have shown that,fora:::0.1,CSMAlCDprovides
adequate channel capacity, 0.65 or higher. For the definition of high-speed LAN given in
Exercise 2.9, show that CSMA/CD scheme is unsuitable for high-speed LANs.
2.11.What ismeant byinternetworking'lWhat arethemain issues inintemetworking?Explain the
difference among the following terms:
(a) Bridge
(b) Router
(c) Gateway
2.12. Suppose that the following sequence of message exchanges are involved in transmitting a
piece ofdata when circuit-switching technique is used:
(a) connect_request(from sender to receiver)
(b) connectionjacknowledgment(from receiverto sender)
(c) send_data(data transfer from sender to receiver)
(d) dataacknowledgment (from receiverto sender)
(e) disconnectrequest (from sender toreceiver)
(t) disconnectionocknowledgment(from receiver to sender)
Suppose that the time to transfer each of the above message is t, except for the send_data
message, for which the time taken is t +ds, where d is a constant and s is the size of the
data in bytes. Now suppose that on the same network t is also the time to transfer a packet
of 100 bytes and a packet-switching technique used returns an acknowledgment from
receiver to sender for every n packets received. Compute the threshold value of s at which
both the circuit-switching and packet-switching techniques perform at the same data
transmission rate.

Chap. 2 • Exercises 107
2.13. Suggest three different routing strategies for use in computer networks. List the relative
advantages and disadvantages of the strategies suggested by you.
2.14. A network system uses the dynamic routing strategy and the non-minimal-path selection
policy. Inthissystem,apacket maycontinuetoberouted throughthenetworkbutneverreach
its destination. Devise a mechanism that avoids the occurrence of this situation in the
system.
2.15. Why are communication protocols needed in a network system?Whatare the main reasons
for using the layered approach to communication protocol specification and design?
2.16. Draw adiagramshowingthe architectureofthe OSImodel. Brieflydescribethe functions of
each layer of this architecture.
2.17. In Figure 2.8, we see that each layer adds itsown header to the message being transmitted
across thenetwork. Insteadofaddingaseparateheaderateach layer,itwould have beenmore
efficient to add a single headercontaining thecontrol information inall these headers to the
message before its transmission across the network. Explain why this is not done.
2.18. Mostcomputernetworksusefewer layersthanthose specifiedintheOSImodel.Explainwhat
might be the reason for this. What problems, ifany, could this lead to?
2.19. What are the main differences between connection-orientedand connectionlesscommunica
tion protocols? Indicate which of the two protocols ispreferable for the transmission of the
following types of information:
(a) Voice
(b) Video
(c) Bursty data
2.20. The packetsof a message may arrive at their destination inan order differentfrom the order
in which they were sent. Indicate for which of the following types of networks is this
statement true:
(a) Ethernet LAN
(b) WAN
(c) ATM LAN
(d) ATM WAN
Give reasons for your answers.
2.21. Why is the OSI model considered to be unsuitable for use ina LAN environment?Give the
architecture of a communication protocol model suitable for LANs. Briefly describe the
functions of each layer of this architecture.
2.22. Why areconventionalcommunicationprotocolsfor networksystemsgenerallyconsideredto
be unsuitable for distributed systems?
2.23. Explain the mechanism used in the VMTP protocol for each ofthe following:
(a) Handling of lost messages
(b) Group communication
(c) Flow control
(d) Transparentcommunication.
2.24. Explain the mechanism used in the FLIP protocol for each of the following:
(a) Transparent communication
(b) Group communication
(c) Secure communication
(d) Easy network management.

108 Chap. 2 • ComputerNetworks
2.25. What are themain attractive features ofATMtechnology? What typeof impact willeach of
these features have on future distributed systems?
2.26. Describe the functionalities of thedifferent layers of theATMprotocol reference model.
2.27. Explain how the following can beachieved:
(a) LAN emulation overATM
(b) IP overATM
2.28. Give three different methods that may be used in ATMnetworks to allocate bandwidth to
applicationsto make best useof theavailable bandwidth while satisfying the requests of the
applications. Also give the relative advantages and limitations of the three methods.
2.29. Give examples toillustrate theproblems thatoccur due tothe latencylbandwidth trade-off in
high-speed, wide-area ATMnetworks.
BIBLIOGRAPHY
[AbeysundaraandKamal1991]Abeysundara, B.W.,and Kamal,A.E.,"High-SpeedLocalArea
Networks and Their Performance: A Survey," ACM Computing Surveys, Vol. 23, No.2, pp.
221-264 (1991).
[ATM Forum 1993] ATM User-Network Interface Specification Version 3.0, Prentice-Hall,
Englewood Cliffs, NJ (1993).
[ATM Forum 1994] "LAN Emulation Over ATM," Draft Specification-Revision 5 (ATM
FORUM 94-0035R5), LAN Emulation Sub-Working Group of the ATM Forum Technical
Committee (August 1994).
[Black1993]Black,V.,ComputerNetworks: Protocols. Standards, andInterface,2nded.,Prentice
Hall, Englewood Cliffs, NJ (1993).
[Black 1995a] Black, V., ATM: Foundation for Broadband Networks, Prentice-Hall, Englewood
Cliffs, NJ (1995).
[Black 1995b] Black, V., TCPIIP andRelatedProtocols, 2nd ed., IEEE ComputerSociety Press,
LosAlamitos, CA (1995).
[Brazdziunas 1994] Brazdziunas, C., "IPing Support forATMServices," Internet RFC No. 1680
(1994).
[Chaoet al, 1994]Chao, H.1, Ghosal, D., Saha, D.,andTripathi, S. K.,"IP onATMLocalArea
Networks," IEEECommunications Magazine, pp. 52-59, New York,NY (August 1994).
[Cheriton 1986J Cheriton, D. R., "VMTP: A Transport Protocol for the Next Generation of
Communication Systems," In: Proceedings ofthe SIGCOMM'86, pp. 406-415 (August 1986).
[Cheriton 1988]Cheriton, D. R.,"The VDistributed System," CommunicationsoftheACM, Vol.
31, No.3, pp. 314-333,ACM, New York,NY (1988).
[Cheriton.andWilliamson1989]Cheriton, D.R.,andWilliamson,C.L.,"VMTPAstheTransport
Layer for High-Performance Distributed Systems," IEEE Communications Magazine, Vol.27,
No.6, pp. 37-44, New York,NY (1989).
[Comer 1995]Comer, D. E., Intemetworking with TCPI/P: Volumel-s-Principles, Protocols. and
Architectures, 3rd ed., Prentice-Hall, Englewood Cliffs, NJ (1995).
[ComerandStevens1993]Comer,D.E.,andStevens,D.L.,lntemetworkingwithTCPIIP: Volume
III-Client-Server Programming andApplications: BSD Socket Version, Prentice-Hall, Engle
wood Cliffs, NJ (1993).

Chap. 2 • Bibliography 109
[ComerandStevens1994]Comer,D.E.,andStevens,D.L.,InternetworkingwithTCPIIP:Volume
II-Design, Implementation, and Internals, 2nd ed., Prentice-Hall, Englewood Cliffs, NJ
(1994).
[Datapro 1990] Datapro, "An Overview of Simple Network Management Protocol," Datapro
Network Management, NM40-300-201 (February 1990).
[Datapro 1993] Open Software Foundation (OSF) Distributed ManagementEnvironment (DME),
A Datapro Report, Datapro Network Management, NM40-684-07 (April 1993).
[DePrycker 1993] DePrycker, M., Asynchronous Transfer Mode: Solution for Broadband ISDN,
2nd ed., Ellis Horwood (1993).
[DePrycker et al. 1993] DePrycker, M., Peschi, R., and Landegem, T., "B-ISDN and the OSI
Protocol Reference Model," IEEE Network, Vol.7, No.2, pp. 10-18, New York,NY (March
1993).
[DuttonandLenhard1995] Dutton, Jr.,H.,andLenhard, P.,High-SpeedNetworking Technology:
An Introductory Survey, Prentice-Hall, Englewood Cliffs, NJ (1995).
[Eckberg1992]Eckberg,A.,"B-ISDN/ATMTrafficandCongestion Control," IEEENetwork, Vol.
6, No.5, pp. 28-37, New York,NY (1992).
[Finlaysonet al.1984]Finlayson, R., Mann,T.,Mogul,1.,andTheimer, M., "A ReverseAddress
Resolution Protocol," RFC No. 903 (June 1984).
[.~ischer et al. 1994] Fischer, W., WalJmeier,E., Worster,T.,Davis, S. P.,and Hayter,A., "Data
Communications Using ATl\1: Architectures, Protocols, and Resource Management," IEEE
Communications Magazine, pp. 24-33, New York,NY (August 1994).
[Furhtand Milenkovic1995] Furht, B.,andMilenkovic, M.,GuidedTourofMultimedia Systems
and Applications, IEEE ComputerSociety Press, LosAlamitos, CA (1995).
[Haendel et al, 1994] Haendel, R., Huber, M. N., and Schroeder, S., ATM Networks: Concepts,
Protocols, Applications,Addison-Wesley, Reading, MA (1994).
[Helgert 1991] Helgert, H. J., Integrated Services Digital Networks.' Architectures, Protocols,
Standards, Addison-Wesley, Reading, MA (1991).
(Hong and Suda 1991] Hong, D., and Suda, T., "Congestion Control and Prevention in ATM
Networks," IEEE Network, Vol.5, No.4, pp. 10-16, New York,NY (1991).
[Hughes 1994] Hughes, K., "Entering the World-Wide Web: A Guide to Cyberspace," ftp:/
taurus.cs.nps.navy.mil:/pub/mbmy/world-wide-web-guide.ps.Z (1994).
[IEEE1985a]CarrierSenseMultipleAccess withCollision Detect(("'SMA/CD)AccessMethodand
Physical Layer Specifications, ANSI/IEEE 802.3 (lSOIDIS 8802/3), IEEE, New York(1985).
[IEEE 1985b] Token-Passing Bus AccessMethodand Physical Layer Specifications, ANSI/IEEE
802.4 (ISO/DIS 8802/4), IEEE, NewYork(1985).
[IEEE 1985c] T()ken Ring Access Method and Physical Layer Specifications, ANSIIIEEE 802.5
(lSOIDIS 8802/5), IEEE, New York(1985).
[IEEE 1990] IEEE Standard802: Overview andArchitecture,American National Standard ANSI/
IEEE 802, IEEE ComputerSociety Press, LosAlamitos, CA (1990).
[IT 1990] Information Technology-Open Systems Interconnection-Management Information
ProtocolSpecification-CommonManagement Information Protocol, ISOIIEC9596-1,ISOIIEC
JTC1/SC21 N5303 (November 1990).

110 Chap. 2 • Computer Networks
[Janet 1993]Janet, E. L., "Selectinga Network ManagementProtocol, Functional Superiority vs.
PopularAppeal:' Telephony(November 1993).
(Kaashoek et al, 1993]Kaashoek, M. F.,VanRenesse, R., Staveren, H., and Tanenbaum, A. S.,
"FLIP: An Internetwork Protocol for Supporting Distributed Systems," ACM Transactions on
ComputerSystems, Vol.11,No.1, pp. 73-106 (1993). ©ACM, Inc., 1993.
[Kawarasaki and Jabbari 1991]Kawarasaki, M., and Jabbari, B., "B-ISDN Architecture and
Protocol," IEEE Journal of Selected Areas on Communications, Vol. SAC-9, No.9, pp.
1405-1415, New York,NY (1991).
[Kim and Wang 1995] Kim, B. G., and Wang, P., "ATM Network: Goals and Challenges,"
Communications oftheACM, Vol.38, No.2, pp. 39-44 (1995).
[Kingand Mitrani 1987]King, P 1. B., and Mitrani, I., "Modeling a Slotted Ring Local Area
Network:' IEEE Transactions on Computers, Vol.C-36, No.5, pp. 554-561, Piscataway, NJ
(1987).
[Kleinrock 1992]Kleinrock, L., "The Latency/Bandwidth Tradeoff in Gigabit Networks," IEEE
Communications Magazine, Vol.30, No.4, pp. 36-40, New York,NY (1992).
[Kroll994]Krol, E.,TheWholeInternet: User'sGuideandCatalog,2nded.,O'Reilly,Sebastopol,
CA (1994).
[Kung 1992) Kung, H. T., "Gigabit Local Area Networks: A Systems Perspective," IEE'E
Communications Magazine, Vol.30, No.4, pp. 79-89, New York,NY (1992).
[Larmouth 1993]Larmouth, J., Understanding OSI, Prentice-Hall, London, UK (1993).
[Laubach 1994] Laubach, M., "Classical IP and ARP over ATM," RFC No. 1577 (January
1994).
[Leinwandand Conroy1996]Leinwand,A.,andConroy, K.F.,NetworkManagement:A Practical
Perspective, 2nd ed., Addison-Wesley, Reading, MA (1996).
[Macedonia and Brutzman 1994]Macedonia, M. R., and Brotzman, D. P., "MBone Provides
Audioand VideoAcross the Internet," IEEE Computer, pp. 30-36(April 1994).
[Malamud1992]Malamud,C.,STACKS:lnteroperabilityinToday'sComputerNetworks, Prentice
Hall, EnglewoodCliffs, NJ (1992).
[Malamud 1993]Malamud, C., Exploring the Internet: A Technical Travelogue, Prentice-Hall,
Englewood Cliffs, NJ (1993)."
[Martin 1993]Martin, 1.L., "Travels with Gopher," IEEE Computer, Vol26., No.5, pp. 84-87
(1993).
[Miller1995]Miller, M.A.,InterNetworking: A GuidetoNetwork CommunicationsLAN toLAN;
LAN to WAN,2nd ed., M&T Books, New York,NY (1995).
[Mosaic1994]"What'sNewwithNCSA Mosaic,"http:www.ncsa.uiuc.edu/SDG/SoftwarelMosaic/
Docs/whatsnew.html (June 1994).
[Mullenderetal.1990]Mullender, S.1.,VanRossum, G.,Tanenbaum,A.S., VanRenesse, R.,and
VanStaverene, H., "Amoeba: A Distributed Operating System for the 1990s," IEEE Computer,
Vol.23., No.5, pp. 44-53 (1990).
[Nejmeh 1994] Nejmeh, B. A., "Internet: A Strategic Tool for the Software Enterprise,"
CommunicationsoftheACM, Vol.37, No. 11,pp. 23-27 (1994). ©ACM, Inc., 1994.
[Newman1994]Newman,P, "ATMLocalArea Networks,"IEEECommunicationsMagazine, Vol.
32, No.3, pp. 86-98 (1994).

Chap.2 • Bibliography 111
[Omidyar and Aldridge 1993] Omidyar, C., and Aldridge, A., "Introduction to SDH/SONET,"
IEEE Communications Magazine, Vol.31, No.9, pp, 30-33 (1993).
[Partridge 1994]Partridge, C., Gigabit Networking, Addison-Wesley, Reading, MA (1994).
[Perlman 1992]Perlman, R., Interconnections: Bridges and Routers,.Addison-Wesley, Reading,
MA (1992).
[Plummer 1982] Plummer, D. C., "An Ethernet Address Resolution Protocol," RFC No. 826
(November 1982).
[Postel 1980] Postel, J., "User Datagram Protocol," RFC No. 768, USC Information Sciences
Institute (August 1980).
[Postel 1981a] Postel, J., "Internet Protocol: DARPA Internet Program Protocol Specification,"
RFC No. 791 (September 1981).
[Postel 1981b]Postel, 1., "Internet Control Message Protocol," RFC No. 792 (September 1981).
[Postel 1981c] Postel, 1., "Transmission Control Protocol: DARPA Internet Program Protocol
Specification," RFC No. 793 (September 1981).
[Press 1994]Press, L.,"Commercializationofthe Internet," CommunicationsoftheACM, Vol.37,
No. 11,pp. 17-21(1994).
[Ramos et al. 1996JRamos, E., Schroeder, A., and Beheler, A., ComputerNetworking Concepts,
Prentice-Hall, Englewood Cliffs, NJ(1996).
[Rooholamini 19951 Rooholamini, R., "ATM-Based Multimedia Servers," IEEE Multimedia, pp.
39-52 (Spring 1995).
[Santifaller1994]Santifaller,M., TCPIIPandONC/NFS, InternetworkinginaUNIXEnvironment,
2nd ed., Addison-Wesley, Reading, MA (1994).
[Shevenell 1994J ShevenelJ, M., "NMPv2 Needs Reworking to Emerge as a Viable Net
Management Platfornl," Network World(March 7, 1994).
[Smythe 1995]Smythe, C., lnternetworking: Designing the Right Architectures, Addison-Wesley,
Reading, MA (1995).
[Stallings 1992a] Stallings, W. (Ed.), Advances in ISDN and Broadband ISDN, IEEE Computer
Society Press, Los Alamitos, CA (1992).
[Stallings 1992b] Stallings, W., Computer Communications: Architectures, Protocols and
Standards, 3rd ed., IEEEComputerSociety Press, Los Alamitos, CA (1992).
[Stallings 1993a] Stallings, W. (Ed.), Advances in Local and Metropolitan Area Networks, IEEE
Computer Society Press, Los Alamitos, CA (1993).
[Stallings 1993b]Stallings, W. (Ed.), Network Management, IEEE Computer Society Press, Los
Alamitos, CA (1993).
[Stallings 1993c]Stallings, W., Networking Standards: A Guide to OSI, ISDN, LAN, and MAN
Standards, Addison-Wesley, Reading, MA (1993).
[Stallings 1995]Stallings, W., ISDN and Broadband ISDN with Frame Relay and ATM, 3rd ed.,
Prentice-Hall, EnglewoodCliffs, NJ (1995).

112 Chap. 2 • Computer Networks
[Suzuki 1994J Suzuki, T., "ATM Adaptation Layer Protocol," IEEE Communications Magazine,
Vol.32, No.4, pp. 80-83 (1994).
[Tanenbaum 1988] Tanenbaum, A. S., Computer Networks, 2nd ed., Prentice-Hall, Englewood
Cliffs, NJ (1988).
[18nenbaum 1995JTanenbaum, A. S., DistributedOperating Systems, Prentice-Hall, Englewood
Cliffs, NJ (1995).
[Titteland James 1996] Tittel, E., and James, S., ISDN Networking Essentials, Academic Press,
San Diego, CA (1996).
[Trajkovic and Golestani 1992] Trajkovic, L., and Golestani, S. J., "Congestion Control for
Multimedia Services," IEEE Network, Vol.6, No.5, pp. 20-26 (1992).
[Thrner 1992] Turner, J., "Managing Bandwidth in ATM Networks with Bursty Traffic," IEEE
Network, Vol.6, No.5, pp. 50-58 (1992).
[Umar 1993] Umar, A., DistributedComputing: A Practical Approach, Prentice-Hall, Englewood
Cliffs, NJ (1993).
[Verma1990]Verma,P.K.,ISDNSystems:Architecture, Technology&Applications,Prentice-Hall,
Englewood Cliffs, NJ (1990).
[Vetter1995J Vetter,R.J.,"ATMConcepts:Architectures,and Protocols,"Communicationsofthe
ACM, Vol.38, No.2, pp. 31-38 (1995).
[Vetterand Du 1993] Vetter,R. J., and Du, D. H. C., "Distributed Computing with High-Speed
Optical Networks," IEEE Computer, Vol.26, No.2, pp. 8-18 (1993).
[Vetteretal, 1994] Vetter,R.J.,Spell, C.,andWard,C.,"MosaicandtheWorld-Wide Web," IEEE
Computer, pp. 49-56 (October 1994).
[Vickersand Suda 1994] Vickers, B. J., and Suda, T., "Connectionless Service for Public ATM
Networks," IEEE Communications Magazine, pp. 34-42 (August 1994).
[Voruganti 1994] Voruganti, R. R., "A Global Network Management Framework for the '90s,"
IEEE Communications Magazine, pp. 74-83 (August 1994).
[Wilkes and Wheeler 1979] Wilkes, M. V., and Wheeler, D. J., "The Cambridge Digital
CommunicationRing,"In:Proceedingsofthe LocalAreaCommunicationsNetwork Symposium,
Boston, pp. 47-61 (May 1979).
[Wittie1991] Wittie, L. D., "ComputerNetworks and DistributedSystems,"IEEE Computer, Vol.
24, No.9, pp. 67-75 (1991).
[WWW 1994&] "World-Wide WebGrowth,"ftp:nic.merit.edu (1994).
[WWW 1994b] "TheWWW Virtual Library," http:info.cern.chlhypertextlDataSourceslbySubjectl
Overview.html (June 1994).
(Yazidand Mouftah 1992] Yazid, S.,. and Mouftah,H. T., "Congestion Control Methods for
B-ISDN," IEEE Communications Magazine, Vol.30, No.7, pp. 42-47 (1992).
POINTERS TO818UOGRAPHIES ONTHE INTERNET
Bibliographies containing references on Computer Networking can be found at:
ftp:ftp.cs.umanitoba.ca/publbibliographieslDistributedlnetwork.html
ftp:ftp.cs.umanitoba.ca/publbibliographieslDistributedlCCR.html

Chap. 2 • Pointers toBibliographies ontheInternet 113
Bibliography containing references on Communication and Routing in Interconnection
Networks can be found at:
ftp:ftp.cs.umanitoba.calpub/bibliographieslParallel/par.comm.html
Bibliography containing references on the Internet can be found at:
ftp:ftp.cs.umanitoba.ca/pub/bibliographieslMisc/internet.html
Bibliography containing references on Gigabit or High-Speed Networks can be found
at:
ftp:ftp.cs.umanitoba.ca/pub/bibliographies/Distributed/gigabit.html
Bibliography containing references on ATMNetworks can be found at:
ftp:ftp.cs.umanitoba.calpublbibliographieslDislributed/ATM.html

3
CHAPTER
MessQge Passing
3.1 INTRODumON
A process is a program in execution. When we say that two computers of a distributed
system arecommunicating witheach other, we mean that two processes, one running on
each computer, are in communication with each other.Ina distributed system, processes
executing on different computers often need to communicate with each other to achieve
some common goal. For example, each computer of a distributed system may have a
resourcemanagerprocesstomonitorthecurrent statusofusageofitslocalresources,and
theresource managersofallthecomputers mightcommunicate witheachother fromtime
to time to dynamically balance the system load among all the computers. Therefore, a
distributed operating system needs to provide interprocess communication (lPC)
mechanisms to facilitate such communication activities.
Interprocess communication basically requires information sharing among two or
more processes.The two basic methods for information sharing are as follows:
1. Original sharing, or shared-data approach
2. Copy sharing, or message-passing approach
In the shared-data approach, the information to be shared is placed in a common
memory area that is accessible to all the processes involved in an IPC. The shared-data
114

Sec.3.2 • Desirable Features ofaGoodMessage-Passing System 115
paradigm gives theconceptualcommunicationpattern illustrated in Figure 3.1(a). On the
other hand, in the message-passing approach, the information to be shared is physically
copied from the sender process's address space to the address spaces of all the receiver
processes, and this isdone by transmitting the data to becopied in the form of messages
(amessageisablockofinformation). The message-passingparadigm givestheconceptual
communication pattern illustrated in Figure 3.1(b). That is, the communicatingprocesses
interact directly with each other.
Sharedcommon
memoryarea
(a)
(b)
Fig. 3.1 The two basic interprocesscommunicationparadigms: (a) The shared-data
approach. (b)The message-passingapproach.
Since computersinanetwork donotshare memory,processes inadistributed system
normally communicate by exchanging messages rather than through shared data.
Therefore, message passing is the basic IPe mechanism in distributed systems.
A message-passing system is a subsystem of a distributed operating system that
provides a set of message-based IPe protocols and does so by shielding the details of
complex network protocols and multiple heterogeneous platforms from programmers. It
enables processes to communicate by exchanging messages and allows programs to be
written byusing simple communicationprimitives, suchassendandreceive. Itserves asa
suitableinfrastructureforbuildingotherhigherlevellPCsystems,suchasremoteprocedure
call (RPC; seeChapter4)anddistributed shared memory (DSM;seeChapter5).
3.2 DESIRABlE FEATURES OF AGOOD MESSAGE·PASSING
SYSTEM
3.2.1 Simplicity
A message-passing system should be sitnple and easy to use. It must be straightforward
to construct new applications and to communicate with existing ones by using the
primitives provided by the message-passing system. It should also be possible for a

116 Chap. 3 • Message Passing
programmertodesignatethedifferentmodulesofadistributedapplicationand tosend and
receivemessagesbetweenthem in a way as simpleas possiblewithoutthe need to worry
about the system and/or network aspects that are not relevant for the application level.
Clean and simple semantics ofthe IPe protocols ofa message-passing system make it
easier to build distributed applications and to get them right.
3.2.1 Unlrorm Semantics
In a distributed system, a message-passing system may beused for the following two
types ofinterprocess communication:
1. Local communication, in which the communicating processes are on the same
node
2. Remote communication, in which the communicating processes are on different
nodes
An important issue in the design ofa message-passing system is that the semantics
ofremote communications should be as close as possible to those oflocal communica
tions. This is an important requirement for ensuring that the message-passing system is
easy to use.
3.2.3 Efficiency
Efficiency is.normally a critical issue for a message-passing system to be acceptable by
the users. Ifthe message-passingsystemisnot efficient, interprocesscommunicationmay
becomesoexpensivethatapplicationdesignerswillstrenuouslytrytoavoiditsuseintheir
applications. As aresult, the developed application programs would be distorted. An IPe
protocol ofa message-passing system can be made efficient by reducing the number of
message exchanges, as far as practicable, during the communication process. Some
optimizations normally adopted for efficiency include the following:
• Avoiding the costs ofestablishing and terminatingconnections between the same
pairofprocesses for each and every message exchange between them
• Minimizing the costs ofmaintaining the connections
• Piggybacking of acknowledgment of previous messages with the next message
duringaconnectionbetweenasenderand areceiverthat involvesseveralmessage
exchanges
3.1.4 Reliability
Distributed systems are prone to different catastrophic events such as node crashes or
communication link failures.. Such events may interrupt a communication that was in
progressbetweentwoprocesses,resultingintheloss ofamessage. AreliableIPeprotocol
can copewith failureproblemsand guaranteesthedeliveryofamessage. Handlingoflost

Sec.3.2 • Desirable Features of aGood Message-Passing System 117
messages usually involves acknowledgments and retransmissions on the basis of
timeouts.
Another issue related toreliability isthatofduplicate messages. Duplicate messages
may besentintheevent offailures orbecause oftimeouts.Areliable fPCprotocol isalso
capable of detecting and handling duplicates. Duplicate handling usually involves
generating and assigning appropriate sequence numbers to messages.
Agood message-passing system musthave IPCprotocols tosupport thesereliability
features.
3.2.5 Correctness
Amessage-passing system often hasIPCprotocols forgroup communicationthatallow a
sender tosend amessage toagroup of receivers and areceiver toreceive messages from
several senders. Correctness is a feature related to IPC protocols for group communica
tion. Although not always required, correctness may be useful for some applications.
Issues related to correctness are as follows [Navratnam et al. 1988]:
• Atomicity
• Ordered delivery
• Survivability
Atomicity ensures that every message sent to a group of receivers willbe delivered
toeither allof themor noneof them. Ordered delivery ensures that messages arrive atall
receivers inanorder acceptable tothe application. Survivability guarantees thatmessages
will bedelivered correctly despite partial failures ofprocesses, machines, orcommunica
tion links. Survivability is a difficult property to achieve.
3.2.6 Flexibility
Not all applications require the same degree of reliability and correctness of the fPC
protocols. For example, in adaptive routing, it may be necessary to distribute the
information regarding queuing delays in different parts of the network. A broadcast
protocolcould beusedforthispurpose. However,ifabroadcastmessageislateincoming,
due to communication failures, it might just as well not arrive at all as it will soon be
outdated by a more recent one anyway. Similarly, many applications do not require
atomicity or ordered delivery of messages. For example, aclient maymulticast arequest
message to agroup of servers and offer thejob to the first server thatreplies. Obviously,
atomicity of message delivery is not required in this case. Thus the IPC protocols of a
message-passing system must beflexible enough tocater tothevariousneedsofdifferent
applications. That is, the IPC primitives should be such that the users have the flexibility
to choose and specify the types and levels of reliability and correctness requirements of
their applications. Moreover, IPC primitives must also have the flexibility to permit any
kind of control flow between the cooperating processes, including synchronous and
asynchronous send/receive.

118 Chap. 3 • Message Passing
A good message-passing system must also be capable of providing a secure end-to-end
communication. That is,a message in transit on the network should not be accessible to
any user other than those to whom it is addressed and the sender. Steps necessary for
secure communication include the following:
• Authentication of the receiver(s) of a message by the sender
• Authentication of the sender of a message by its receiver(s)
• Encryption of a message before sending it over the network
These issues will bedescribed in detail in Chapter 11.
3.1.8 Portability
There are two different aspects of portability in a message-passing system:
1. The message-passing system should itself be portable. That is, it should be
possible to easily construct a new IPC facility on another system by reusing the basic
design of the existing message-passing system.
2. The applications written by using the primitives of the IPC protocols of the
message-passing system should be portable. This requires that heterogeneity must be
considered while designing a message-passing system. This may require the use of an
external data representation format for the communications taking place between two or
moreprocesses runningoncomputers ofdifferent architectures. The design of high-level
primitivesfortheIPCprotocolsofamessage-passing systemshouldbedonesoastohide
the heterogeneous nature of the network.
3.3 ISSUES INIPC BY MESSAGE PASSING
Amessageisablockofinformationformattedbyasendingprocess insuchamannerthat
it is meaningful to the receiving process. It consists of a fixed-length header and a
variable-size collection of typeddata objects.Asshown in Figure 3.2, the header usually
consists of the following elements:
• Address. It contains characters that uniquely identify the sending and receiv
ing processes in the network. Thus, this element has two parts-one part is
the sending process address and the other part is the receiving process
address.
• Sequence number. This is the message identifier (ID), which is very useful
for identifying lost messages and duplicate messages in case of system
failures.

Sec.3.3 • Issues inIPCbyMessage Passing 119
Structuralinformation Addresses
Actualdata Sequence
orpointer Numberof Type number ReceivingSending
tothedata bytes/ ormessage 10 process process
elements address address
~ 141-------- ..I
Variable- Fixed-lengthheader-----
-4.....
sizecollection
oftypeddata
Fig.3.2 Atypical message structure.
• Structural information. This element also has two parts. The type part specifies
whether thedata tobepassed ontothe receiver isincluded withinthemessage or
themessageonlycontainsapointertothedata, whichisstoredsomewhereoutside
the contiguous portion of the message. The second part of this element specifies
the length of the variable-size message data.
In a message-oriented IPC protocol, the sending process determines the actual
contents of a message and the receiving process is aware of how to interpret the
contents. Special primitives are explicitly used for sending and receiving the mes
sages. Therefore, in this method, the users are fully aware of the message formats
used in the communication process and the mechanisms used to send and receive
messages.
In the design of an IPe protocol for a message-passing system, the following
important issues need to be considered:
• Who is the sender?
• Who is the receiver?
• Is there one receiver or many receivers?
• Is the message guaranteed to have been accepted by its receiver(s)?
• Does the sender need to wait for a reply?
• What should be done if a catastrophic event such as a node crash or a
communication link failure occurs during the course of communication?
• What should be done if the receiver is not ready to accept the message: Will the
message bediscarded or stored ina buffer? In the case of buffering, whatshould
be done if the buffer is full?
• Ifthere areseveral outstanding messages forareceiver,can itchoose theorder in
which to service the outstanding messages?
These issues are addressed by the semantics of the set of communication primitives
provided by the fPC protocol. A general description of the various ways in which these
issues are addressed by message-oriented IPC protocols is presented below.

120 Chap.3 • Message Passing
3.4 SYNCHRONIZATION
A central issue in the communication structure is the synchronization imposed on the
communicating processes by the communication primitives. The semantics used for
synchronizationmaybebroadlyclassified asblockingandnonblockingtypes.Aprimitive
issaid tohave nonblocking semantics ifitsinvocation does notblock theexecution ofits
invoker (the control returns almost immediately to the invoker); otherwise a primitive is
said to be of the blocking type. The synchronization imposed on the communicating
processes basically depends on one of the two types of semantics used for the sendand
receiveprimitives.
In case of a blocking send primitive, after execution of the send statement, the
sending process isblockeduntil itreceives anacknowledgmentfrom thereceiver that the
message has been received. On the other hand, for nonblocking send primitive, after
execution of the send statement, the sending process is allowed to proceed with its
execution as soon as the message has been copied to a buffer.
In the case of a blocking receiveprimitive, after execution of the receivestatement,
the receiving process is blocked until it receives a message. On the other hand, for a
nonblocking receiveprimitive, the receiving process proceeds with its execution after
execution ofthe receivestatement, which returns control almost immediately just after
telling the kernel where the message buffer is.
An important issue in a nonblocking receiveprimitive is how the receiving process
knows that the message has arrived in the message buffer. One of the following two
methods is commonly used for this purpose:
1. Polling.In thismethod,atestprimitive isprovided toallow thereceiver tocheck
the buffer status. The receiver uses this primitive to periodically poll the kernel to check
if the message is already available in the buffer.
2. Interrupt.In this method, when the message has been filled in the buffer and is
ready for use by the receiver, asoftware interrupt is used to notify the receiving process.
This method permits the receiving process to continue with itsexecution without having
to issue unsuccessful test requests. Although this method is highly efficient and allows
maximum parallelism, its maindrawback is that user-level interrupts make programming
difficult [Tanenbaum 1995].
A variant of the nonblocking receiveprimitive is the conditional receiveprimitive,
which also returns control to the invoking process almost immediately, either with a
message or with an indicator that no message is available.
In a blocking send primitive, the sending process could get blocked forever in
situations where the potential receiving process hascrashed or thesent message has been
lostonthe network duetocommunicationfailure.Toprevent thissituation, blockingsend
primitivesoften useatimeout valuethatspecifies anintervaloftimeafter whichthesend
operation is terminated with an error status. Either the timeout value may be a default
value or the users may beprovided with theflexibility to specify itas a parameter of the
sendprimitive.

Sec.3.4 • Synchronization 121
Atimeout value may also beassociated with ablocking receiveprimitiveto prevent
the receiving process from getting blocked indefinitely in situations where the potential
sending process hascrashed or theexpected message has been lost on thenetwork due to
communication failure.
When both the send and receive primitives of a communication between two
processes useblocking semantics, thecommunicationissaidtobesynchronous;otherwise
it is asynchronous. That is, for synchronous communication, the sender and the receiver
must be synchronized to exchange a message. This is illustrated in Figure 3.3.
Conceptually, thesending process sendsamessage tothereceiving process, thenwaitsfor
an acknowledgment. After executing the receivestatement, the receiver remains blocked
until it receives the message sent by the sender. On receiving the message, the receiver
sendsanacknowledgmentmessage tothesender.The senderresumes execution onlyafter
receiving this acknowledgment message.
Sender's Receiver's
execution execution
Receive(message);
Iexecutionsuspended
I
I
Send(message); I
I
executionsuspended
I
I I
I I
I Executionresumed
I
I
I Send(acknowledgment)
I
I
ExecutionresumedI
Blockedstate
Executingstate
Fig.3.3 Synchronous mode ofcommunicationwith bothsendand receiveprimitives
having blocking-typesemantics.
Ascompared toasynchronouscommunication,synchronouscommunicationissimple
and easy to implement. It also contributes to reliability because it assures the sending
process that itsmessage hasbeen accepted before the sending process resumes execution.
As a result, if the message gets lost or is undelivered, no backward error recovery is

122 Chap. 3 • Message Passing
necessaryforthesendingprocesstoestablishaconsistentstateandresumeexecution[Shatz
1984]. However, the main drawback of synchronous communication is that it limits
concurrency and is subject to communication deadlocks (communication deadlock is
described inChapter6). It is less flexible than asynchronouscommunicationbecause the
sendingprocessalwayshastowaitforanacknowledgmentfromthereceivingprocesseven
whenthisisnotnecessary.Inasystemthatsupports multiplethreads inasingleprocess(see
Chapter 8), the blocking primitives can be used without the disadvantage of limited
concurrency. Howthisismadepossible isexplainedinChapter8.
A flexible message-passing system usually provides both blocking and nonblocking
primitivesforsendand receive sothatuserscanchoose themostsuitable one tomatch the
specific needs of their applications.
3.5 BUFFERING
Messages can betransmitted from one process to another by copying the body of the
message fromtheaddressspaceofthesendingprocesstotheaddress spaceofthereceiving
process (possibly via the address spaces of the kernels of the sending and receiving
computers). In some cases, the receiving process may not be ready to receive a message
transmittedtoitbutitwantstheoperating systemtosavethatmessage forlaterreception. In
thesecases,theoperatingsystemwillrelyonthereceiverhavingabufferinwhichmessages
canbestored priortothereceiving processexecuting specificcode toreceive themessage.
In interprocesscommunication, the message-buffering strategy is strongly related to
synchronization strategy. The synchronous and asynchronous modes of communication
correspond respectively to the two extremes of buffering: a null buffer, or no buffering,
and abufferwith unboundedcapacity. Other two commonly used buffering strategies are
single-message and finite-bound, or multiple-message, buffers. These four types of
buffering strategies are described below.
3.5.1 Null luff.r (orNoluff.rlng)
In case of no buffering, there is no place to temporarily store the message. Hence one of
the following implementation strategies may be used:
1. The message remains in the sender process's address space and the execution of
the sendis delayed until the.receiverexecutes the corresponding receive. Todo this, the
sender process is backed up and suspended in such a way that when it is unblocked, it
starts by reexecuting the send statement. When the receiver executes receive, an
acknowledgment is sent to the sender's kernel saying that the sender can now send the
message. Onreceiving theacknowledgmentmessage, thesenderisunblocked,causing the
sendtobeexecutedonceagain.Thistime,themessage issuccessfullytransferred fromthe
sender's address space to the receiver's address space because the receiver is waiting to
receive the message.
2. The message issimply discarded andthetimeout mechanismisusedtoresend the
message after atimeout period. That is,after executing send, the sender process waits for

Sec.3.5 • Buffering 123
an acknowledgment from the receiver process. If no acknowledgment is received within
the timeout period, it assumes that its message was discarded and tries again hoping that
this time the receiver has already executed receive. The sender may have to try several
times before succeeding. The sender gives up after retrying for a predecided number of
times.
As shown in Figure 3.4(a), in the case of no buffering, the logical path ofmessage
transfer is directly from the sender's address space to the receiver's address space,
involving a single copy operation.
Sending Receiving
~~------------~@
(a)
Sending Receiving
process process
Node
boundary
(b)
Sending
process
IMessage21
IMessage31
Multiple·message
buffer/mailbox/port
(c)
Fig. 3.4 The three types ofbuffering strategies used ininterprocesscomunication
mechanisms: (a) Message transfer insynchronoussend withnobuffering
strategy (onlyone copy operation isneeded). (b) Message transfer in
synchronoussend withsingle-messagebuffering strategy (two copy
operationsare needed). (c) Message transfer inasynchronoussend with
multiple-messagebuffering strategy (two copy operationsare needed).

124 Chap. 3 • Message Passing
3.5.1 Single-Messap luff.r
The null bufferstrategy isgenerallynotsuitablefor synchronouscommunicationbetween
two processes in adistributed system because if the receiver is not ready, a message has
to be transferred two or more times, and the receiver ofthe message has to wait for the
entire time taken to transfer the message across the network. In a distributed system,
message transfer across the network may require significant time in some cases.
Therefore, instead of using the null buffer strategy, synchronous communication
mechanisms in network/distributed systems use a single-message buffer strategy. In this
strategy, a buffer having a capacity to store a single message is used on the receiver's
node. This is because in systems based on synchronous communication, an application
module may have at most one message outstanding at a time. The main idea behind the
single-message buffer strategy is to keep the message ready for use at the location of the
receiver. Therefore, inthismethod, the request messageisbuffered on the receiver'snode
if the receiver is not ready to receive the message. The message buffer may either be
located in the kernel's address space or inthe receiverprocess'saddress space. As shown
in Figure 3.4(b), in this case the logical path of message transfer involves two copy
operations.
3.5.3 Unbounded-Capacity luffe,
Intheasynchronousmodeofcommunication,sinceasenderdoes notwait forthereceiver
tobeready,there maybeseveral pendingmessagesthat have notyet been acceptedbythe
receiver. Therefore, an unbounded-capacity message buffer that can store all unreceived
messages is needed to support asynchronous communication with the assurance that all
the messages sent to the receiver will be delivered.
3.5.4 Flnlte-80und (orMultlpl.-MuSQge) lu".r
Unboundedcapacity ofa buffer is practically impossible. Therefore, in practice, systems
using asynchronous mode of communication use finite-bound buffers, also known as
multiple-messagebuffers. When the buffer has finite bounds, astrategy isalso needed for
handling the problem of a possible bufferoverflow. The buffer overflow problemcan be
dealt with in one ofthe following two ways:
1. Unsuccessful communication. In this method, message transfers simply fail
wheneverthere isnomore bufferspace.Thesendnormally returns anerrormessagetothe
sending process,indicatingthatthemessagecould notbedeliveredtothereceiverbecause
the buffer is full. Unfortunately, the use of this method makes message passing less
reliable.
2. Flow-controlledcommunication.The second method istouseflowcontrol,which
means that the senderis blocked until the receiver accepts some messages, thus creating

Sec.3.6 • Multidatagram Messages 125
spaceinthe buffer for newmessages. This method introduces asynchronizationbetween
thesender andthereceiver andmay result inunexpected deadlocks. Moreover,due tothe
synchronization imposed, the asynchronous send does not operate in the truly
asynchronous mode for all send commands.
The amount of buffer space to be allocated in the bounded-buffer strategy is a
matter of implementation. In the most often used approach, a create_buffer system
call is provided to the users. This system call, when executed by a receiver process,
creates a buffer (sometimes called a mailbox or port) of a size specified by the
receiver. The receiver's mailbox may be located either in the kernel's address space
or in the receiver process's address space. If it is located in the kernel's address
space, mailboxes are a system resource that must be allocated to processes as and
when required. This will tend to limit the number of messages that an individual
process may keep in its mailbox. On the other hand,if the mailbox is located in the
receiver process's address space, the operating system will have to rely on the
process allocating an appropriate amount of memory, protecting the mailbox from
mishaps, and so on.
As shown in Figure 3.4(c), in the case of asynchronous send with bounded-buffer
strategy,themessage isfirstcopied fromthesendingprocess'smemory intothereceiving
process's mailbox and then copied from the mailbox to the receiver's memory when the
receiver calls for the message. Therefore, in this case also, the logical path of message
transfer involves two copy operations.
Although message communication based on multiple-message-buffering capability
provides betterconcurrencyandflexibility ascompared tonobufferingorsingle-message
buffering, it is more complex to design and use. This is because of the extra work and
overhead involved in the mechanisms needed for the creation, deletion, protection, and
other issues involved in buffer management.
3.6 MUlTIDATAGRAM MESSAGES
Almost all networks have an upper bound on the size of data that can be transmitted at a
time. This size is known as the maximum transfer unit (MTU) of a network.A message
whose sizeisgreater than the MTU hasto befragmented into multiples oftheMTU, and
then each fragment has to be sent separately. Each fragment is sent in a packet that has
some control information in addition to the message data. Each packet is known as a
datagram. Messages smaller than theMl'Uof the network can be sent ina single packet
andare known assingle-datagrammessages. Ontheother hand, messages largerthan the
MTU of the network have tobe fragmented and sent inmultiple packets. Such messages
are known as multidatagram messages. Obviously, different packets of a multidatagram
message bear a sequential relationship to one another. The disassembling of a
multidatagram message into multiple packets on the sender side and the reassembling of
the packets on the receiver side is usually the responsibility of the message-passing
system.

126 Chap. 3 • Message Passing
3.7 ENCODING AND DECODING OFMESSAGE DATA
A message data should be meaningful to thereceiving process. This implies that, ideally,
thestructure ofprogramobjects shouldbepreserved whiletheyarebeing transmittedfrom
theaddress spaceofthesending process totheaddressspaceofthereceivingprocess.This
obviously is not possible in a heterogeneous system in which the sending and receiving
processes are on computers of different architectures. However, even in homogeneous
systems, itis very difficult to achieve this goal mainly because oftwo reasons:
1. An absolute pointer value loses its meaning when transferred from one process
address space toanother.Therefore, suchprogram objects that useabsolute pointervalues
cannot be transferred in their original form, and some other form of representation must
be used to transfer them. For example, to transmit a tree object, each element of the tree
mustbecopied inaleafrecord andproperly aligned insomefixedorder inabuffer before
it can be sent to another process. The leaf records themselves have no meaning in the
address space of the receiving process, but the tree can be regenerated easily from them.
To facilitate such regeneration, object-type information must be passed between the
senderandreceiver,indicating notonly thatatreeobject isbeingpassed butalso theorder
inwhichtheleafrecords arealigned.Thisprocess offlattening andshaping oftreeobjects
also extends to other structured program objects, such as linked lists.
2. Different program objects occupy varying amount of storage space. To be
meaningful, a message must normally contain several types of program objects, such as
long integers, short integers, variable-length character strings, and so on. In this.case, to
make the message meaningful tothe receiver, there must besome way for the receiverto
identify which program object isstored where inthemessage buffer and how much space
each program object occupies.
Due to the problems mentioned above in transferring program objects in their
original form, theyarefirstconvertedtoastreamformthatissuitable fortransmissionand
placed into a message buffer.This conversion process takes place on the sender side and
is known as encoding ofa message data. The encoded message, when received by the
receiver, must be converted back from the stream form to the original program objects
before itcanbeused.Theprocess ofreconstructionofprogram objects from message data
on the receiver side is known as decoding of the message data.
Oneofthefollowing tworepresentationsmaybeusedfortheencodingand'decoding
of a message data:
1. In tagged representation the type of each program object along with its value is
encoded in the message. In this method, it is a simple matter for the receiving process to
check thetypeofeachprogram object inthemessage becauseoftheself-describingnature
of the coded data format.
2. In untaggedrepresentation the message data only contains program objects. No
informationisincluded inthe message datatospecify thetypeofeach program object. In

Sec.3.8 • ProcessAddressing 127
this method, the receiving process must have a prior knowledge of how to decode the
received data because the coded data format is not self-describing.
The untagged representation is used in Sun XDR (eXternal Data Representation)
[Sun 1990] and Courier [Xerox 1981], whereas the tagged representation is used in the
ASN.1 (Abstract Syntax Notation) standard [CCITf 1985] and the Mach distributed
operating system [Fitzgerald and Rashid 1986].
In general, tagged representation is more expensive than untagged representation,
both in terms of the quantity of data transferred and the processing time needed at each
sidetoencode anddecode themessage data.No matter whichrepresentationisused, both
thesenderandthereceiver mustbefullyawareoftheformatofdatacoded inthemessage.
The sender possesses the encoding routine for the coded data format and the receiver
possesses thecorresponding decoding routine. The encoding anddecoding operations are
perfectly symmetrical in the sense that decoding exactly reproduces the data that was
encoded, allowing for differences in local representations. Sometimes, a receiver may
receive a badly encoded data, such as encoded data that exceeds a maximum-length
argument. In such a situation, the receiver cannot successfully decode the received data
and normally returns an error message to the sender indicating that the data is not
intelligible.
3.8 PROCESS ADDRESSING
Another important issue in message-based communication is addressing (or naming) of
the parties involved in an interaction: To whom does the sender wish to send its
message and, conversely, from whom does the receiver wish to accept a message? For
greater flexibility, a message-passing system usually supports two types of process
addressing:
1. Explicit addressing. The process with which communication is desired is
explicitly named as a parameter in the communication primitive used. Primitives (a)
and (b) of Figure 3.5 require explicit process addressing.
2. Implicit addressing. A process willing to communicate does not explicitly name
a process for communication. Primitives (c) and (d) of Figure 3.5 support implicit
process addressing. In primitive (c), the sender names a service instead of a process.
This type of primitive is useful in client-server communications when the client is not
concerned with which particular server out of a set of servers providing the service
desired by the client actually services its request. This type of process addressing is also
known as functional addressing because the address used in the communication
primitive identifies a service rather than a process.
On the other hand, in primitive (d), the receiver is willing to accept a message
from any sender. This type of primitive is again useful in client-server communications
when the server is meant to service requests of all clients that are authorized to use its
service.

128 Chap. 3 • Message Passing
(a)send(process_id,message)
Sendamessagetotheprocessidentifiedby"process_id".
(b)receive (process_id,message)
Receiveamessagefromtheprocessidentifiedby"process_id".
(c)send_any (service_id,message)
Sendamessagetoanyprocessthatprovidestheserviceoftype
"service_ief'.
(d)receive_any (process_id,message)
Receiveamessagefromanyprocessandreturntheprocess
identifier("process_id")oftheprocessfromwhichthemessage
wasreceived.
Fig.3.5 Primitives forexplicit andimplicitaddressingofprocesses.
Withthetwobasictypesofprocessaddressing usedincommunicationprimitives, we
now look at the commonly used methods for process addressing.
Asimple method toidentify aprocess isbyacombination ofmachine_idand local_
id, such as machine_id@local_id. The local_id part is a process identifier, or a port
identifierof a receiving process, or something else that can be used to uniquely identify
aprocess on amachine.Aprocess willing tosend amessage toanother process specifies
thereceivingprocess'saddress intheformmachine_id@local_id. Themachine_idpartof
the address is used bythe sending machine's kernel tosend the message to the receiving
process's machine, and the local_id part of the address is then used by the kernel of the
receiving process'smachinetoforwardthemessagetotheprocessforwhichitisintended.
This method of process addressing is used in Berkeley UNIX with 32-bit Internet
addresses for machine_idand 16-bit numbers for local_id.
An attractive feature of this method is that no global coordination is needed to
generate systemwide unique process identifiers because local_ids need tobeunique only
for one machine and can be generated locally without consultation with other machines.
However, a drawback of this method is that it does not allow a process to migrate from
one machine to'another if such a need arises. For instance, one or more processes of a
heavily loaded machine may be migrated to a lightly loaded machine to balance the
overall system load.
To overcome the limitation of the above method, processes can be identified by a
combination of the following three fields: machineld, local_id, andmachineid.
1. The first field identifies the node on which the process is created
2. The second fieldisalocalindentifier generated bythenodeonwhich theprocess
is created
3. The third field identifies the last known location (node) of the process.
During the lifetime of a process, the values of the first two fields of its identifier
never change; the third field, however, may.This method ofprocess addressing isknown
as link-basedprocess addressing. For this method to work properly, when a process is

Sec.3.8 • Process Addressing 129
migrated from its current node to a new node, a linkinformation (process identifier with
the value of its third field equal to the machineidof the process's new node) is left on
itsprevious node, and on the newnode, anew local_idisassigned tothe process, and its
process identifier and the new local_id is entered in a mapping table maintained by the
kernelofthenewnodeforallprocessescreated onanother nodebutrunningonthisnode.
Note that the value of the third field of a process identifier is set equal to its first field
when the process is created.
A process willing to send a message to another process specifies the receiving
process's address in the form, say, machine_id@local_id@machine_id. The kernel of
the sending machine delivers the message to the machine whose machine_idis specified
in the third field of the receiving process's address. If the value of the third field is
equal to the first field, the message will be sent to the node on which the process was
created. If the receiving process was not migrated, the message is delivered to it by
using the local_id information in the process identifier. On the other hand, if the
receiving process was migrated, the link information left for it on that node is used to
forward the message to the node to which the receiving process was migrated from this
node. In this manner, the message may get forwarded from one node to another several
times before it reaches the current node of the receiving process. When the message
reaches the current node of the receiving process, the kernel of that node extracts the
process identifiers of the sending and receiving processes from the message. The first
two fields of the process identifier of the receiving process are used as its unique
identifier to extract its localid from the mapping table and then to deliver the message
to the proper process. On the other hand, the process identifier of the sending process is
used to return to it the current location of the receiving process. The sending process
uses this information to update the value of the third field of the receiving process's
identifier, which it caches in a local cache, so that from the next time the sending
process can directly send a message for the receiving process to this location of the
receiving process instead of sending it via the node on which the receiving process was
created. A variant of this method of process addressing is used in DEMOSIMP [Miller
et a1. 1987] and Charlotte [Artsy et al. 1987]. Although this method of process
addressing supports the process migration facility, it suffers from two main drawbacks:
1. The overload of locating a process may be large if the process has migrated
several times during its lifetime.
2. It maynot be possible to locate a process if an intermediate node on which the
process once resided during its lifetime is down.
Bothprocess-addressingmethods previously described arenontransparent duetothe
needtospecifythemachine identifier.Theuseriswellawareofthelocationoftheprocess
(or at least the location on which the process wascreated). However, we saw in Chapter
1that location transparency is one of the main goals of a distributed operating system.
Hence a location-transparent process-addressing mechanism is more desirable for a
message-passing system. A simple method to achieve this goal is to ensure that the
systemwide unique identifier of a process does not contain an embedded machine
identifier.A centralized process identifier allocator that maintains a counter can be used

130 Chap. 3 • Message Passing
for thispurpose. When itreceives arequest for anidentifier,it simplyreturns thecurrent
value ofthe counter and then increments it by 1.This scheme, however, suffers from the
problems of poor reliability and poor scalability.
Another methodtoachieve thegoaloflocation transparency inprocess addressing is
to use a two-level naming scheme for processes. In this method, each process has two
identifiers: a high-level name that is machine independent (an ASCII string) and a low
level name that is machine dependent (such as machine_id@local_id). A name server is
used to maintain a mapping table that maps high-level names ofprocesses to their low
levelnames. Whenthismethodofprocess addressing isused,aprocess thatwantstosend
amessage toanother process specifies thehigh-level nameofthereceiving process inthe
communication primitive. The kernel of the sending machine first contacts the name
server (whose address is well known to all machines) to get the low-level name of the
receiving processfromitshigh-level name.Usingthelow-levelname,thekernelsendsthe
message to the proper machine, where the receiving kernel delivers the message to the
receiving process. The sendingkernel alsocaches thehigh-level name tolow-level name
mapping information ofthe receiving process in a local cache for future use, so that the
name server need not becontacted when a message has to be sent again to the receiving
process.
Notice thatthenameserverapproach allows aprocess tobemigrated fromonenode
toanother without the need tochange thecode in the program of any process that wants
to communicate with it.This is because when a process migrates its low-level identifier
changes, andthischangeisincorporated inthenameserver'smapping table.However,the
high-level name of the process remains unchanged.
The name server approach is also suitable for functional addressing. In this case, a
high-level name identifies a service instead of a process, and the name server maps a
service identifier to one or more processes that provide that service.
Thename serverapproach alsosuffersfromtheproblemsofpoorreliability andpoor
scalability because the name server isacentralized component ofthesystem. One wayto
overcome these problems is to replicate the name server. However, this leads to extra
overhead needed in keeping the replicas consistent.
3.9 FAilURE HANDLING
While a distributed system may offer potential for parallelism, itis also prone to partial
failures such as a node crash or a communication link failure. As shown in Figure 3.6,
during interprocess communication, such failures may lead to the following problems:
1. Loss of request message. This may happen either due to the failure of
communication link between the sender and receiver or because the receiver's node is
down at the time the request message reaches there.
2. Loss of response message. This may happen either due to the failure of
communicationlinkbetweenthesenderandreceiverorbecausethesender'snodeisdown
at the time the response message reaches there.

Sec. 3.9 • Failure Handling 131
Sender Receiver
Send request
Lost
(a)
Sender
Send request
Successful request
execution
Send response
Lost
(b)
Sender Receiver
Send request
\Restarted
(c)
Fig.3.6 Possible problems inIPe due todifferent types ofsystem failures. (a)
Request message islost.(b) Response message is lost.(c) Receiver's
computercrashed.
3. Unsuccessful execution ofthe request. This happens due to the receiver's node
crashing while the request is being processed.
Tocope with these problems, areliable IPC protocol ofamessage-passing system is
normallydesigned basedontheideaofinternal retransmissionsofmessagesaftertimeouts
and the return of an acknowledgment message to the sending machine's kernel by the
receiving machine's kernel. That is, the kernel of the sending machine is responsible for
retransmitting the message after waiting for a timeout period if no acknowledgment is
received from the receiver's machine within thistime. The kernel ofthe sending machine
frees the sending process only when the acknowledgment is received. The time duration

132 Chap. 3 • Message Passing
forwhichthesenderwaitsbeforeretransmittingtherequestisnormallyslightlymorethan
the approximate round-trip time between the sender and the receiver nodes plus the
averagetime required for executing the request.
Based on the above idea, a four-message reliable IPC protocol for client-server
communication between two processes works as follows (see Fig. 3.7):
Client
Fig. 3.7 The four-message reliable IPC
protocol forclient-server
---- Blockedstate communicationbetween two
-.- Executingstate processes.
1. The client sends a request message to the server.
2. When the request message is received at the server's machine, the kernel of that
machine returns an acknowledgment message to the kernel of the client machine. If the
acknowledgment is not received within the timeout period, the kernel of the client
machine retransmits the request message.
3. Whentheserverfinishesprocessing theclient's request,itreturnsareplymessage
(containing the result of processing) to the client.
4. When the reply message is received at the client's machine, the kernel of that
machine returns an acknowledgment message to the kernel of the server machine. If the
acknowledgment message is not received within the timeout period, the kernel of the
server machine retransmits the reply message.

Sec.3.9 • Failure Handling 133
In client-server communication, the result of the processed request is sufficient
acknowledgmentthatthe request message was receivedby the server. Basedon this idea,
a three-message reliable IPC protocol for client-server communication between two
processes works as follows (see Fig. 3.8):
Fig.3.8 Thethree-message reliable IPC
protocol forclient-server
communication between two ----- Blocked state
processes. -- Executingstate
1. The client sends a request message to the server.
2. Whenthe serverfinishesprocessingtheclient'srequest, itreturnsareplymessage
(containing the result of processing) to the client. The client remains blocked until the
reply is received. If the reply is not received within the timeout period, the kernel ofthe
client machine retransmits the request message.
3. When the reply message is received at the client's machine, the kernel of that
machine returns an acknowledgment message to the kernel ofthe server machine. If the
acknowledgment message is not received within the timeout period, the kernel of the
server machine retransmits the reply message.
In the protocol ofFigure 3.8, a problem occurs if a request processing takes a long
time. If the request message is lost, it will be retransmitted only afterthe timeoutperiod,
which has been set to a large value to avoid unnecessary retransmissions ofthe request
message. On the other hand, if the timeout value is not set properly taking into
considerationthe long time neededfor requestprocessing, unnecessaryretransmissionsof

134 Chap. 3 • Message Passing
the request message will take place. The following protocol may be used to handle this
problem:
1. The client sends a request message to the server.
2. When the request message isreceived at the server's machine, the kernel of that
machine startsatimer.Iftheserverfinishes processing theclient'srequest andreturnsthe
replymessagetotheclientbeforethetimerexpires,thereplyservesastheacknowledgment
oftherequest message. Otherwise, aseparate acknowledgment issentbythekernelofthe
servermachine toacknowledge therequestmessage.Ifanacknowledgmentisnotreceived
within the timeout period, the kernel of the client machine retransmits the request
message.
3. When the reply message is received at the client's machine, the kernel of that
machine returns an acknowledgment message to the kernel of the server machine. If the
acknowledgment message is not received within the timeout period, the kernel of the
server machine retransmits the reply message.
Notice that the acknowledgment message from client to server machine in the
protocol of Figure 3.8 is convenient but not a necessity. This is because if the reply
message is lost, the request message will be retransmitted after timeout. The server can
process the request once again and return the reply to the client. Therefore, a message
passingsystemmaybedesigned tousethefollowing two-message IPCprotocol forclient
server communication between two processes (see Fig. 3.9):
Fig.3.9 Thetwo-message IPCprotocol used
inmanysystems forclient-server
- - - Blockedstate communication between two
-- Executingstate
processes.

Sec.3.9 • Failure Handling 135
1. Theclient sends arequest message tothe server andremains blocked untilareply
is received from the server.
2. Whentheserverfinishes processing theclient'srequest, itreturns areply message
(containing the result of processing) to the client. If the reply is not received
withinthe timeout period, the kernel oftheclient machine retransmits therequest
message.
Based on the protocol of Figure 3.9, an example of failure handling during
communication between two processes is shown in Figure 3.10. The protocol of Figure
3.9issaidtoobeyat-Least-oncesemantics, whichensures thatatleastoneexecution ofthe
receiver'soperation has beenperformed (butpossibly more). Itismoreappropriate tocall
Client Server
T Send
request
Timeout
Lost
Send
request
Timeout
Crash Unsuccessful
request execution
Restarted
Successful
request execution
'\
T1imeout
Send response
Thesetwosuccessful
executionsofthesame
Send ) requestmayproduce
request different results.
Successful
requestexecution
Send response
Fig.3.10 Anexample offault-tolerantcommunicationbetween aclient andaserver.

136 Chap. 3 • Message Passing
this semantics the last-one semantics because the results of the last execution of the
request areusedbythesender,although earlier(abandoned) executionsoftherequest may
have had side effects that survived the failure. Asexplained later,this semantics may not
be acceptable to several applications.
or
3.9.1 Idempotencyand Handling DuplicateRequest
Messages
Idempotencybasicallymeans"repeatability."Thatis,anidempotentoperationproducesthe
same results without any side effects no matter how many.times it is performed with the
same arguments. An example of an idempotentroutine is a simple GetSqrtprocedure for
calculatingthesquarerootofagivennumber.Forexample, GetSqrt(64)always returns 8.
On theother hand, operations thatdo not necessarily produce the same results when
executedrepeatedly with thesame arguments are said tobenonidempotent. Forexample,
consider the following routine ofa server process that debits a specified amount from a
bank account and returns the balance amount to a requesting client:
debit (amount)
if(balance ~ amount)
{balance =balance - amount;
return ("success", balance);}
else return ("failure", balance);
end;
Figure 3.11shows asequence ofdebitiI00) requests made byaclient forprocessing
the debit routine. The first request asks the server to debit an amount of 100 from the
balance. The server receives the request and processes it.Suppose the initial balance was
1000,so the server sends areply ("success," 900) totheclient indicating that the balance
remaining is 900. This reply, for some reason, could not be delivered to the client. The
client then times out waiting for the response ofitsrequest and retransmits thedebiti100)
request. The server processes the debit(100) request once again and sends a reply
("success," 8(0) to the client indicating that the remaining balance is 800, which is not
correct. Therefore, we see from this example that multiple executions of nonidempotent
routines produce undesirable results.
Clearly, when no response is received by the client, it is impossible to determine
whetherthefailurewasduetoaservercrash.orthelossoftherequestorresponse message.
Therefore, ascanbeseeninFigure 3.10,due tothe useoftimeout-based retransmission of
requests, the server mayexecute (either partially or fully) the same request message more
than once. This behavior mayor may not be tolerable depending on whether multiple
executions of the request have the same effect as a single execution (as in idempotent
routines). Iftheexecution oftherequest isnonidempotent, thenitsrepeatedexecution will
destroytheconsistencyofinformation.Thereforesuch"orphan"executionsmust,ingeneral,
beavoided. The orphan phenomenon has led to the identification and use of exactly-once
semantics, which ensures that only one execution of the server's operation is performed.
Primitives basedonexactly-oncesemanticsaremostdesiredbutdifficulttoimplement.

Sec.3.9 • Failure Handling 137
Server
T Client
(balance=1000)
Send
request
Processdebitroutine
= =
balance 1000-100 900
Timeout
Return(success,900)
~nd
_1
request
Processdebitroutine
=
balance 900-100=800
Return(success,800)
Receive = (success,800)
balance 800
debit(amount)
{
if(balance>=amount)
{ balance=balance -amount;
retum("success",balance);
}
elsereturn(Ufailure",balance);
Fig.3.11 Anonidempotentroutine.
One way to implementexactly-once semantics istouse aunique identifierforevery
request thattheclient makes andtosetupareplycache inthekernel'saddress spaceonthe
server machine to cache replies. In this case, before forwarding a request to a server for
processing, the kernel ofthe servermachine checks to see if a reply already exists in the
replycache fortherequest. Ifyes,thismeansthatthisisaduplicaterequestthathasalready
beenprocessed.Therefore,thepreviouslycomputedresultisextractedfromthereplycache
andanewresponse message issenttotheclient.Otherwise,therequestisanewone.Inthis
case,thekernel forwards therequesttotheappropriateserverforprocessing,andwhenthe
processingisover,itcaches therequest identifieralong with theresultofprocessinginthe
replycache before sending aresponse message totheclient.
An example of implementing exactly-once semantics is shown in Figure 3.12. This
figure issimilarto Figure 3.11exceptthat requests are now numbered, and areply cache
has been added to the server machine. The client makes request-I; the server machine's
kernel receives request-l and then checks the reply cache to see ifthere isacachedreply

138 Chap.3 • Message Passing
Request Replyto
identifier besent
-...
request-1 success,900)
10-----
~----_.
Replycache
Server
I Client
(balance=1000)
Send
request-1
Checkreplycacheforrequest...1
Nomatchfound,soprocessrequest...1
Timeout Savereply ---.J
Return(success,900)
Send
request...1
Checkreplycacheforrequest...1
I
Matchfound
..-----J
Extractreply
Return(success.900)
Receive
=
balance 900
Fig.3.12 Anexampleofexactly-once semantics usingrequestidentifiersandreplycache.
for request-I, There isno match, so itforwards therequest totheappropriateserver.The
server processes the request and returns the result to the kernel. The kernel copies the
request identifierand the result of execution to the reply cache and then sends the result
intheformofaresponse message totheclient."Thisreplyislost,and theclient timesout
on request...l and retransmits request-I, The server machine's kernel receives request-I
once again andchecks thereply cache toseeifthereisacached reply for request-I, This
time a match is found so it extracts the result corresponding to request-Jfrom the reply
cache and once again sends it to the client as a response message. Thus the reprocessing
of aduplicate request is avoided. Note that the range of the request identifiers should be
much larger than the number of entries in the cache.

Sec. 3.10• GroupCommunication 139
It is important to remember that the use of a reply cache does not make a
nonidempotent routine idempotent. The cache is simply one possible way to implement
nonidempotent routines with exactly-once semantics.
3.9.2 K••plng Track of lost Qnd Out-of-Sequence Packets
inMultldGtGgram MessGges
Inthecaseofmultidatagrammessages,thelogical transferofamessageconsistsofphysical
transfer of several packets. Therefore, a message t.ransmission can be considered to be
complete only when all the packets of the message have been received by the process to
which it is sent. For successful completion ofa multidatagram message transfer, reliable
delivery ofevery packetisimportant.Asimpleway toensurethis istoacknowledgeeach
packet separately (called stop-and-waitprotocol). But aseparateacknowledgmentpacket
for each request packet leads to a communication overhead. Therefore, to improve
communicationperformance, abetter approach is to use a single acknowledgmentpacket
forallthepackets ofamultidatagrammessage(calledblastprotocol).However, when this
approach is used, a node crash or acommunicationlink failure may lead to the following
problems:
• One or more packets of the multidatagram message are lost in communication.
• The packets arc received out of sequence by the receiver.
Anefficientmechanismtocope with these problemsisto useabitmaptoidentify the
packetsofamessage. Inthismechanism,theheaderpartofeachpacketconsistsoftwoextra
fields, oneofwhich specifiesthetotalnumberofpacketsinthemultidatagrammessageand
the other is the bitmap field that specifies the position of this packet in the complete
message. The first field helps thereceivingprocesstosetaside asuitably sized bufferarea
for the message and the second field helps in deciding the position of this packet in that
buffer.Sinceallpacketshaveinformationabout thetotalnumberofpackets inthemessage,
so even inthe case of out-of-sequence receipt of the packets, that is, even when the first
packetisnotreceivedfirst,asuitably sizedbufferareacanbesetasidebythereceiverforthe
entire message andthereceivedpacketcanbeplacedinitsproperpositioninsidethebuffer
area. After timeout, if all packets have not yet been received, a bitmap indicating the
unreceived packets is sent to the sender. Using the bitmap information, the sender
retransmitsonlythose packets thathavenotbeen receivedbythereceiver.Thistechniqueis
called selective repeat.When allthe packetsofamultidatagrammessage are received, the
message transfer is complete, and the receiver sends an acknowledgment message to the
sending process. This method ofmultidatagrammessage communicationisillustratedwith
anexampleinFigure3.13 inwhich themultidatagrammessageconsistsoffivepackets.
3.10 GROUP COMMUNICATION
The most elementary form of message-based interaction is one-to-one communication
(also known as point-to-point, or unicast, communication) in which a single-sender
process sends amessage toasingle-receiverprocess. However, for performanceand ease

140 Chap. 3 • Message Passing
Senderofa
multidatagram
messagethat Receiverofthe
consistsof multidatagram
fivepackets messagTe
Sendrequestmessage
Timeout
Bufferfor5packets
Createbufferfor5 ~ 1
packetsandplace
thispacketin ..
Packets position2 2
ofthe
response Placethispacket .. 3
I inposition3
.....H 4
Fourthof5packets
Lost
I ~~ 5
Placethispacket
inposition5---1
(5,01001~) __
quest
Missingpacketsinformation
---r
cket
--
Resendmissing
packets
---L
cket
Sendacknowledgment
Acknowledgment
Fig.3.13 Anexampleoftheuseofabitmaptokeeptrackoflostandoutofsequence
packetsinamultidatagrammessagetransmission.
of programming, several highly parallel distributed applications require that a message
passing system shouldalsoprovidegroupcommunication facility.Depending onsingleor
multiple senders and receivers, the following three types of group communication are
possible:
1. One to many (single sender and multiple receivers)
2. Many to one (multiple senders and single receiver)

Sec. 3.10• Group Communication 141
3. Many to many (multiple senders and multiple receivers)
The issues related to these communication schemes are described below.
3.10.1 On8-to-Many Communication
Inthis scheme, there aremultiple receivers foramessage sentby asingle sender.One-to
many scheme is also known as multicast communication. A special case ofmulticast
communication is broadcast communication, in which the message is sent to all
processors connected to a network.
Multicastlbroadcastcommunication is very useful for several practical applications.
For example, consider a server manager managing a group of server processes all
providing thesametype ofservice.The server manager can multicast amessagetoallthe
serverprocesses, requesting thatafreeserver volunteer toservethecurrent request.Itthen
selects the first server that responds. The server manager does not have to keep track of
the free servers. Similarly, to locate a processor providing a specific service, an inquiry
message maybebroadcast. Inthiscase,itisnotnecessary toreceive ananswerfromevery
processor; just finding one instance of the desired service is sufficient.
Group Management
In case of one-to-many communication, receiver processes of a message form a group.
Such groups are of two types--closed andopen.Aclosedgroup isone inwhichonly the
members of the group can send a message to the group. An outside process cannot send
a message to the group as a whole, although it may send a message to an individual
member of the group. On the other hand, an open group is one in which any process in
the system can send a message to the group as a whole.
Whether to use a closed group or an open group is application dependent. For
example, agroupofprocesses workingonacommon problem neednotcommunicatewith
outside processes and can form aclosed group. On the other hand, agroup of replicated
servers meantfordistributedprocessing ofclientrequests mustformanopengroupsothat
client processes can send their requests to them. Therefore, a flexible message-passing
system with group communication facility should support both types of groups.
Amessage-passingsystemwithgroupcommunicationfacilityprovidestheflexibility
tocreate and delete groups dynamically and toallow aprocess tojoin orleave agroup at
any time. Obviously, the message-passing system must have a mechanism tomanage the
groups and their membership information. A simple mechanism for this is to use a
centralized group serverprocess. Allrequests to create agroup, to delete agroup, to add
a member to a group, or to remove a member from a group are sent to this process.
Therefore, itiseasy forthegroupserver tomaintain up-to-date information ofallexisting
groups andtheir exact membership.This approach, however, suffersfromtheproblems of
poor reliability and poor scalability common to all centralized techniques. Replication of
the group server may be done to solve these problems to some extent. However,
replication leads to the extra overhead involved in keeping the group information of all
group servers consistent.

142 Chap. 3 • Message Passing
GroupAddressing
Atwo-level naming scheme isnormally usedforgroup addressing. The high-level group
name is anASCII string that is independent of the location information of the processes
in the group. On the other hand, the low-level group name depends to a large extent on
the underlying hardware. Forexample, onsomenetworks itispossible tocreate aspecial
network address to which multiple machines can listen. Sucha network address iscalled
amulticastaddress.Apacket sent to amulticast address isautomatically delivered toall
machines listening to the address. Therefore, in such systems a multicast address is used
as a low-level name for a group.
Some networks that do not have the facility to create multicast addresses may have
broadcasting facility. Networks with broadcasting facility declare a certain address, such
as zero, as a broadcast address. A packet sent to a broadcast address is automatically
delivered to all machines on the network. Therefore, the broadcast address of a network
may be used as a low-level name for a group. In thiscase, the software of each machine
must check to see if the packet is intended for it. If not, the packet is simply discarded.
Sinceallmachinesreceiveeverybroadcastpacketandmustcheckifthepacket isintended
forit,theuseofabroadcastaddress islessefficientthantheuseofamulticast address for
group addressing. Also notice that in a system that uses a broadcast address for group
addressing, aU groups have the same low-level name, the broadcast address.
If a network does not support either the facility to create multicast addresses or the
broadcasting facility, a one-to-one communication mechanism has to be used to
implement the group communication facility.That is, the kernel of the sending machine
sends the message packet separately to each machine that has a process belonging to the
group. Therefore, in this case, the low-level name of a group contains a list of machine
identifiers of an machines that have a process belonging to the group.
Notice thatinthefirsttwomethodsasinglemessagepacketissentover thenetwork,
whereas in the third method the number ofpackets sent over the network depends on the
number of machines that have one or more processes belonging to the group. Therefore
the third method generates more network traffic than the other two methods and is in
general less efficient. However, it is better than the broadcasting method in systems in
which most groups involve only a fewout of many machines on the network. Moreover
the first two methods are suitable for use only on a single LAN. If the network contains
multiple LANs interconnected by gateways and theprocesses of agroup are spread over
multiple LANs, the third method is simpler and easier to implement than the other two
methods.
Message Delivery to Receiver Processes
User applications use high-level group names inprograms. The centralized group server
maintains a mapping of high-level group names to their low-level names. The group
server also maintains a list of the process identifiers of all the processes for each
group.
When asendersendsamessage toagroupspecifying itshigh..levelname, thekernel
ofthesendingmachinecontactsthegroupservertoobtainthelow-levelnameofthegroup

Sec. 3.10 • Group Communication 143
and the list of process identifiers of the processes belonging to the group. The list of
processidentifiers isinserted inthe message packet. Ifthe low-level group name iseither
a multicast address or a broadcast address, the kernel simply sends the packet to the
multicastlbroadcast address. On the other hand, if the low-level group name is a list of
machine identifiers, the kernel sends a copy of the packet separately to each machine in
the list.
When the packet reaches a machine, the kernel of that machine extracts the list of
process identifiers from the packet and forwards the message in the packet to those
processes inthe list that belong toitsown machine. Note that when the broadcastaddress
is used as a low-level group name, the kernel of a machine may find that none of the
processes in the list belongs to its own machine. In this case, the kernel simply discards
the packet.
Notice that a sender is not at all aware of either the size of the group or the actual
mechanism used for group addressing. The sender simply sends a message to a group
specifyingitshigh-level name, andtheoperating system takes theresponsibilitytodeliver
the message to all the group members.
Buffered and Unbuffered Multicast
Multicasting is an asynchronous communication mechanism. This is because multicast
send cannot be synchronous due to the following reasons [Gehani 1984]:
1. Itisunrealistictoexpect asending process towaituntilallthereceiving processes
that belong to the multicast group are ready to receive the multicast message.
2. The sending process maynot be aware of all the receiving processes that belong
to the multicast group.
How a multicast message is treated on a receiving process side depends on whether
the multicast mechanism is buffered or unbuffered. For an unbuffered multicast, the
message isnot buffered forthereceiving process and islostifthe receiving process isnot
in a state ready to receive it.Therefore, the message is received only by those processes
of the multicast group that are ready to receive it. On the other hand, for a buffered
multicast, the message is buffered for the receiving processes, so each process of the
multicast group will eventually receive the message.
Send-to-All andBulletin-Board Semantics
Ahamad and Bernstein [1985] described the following two types of semantics forone-to
many communications:
1. Send-to-all semantics. A copy of the message is sent to each process of the
multicast group and the message is buffered until it is accepted by the process.
2. Bulletin-board semantics. A message to be multicast is addressed to a channel
instead of being sent to every individual process of the multicast group. From a logical

144 Chap. 3 • Message Passing
point ofview,the channel plays the role of a bulletin board.A receiving process copies
the message from thechannel instead of removing it whenit makes a receiverequest on
the channel. Thus a multicast message remains available to other processes as if it has
been posted on the bulletin board. The processes that have receive access right on the
channel constitute the multicast group.
Bulletin-boardsemantics ismore flexible thansend-to-all semantics because ittakes
care of the following two factors that are ignored bysend-to-all semantics [Ahamad and
Bernstein 1985]:
1. The relevance of amessage toaparticularreceiver may depend on the receiver's
state.
2. Messages notaccepted within acertain time after transmission may no longer be
useful; their value may depend on the sender's state.
To illustrate this, let us once again consider the example of a server manager
multicasting amessagetoalltheserverprocesses tovolunteertoserve thecurrentrequest.
Using send-to-all semantics, itwouldbe necessary tomulticast toall the servers, causing
many contractors to process extraneous messages. Using bulletin-board semantics, only
those contractors that are idle and in a state suitable for serving requests will make a
receiverequest on the concerned channel, and thus. only contractors in the correct state
will process such messages [Ahamad and Bernstein 1985].Furthermore, the message is
withdrawn from thechannel bytheserver manager as soonasthe bidperiod isover; that
is, the first bidder is selected (in thiscase). Therefore, the message remains available for
being received only as long as the server manager is in a state in which bids are
acceptable. While this does not completely eliminate extraneous messages (contractors
may still reply after the bid period is over), it does help in reducing them.
Flexible Reliability in Multicast Communication
Different applications require different degrees of reliability. Therefore multicast
primitives normally provide the flexibility for user-definable reliability.Thus, the sender
of a multicast message can specify the number of receivers from which a response
message isexpected. Inone-to-many communication, thedegreeofreliability isnormally
expressed in the following forms:
1. The O-reliable. No response isexpected bythe sender from any of the receivers.
This isuseful forapplications usingasynchronous multicast in which the sender does not
wait for any response after multicasting the message. An example of this type of
application is a time signal generator.
2. The l-reliable. The sender expects a response from any of the receivers. The
already described application in which a server manager multicasts a message to all the
servers to volunteer to serve the current request and selects the first server that responds
is an example of l-reliable multicast communication.

Sec.3.10• GroupCommunication 145
3. The m-out-of-n-reliable. The multicast group consists of n receivers and the
sender expects a response from m (1 <m <n) of the n receivers. Majority consensus
algorithms (described in Chapter 9) used for the consistency control of replicated
information use this form of reliability, with the value m= n/2.
4. All-reliable.The sender expects aresponse message from all the receivers of the
multicast group. For example, suppose a message for updating the replicas of a file is
multicast toallthefileservers havingareplicaofthefile.Naturally,suchasenderprocess
will expect a response from all the concerned file servers.
Atomic Multicast
Atomic multicasthasanall-or-nothing property.Thatis,whenamessage issenttoagroup
byatomic multicast, itiseither receivedbyalltheprocesses thataremembers ofthegroup
or else it is not received by any of them. Animplicit assumption usually made in atomic
multicast is that when a process fails, it is no longer a member of the multicast group.
When the process comes up after failure, it must join the group afresh.
Atomic multicast is not always necessary. For example, applications for which the
degree of reliability requirement is O-reliable., l-reliable, or m-out-of-n-reliable do not
need atomic multicast facility. On the other hand, applications for which the degree of
reliability requirement is all-reliable need atomic multicast facility. Therefore, a flexible
message-passing system should support both atomic and nonatomic multicast facilities
and should provide the flexibility to the sender of a multicast message to specify in the
send primitive whether atomicity property is required or not for the message being
multicast.
Asimple method to implement atomic multicast isto multicast a message, with the
degree of reliability requirementbeing all-reliable. In this case, the kernel of the sending
machine sends the message toallmembers ofthegroupand waitsforanacknowledgment
from each member (we assume that a one-to-one communication mechanism is used to
implement the multicast facility). After a timeout period, the kernel retransmits the
message to all those members from whom an acknowledgment message has not yet been
received. The timeout-based retransmission of the message is repeated until an
acknowledgment is received from all members of the group. When all acknowledgments
have been received, the kernel confirms tothe sender thatthe atomic multicast process is
complete.
The above method works fineonlyaslongasthemachines ofthesenderprocess and
the receiver processes do not failduring anatomic multicast operation. This isbecause if
the machine of the sender process fails, the message cannot be retransmitted if one or
more members did not receive the message due to packet loss or some other reason.
Similarly, ifthe machine of a receiver process fails and remains down for some time, the
message cannot be delivered to that process because retransmissions of the message
cannot be continued indefinitely and have to be aborted after some predetermined time.
Therefore, a fault-tolerant atomic multicast protocol must ensure that a multicast will be
delivered toallmembers ofthemulticast groupeven intheeventoffailure ofthesender's
machine or a receiver's machine. One method to implement such a protocol is described
next [Tanenbaum 1995}.

146 Chap. 3 • Message Passing
In this method,each message has a message identifierfield todistinguish itfrom all
other messages and a field to indicate that it is an atomic multicast message. The sender
sends the message to a multicast group. The kernel of the sending machine sends the
message to all members of the group and uses timeout-based retransmissions as in the
previous method. Aprocess that receives the message checks its message identifier field
to seeifit isanew message. Ifnot,itissimplydiscarded.Otherwise, the receiver checks
to see.if it is an atomic multicast message. If so, the receiver also performs an atomic
multicast of the same message, sending ittothe same multicastgroup. The kernel of this
machine treats this message as an ordinary atomic multicast message and uses timeout
based retransmissions when needed. In this way, each receiver of an atomic multicast
message willperformanatomicmulticastofthemessagetothesamemulticast group.The
method ensures that eventually all the surviving processes of the multicast group will
receive the message even if the sender machine fails after sending the message or a
receiver machine fails after receiving the message.
Notice thatanatomicmulticast isingeneral veryexpensiveascomparedtoanormal
multicast due tothelargenumber of messages involved initsimplementation.Therefore,
a message-passing system should not use the atomicity property as a default property of
multicast messages but should provide this facility as an option.
Group Communication Primitives
In both one-to-one communication and one-to-many communication, the sender of a
process basically has to specify two parameters: destination address and a pointer to the
message data. Therefore ideally the same sendprimitive can beused for bothone-to-one
communication and one-to-many communication. If the destination address specified in
thesendprimitive is that of a single process, the message is sentto that one process. On
the other hand, if the destination address is a group address, the message is sent to all
processes that belong to that group.
However, most systems having a group communication facility provide a different
primitive (such as sendgroup) for sending a message to a group. There are two main
reasons for this. First, it simplifies the design and implementation of a group
communication facility. For example, suppose the two-level naming mechanism is
used for both process addressing and group addressing. The high-level to low-level
name mapping for processes is done by the name server, and for groups it is done by
the group server. With this design, if a single send primitive is used for both one-to
one communication and one-to-many communication, the kernel of the sending
machine cannot know whether the destination address specified by a user is a single
process address or a group address. Consequently, it does not know whether the name
server or the group server should be contacted for obtaining the low-level name of the
specified destination address. Implementation methods to solve this problem are
possible theoretically, but the design will become complicated. On the other hand, if
separate primitives such as send and sendgroup are used, the kernel can easily make
out whether the specified destination address is a single process address or a group
address and can contact the appropriate server to obtain the corresponding low-level
name.

Sec.3.10• GroupCommunication 147
Second, it helps in providing greater flexibility to the users. For instance»a separate
parameter may be used in the sendgroup primitive to allow users to specify the degree
of reliability desired (number of receivers from which a response message is expected),
and another parametermay be used to specify whether the atomicity property isrequired
or not.
3.10.2 Many-to-One Communication
In this scheme, multiple senders send messages to a single receiver. The single receiver
maybeselectiveornonselective.Aselective receiverspecifies aunique sender; amessage
exchange takes place only if that sender sends a message. On the other hand, a
nonselective receiver specifies a set of senders, and if anyone sender in the set sends a
message to this receiver, a message exchange takes place.
Thus we see that an important issue related to the many-to-one communication
scheme is nondeterminism. The receiver may want to wait for information from any ofa
group of senders, rather than from one specific sender. As it is not known in advance
which member (or members) of the group will have its information available first, such
behavior is nondeterministic. In some cases it is useful to dynamicalJy control the group
of senders from whom to accept message. For example, a buffer process may accept a
request from a producerprocess to store an item inthe buffer whenever the buffer is not
full; it may accept a request from a consumer process to get an item from the buffer
whenever the buffer is not empty. To program such behavior, a notation is needed to
express and control nondeterminism. One such construct is the "guarded command"
statement introduced by Dijkstra l19751. Since this issue is related to programming
languages rather than operating systems, we will not discuss it any further.
3.10.3 Many-to-Many Communication
In this scheme, multiple senders send messages to multiple receivers. The one-to-many
and many-to-one schemes are implicit in this scheme. Hence the issues related toone-to
many and many-to-one schemes, which have already been describedabove, also apply to
the many-to-many communication scheme. In addition, an important issue related to
many-to-many communication scheme is that of ordered message delivery.
Ordered message delivery ensures that all messages are delivered to all receivers in
an order acceptable to the application. This property is needed by many applications for
their correct functioning. For example, suppose two senders send messages to update the
same record of a database to two server processes having areplica of the database. Ifthe
messages of the two senders are received by the two servers in differentorders, then the
final values of the updated record of the database may be different in its two replicas.
Therefore, this application requires thatall messages bedelivered inthe same order toall
receivers.
Ordered message delivery requires message sequencing. In a system with a single
senderand multiple receivers (one-to-many communication), sequencing messages to all
the receivers is trivial. If the sender initiates the next multicast transmission only after
confirmingthat the previous multicast message hasbeen received byall themembers, the

148 Chap.3 • Message Passing
messages win bedeliveredinthesameorder.Ontheotherhand.inasystem withmultiple
senders and a single receiver (many-to-one communication), the messages will be
delivered to the receiver in the order in which they arrive at the receiver's machine.
Ordering in thiscase issimply handled bythereceiver.Thus weseethat it isnotdifficult
to ensure ordered delivery of messages in many-to-one'or one-to-many communication
schemes.
However, in many-to-many communication, a message sent from a sender may
arrive at a receiver's destination before the arrival of a message from another sender;
but this order may be reversed at another receiver's destination (see Fig. 3.14). The
reason why messages of different senders may arrive at the machines of different
receivers in different orders is that when two processes are contending for access to a
LAN, the order in which messages of the two processes are sent over the LAN is
nondeterministic. Moreover, in a WANenvironment, the messages of different senders
may be routed to the same destination using different routes that take different amounts
of time (which cannot be correctly predicted) to the destination. Therefore, ensuring
ordered message delivery requires a special message-handling mechanism in many-to
many communication scheme.
The commonly used semantics for ordered delivery of multicast messages are
absolute ordering, consistent ordering, and causal ordering. These are described
below.
Time
1
Fig.3.14 Noorderingconstraintfor
messagedelivery.
Absolute Ordering
Thissemantics ensuresthatallmessagesaredelivered toallreceiverprocesses intheexact
order in which theyweresent(seeFig. 3.15). One method toimplement this semantics is
to use global timestamps as message identifiers. That is, the system isassumed tohave a
clock ateach machine andallclocks aresynchronized witheachother,and whenasender
sendsamessage, theclock value(timestamp) istakenastheidentifierofthatmessageand
embedded in the message.
The kernel of each receiver's machine saves all incoming messages meant for a
receiver in a separate queue. A sliding-window mechanism is used to periodically

Sec.3.10 • GroupCommunication 149
Time
1t1<~
Fig.3.]5 Absoluteorderingofmessages.
deliver the message from the queue to the receiver. That is, a fixed time interval is
selected as the window size, and periodically all messages whose timestamp values
fall within the current window are delivered to the receiver. Messages whose
timestamp values fall outside the window are left in the queue because of the
possibility that a tardy message having a timestamp value lower than that of any of
the messages in the queue might still arrive. The window size is properly chosen
taking into consideration the maximum possible time that may be required by a
message to go from one machine to any other machine in the network.
Consistent Ordering
Absolute-ordering semantics requires globally synchronized clocks, which are not easy
to implement. Moreover, absolute ordering is not really what many applications need
to function correctly. For instance, in the replicated database updation example, it is
sufficient to ensure that both servers receive the update messages of the two senders
in the same order even if this order is not the real order in which the two messages
were sent. Therefore, instead of supporting absolute-ordering semantics, most systems
support consistent-ordering semantics. This semantics ensures that all messages are
delivered to all receiver processes in the same order. However, this order may be
different from the order in which messages were sent (see Fig. 3.16).
One method to implement consistent-ordering semantics is to make the many-to
many scheme appear as a combination of many-to-one and one-to-many schemes
[Chang and Maxemchuk 1985]. That is, the kernels of the sending machines send
messages to a single receiver (known as a sequencer) that assigns a sequence number
to each message and then multicasts it. The kernel of each receiver's machine saves
all incoming messages meant for a receiver in a separate queue. Messages in a queue
are delivered immediately to the receiver unless there is a gap in the message
identifiers, in which case messages after the gap are not delivered until the ones in the
gap have arrived.

150 Chap. 3 • Message Passing
Time
1tl<~
Fig.3.16 Consistentordering of messages.
The sequencer-based method for implementing consistent-ordering semantics is
subjecttosinglepointoffailure and hencehas poorreliability. Adistributedalgorithmfor
implementingconsistent-ordering semantics that does not suffer from this problem is the
ABCASTprotocol of the ISIS system [Birman and Van Renesse 1994, Birman 1993,
Birmanet al. 1991,Birmanand Joseph 1987].It assigns asequencenumberto a message
by distributed agreement among the group members and the sender and works as
follows:
1. The sender assigns a temporary sequence number to the message and sends it
to all the members of the multicast group. The sequence number assigned by the
sender must be larger than any previous sequence number used by the sender.
Therefore, a simple counter can be used by the sender to assign sequence numbers to
its messages.
2. On receiving the message, each member of the group returns a proposed
sequence number to the sender. A member (i) calculates its proposed sequencenumber
by using the function
max(Fmax' P max) + 1+ ilN
whereFmaxisthelargestfinal sequencenumberagreedupon sofarfor amessagereceived
bythe group(eachmembermakesarecordofthis when afinal sequencenumberisagreed
upon), Pmax is the largest proposed sequence number by this member, and N is the total
number ofmembers in the multicast group.
3. When the sender has received the proposed sequence numbers from all the
members,itselectsthe largestone asthe final sequencenumberfor themessageand sends
it to all members in a commit message. The chosen final sequence number is guaranteed
tobe uniquebecauseofthe term UNin the function used for the calculationofaproposed
sequence number.
4. On receiving the commit message, each member attaches the final sequence
number to the message.

Sec. 3.10 • Group Communication 151
5. Committedmessages withfinal sequence numbers aredeliveredtotheapplication
programs in order of their final sequence numbers. Note that the algorithm for sequence
number assignment to a message is a part of the runtime system, not the user
processes.
It can be shown that this protocol ensures consistent ordering semantics.
Causal Ordering
For some applications consistent-ordering semantics is not necessary and even weaker
semantics is acceptable. Therefore, an application can have better performance if the
message-passing system used supports a weaker ordering semantics that is acceptable to
the application. .One such weaker ordering semantics that is acceptable to many
applications is the causal-ordering semantics. This semantics ensures that ifthe event of
sending one message iscausally related to theevent ofsending another message, the two
messages are delivered to all receivers in the correct order. However, if two message
sending events are not causally related, the two messages may be delivered to the
receivers inany order.Twomessage-sendingevents are saidto becausally related ifthey
arecorelated bythehappened-beforerelation (foradefinition ofhappened-beforerelation
see Chapter 6). That is, two message-sending events are causally related if there is any
possibility of the second one being influenced inany waybythefirst one.The basic idea
behind causal-ordering semantics is that when it matters, messages are always delivered
in the proper order, but when it does not matter, they may be delivered in any arbitrary
order.
Anexample of causal ordering of messages isgiven inFigure 3.17. Inthisexample,
sender SI sends message mJto receivers R], R 2, and R 3 and sender S2sends message m2
to receivers R 2, and R 3. On receiving nu ,receiver R] inspects it,creates a new message
ms, and sends m-;to R 2 and R 3. Note that the event of sending m3is causally related to
theevent of sending m, because thecontents of m-;might have beenderived inpart from
ml; hence the two messages must be delivered to both R and R in the proper order, m,
2 3
Time
1
F'ig.3.17 Causal ordering of messages.

152 Chap. 3 • Message Passing
before m3' Also note that since m2is not causally related to either m, or m3' m2can be
delivered at any time to R and R irrespective of m, or m-, This is exactly what the
2 3
example ofFigure 3.17 shows.
One methodforimplementing causal-ordering semanticsistheCBCASTprotocolof
the ISIS system [Birman et al. 1991].It works as follows:
1. Each memberprocess ofagroup maintainsavectorofncomponents, wherenis
the total number of members inthe group. Each memberis assigned a sequence number
from 0 to n, and the ith component of the vectors corresponds to the member with
sequence number i. Inparticular, the valueof the ithcomponentofa member's vectoris
equal to the number of the last message received in sequence by this member from
member i.
2. To send a message, a process increments the value of its own component in its
own vector and sends the vector as part of the message.
3. Whenthemessagearrivesatareceiverprocess's site,itisbufferedbytheruntime
system. The runtime system tests the two conditions given below to decide whether the
messagecan be delivered to the user process or its delivery must be delayed to ensure
causaJ...orderingsemantics.LetSbethe vectorofthesenderprocessthatisattachedtothe
messageandRbethevectorofthereceiver process.Alsoletibethesequence numberof
the sender process. Then the two conditions to be tested are
S[i] = R[i] + 1 and SUl::; RUJ for allj ~ i
The firstcondition ensures thatthereceiver hasnotmissedany messagefromthe
sender. This test is needed because two messages from the same sender are always
causally related. The second condition ensures that the sender has not received any
message that the receiver has not yet received. This test is needed to make sure that the
sender's message is not causally related toa message missed by the receiver.
If the message passes these two tests, the runtime system delivers it to the user
process. Otherwise, the message is left in the buffer and the test is carried out again for
it when a new message arrives.
A simple example to illustrate the algorithm is given in Figure 3.18. In this
example, there'are four processes A, B, C, and D. The status of their vectors at some
instance of time is (3, 2, 5, I), (3, 2, 5, 1), (2, 2, 5, 1), and (3, 2, 4, 1), respectively.
This means that, until now, A has sent three messages, B has sent two messages, C
has sent five messages, and D has sent one message to other processes. Now A sends
a new message to other processes. Therefore, the vector attached to the message will
be (4, 2, 5, 1). The message can be delivered to B because it passes both tests.
However, the message has to be delayed by the runtime systems of sites of processes
C and D because the first test fails at the site of process C and the second test fails
at the site of process D.
A good message-passing system should support at least consistent- and causal
ordering semantics and should provide the flexibility tothe userstochoose one of these
in their applications.

Sec.3.11 • CaseStudy: 4.3BSD UNIX IPCMechanism 153
Statusofvectorsatsomeinstanceoftime
I
Vectorof Vectorof Vectorof : Vectorof
processA I processB processC : processD
~l~ [iliIili]:~
I
I
ProcessAsendsanew :
messagetootherprocesses:
CiliTI:E!M=gel!
I
I
I-----_:~ Deliver
I
I
:Delay
:becausethe
:condition
I
:A[1]=C[1]+1
doesnothold
I
I
I
I
I
I
I
I
I
I
I
Fig. 3.18 An exampletoillustratetheCBCASTprotocol forimplementingcausal
orderingsemantics.
3.11 CASE STUDY: 4.38SD UNIX IPC MECHANISM
The socket-based IPC of the 4.3BSD UNIX system illustrates how a message-passing
systemcanbedesignedusingtheconceptsandmechanismspresentedinthischapter.The
system was produced by the Computer Systems Research Group (CSRG) of the
Universityof California at Berkeleyand is the most widely used and welldocumented
message-passingsystem.
3.11.1 8aslc Concepts and Main Features
The IPe mechanismof the 4.3BSD UNIXprovidesa generalinterfaceforconstructing
network-basedapplications.Its basicconcepts and mainfeaturesareasfollows:
1. Itisnetworkindependentinthesensethatitcansupportcommunicationnetworks
thatusedifferentsetsofprotocols,differentnamingconventions,differenthardware,and
soon.Forthis,itusesthenotionofcommunicationdomain, whichreferstoastandardset
ofcommunicationproperties.Inthischapterwehaveseenthattherearedifferentmethods
ofnamingacommunicationendpoint.Wealsohaveseenthattherearedifferentsemantics
of communication related to synchronization,reliability,ordering, and so on. Different
networksoften use different naming conventions for naming communicationendpoints

154 Chap. 3 • Message Passing
and possess different semantics of communication. These properties of a network are
known as its communication properties. Networks with the same communication
propertiesbelongtoacommon communicationdomain (orprotocol family). Byproviding
the flexibility to specify a communication domain as a parameterof the communication
primitive used, the IPC mechanism of the 4.3B5D UNIX allows the users to select a
domain appropriate to their applications.
2. Itusesaunifiedabstraction, called socket,foranendpoint ofcommunication.That
is, a socket is an abstract object from which messages are sent and received. The IPC
operations are based on socket pairs, one belonging to each of a pair ofcommunicating
processes thatmaybeonthesameordifferentcomputers.Apairofsocketsmaybeusedfor
unidirectionalorbidirectionalcommunicationbetweentwoprocesses.Amessagesentbya
sendingprocessisqueuedinitssocketuntilithasbeentransmittedacrossthenetworkbythe
networking protocol and an acknowledgment has been received (only if the protocol
requires one).Onthereceiver side,themessage isqueued inthereceiving process'ssocket
untilthereceiving processmakesanappropriatesystemcalltoreceiveit.
Any process can create a socket for use in communication with another process.
Sockets are created within a communication domain. A created socket exists until it is
explicitly closed or until every process having a reference to it exits.
3. For location transparency, it uses a two-level naming scheme for naming
communication endpoints. That is, a socket can be assigned a high-level name that is a
human-readable string. The low-level name of a socket is communication-domain
dependent. Forexample, itmayconsist ofalocalportnumberandanInternet address. For
translation of high-level socket names to their low-level names, 4.3B50 provides
functions for application programs rather than placing the translation functions in the
kernel. Note that a socket's high-level name is meaningful only within the context of the
communication domain in which the socket is created.
4. It is highly flexible in the sense that it uses a typing mechanism for sockets to
provide thesemantic aspectsofcommunicationtoapplications inacontrolledanduniform
manner. That is, all sockets are typed according to their communication semantics, such
as ordered delivery, unduplicated delivery, reliable delivery, connectionless communica
tion, connection-oriented communication, and so on. The system defines some standard
socket types and provides the flexibility to the users to define and use their own socket
types whenneeded.Forexample, asocketoftypedatagrammodelspotentiallyunreliable,
connectionless packet communication, and a socket of type stream models a reliable
connection-based byte stream.
5. Messages can be broadcast if the underlying network provides broadcast
facility.
3.11.2 TMIPC Prlmltlv.s
The primitives of the 4.3BSD UNIX IPC mechanism are provided as system calls
implemented as a layer on top of network communication protocols such as TCP, UDP,
and so on. Layering the IPC mechanism directly on top of network communication

Sec.3.11 • CaseStudy: 4.3BSDUNIXIPe Mechanism 155
protocols helps in making it efficient. The most important available IPC primitives are
briefly described below.
=
s socket(domain, type, protocol)
When a process wants to communicate with another process, it must first create a socket
by using the socket system call. The first parameter of this call specifies the
communicationdomain. The mostcommonlyused domain isthe Internetcommunication
domain because alarge number of hosts inthe world support the Internetcommunication
protocols.The second parameterspecifies thesocket typethatisselectedaccording tothe
communication semantics requirements of the application. The third parameter specifies
the communication protocol (e.g., TCP/IP or UDPIIP) to be used for the socket's
operation. If the value of this parameter is specified as zero, the system chooses an
appropriate protocol. The socket call returns a descriptor by which the socket may be
referenced insubsequentsystemcalls.A created socket isdiscarded withthenormalclose
system call.
bind (s, addr, addrlen)
Aftercreating asocket, thereceiver must bindittoasocket address. Notethatiftwo-way
communication is desired between two processes, both processes have to receive
messages, andhence both must separately bindtheir sockets toasocketaddress.Thebind
system call isused for this purpose. The three parameters of thiscall arethedescriptor of
the created socket, a reference to a structure containing the socket address to which the
socket is to be bound, and the number of bytes in the socket address. Once a socket has
been bound, its address cannot be changed.
It might seem more reasonable to combine the system calls for socket creation and
binding a socket to a socket address (name) in a single system call. There are two main
reasons for separating these two operations in different system calls. First, with this
approach a socket can be useful without names. Forcing users to name every socket that
is created causes extra burden on users and maylead to the assignment of meaningless
names. Second, some communication domains might require additional, nonstandard
information (suchastypeofservice) forbindingofanametoasocket.Theneedtosupply
this information at socket creation time will further complicate the interface.
connect(s, server_addr, server_addrlen)
Thetwomostcommonly usedcommunicationtypesinthe4.3BSDUNIXIPCmechanism
are connection-based (stream) communication and connectionless (datagram) commu
nication. In connection-based communication, two processes first establish a connection
between their pairs of sockets. The connection establishment process is asymmetric
because one of the processes keeps waiting for a request for a connection and the other
makes a request for a connection. Once connection has been established, data can be
transmitted between the two processes in either direction. This type of communication is
usefulforimplementingclient-serverapplications. A servercreates asocket,bindsaname

156 Chap. 3 • Message Passing
to it, and makes the name publicly known. It then waits for a connection request from
client processes. Clients send connection requests to the server. Once the connection is
established, they can exchange request and reply messages. Connection-based commu
nication supports reliable exchange of messages.
In connectionless communication, a socket pair is identified each time a
communication ismade.Forthis,thesending process specifiesitslocal socketdescriptor
and the socket address of the receiving process's socket each time it sends a message.
Connectionless communication is potentially unreliable.
The connect system call is used in connection-based communication by a client
processtorequestaconnectionestablishment betweenitsownsocketandthesocketofthe
serverprocess with whichit wants tocommunicate. Thethree parameters ofthiscall are
thedescriptoroftheclient's socket,areferencetoastructurecontainingthesocketaddress
of the server's socket, and the number of bytes in the socket address. The connect call
automatically bindsasocketaddress (name) totheclient'ssocket.Henceprior bindingis
not needed.
listen(s, backlog)
The listen system call is used in case of connection-based communication by a server
process to listen on its socket forclient requests forconnections. The two parameters of
this call are the descriptor of the server's socket and the maximum number ofpending
connections that should bequeued for acceptance.
snew = accept(s, client_addr, client_addrlen)
Theacceptsystemcan isusedinaconnection-based communication byaserver process
to accept a request for a connection establishment made by aclient and to obtain a new
socket for communication with that client. The three parameters of this call are the
descriptor of the server's socket, a reference to a structure containing the socket address
of the client's socket, and the number of bytes in the socket address. Note that the call
returns a descriptor (snew) that is the descriptor of a new socket that is automatically
created upon execution of the accept call. This new socket is paired with the client's
socket so that the server can continue to use the original socket with descriptor s for
accepting further connection requests from other clients.
Primitives for Sending and Receiving Data
A variety of system calls are available for sending and receiving data. The four most
commonly used are:
nbytes = read (sneMl, buffer,amount)
write(s, "message," msg_length)
amount = recvfrom (s, buffer,sender_address)
sendtots, "message," receiver_address)

Sec. 3.12 • Summary 157
The read and write system calls are most suitable for use in connection-based
communication.The writeoperation is usedbyaclient tosend amessage toaserver.The
socket tobeused forsending themessage, the message, and thelength ofthemessage are
specified as parameters to the call. The readoperation is used by the server process to
receivethemessage sentbytheclient.The socketoftheservertowhichtheclient'ssocket
is connected and the buffer for storing the received message are specified as parameters
to the call. The call returns the actual number of characters received. The socket
connection establishment between the client and the server behaves like a channel of
stream data that does not contain any message boundary indications. That is, the sender
pumps data intothe channel and the receiver reads them in the same sequence as written
bythecorrespondingwrite operations. The channel size islimited by abounded queue at
the receiving socket. The sender blocks if the queue is full and the receiver blocks if the
queue is empty.
On theother hand, the recvfromand sendtosystem calls are most suitable for usein
case of connectionless communication. The sendtooperation is used by a sender to send
amessagetoaparticularreceiver.The socketthrough which themessage istobesent,the
message, and a reference to a structure containing the socket address of the receiver to
which the message is to be sent are specified as parameters to this call. The reevfrom
operation is used by a receivertoreceivea message from a particular sender.The socket
through whichthe message istobereceived, thebuffer where themessage istobestored,
and a reference to a structure containing the socket address of the sender from which the
message is to be received are specified as parameters to this call. The recvfrom call
collects the first message in the queue at the socket. However, if the queue is empty, it
blocks until a message arrives.
Figure 3.19 illustrates the useof sockets forconnectionlesscommunicationbetween
two processes. In the socket call, the specification of AF_INET as the first parameter
indicates that the communication domain is the Internet communication domain, and the
specification ofSOCK_DGRAMasthesecondparameterindicates thatthesocketisofthe
datagram type (used for unreliable, connectionless communication).
Alternatively, Figure 3.20 illustrates the use of sockets for connection-based
communicationbetween aclient processandaserverprocess.Thespecification ofSOCK_
STREAM as the second parameter of the socket call indicates that the socket is of the
stream type (used for reliable, connection-based communication).
3.12 SUMMARY
Interprocess communication (IPC) requires information sharing among two or more
processes. The two basic methods for information sharing are original sharing (shared
data approach) and copy sharing (message-passing approach). Since computers in a
network do not share memory, the message-passing approach is most commonly used in
distributed systems.
A message-passing system is a subsystem of a distributed operating system that
provides a set of message-based protocols, and it does so by shielding the details of
complex network protocols and multiple heterogeneous platforms from programmers,

158 Chap. 3 • Message Passing
s=socket(AF_'NET,SOCK_DGRAM,0);
bind(s,sender_address,server_address_length);
sendto(s,"message",receiver_address);
close(s);
(a)
s
=socket(AF_INET,SOCK_DGRAM,0);
bind(s,receiver_address,receiver_address_length)
Fig. 3.19 Useofsockets forconnectionless
amount = recvfrom(s,buffer,sender_address); communication between two
processes. (a) Socket-related
systemcalls insender'sprogram.
close(8);
(b) Socket-related systemcalls in
(b) receiver'sprogram.
Some of the desirable features of a good message-passing system are simplicity,
uniform semantics, efficiency, reliability, correctness, flexibility, security, and port
ability.
Thesenderandreceiverofamessagemaycommunicateeitherinthesynchronousor
asynchronous mode. As compared to the synchronous mode of communication, the
asynchronous mode provides better concurrency, reduced message traffic, and better
flexibility. However, the asynchronous communication mode is more complicated to
implement, needs message buffering, and requires programmers to deviate from the
traditional centralized programming paradigm.
The four types of buffering strategies that may be used in the design of IPC
mechanisms are a null buffer,or no buffering; a simple-message buffer; an unbounded
capacity buffer; and a finite-bound, or multiple-message, buffer.
Messages are transmitted over a transmission channel in the form of packets.
Therefore, for transmitting a message that is greater than the maximum size of a packet,
the logical message has to be separated (disassembled) and transmitted in multiple
packets. Such a message is called a multipacket or a multidatagram message.
Encoding is the process of converting the program objects of a message to a
stream form that is suitable for transmission over a transmission channel. This process
takes place on the sender side of the message. The reverse process of reconstructing
program objects from message data on the receiver side is known as decoding of the
message data.

Sec. 3.12• Summary 159
5=
socket(AF_INET,SOCK_STREAM,0);
connect(s,server_address,server_address_length);
write(s,"message",mSQ_length);
close(s):
(a)
s=socket(AF_INET,SOCK__STREAM,0);
bind(5,server_address,server_address_length);
listen(s,backlog);
=
snew accept(s,client_address,client_address_length);
nbytes=read(snew,buffer,amount);
Fig.3.20 Useofsockets for
connection-basedcommunication
between aclient and aserver.
close(snew);
(a) Socket-relatedsystem calls in close(s);
client'sprogram. (b) Socket-related
system calls inserver'sprogram. (b)
Another major issue in message passing is addressing (or naming) of the parties
involved inaninteraction. Aprocess mayormaynotexplicitlynameaprocess withwhich
it wants to communicate depending upon whether it wants to communicate only with a
specific process or with any process of a particular type. An important goal in process
addressing is to provide location transparency. The most commonly used method to
achieve this goal isto usc a two-level naming schemefor processes and aname server to
map high-level, machine-independent process names to their low-level, machine
dependent names.
Failure handling is another important issue inthe design of an IPe mechanism. The
twocommonly usedmethods inthedesignofareliable IPeprotocolaretheuseofinternal
retransmissions based on timeouts and the use of explicit acknowledgment packets. Two
important issues related to failure handling in IPC mechanisms are idempotency and
handling of duplicate request messages and keeping track of lost and out-of-sequence
packets in multidatagram messages.
Themostelementaryformofmessage-basedinteractionisone-to-onecommunication
inwhich asingle sending process sendsamessage toasingle receiving process. However,
for better performance and flexibility, several distributed systems provide group com-

160 Chap. 3 • Message Passing
municationfacilitiesthatmayallowone-to-many,many-to-one,andmany-to-manytypesof
interactionsbetweenthesendersandreceivers. Someissuesrelatedtogroupcommunica
tionaregroupmanagement,groupaddressing,atomicity,andorderedmessagedelivery.
EXERCISES
3.1. What are the.main reliability issues in designing a message-passing system? Describe a
suitable mechanism for handling each of these issues.
3.2. Whatarethemainissuesrelatedtothecorrectness oftheIPCprotocolsofamessage-passing
system? Describe a suitable mechanism for handling each of these issues.
3.3. Describe someflexibility features thatamessage-passing system should provide toitsusers.
Write suitable IPC primitives that will allow the users to take advantage of these flexibility
features.
3.4. Describe blocking and nonblocking types of fPC. Which is easier to implement and why?
Discuss their relative advantages and disadvantages.
3.5. Writethecodeforimplementingaproducer-consumerpairofprocesses forthefollowing two
cases:
(a) They usea single-message buffer.
(b) They usea bufferthatcan accommodate up ton messages.
The producer produces messages and puts them in the message buffer, and the consumer
consumes messages from the message buffer.Assume that all messages are of fixed size.
3.6. Whatisadatagram?WhyaremultidatagrammessagesusedinIPC?Whatarethemainissuesin
IPCofmultidatagrammessages?Describeamechanismforhandlingeachofthese issues.
3.7. In a multidatagram communication, explain how the recipient can properly recognize and
arrange thedatagrams of the same message and howcan itrecognize the Jastdatagram ofa
message.
3.8. Alinkedlistofcharactersistobesentacrossanetworkintheformofastreamofbytes.Write
thecode inanysuitableprogramming language forencoding thedata structure onthesender
side and decoding iton the receiver side.
3.9. Write the code for implementing the bulletin-board semantics for one-to-many communica
tion for the following cases:
(a) A message posted on the bulletin board isautomatically removed after n receivers have
read it.
(b) A message postedon thebulletin board isautomatically removed after the lapseof time
t.Time tisa parameter specified by the sender of a message.
Assume that only one message can beposted on the bulletin board at a time. If the bulletin
board is not empty at the time a sender wants to post a message, a "not empty, try again"
message is returned to the sender. You may make any other assumptions that you feel
necessary, but clearly state your assumptions.
3.10. Give two examples of applications for which each of the following types of multicast
communication is most suitable:
(a) The O-reliable
(b) The l-reliable
(c) The m-out-of-n reliable (1<m< n)
(d) All-reliable

Chap. 3 • Exercises 161
3.11. What is meant by"orderedmessage delivery"? Do all applications need the same semantics
for this property? If yes, explain why. If no, give examples of two applications that need
different semantics for this property.
3.12. Explain what is meant by absolute ordering, consistent ordering, and causal ordering of
messages. Give a mechanism to implement each one.
3.13. Describe a mechanism for implementing consistent ordering of messages in each of the
following cases:
(a) One-to-many communication
(b) Many-to-one communication
(c) Many-to-many communication
3.14. Describe three different process-addressing mechanisms. Discuss their relative advantages
anddisadvantages. Whichofthemechanismsdescribed byyouismostsuitableforeachofthe
following cases (give reasons for your answer):
(a) For communication between a server process and several client processes. The client
processes send request messages to the server process and the server process returns a
reply for each client request.
(b) For allowing a sender process to send messages to a group of processes.
(c) For allowing a sender process to send messages to a receiver process that is allowed to
migrate from one node to another.
(d) For allowing asender process to send messages to a receiver process that is allowed to
migratefromonenodetoanother andtoallowthereceiver process toreturnareplytothe
sender process.
(e) For allowing a client process to receive service from anyone of the several server
processes providing that service.
3.15. What is an idempotent operation? Which of the following operations are idempotent:
(a) read_next_record(filename)
(b) readrecord (filename, recordnumber)
(c) append_record(filename, record)
(d) writerecord (filename, after_record_n, record)
(e) seek (filename, position)
(f) add (integer_I, integer_2)
(g) increment (variable_name)
3.16. Suggest a suitable mechanism for implementing each of the following types of IPe
semantics:
(a) Last one
(b) At least once
(c) Exactly once
3.17. The operations performed by a server are nonidempotent. Describe a mechanism for
implementing exactly-once IPe semantics inthis case.
3.18. Suggest whether at-least-once or exactly-once semantics should be used for each of the
following applications (give reasons for your answer):
(a) For making a request to a file server to read a file
(b) For making a request to a file server to append some data to anexisting file
(c) For making a request toacompilation server tocompile a file
(d) For making arequest to adatabase server to update a bank account
(e) For making arequest to adatabase server to get the current balance of a bank account
(f) For making a request to a booking server to cancel an already booked seat

162 Chap.3 • MessagePassing
3.19. SuggestasuitablemechanismforimplementingreliableIPCwithexactly-oncesemanticsin
eachofthefollowingcases:
(a) Thecomputersofthesenderandreceiverprocessesarereliablebutthecommunication
linksconnectingthemareunreliable.
(b) Thecomputersofthesenderandreceiverprocessesareunreliablebutthecommunication
linksconnectingthemarereliable.
(c) The computer of the sender process and the communication links connecting the
computers of the sender and receiver processes are reliable but the computer of the
receiverprocessisunreliable.
(d) The computer of the receiver process and the communication links connecting the
computersofthesenderandreceiverprocessesarereliablebutthecomputerofthesender
processisunreliable.
3.20. Isitalwaysnecessaryforthesenderofamessagetoknowthatthemessagearrivedsafelyat
itsdestination?Ifyes,explainwhy.Ifno,givetwoexamplesinsupportofyouranswer.
3.21. What is the main purposeof usingan acknowledgmentmessagein an IPC protocol?Are
acknowledgmentmessagesalwaysneededforreliablecommunication?Givereasonsforyour
answer.
3.22. Whatis "piggybacking"of a message?Howdoesithelpinreducingnetworktraffic?Give
someexamplesofwherethepiggybackingschememaybeusedtoimproveperformancein
distributedsystems.
3.23. Inmanyclient-serversystems,thetimeoutmechanismisusedtoguardagainstthehangingof
a client forever if the server crashes.That is, in these systems, if after sending a request
messagetoa serveraclientdoesnotreceivea replyfromtheserverwithinafixedtimeout
interval, the client assumesthat the server has crashed and can take necessarycorrective
actions. What should bethe ideal length ofthe timeout period inthese systems?Iftheserver
computer isfullyreliable,isitstill usefulto usethetimeoutmechanism?Give reasonsfor
youranswer.
3.24. A file server services file read/write requests of multiple clients. Clients can directly
communicate with the file server by sending messagesto it and receivingmessagesfrom
it.
(a) Describethecontentsofthemessagethataclientmustsendtothefileserverforreading
aportionofafile.
(b) Describethecontentsofthemessagethattheservermustreturntotheclientinreplyto
part(a).
(c) Describethecontentsofthemessagethataclientmustsendtothefileserverforwriting
somedatatoanexistingfile.
(d) Describethecontentsofthemessagethattheservermustreturntotheclientinreplyto
part(c).
(e) Describe a mechanismby whichthe file servercan cope withmultipleclient requests
arrivingalmostsimultaneously.
3.25. Inaclient-serverIPC,aclientsendsarequestmessagetoaserver,andtheserverprocesses
the request and returnsthe resultof the requestprocessingto the client. Is it usefulfor a
processtobehavebothasaclientandaserver?Ifno,explainwhy.Ifyes,giveanexample
insupportofyouranswer.
3.26. Afilestorageandretrievalsystemconsistsofthefollowingtypesofprocesses:severalclient
processes, a file manager process, a cache manager process, and several disk manager
processes.Theseprocessesperformthefollowingjobs:

Chap. 3 • Bibliography 163
(a) Aclient process sends file access requests to thefile managerprocess.
(b) The file manager process, on receiving aclient request, sends the request to the cache
manager.
(c) If the data is available in the cache, the cache managerextracts the data from the cache
and sends it in a reply message to the file manager. Otherwise, the cache managersends
a request for the data to all the disk managers.
(d) On receiving a request from the cache manager, a disk managersearches for the datain
its own disk and returns a suitablereply to the cache manager.Thatis, if the data is not
found, a "notfound" message is returned; otherwise the requested data is returned in a
message.
(e) Upon receipt ofthe data, the cache managercachesthe datain the cache and also sends
it to the file manager.
(0 The file manager performs the requested operation on the data and finally returns a
suitable reply to the client process.
What form ofIPC do the different types ofprocesses ofthis application use for interaction
among them? Write properIPC primitives to define the interface between these processes.
3.27. Write the skeleton ofthe processes of the file storage and retrieval system ofExercise 3.26
using the primitives of the 4.3BSD UNIX IPC mechanism to show how they will
communicate with each other. Use connection-based communication.
BIBLIOGRAPHY
[Ahamad and Bernstein1985]Ahamad,M.,and Bernstein,A.J., "AnApplicationofNameBased
AddressingtoLow LevelDistributedAlgorithms,"IEEETransactions onSoftwareEngineering,
Vol.SE-l1,No.1, pp. 59-67 (1985).
[Andrews 1991] Andrews, G., "Paradigms for Process Interaction inDistributedPrograms,"ACM
Computing Surveys, Vol. 23, No. r,pp. 49-90 (1991).
[Artsyet ale1987]Artsy, Y.,Chang,H.,andFinkel,R.,"InterprocessCommunicationinCharlotte,"
IEEE Software, pp. 22--28 (1987).
[Bal and Tanenbaum 1989] Bal, H. E., and Tanenbaum, A. S., "Programming Languages for
Distributed Computing Systems," ACM Computing Surveys, Vol. 21, No.3, pp. 261-322
(1989).
[Birman 1993]Birman, K.P.,"The Process GroupApproach to Reliable DistributedComputing,"
Communications oftheACM, Vol.36, pp. 36-53 (1993).
[Birman and Joseph 1987]Birman, K. P., and Joseph, T. A., "Reliable Communication in the
Presence of Failures," ACM Transactions on Computer Systems, Vol. 5, No.1, pp. 47-76
(1987).
[Birman and Van Renesse 1994] Birman, K. P., and Van Renesse, R., Reliable Distributed
Computing with the ISIS Toolkit,IEEE ComputerSociety Press, Los Alamitos, CA (1994).
[Birman et ale1991] Birman, K. P., Schiper, A., and Stephenson, P., "Lightweight Causal and
AtomicGroup Multicast,"ACM Transactions on Computer Systems, Vol.9, No.3,pp. 272-314
(1991).
[CCITT1985]RecommendationX.409:PresentationTransferSyntaxand Notation.Red Book,Vol.
VIII, International Telecommunications Union, Geneva, Switzerland (1985).

164 Chap.3 • Message Passing
[Chang and Maxemehuk 1984] Chang, J. M., and Maxemchuk, N. F., "Reliable Broadcast
Protocols,"ACM Transactionson ComputerSystems,Vol.2,pp. 39-59(1984).
[Changand Maxemchuk 1985]Chang, J.M.,and Maxemchuk, N.F.,"A BroadcastProtocol for
Broadcast Networks,"ACM Transactionson ComputerSystems,Vol.3, No.1 (1985).
[Dijkstra 1975] Dijkstra, E.W.,"GuardedCommands,Nondeterminacy, and Formal Derivationof
Programs," CommunicationsoftheACM, Vol.18,No.8, pp. 453-457 (1975).
[Draves 1990]Draves, R.P.,"TheRevised IPCInterface," In:Proceedingsofthe FirstUSENIX
Mach Symposium, USENIX, Berkeley, CA, pp. 101-121, (1990).
[Ezhilcbelvanet al, 1995]Ezhilchelvan,P.D.,Macedo, R.A.,andShrivastava,S. K.,"Newtop:A
Fault-Tolerant Group Communication Protocol," In: Proceedings of the 15th International
Conferenceon DistributedComputingSystems, IEEE (1995).
[Fitzgerald and Rashid 1986] Fitzgerald, R., and Rashid, R. F., "The Integration of Virtual
Memory Management and Interprocess Communication in Accent," ACM Transactions on
ComputerSystems, Vol.4, No.2, pp. 147-177 (1986).
[Franketal, 1985] Frank, A.J.,Wittie, L.D.,and Bernstein,A.J., "MulticastCommunicationon
Network Computers," IEEESoftware,Vol.2,No.3, pp. 49-61 (1985).
[Gammage and Casey 1985] Gammage, N., and Casey, L., "XMS: A Rendezvous Based
Distributed System Software Architecture," IEEE Software, Vol. 2, No.3, pp. 9-19
(1985).
[Gammage et al. 1987]Gammage, N. D., Kamel, R. F.,and Casey, L., "Remote Rendezvous,"
Software-Practiceand Experience, Vol.17, No. 10,pp.741-755 (1987).
[Garcia-Molinaand Spauster1991]Garcia-Molina, H.,and Spauster, A.,"Orderedand Reliable
MulticastCommunication,"ACMTransactions onComputerSystems, Vol.9,No.3,pp.242-271
(1991).
[Garg 1996] Garg, V. K., Principles ofDistributed Systems, Kluwer Academic, Norwell, MA
(1996).
[Gehani 1984]Gehani, N. H., "Broadcast Sequential Processes (BSP)," IEEE Transactions on
SoftwareEngineering, Vol.SE-IO, No.4, pp. 343-351 (1984).
[Gehani 1987] Gehani,N.H., "MessagePassing: SynchronousversusAsynchronous,"AT&TBell
Laboratories, Murray Hill, NJ (1987).
[Islam and Roy 1995]Islam, N., and Roy, H., "Techniques for Global Optimization ofMessage
Passing Communication on Unreliable Networks," In: Proceedings ofthe 15th International
Conferenceon Distributed ComputingSystems, IEEE, Piscataway, NJ (1995).
[Jalote 1994]Jalote,P.,Fault ToleranceinDistributedSystems,Prentice-Hall, Englewood Cliffs,
NJ(1994).
[Jia 1995] Jia, X., "A Total Ordering Multicast Protocol Using Propagation Trees," IEEE
Transactionson Paralleland DistributedSystems,Vol.6, No.6, pp.617-627 (1995).
[Kaashoekand Tanenbaum1991JKaashoek,M.F.,andTanenbaum,A. S.,"GroupCommunica
tion in the AmoebaDistributed Operating System," In:Proceedingsofthe 11thInternational
Conference on Distributed Computing Systems, IEEE Press, Piscataway, NJ, pp. 222-230
(1991).
[Kaashoek et al. 1989]Kaashoek, M.F.,Tanenbaum, A. S., Hummel, S., and Bal, H. E., "An
Efficient Reliable Broadcast Protocol," Operating Systems Review, Vol. 23, pp. 5-19
(1989).

Chap.3 • Bibliography 165
[Kranz et al, 1993] Kranz, D., Johnson, K., Agarwal, A., Kubiatowicz, 1. 1., and Lim, B.,
"IntegratingMessage PassingandSharedMemory:EarlyExperiences,"In:Proceedings ofthe
4thSymposium onPrinciples andPracticeofParallel Programming,AssociationforComputing
Machinery,NewYork,NY, pp.54-63 (1993).
[Liang et al, 1990] Liang, L., Chanson, S. T., and Neufeld, W.,"Process Groups and Group
Communications: Classifications and Requirements," IEEE Computer, Vol. 23, pp. 56-66
(1990).
[Luan and Gligor 1990] Luan, S. W.,andGligor,V. D., "A Fault-TolerantProtocolforAtomic
Broadcast," JEEE Transactions on Parallel and Distributed Systems, Vol. I, pp. 271-285
(1990).
[Meliar-Smith et al, 1990] Meliar-Smith, P. M., Moser, L. E., and Agrawala, V.,"Broadcast
ProtocolsforDistributedSystems,"IEEETransactionsonParallelandDistributedSystems, Vol.
1,pp. 17-25 (1990).
[Milenkovic 1992]Milenkovic,M.,Operating Systems: Concepts and Design, 2nded.,McGraw
Hill,NewYork,NY(1992).
[Miller et ale 1987] Miller, B. P, Presotto, D. L., and Powell, M. L., "DEMOSIMP: The
DevelopmentofaDistributedOperatingSystem,"Software-Practiceand Experience, Vol. 17,
No.4, pp.277-290 (1987).
[Mullender 1993] "Interprocess Communication," In: S. Mullender(Eds.),Distributed Systems,
2nded., AssociationforComputingMachinery,NewYork,NY,pp.217-250 (1993).
[Natrajan 1985] Natrajan, N., "Communication and SynchronizationPrimitives for Distributed
Programs," IEEE Transactions on Software Engineering, Vol. SE-l1, No.4, pp. 396-416
(1985).
[Navratnam et al, 1988] Navratnarn, S., Chanson, S:, and Neufeld, G., "Reliable Group
CommunicationinDistributedSystems,"In:Proceedings ofthe8thInternational Conference on
Distributed Computing Systems, IEEEPress,Piscataway,NJ,pp.439-446 (1988).
[Ramanathan and Shin 1992] Ramanathan, P., and Shin, K. G., "Delivery of Time-Critical
Messages Using aMultipleCopy Approach,"ACMTransactions on ComputerSystems, Vol. 10,
pp. 144-166 (1992).
[Shatz 1984] Shatz,S.M.,"CommunicationMechanismsforProgrammingDistributedSystems,"
IEE~E Computer, pp.21-28 (June 1984).
[Stempleet al. 1986] Stemple,D.W.,VinterS.T.,andRamamritham,K.,"FunctionalAddressing
inGutenberg:InterprocessCommunicationwithoutProcessIdentifiers,"IEEE Transactions on
Software Engineering, Vol. SE-12, No. 11,pp. 1056-1066 (1986).
[Sun1990]SunMicrosystemsInc.,Network Programming, SunMicrosystems,MountainView,CA
(March 1990).
[Tanenbaum 1995] Tanenbaum,A.S.,Distributed Operating Systems, Prentice-Hall,Englewood
Cliffs,NJ(1995).
[Tanenbaum et al, 1992] Tanenbaum, A. S., Kaashoek, M. F., and Bal, H. E., "Parallel
Programming Using Shared Objects and Broadcasting," JEEE~ Computer, pp. 10-19 (August
1992).
[Xerox 1981]XeroxCorporation, Courier: The Remote Procedure Call Protocol. Xerox Systems
Integration Standards, XeroxCorporation,Standard,CT(1981).
[Yeung and Yum 1995] Yeung,K. H., and Yum,T. S., "Selective BroadcastData Distribution
Systems," In: Proceedings of the 15th International Conference on Distributed Computing
Systems, IEEEPress,Piscataway,NJ(1995).

166 Chap. 3 • Message Passing
POINTERS TO818ll0GRAPHIES ONTHE INTERNET
Icouldnotfindabibliographydedicatedonlytomessagepassing.However,thefollowing
bibliographies containreferences onthetopicscoveredinthischapter:
ftp:ftp.cs.umanitoba.calpublbibliographiesIOsIlMMD_IV.html
ftp:ftp.cs.umanitoba.calpublbibliographies/Os/os.html
ftp:ftp.cs.umanitoba.calpublbibliographieslParalleUJPDC.html
ftp:ftp.cs.umanitoba.calpublbibliographieslParalleUpvrn.htrnl

4
CHAPTER
Remote Procedure
Calls
4.1 INTRODUOION
The general message-passing model of interprocess communication (IPC) was pre
sented in the previous chapter. The IPe part of a distributed application can often
be adequately and efficiently handled by using an IPe protocol based on the
message-passing model. However, an independently developed IPC protocol is tai
lored specifically to one application and does not provide a foundation on which to
build a variety of distributed applications. Therefore, a need was felt for a general
IPe protocol that can be used for designing several distributed applications. The
Remote Procedure Call (RPC) facility emerged out of this need. It is a special case
of the general message-passing model of IPC. Providing the programmers with a
familiar mechanism for building distributed systems is one of the primary motiva
tions for developing the RPC facility. While the RPe facility is not a universal
panacea for all types of distributed applications, it does provide a valuable commu
nication mechanism that is suitable for building a fairly large number of distributed
applications.
167

168 Chap. 4 • Remote Procedure Calls
The RPC has become a widely accepted IPC mechanism in distributedsystems. The
popularityofRPC as the primary communication mechanism for distributed applications
is due to its following features:
1. Simple call syntax.
2. Familiar semantics (because ofits similarity to local procedure calls).
3. Its specification of a well-defined interface. This property is used to support
compile-time type checking and automated interface generation.
4. Itsease ofuse.Theclean and simple semanticsofaprocedurecall makes iteasier
to build distributed computations and to get them right.
5. Its generality. This feature is owing to the fact that in single-machine
computations procedure calls are often the most important mechanism for
communication between parts ofthe algorithm [Birrell and Nelson 1984].
6. Its efficiency. Procedure calls are simple enough for communication to be quite
rapid.
7. It can be used as an IPC mechanism to communicate between processes on
different machines as well as between different processes on the same
machine.
4.1 THE RPe MODEL
The RPC model is similar to the well-known and well-understood procedure call model
used for the transfer ofcontrol and data within a program in the following manner:
1. For making aprocedurecall, thecallerplaces argumentstotheprocedureinsome
well-specified location.
2. Controlisthen transferredtothesequenceofinstructionsthatconstitutesthebody
ofthe procedure.
3. The procedure body is executed in a newly created execution environment that
includes copies ofthe arguments given in the calling instruction.
4. After the procedure's execution is over, control returns to the calling point,
possibly returning a result.
The RPC mechanism is an extension ofthe procedure call mechanism in the sense
that it enables a call to bemade to a procedure that does not reside in theaddress space
ofthe callingprocess. The calledprocedure(commonlycalled remoteprocedure)may be
on the same computer as the calling process or on a different computer.
Incase ofRPC, since thecallerand the calleeprocesseshave disjointaddress spaces
(possibly on different computers), the remote procedure has no access to data and
variables ofthe caller'senvironment. Therefore the RPC facility uses a message-passing
scheme for information exchange between the callerand the callee processes. As shown
in Figure 4.1, when a remote procedure call is made, the callerand the callee processes
interact in the following manner:

Sec.4.2 • The RPC Model 169
Caller Callee
(clientprocess) (serverprocess)
I
I
,
I
Requestmessage f
Callprocedureand (containsremoteprocedure'sparameters~
waitforreply
I
I Receiverequestand
startprocedure
execution
Procedure
executes
Sendreplyandwait
fornextrequest
Resumeexecution
Fig. 4.1 Atypical model ofRemote Procedure Call.
1. The caller (commonly known asclient process) sends acall (request) message to
the callee (commonly known as server process) and waits (blocks) for a reply
message. The request message contains the remote procedure's parameters,
among other things.
2. Theserverprocessexecutes theprocedure andthenreturns theresultofprocedure
execution ina reply message to the client process.
3. Oncethereply message isreceived, theresultofprocedure execution isextracted,
and the caller's execution is resumed.
The server process is normally dormant, awaiting the arrival of a request message.
When one arrives, the server process extracts the procedure's parameters, computes the
result, sends a reply message, and then awaits the next call message.
Note that in this model of RPC, only one of the two processes is active at any
given time. However, in general, the RPC protocol makes no restrictions on the
concurrency model implemented, and other models of RPC are possible depending on
the details of the parallelism of the caller's and callee's environments and the RPC
implementation. For example, an implementation may choose to have RPC calls to be
asynchronous, so that the client may do useful work while waiting for the reply from
the server. Another possibility is to have the server create a thread (threads are
described in Chapter 8) to process an incoming request, so that the server can be free
to receive other requests.

170 Chap.4 • Remote Procedure Calls
4.3 TRANSPARENCY OF RPC
A majorissue in the design ofan RPC facility is its transparency property. A transparent
RPC mechanismisone in which local proceduresand remoteproceduresare (effectively)
indistinguishabletoprogrammers.This requires the following two types oftransparencies
[Wilbur and Bacarisse 1987]:
I. Syntactic transparencymeans thataremoteprocedurecall shouldhave exactlythe
same syntax as a local procedure call.
2. Semantic transparency means that the semantics of a remote procedure call are
identical to those ofa local procedure call.
It is not very difficult to achieve syntactic transparency ofan RPC mechanism, and
we have seen that the semantics ofremote procedure calls are also analogous to that of
local procedure calls for most parts:
• The calling process is suspended until the called procedure returns.
• The callercan pass arguments to the called procedure (remote procedure).
• The called procedure (remote procedure) can return results to the caller.
Unfortunately, achieving exactly the same semantics for remote procedure calls as
for local procedurecalls is close to impossible [Tanenbaumand VanRenesse 1988].This
is mainly because ofthe following differences between remote procedure calls and local
procedure calls:
1. Unlike local procedurecalls, with remoteprocedurecalls, the called procedure is
executedinanaddressspace that isdisjointfrom thecallingprogram'saddressspace. Due
to this reason, the called (remote) procedure cannot have access to any variables or data
values in the calling program's environment. Thus in the absence ofshared memory, it is
meaningless to pass addresses in arguments, making call-by..reference pointers highly
unattractive. Similarly, it is meaningless to pass argument values containing pointer
structures (e.g., linked lists), since pointers are normally represented by memory
addresses.According toBal et al. [1989], dereferencingapointerpassedby the callerhas
to be done at the caller's side, which implies extra communication. An alternative
implementation is to send a copy ofthe value pointed at the receiver, but this has subtly
differentsemanticsand may be difficulttoimplementifthepointerpointsinto the middle
ofa complex data structure, such as a directed graph. Similarly, call by reference can be
replaced by copy in/copy out, but at the cost ofslightly different semantics.
2. Remote procedurecalls are more vulnerable to failure than local procedurecalls,
since they involve two different processes and possibly a network and two different
computers. Therefore programs that make use ofremote procedure calls must have the
capability ofhandling even those errors that cannot occur in local procedure calls. The
need for the ability totake care ofthe possibilityofprocessorcrashesand communication

Sec.4.4 • Implementing RPCMechanism 171
problems of a network makes it even more difficult to obtain the same semantics for
remote procedure calls as for local procedure calls.
3. Remote procedure calls consume much more time (100-1000 times more) than
local procedurecalls. This is mainly due to the involvement ofacommunicationnetwork
in RPCs. Therefore applications using RPCs must also have the capability to handle the
long delays that may possibly occur due to network congestion.
Becauseofthesedifficultiesinachievingnormalcall semanticsfor remoteprocedure
calls, some researchers feel that the RPCfacility should be nontransparent. Forexample,
Hamilton [1984] argues that remote procedures should be treated differently from local
procedures from the start, resulting in a nontransparent RPC mechanism. Similarly, the
designers ofRPC inArgus [Liskov and Scheifler 1983]wereofthe opinionthat although
the RPC system should hide low-level details ofmessage passing from the users, failures
and long delays should not be hidden from the caller. That is, the callershould have the
flexibility of handling failures and long delays in an application-dependent manner. In
conclusion, although in most environments total semantic transparency is impossible,
enough can be done to ensure that distributed application programmers feel
comfortable.
4.4 IMPLEMENTING RPC MECHANISM
To achieve the goal ofsemantic transparency, the implementationofan RPCmechanism
is based on the concept ofstubs, which provide a perfectly normal (local) procedure call
abstraction by concealing from programs the interface to the underlying RPC system. We
saw that an RPC involves aclientprocess and a serverprocess. Therefore, to conceal the
interface of the underlying RPC system from both the client and server processes, a
separatestub procedureisassociatedwith eachofthe two processes. Moreover,tohide the
existence and functional details of the underlying network, an RPC communication
package (known as RPCRuntime) is used on both the client and server sides. Thus,
implementation of an RP(~ mechanism usually involves the following five elements of
program [Birrell and Nelson 1984]:
1. The client
2. The client stub
3. The RPCRuntime
4. The server stub
5. The server
The interactionbetween them is shown in Figure4.2. Theclient, the clientstub, and
one instance ofRPCRuntime execute on the client machine, while the server, the server
stub, and anotherinstanceofRPCRuntimeexecuteon the servermachine.Thejobofeach
ofthese elements is described below.

172 Chap. 4 • Remote Procedure Calls
Clientmachine Servermachine
Client Server
Call
Clientstub Serverstub
Pack
APCRuntime RPCRuntime
Send
Callpacket
Resultpacket
Fig. 4.2 Implementationof RPCmechanism.
4.4.1 Client
The client is a user process that initiates a remote procedure call. To make a remote
procedurecall, theclientmakesaperfectly normallocalcallthatinvokes acorresponding
procedure in the client stub.
4.4.1 Client Stub
The client stub is responsible for carrying out the following two tasks:
• On receipt of a call request from the client, it packs a specification of the target
procedure andtheargumentsintoa message andthenasksthelocal RPCRuntime
to send it to the server stub.

Sec. 4.4 • Implementing RPC Mechanism 173
• On receiptofthe result of procedureexecution, it unpacks the result and passes it
to the client.
4.4.3 RPCRuntlme
The RPCRuntime handles transmission of messages across the network between client
and server machines. It is responsible for retransmissions, acknowledgments, packet
routing, and encryption.TheRPCRuntimeon the clientmachinereceives the call request
message from the client stub and sends it to the server machine. It also receives the
messagecontainingthe resultofprocedureexecution from the servermachineand passes
it to the client stub.
On the other hand, the RPCRuntime on the server machine receives the message
containingthe resultofprocedureexecutionfrom the serverstub and sends it tothe client
machine. It also receives the call request message from the client machine and passes it
to the server stub.
4.4.4 ServerStub
The job of the server stub is very similar to that of the client stub. It performs the
following two tasks:
• On receiptofthe call requestmessagefrom the local RPCRuntime,the serverstub
unpacks it and makes a perfectly normal call to invoke the appropriate procedure
in the server.
• On receipt of the result of procedure execution from the server, the server stub
packs the result into a message and then asks the local RPCRuntime to send it to
the client stub.
4.4.5 Server
On receiving a call request from the server stub, the server executes the appropriate
procedure and returns the result of procedure execution to the server stub.
Note here that the beauty ofthe whole scheme is the total ignorance on the part ofthe
clientthat the work was done remotely insteadofby the local kernel.Whentheclientgets
control following the procedure call that it made, all it knows is that the results of the
procedureexecutionare availabletoit.Therefore, as far as the clientisconcerned,remote
services are accessed by making ordinary (local) procedure calls, not by using the send
and receive primitives ofChapter 3.All the details ofthe message passing are hidden in
the clientand serverstubs, makingthe steps involvedinmessagepassinginvisibleto both
the client and the server.

174 Chap. 4 • Remote Procedure Calls
4.5 STU8 GENERATION
Stubs can begenerated in one of the following two ways:
1. Manually. In this method, the RPC implementor provides a set of translation
functions from which a usercan constructhis or her own stubs. This method is simple to
implement andcan handle very complex parameter types.
2. Automatically. This is the more commonly used method for stub generation. It
uses Interface Definition Language (JDL) that is used to define the interface between a
clientand a server.An interface definition is mainly a list of procedure names supported
by the interface, together with the types of their arguments and results. This is sufficient
information for the client and server to independently perform compile-time type
checking and to generate appropriate calling sequences. However, an interface definition
alsocontainsother information thathelps RPCreduce data storage and theamount ofdata
transferred over the network. For example, an interface definition has information to
indicate whethereach argument is input, output, or both-only input arguments need be
copied from client to server and only output arguments need be copied from server to
client. Similarly, an interface definition also has information about type definitions,
enumeratedtypes, and defined constants thateach side uses tomanipulatedata from RPC
calls, making it unnecessary for both the client and the server to store this information
separately. (See Figure 4.21 for an example of an interface definition.)
A server program that implements procedures in an interface is said to exportthe
interface, and aclient program thatcalls proceduresfroman interface issaid toimportthe
interface. When writing a distributed application, a programmer first writes an interface
definition using the IDL. He or she can then write the client program that imports the
interface and the server program that exports the interface. The interface definition is
processedusinganIDLcompilertogenerate componentsthatcanbecombinedwithclient
and server programs, without making anychanges totheexisting compilers. Inparticular,
fromaninterfacedefinition, anIDLcompilergeneratesaclientstubprocedureandaserver
stub procedure for each procedure in the interface, the appropriate marshaling and
unmarshaling operations (described later in this chapter) in each stub procedure, and a
headerfilethatsupportsthedatatypesintheinterfacedefinition. Theheaderfile isincluded
in the source files ofboth the client and server programs, the client stub procedures are
compiledand linked withtheclient program, and the server stub proceduresarecompiled
andlinked withtheserver program.AnIDLcompilercan bedesigned toprocess interface
definitionsforusewithdifferentlanguages, enablingclients andservers writtenindifferent
languages,tocommunicatebyusingremote procedure calls.
4.6 RPe MESSAGES
Any remote procedurecall involves aclient process and aserver process that arepossibly
located on different computers. The mode ofinteraction between the client and server is
that the client asks the server to execute a remote procedure and the server returns the

Sec.4.6 • RPCMessages 175
result of execution of the concerned procedure to the client. Based on this mode of
interaction, the two types of messages involved in the implementation of an RPC system
are as follows:
1. Call messages that are sent by the client tothe serverfor requesting execution of
a particular remote procedure
2. Reply messages that are sent by the server to the client for returning the result of
remote procedure execution
The protocol of the concerned RPC system defines the format of these two types of
messages. Normally, an RPC protocol isindependentof transportprotocols. That is,RPC
does not care how a message is passed from one process to another. Therefore an RPC
protocol deals only with the specification and interpretation of these two types of
messages.
4.6.1 Call M.ssag.s
Since acall message isusedtorequest executionofaparticularremote procedure,thetwo
basic components necessary in a call message are as follows:
1. The identification information of the remote procedure to be executed
2. The arguments necessary for the execution of the procedure
In addition to these two fields, a call message normally has the following fields:
3. A message identification field that consists of a sequence number. This field is
useful intwo ways-foridentifyinglost messages andduplicatemessagesincase
of system failures and for properly matching reply messages to outstanding call
messages, especially in those cases where the replies of several outstanding call
messages arrive out of order.
4. A message type field that is used to distinguish call messages from reply
messages. For example, in an RPC system, this field may be set to 0 for all call
messages and set to 1for all reply messages.
5. A client identification field that may be used for two purposes-to allow the
server of the RPC to identify the client to whom the reply message has to be
returned and to allow the server to check the authentication of the client process
for executing the concerned procedure.
Thus, a typical RPC call message format may be of the form shown in Figure 4.3.
4.6.2 R.ply Messages
When the serverof an RPC receives a call message from a client, itcould befaced with
one of the following conditions. Inthe list below, it is assumed for a particularcondition
that no problem was detected by the server for any of the previously listed conditions:

176 Chap. 4 • Remote Procedure Calls
(
Remoteprocedureidentifier )
Message Message Client
identifier type identifier Program Version Procedure Arguments
number number number
(
.)
Fig.4.3 AtypicalRPCcallmessageformat.
1. The server finds that the call message is not intelligible to it. This may happen
when a call message violates the RPC protocol. Obviously the server will reject
such calls.
2. The server detects by scanning the client's identifier field that the client is not
authorized tousetheservice.The server willreturn anunsuccessful reply without
bothering to make an attempt to execute the procedure.
3. The server finds thattheremote program, version,or procedure number specified
in the remote procedure identifier field of the call message is not available with
it.Againtheserverwillreturnanunsuccessful replywithoutbothering tomakean
attempt to execute the procedure.
4. If this stage isreached, an attempt will be made toexecute the remote procedure
specified in thecall message. Therefore itmay happen that the remote procedure
is not able to decode the supplied arguments. This may happen due to an
incompatible RPC interface being used by the client and server.
5. An exception condition (such as division by zero) occurs while executing the
specified remote procedure.
6. The specified remote procedure is executed successfully.
Obviously, in the first five cases, an unsuccessful reply has to besent to the client
with the reason for failure inprocessing the request and a successful reply has to be sent
in the sixth case with the result of procedure execution. Therefore the format of a
successful replymessageandanunsuccessful replymessage isnormally slightlydifferent.
Atypical RPCreply message formatforsuccessful andunsuccessful replies maybeofthe
form shown in Figure 4.4.
The message identifier field of a reply message is the same as that of its
correspondingcall message sothatareply message can beproperly matched with itscall
message. The message type field isproperly settoindicate that itisareply message. For
asuccessful reply,thereplystatusfieldisnormally settozeroandisfollowed bythefield
containing the result of procedure execution. For an unsuccessful reply, the reply status
field iseither setto 1ortoanonzero valuetoindicate failure. Inthelattercase, the value
of the reply status field indicates the type of error. However, in either case, normally a
short statementdescribing thereason forfailure isplaced inaseparate field following the
reply status field.
Since RPC protocols are generally independent of transport protocols, it is not
possible for an RPC protocol designer to fix the maximum length of call and reply

Sec.4.7 • Marshaling Arguments andResults 177
c
)
Message Message Reply
identifier type status Resuit
(successful)
(
.,J
(a)
Message Message Reply Reasonfor
identifier type status failure
Fig. 4.4 Atypical RPC reply message (unsuccessful)
format: (a) asuccessful reply
message format; (b) an unsuccessful
reply message format. (b)
messages. Therefore, for a distributed application to work for a group of transports, itis
important forthedistributedapplication developers toensure thattheirRPCcallandreply
messages do not exceed the maximum length specified by any of the transports of the
concerned group.
4.7 MARSHALING ARGUMENTS AND RESULTS
Implementation of remote procedure calls involves the transfer of arguments from the
client process to the server process and the transfer of results from the server process to
theclient process. These arguments andresults arebasicaIJylanguage-leveldatastructures
(program objects), which are transferred in the form of message data between the two
computers involved in the call. We have seen in the previous chapter that transfer of
message data between two computers requires encoding and decoding of the message
data. ForRPCsthisoperationisknownasmarshalingand basically involves thefollowing
actions:
1. Taking the arguments (of a client process) or the result (of a server process) that
will form the message data to be sent to the remote process.
2. Encoding the message data of step 1 above on the sender's computer. This
encoding process involves the conversion of program objects into a stream form
that is suitable for transmission and placing them into a message buffer.
3. Decoding of the message data on the receiver's computer. This decoding process
involves the reconstruction of program objects from the message data that was
received in stream form.
In order that encoding and decoding of an RPC message can be performed
successfully, the order and the representation method (tagged or untagged) used to

178 Chap. 4 • Remote Procedure Calls
marshalargumentsand resultsmustbeknownto both theclientand the serverofthe RPC.
Thisprovidesadegreeoftype safetybetweenaclientand aserverbecausethe serverwill
not accept a call from a client until the client uses the same interface definition as the
server. Type safety is ofparticular importance to servers since it allows them to survive
against corrupt call requests.
Themarshalingprocessmustreflectthestructureofalltypes ofprogramobjectsused
in the concerned language. These include primitive types, structured types, and user
defined types. Marshaling procedures may be classified into two groups:
1. Those provided as a part ofthe RPC software. Normally marshaling procedures
for scalardatatypes, together with procedures to marshal compound types built from the
scalar ones, fall in this group.
2. Those that are defined by the users of the RPC system. This group contains
marshalingproceduresforuser-defineddata types and data types that includepointers.For
example, in ConcurrentCLU, developed for use in the Cambridge DistributedComputer
System [Bacon and Hamilton 1987], for user-defined types, the type definition must
contain procedures for marshaling.
AgoodRPC systemshouldalwaysgeneratein-linemarshalingcodefor everyremote
call so that the users are relieved of the burden of writing their own marshaling
procedures. However, practically it is difficult to achieve this goal because of the
unacceptable large amounts of.code that may have to be generated for handling all
possible data types.
4.8 SERVER MANAGEMENT
In RPC-based applications, two important issues that need to be considered for server
management are server implementation and server creation.
4.8.1 ServerImplementation
Based on the style of implementation used, servers may be of two types: stateful and
stateless.
Stateful Servers
Astatefulservermaintainsclients' state informationfrom one remoteprocedurecall tothe
next. That is, in case oftwo subsequent calls by a client to a stateful server, some state
information pertaining to the service performed for the client as a result ofthe first call
execution is stored by the serverprocess. Theseclients' state information is subsequently
used at the time ofexecuting the second call.
Forexample, let us consider a server for byte-stream files that allows the following
operations on files:

Sec.4.8 • ServerManagement 179
Open (filename, mode): This operation is used to open a file identified byfilename
inthe specifiedmode. When the server executesthis operation, itcreatesanentry for
this file in afile-table that it uses for maintaining the file state informationof all the
open files.The file state informationnormally consistsoftheidentifierofthefile, the
open mode, andthecurrentpositionofanonnegativeintegerpointer, called theread
write pointer. When a file is opened, its read-write pointer is set to zero and the
server returns to the client a file identifier (jid), which is used by the client for
subsequent accesses to that file.
Read tfid, n, buffer): This operation is used to get n bytes of data from the file
identifiedbY.tidintothebuffernamed buffer.Whentheserverexecutesthisoperation,
itreturns totheclient nbytesoffiledatastarting fromthebytecurrentlyaddressedby
theread-writepointerandthen incrementstheread-writepointerbyn.
Write (fid, n,buffer): Onexecutionofthisoperation, theservertakes nbytes ofdata
from the specified buffer, writes it into the file identified byfid at the byte position
currently addressed by the read-write pointer, and then increments the read-write
pointer by n.
Seektfid, position): This operationcauses theserver tochangethe valueoftheread
write pointer of the file identified bylid to the new value specified as position.
Close(fid): This statementcauses the server todelete from itsfile-table the filestate
information of the file identified byfide
The file server mentioned above is stateful because it maintains the current state
information for a file that has been opened for use by a client. Therefore, as shown in
Figure 4.5, after opening a file, if a client makes two subsequent Read tfid, 100, buJ)
calls, the first call will return the first 100 bytes (bytes 0-99) and the second call will
return the next 100 bytes (bytes 100-199).
Clientprocess Serverprocess
Open(filename. mode)
Filetable
RIW
Return(tid) fid Mode pointer
Read(tid,100,buf)
Return(bytes0to99)
Read(fid,100,buf)
Return(bytes100to199)
Fig. 4.5 Anexampleof astateful file server.

180 Chap. 4 • Remote Procedure Calls
Stateless Servers
A stateless server does not maintain any client state information. Thereforeevery request
from aclient must beaccompaniedwithallthenecessary parameterstosuccessfullycarry
out the desired operation. For example, a server for byte stream files that allows the
following operations on files is stateless.
Read (filename, position, n, buffer): On execution of this operation, the server
returns to the client n bytes of data ofthe file identified byfilename. The returned
data is placed in the buffer named buffer. The value of actual number of bytes read
isalso returned totheclient. The position within thefilefrom where tobegin reading
is specified as theposition parameter.
Write (filename, position, n, buffer): When the server executes this operation, it
takes nbytes ofdata from the specified bufferand writesitinto the file identified by
filename. The position parameter specifies the byte position within the file from
where to start writing. The server returns to the client the actual number of bytes
written.
As shown in Figure 4.6, this file server does not keep track of any file state
information resulting from a previous operation. Therefore if a client wishes to have
similar effect as that in Figure 4.5, the foJJowingtwo Read operations must be carried
out:
Read (filename, 0, 100, buf)
Read (filename, 100, 100, buf)
Notice that in this case the client has to keep track of the file state information.
Clientprocess Serverprocess
Filestateinformation
n F a il m e- e Modep R oi I n W ter Read(filename,0,100,bUf) ...
Return(0to99bytes)
"'
Read(filename,100,100,buf)
...
.... Return(100to199bytes)
Fig.4.6 Anexample ofastateless fileserver.

Sec. 4.8 • Server Management 181
Why Stateless Servers?
From the description of stateful and stateless servers, readers might have observed that
stateful servers provide an easier programming paradigm because they relieve the clients
from the task of keeping track of state information. In addition, stateful servers are
typically more efficient than stateless servers. Therefore, the obvious question that arises
is why should stateless servers be used at all.
The use ofstateless servers in many distributed applications isjustified by the fact
that stateless servers have a distinct advantage over stateful servers in the event of a
failure. For example, with stateful servers, if a server crashes and then restarts, the state
information that it was holding may be lost and the clientprocess mightcontinue its task
unaware of the crash, producing inconsistent results. Similarly, when a client process
crashes and then restarts its task, the server is left holding state information that is no
longervalid but cannoteasilybe withdrawn.Therefore,the clientofastatefulservermust
be properly designed to detect server crashes so that it can perform necessary error
handling activities. On the other hand, with stateless servers, a client has to only retry a
request until the serverresponds; it does not need to know that the serverhas crashed or
that the network temporarily went down. Therefore, stateless servers, which can be
constructed around repeatable operations, make crash recovery very easy.
Both stateless and stateful servershave theirownadvantagesand disadvantages.The
choice ofusing a stateless or a stateful server is purely application dependent. Therefore,
distributedapplication systemdesigners must carefully examinethe positiveand negative
aspects ofboth approaches for their applications before making a choice.
4.8.2 Server Creation Semantics
In RPC, the remoteprocedure to be executed as a result ofa remote procedure call made
by aclientprocess lies in aserverprocessthat is totally independentofthe clientprocess.
Independencehere meansthat the clientand serverprocesseshave separatelifetimes,they
normally run on separatemachines,and they havetheirown addressspaces.Sinceaserver
process is independent ofaclient process that makes a remote procedurecall to it, server
processesmay eitherbecreatedand installedbeforetheirclientprocessesor becreatedon
a demand basis. Based on the time duration for which RPC servers survive, they may be
classifiedas instance-per-calJ servers, instance-per-transactionlsession servers, or persist
ent servers.
Instance-per-Call Servers
Servers belonging to this category exist only for the duration ofa single call. A serverof
this type is created by RPCRuntime on the server machine only when a call message
arrives. The server is deleted after the call has been executed.
This approach for server creation is not commonly used because of the following
problems associated with it:
• The servers ofthis type are stateless because they are killed as soon as they have
serviced the call for which they were created. Therefore, any state that has to be

182 Chap. 4 • Remote Procedure Calls
preserved across server calls must be taken care of byeither the client process or
the supporting operating system. The involvement of the operating system in
maintaining intercall state information will make the remote procedure calls
expensive. Ontheotherhand,iftheintercallstateinformation ismaintained bythe
client process, the state information must be passed to and from the server with
each call. This will lead to the loss of data abstraction across the client-server
interface, which will ultimately result in loss of attractiveness of the RPC
mechanism to the programmers.
• When a distributed application needs to successively invoke the same type of
server several times, this approach appears more expensive, since resource
(memory space to provide buffer space and control structures) allocation and
deallocation has to be done many times. Therefore, the overhead involved in
server creation and destruction dominates the cost of remote procedure calls.
Instance-per-Session Servers
Servers belonging to this category exist for the entire session for which a client and a
server interact. Since a server of this type exists for the entire session, it can maintain
intercall state information, and the overhead involved in server creation and destruction
for a client-server session that involves a large number of calls is also minimized.
Inthis method, normally thereisaserver manager foreach type of service. Allthese
server managers are registered with the binding agent (binding agent mechanism for
binding aclient and aserver isdescribed later in thischapter). When aclient contacts the
binding agent, it specifies the type of service needed and the binding agent returns the
address oftheserver manager ofthedesired typetotheclient.Theclient thencontacts the
concerned server manager, requesting ittocreate aserver forit.The server manager then
spawns a new server and passes back its address to the client. The client now directly
interacts withthisserverfortheentiresession.Thisserverisexclusively usedbytheclient
for which it was created and is destroyed when the client informs back to the server
manager of the corresponding type that it no longer needs that server.
A server of this type can retain useful state information between calls and so can
present acleaner, moreabstract interface toitsclients. Notethataserver of this typeonly
services a single client and hence only has to manage a single set of state information.
Persistent Servers
Apersistentserver generally remains inexistence indefinitely. Moreover, wesawthat the
servers of the previous two types cannot be shared by two or more clients because they
areexclusivelycreated foraparticularclient ondemand. Unlike them, apersistentserver
is usually shared by many clients.
Servers of this typeareusually created and installed before theclients thatuse them.
Each server independentlyexports its service byregistering itselfwith the binding agent.
Whenaclient contacts thebindingagentforaparticulartypeofservice,thebinding agent
selects aserver ofthat typeeither arbitrarily orbased onsomein-built policy (such asthe

Sec.4.9 • Parameter-Passing Semantics 183
minimum numberofclientscurrently boundtoit) andreturns theaddress oftheselected
servertotheclient.Theclientthendirectly interacts withthatserver.
Notethatapersistent servermaybesimultaneously boundtoseveralclients.Inthis
case, the serverinterleavesrequestsfrom a numberofclientsand thus has to concurrently
manage several sets of state information. If a persistent server is shared by multiple
clients, the remote procedure that it offers must be designed so that interleaved or
concurrent requests from different clients do not interfere with each other.
Persistent servers may also be used for improving the overall performance and
reliability of the system. For this, several persistent servers that providethe sametype of
service may be installed on different machines to provide either load balancing or some
measure of resilience to failure.
4.9 PARRMETER·PASSING SEMANTICS
The choiceof parameter-passingsemantics iscrucialtothe design of an RPC mechanism.
The two choices are call-by-value and call-by-reference.
4.9.1 (all-by-Value
In the eall-by-value method, all parameters are copied into a message that is transmitted
from the clientto the serverthrough the intervening network. This poses no problems for
simple compact types such as integers, counters, small arrays, and so on. However,
passing larger data types such as multidimensional arrays, trees, and so on, can consume
much time for transmission of data that may not be used. Therefore this method is not
suitable for passing parameters involving voluminous data.
An argument in favor of the high cost incurred in passing large parameters by value
is that it forces the users to be aware ofthe expense ofremote procedure calls for large
parameter lists. In turn, the users are forced to carefully consider their design of the
interface needed between client and server to minimize the passing of unnecessary data.
Therefore, before choosing RPC parameter-passing semantics, itis importantto carefully
review and properly design the client-server interfaces so that parameters become more
specific with minimal data being transmitted.
4.9.2 (all-by-Reference
MostRPC mechanisms use thecall-by-valuesemanticsfor parameterpassingbecausethe
client and the serverexist in different address spaces, possibly even on different types of
machines, so that passing pointers or passing parameters by reterenee is meaningless.
However, a few RPC mechanisms do allow passing ofparameters by reference in which
pointerstothe parametersarepassedfrom theclienttothe server.Theseareusuallyclosed
systems, where a single address space is shared by all processes in the system. For
example, distributed systems having distributed shared-memory mechanisms (described
in Chapter 5) can allow passing ofparameters by reference.

184 Chap. 4 • Remote Procedure Calls
In an object-based system that uses the RPC mechanism for object invocation, the
call-by-reference semantics is known as call-by-object-reference. This is because in an
object-based system, the value of a variable is a reference to an object, so it is this
reference (the object name) that is passed in an invocation.
Emerald [Black et al. 1986, 1987] designers observed that the use of a call-by
object-reference mechanism in distributed systems presents a potentially serious
performance problem because on a remote invocation access by the remote operation
to an argument is likely to cause an additional remote invocation. Therefore to avoid
many remote references, Emerald supports a new parameter-passing mode that is known
as call-by-move. In call...by-move, a parameter is passed by reference, as in the method
of call-by-object-reference, but at the time of the call, the parameter object is moved
to the destination node (site of the callee). Following the call, the argument object may
either return to the caller's node or remain at the callee's node (these two modes are
known as call-by-visit and call-by-move, respectively).
Obviously,the useofthecall-by-move mode forparameter passing requires that the
underlying system supports mobile objects that can be moved from one node toanother.
Emerald objects are mobile..
Notice thatcall-by-move does notchange theparameter-passing semantics, whichis
still call-by-object-reference. Therefore call-by-move is basically convenient and
optimizes performance. This is because call-by-move could be emulated as a two-step
operation:
• First move each call-by-move parameter object to the invokee's node.
• Then invoke the object.
However,performing themovesseparately wouldcausemultiplemessagestobesent
across the network. Thus, providing call-by-move as a parameter-passing mode allows
packaging oftheargumentobjects inthesamenetworkpacketastheinvocation message,
thereby reducing the network traffic and message count.
Although call-by-move reduces the cost of references made by the invokee, it
increases the cost of the invocation itself. If the parameter object is mutable and shared,
it also increases the cost of references by the invoker [Black et al. 1987].
4.10 CAllSEMANTICS
In RPC, the caller and thecallee processes are possibly locatedon different nodes. Thus
it is possible for either the caller or the callee node to fail independently and later to be
restarted. In addition, failure of communication links between the caller and the callee
nodesisalsopossible.Therefore, thenormalfunctioningofanRPCmaygetdisrupteddue
to one or more of the following reasons:
• The call message gets lost.
• The response message gets lost.

Sec.4.10 • Call Semantics 185
• The callee node crashes and is restarted.
• The caller node crashes and is restarted.
Some elementof acaller's node that is involved in the RPC must contain necessary
code tohandle these failures. Obviously, thecode for thecaller'sprocedure should not be
forced to deal with these failures. Therefore, the failure-handling code isgenerally a part
of RPCRuntime. The call semantics of an RPC system that determines how often the
remote procedure may be executed under fault conditions depends on this part of the
RPCRuntimecode. This part of thecode may be designed toprovide theflexibility tothe
application programmers to select from different possible call semantics supported by an
RPC system. The different types of call semantics used in RPC systems are described
below.
4.10.1 Possibly or May-Ie Call Semantics
This is the weakest semantics and is not really appropriate to RPC but ismentioned here
for completeness. In this method, to prevent the caller from waiting indefinitely for a
response from thecallee, atimeout mechanismisused.That is,thecaller waitsuntilapre
determined timeout period and thencontinues with itsexecution. Therefore the semantics
does not guarantee anything about the receipt of the call message or the procedure
execution by the caller. This semantics may be adequate for some applications in which
the response message is not important for the caller and where the application operates
within a local area network having a high probability of successful transmission of
messages.
4.10.2 last-One Call Semantics
This call semantics is similar to the one described in Section 3.9 and illustrated with an
example in Figure 3.10. It uses the idea of retransmitting the call message based on
timeouts until a response is received by the caller. That is, the calling of the remote
procedure bythe caller, theexecution ofthe procedure bythe callee, and thereturn ofthe
result to the caller will eventually be repeated until the result of procedure execution is
received by the caller. Clearly, the results of the last executed call are used by the caller,
although earlier (abandoned) calls may have had side effects that survived the crash.
Hence this semantics is called last-one semantics.
Last-one semantics can beeasilyachieved inthewaydescribed above whenonly two
processors(nodes) are involved intheRPC. However, achieving last-one semantics inthe
presence of crashes turns out to be tricky for nested RPCs that involve more than two
processors (nodes) [Bal et a1. 1989J.For example, suppose process PI of node NI calls
procedureFl onnodeN2, whichinturncalls procedureF2onnodeN3.While theprocess
onN3isworking onF2, nodeN) crashes. NodeNI's processes willberestarted, andPI's
call toF) will berepeated. The second invocation of f"I will again call procedure F2on
node N3. Unfortunately, node N3 is totally unaware of node Nt'8 crash. Therefore
procedure F2will beexecutedtwice on nodeN3and N3 may return theresults ofthe two
executions of F2 in any order, possibly violating last-one semantics.

186 Chap.4 • Remote Procedure Calls
The basicdifficulty inachieving last-onesemantics insuchcasesiscaused byorphan
calls. An orphan call is one whose parent (caller) has expired due to a node crash. To
achieve last-one semantics, these orphan calls must be terminated before restarting the
crashed processes. This is normally done either by waiting for them to finish or by
tracking them down andkilling them ("orphan extermination").Asthis isnotaneasyjob,
other weaker semantics have been proposed for RPC.
4.10.3 last-of-Many Call S....antlcs
This is similarto the last-one semantics exceptthat theorphan calls are neglected [Balet
aI. 1989]. A simple way to neglect orphan calls is to use call identifiers to uniquely
identify each call. When a call is repeated, it is assigned a new call identifier. Each
response message has thecorrespondingcall identifierassociated with it.Acaller accepts
a response only if the call identifier associated with it matches with the identifier of the
most recently repeated call; otherwise it ignores the response message.
4.10.4 At-least-Once Call Semantics
This is an even weaker call semantics than the last-of-many call semantics. It just
guarantees that the call is executed one or more times but does not specify which results
are returned to the caller. It can be implemented simply by using timeout-based
retransmissions without caring for the orphan calls. That is, for nested calls, if there are
any orphan calls, it takes the result ofthe first response message and ignores the others,
whether or not the accepted response is from an orphan.
4.10.5 Exactly-OnceCall Semantics
This is the strongest and the most desirable call semantics because it eliminates the
possibility of a procedure being executed more than once no matter how many times
a call is retransmitted. The last-one, last-of-many, and at-least-once call semantics
cannot guarantee this. The main disadvantage of these cheap semantics is that they
force the application programmer to design idempotent interfaces that guarantee that if
a procedure is executed more than once with the same parameters, the same results
and side effects will be produced. For example, let us consider the example given in
[Wilbur and Bacarisse 1987] for reading and writing a record in a sequential file of
fixed-length records. For reading successive records from such a file, a suitable
procedure is
ReadNextRecord(Filename)
Ignoring initializationandend-of-fileeffects, each executionof thisprocedure willreturn
the next record from the specified file. Obviously, this procedure is not idempotent
because multiple execution ofthis procedure will return the successive records, which is
not desirable for duplicate calls that are retransmitted due to the loss of response
messages. This happens because intheimplementationofthisprocedure, theserver needs

Sec. 4.11 • Communication Protocols for RPCs 187
to keep track of the current record position for each client that has opened the file for
accessing. Therefore to design an idempotent interface for reading the next record from
the file, itis important that each client keeps track of its own currentrecord position and
the serverismadestateless,that is,noclientstate shouldbe maintainedon the serverside.
Basedon this idea, an idempotentprocedurefor readingthe nextrecord from asequential
file is
ReadRecordN(Filename, N)
whichreturnsthe Nth record from the specifiedfile. Inthis case,the clienthas tocorrectly
specify the value ofN to get the desired record from the file.
However, not all nonidempotent interfaces can be so easily transformed to an
idempotent form. For example, consider the following procedure for appending a new
record to the same sequential file:
AppendRecord(Filename, Record)
It is clearly not idempotent since repeated execution will add further copies ofthe same
record to the file. This interface may be converted into an idempotent interface by using
the following two procedures instead of the one defined above:
GetLastRecordNo(Filename)
WriteRecordN(FiJename, Record, N)
The first procedure returns the record number ofthe last record currently in the file, and
the secondprocedurewritesarecordataspecifiedpositioninthe file. Now,for appending
a record, the client win have to use the following two procedures:
Last =GetLastRecordNo(Filename)
WriteRecordN(Filename, Record, Last)
For exactly-once semantics, the programmer is relieved of the burden of
implementing the server procedure in an idempotent manner because the call semantics
itselftakes careofexecutingthe procedureonlyonce.As alreadydescribedinSection3.9
and illustrated with an example in Figure 3.12, the implementation ofexactly-once call
semantics isbased on the use oftimeouts, retransmissions, call identifiers with the same
identifier for repeated calls, and areply cache associated with the callee.
4.11 COMMUNICATION PROTOCOLS FOR RPCs
Different systems, developed on the basis ofremote procedure calls, have different IPe
requirements. Based on the needs ofdifferent systems, several communication protocols
have been proposed for use in RPCs. A brief description of these protocols is given
below.

188 Chap. 4 • Remote Procedure Calls
4.11.1 The hqu8st Protocol
Thisprotocol isalsoknownastheR (request)protocol [Spector 1982].ItisusedinRPCs
in which the called procedure has nothing to return as the result of procedure execution
and the client requires no confirmation that the procedure has been executed. Since no
acknowledgment orreply message isinvolved inthis protocol,only one message percall
istransmitted (fromclienttoserver)(Fig.4.7).Theclientnormallyproceeds immediately
after sending the request message as there is no need to wait for a reply message. The
protocol provides may-be call semantics and requires no retransmission of request
messages.
Client Server
I
--~-----------------~-----
• • •
I Requestmessage I •
• .. I
• Procedure I
FirstRPCI execution I
I• II
I • I
I -----------------I.-----I
I
--------------------~-----
I • I
I Requestmessage I I
I ..... I
NextRPe. • Procequre I •
I execution I
• I
I• I II
-----------------~-----
I
I
Fig.4.7 Therequest(R)protocol.
AnRPCthatusestheRprotocoliscalledasynchronousRPC.Anasynchronous RPC
helps in improving the combined performance of both the client and the server in those
distributed applications in which the client does not need a reply to each request. Client
performance isimproved becausetheclient isnotblocked andcan immediately continue
todoother workaftermakingthecall.Ontheotherhand,serverperformance isimproved
because the server need not generate and send any reply for the request. One such
application is a distributed window system.A distributed window system, such as X-11
[Davison et aI. 1992], is programmed as a server, and application programs wishing to
displayitemsinwindowsonadisplayscreenareitsclients.Todisplayitemsinawindow,
aclient normally sendsmanyrequests (each requestcontaining arelatively smallamount

Sec.4.11 • Communication Protocols forRPCs 189
of information for a small change in the displayed information) to the server one after
another without waiting for a reply for each of these requests because it does not need
replies for the requests.
Notice that for an asynchronous RPC, the RPCRuntime does not take responsibility
for retrying a request in case of communication failure. This means that if an unreliable
datagramtransport protocol such as UDP is used for the RPC, the request message could
be lost without the client's knowledge. Applications using asynchronous RPC with
unreliable transport protocol must be prepared to handle this situation. However, if a
reliable, connection-oriented transport protocol such asTCP is used forthe RPC, there is
noneed toworryabout retransmitting the request message because itisdelivered reliably
in this case.
Asynchronous RPCs with unreliable transport protocol are generally useful for
implementing periodic update services. For example, a time server node in a distributed
system may send time synchronizationmessages every Tseconds toothernodes using the
asynchronous RPC facility. In this case, even if a message is lost, the correct time is
transmitted in the next message. Each node can keep track of the last time it received an
update message toprevent itfrom missing toomany update messages. Anodethatmisses
too many update messages can send a special request message to the time server node to
get a reliable update after some maximum amount of time.
4.11.2 The Request/Reply Protocol
This protocol isalso known astheRR (request/reply)protocol [Spector 1982].Itisuseful
for the design of systems involving simple RPCs. A simple RPC is one in which all the
arguments as well as all the results fit in a single packet buffer and the duration of acall
and the interval between calls arc both short (less than the transmission time for a packet
between theclient andserver) [BirrellandNelson 1984].The protocol isbasedontheidea
of using implicit acknowledgment to eliminate explicit acknowledgment messages.
Therefore in this protocol:
• Aserver'sreply message isregarded asanacknowledgmentoftheclient'srequest
message.
• A subsequent call packet from a client is regarded as an acknowledgment of the
server's reply message of the previous call made by that client.
The exchangeofmessages between aclient and aserver intheRRprotocol isshown
in Figure 4.8. Notice from the figure that the protocol involves thetransmission of only
two packets per call (one in each direction).
The RR protocol in its basic form does not possess failure-handling capabilities.
Therefore to take care of lost messages, the timeouts-and-retries technique is normally
used along with the RR protocol. In t.his technique, a client retransmits its request
message if it does not receive the response message. before a predetermined timeout
period elapses. Obviously, if duplicate request messages are not filtered out, the RR
protocol, compounded with this technique, provides at-least-once call semantics.

190 Chap. 4 • Remote Procedure Calls
Client Server
:-t-----~~~~~~a~-----~----:
I I -.. I
I , Procedure I
FirstRPCI I execution I
I I Replymessage I
I I
I ["II'" Alsoservesasacknowledgment I
I fortherequestmessage I I
--~-----------------I-----
I
~-------------------~-----
I I
I Requestmessage I
I I
I Alsoservesasacknowledgment'" I
NextRPCI forthereplyofthepreviousRPC Procedure I
execution
I I
I Replymessage , I
I .... I
I Alsoservesasacknowledgment I I
_ _ __ _ _ JO.!.t'ler.e<1.U~tE1~s.!g! _ _ _ _ L- _ _ _ _ _
I
I
}'ig.4.8 The request/reply (RR)protocol.
However, servers can support exactly-once call semantics by keeping records of the
replies in a reply cache that enables them to filter out duplicate request messages and
to retransmit reply messages without the need to reprocess a request. The details of this
technique were given in Section 3.9.
4.11.3 The R8quest/Reply/Acknowledge.R.ply Protocol
This protocol is also known as the RRA (requestJreply/acknowledge-reply) protocol
[Spector 1982]. The implementation of exactly-once call semantics with RR protocol
requires the server to maintain a record of the replies in its reply cache. In situations
where a server has a large number of clients, this may result in servers needing to
store large quantities of information. In some implementations, servers restrict the
quantity of such data by discarding it after a limited period of time. However, this
approach is not fully reliable because sometimes it may lead to the loss of those
replies that have not yet been successfully delivered to their clients. To overcome
this limitation of the RR protocol, the RRA protocol is used, which requires clients
to acknowledge the receipt of reply messages. The server deletes an information
from its reply cache only after receiving an acknowledgment for it from the client.
As shown in Figure 4.9, the RRA protocol involves the transmission of three
messages per call (two from the clientto the server and one from the server to the
client).

Sec.4.12 • Complicated RPCs 191
Client Server
,... - I -----
----~~~---~-----~....
I
I Requestmessage
..I
I
I
I
I I I
I I Procedure I
FirstAPe I I execution I
I I Replymessage I
I , . . . . . . I
I Replyacknowledgmentmessage I I
I ......., I
~----------------- I
I
I
r- - ~-----------------
I
I
Requestmessage I
I l-..
I -.r
I
NextAPCI I Procedure
I execution
I
I
I Replymessage -
I
I ~ Replyacknowledgmentmessage I
I ........
~----------------- I
I
Fig. 4.9 The request/reply/acknowledge-reply (RRA)protocol.
In the RRA protocol, there is a possibility that the acknowledgment message may
itself get lost. Therefore implementation of the RRA protocol requires that the unique
message identifiers associated with request messages must be ordered. Each reply
message contains the message identifier of the corresponding request message, and each
acknowledgment message also contains the same message identifier. This helps in
matching a reply with its corresponding request and an acknowledgment with its
corresponding reply.A client acknowledges a reply message only if it has received the
replies to all the requests previous to the request corresponding to this reply. Thus an
acknowledgment message is interpreted as acknowledging the receipt of all reply
messages corresponding to the request messages with lower message identifiers.
Therefore the loss of an acknowledgment message is harmless.
4.12 COMPLICATED RPCs
Birrell and Nelson [1984] categorized the following two types of RPCs as complicated:
1. RPCs involving long-duration calls or large gaps between calls
2. RPCs involving arguments and/or results that are too large to fit in a single
datagram packet
Different protocols are used for handling these two types of complicated RPCs.

192 Chap. 4 • Remote Procedure Calls
4.11.1 APCs Involving Long-Duration Callsor Larg. Gaps
ktw••n Calls
One ofthe following two methods.may be used to handlecomplicated RPCs that belong
to this category [Birrell and Nelson 1984]:
1. Periodicprobing oftheserver by theclient. In this method, after aclientsends a
request message to a server, it periodically sends a probe packet to the server, which the
server is expected to acknowledge. This allows the client to detect a server's crash or
communication link failures and to notify the corresponding user of an exception
condition.The messageidentifieroftheoriginalrequestmessageisincludedineach probe
packet. Therefore, if the original request is lost, in reply to a probepacketcorresponding
to that request message, the server intimates the client that the request message
corresponding to the probe packet has not been received. Upon receipt of such a reply
from the server, the client retransmits the original request.
2. Periodic generation ofan acknowledgment by the server. In this method, if a
server is not able to generate the next packet significantly sooner than the expected
retransmission interval, it spontaneously generates an acknowledgment. Therefore for a
long-durationcall, the servermay have togenerateseveral acknowledgments, the number
ofacknowledgments being directly proportional to the duration ofthe call. If the client
does not receive either the reply for its request or an acknowledgment from the server
within a predetermined timeout period, it assumes that either the server has crashed or
communicationlink failure has occurred. In this case, it notifies the concerneduser ofan
exception condition.
4.11.1 RPCs Involving Long M.ssages
In some RPCs, the.arguments and/or results are too large to fit in a single-datagram
packet. For example, in a file server, quite large quantities ofdata may betransferred as
input arguments to the writeoperation or as results to the readoperation. A simple way
tohandlesuch an RPC istouse severalphysicalRPCs forone logical RPC. Each physical
RPC transfers an amount ofdata that fits in a single-datagram packet. This solution is
inefficientdue to afixed amountofoverheadinvolvedwith each RPC independentofthe
amount ofdata sent.
Another method of handling complicated RPCs of this category is to use
multidatagrammessages.Inthismethod,along RPC argumentorresult isfragmentedand
transmitted in multiple packets. To improve communication performance, a single
acknowledgment packet is used for all the packets ofa multidatagram message. In this
case, the same approachthat was describedinSection3.9 isused tokeep track oflostand
out-of-sequence packets ofamultidatagram RPC message.
Some RPC systems are limited to small sizes. For example, the SunMicrosystem's
RPC is limited to 8 kilobytes. Therefore, in these systems, an RPC involving messages
larger than the allowed limit must be handled by breaking it up into several physical
RPCs.

Sec.4.13 • Client-ServerBinding 193
4.13 CLIENT-SERVER BINDING
It is necessary for a client (actually a client stub) to know the location of a server before
a remote procedure call can take place between them. The process by which a client
becomes associated with a server so that calls can take place is known as binding. From
the application level's point of view, the model of binding is that servers "export"
operationstoregistertheirwillingnesstoprovideserviceandclients' "import"operations,
asking the RPCRuntime system to locate a server and establish any state that may be
needed at each end [Bershad et a1. 1987]. The client-server binding process involves
proper handling of several issues:
1. How does a client specify a server to which it wants to get bound?
2. How does the binding process locate the specified server?
3. When is it proper to bind a client to a server?
4. Is it possible for a client to change a binding during execution?
5. Can a client be simultaneously bound to multiple servers that provide the same
.
service()
These binding issues are described below.
4.13.1 Sarver Naming
The specification by a clientof a server with which it wants to communicate isprimarily
a naming issue. For RPC, Birrell and Nelson [1984] proposed the use of interface names
for this purpose. An interface name has two pal1s-atype and an instance. Type specifies
the interface itself and instance specifies a server providing the services within that
interface. For example, there may be an interface of type file_server, and there may be
several instances of servers providing file service. When a client is not concerned with
which particularserverofan interfaceservices itsrequest, itneed not specify the instance
part of the interface name.
The type part of an interface usually also has a version number field to distinguish
between old and new versions of the interface that may have different sets ofprocedures
or the same set of procedures with different parameters. It is inevitable in the course of
distributedapplicationprogrammingthat anapplication needs to beupdatedafteragiven
version has been released.The use of aversion numberfield allows old and new versions
ofadistributedapplicationtocoexist. One would hope thatthenew version ofaninterface
would eventually replace all the old versions of the interface. However, experience has
shownthat itisalways better to maintain backwardcompatibility with old versionsofthe
software because someone might still be using one ofthe old versions.
According to Birrell and Nelson [1984], the interface name semantics are based
on an arrangement between the exporter and the importer. Therefore, interface names
are created by the users. They are not dictated by the RPC package. The RPC package
only dictates the means by which an importer uses the interface name to locate an
exporter.

194 Chap. 4 • Remote Procedure Calls
4.13.2 Server LocQtlng
The interface name of a server is its unique identifier. Thus when a client specifies the
interfacename ofaserverfor making aremote procedure call, the server must be located
before theclient'srequest message canbesenttoit.This isprimarily alocating issue and
any locating mechanism (locating mechanisms are described in Chapter 10)can be used
for this purpose. The two most commonly used methods are as follows:
1. Broadcasting. In this method, a message to locate the desired server is broadcast
to all the nodes from the client node. The nodes on which the desired server is located
return aresponse message. Notethatthedesired servermaybereplicated onseveral nodes
sotheclient nodewillreceivearesponse fromallthesenodes.Normally,thefirstresponse
that is received at the client's node is given to the client process and all subsequent
responses are discarded.
This method is easy to implement and is suitable for use for small networks.
However, the method isexpensive for largenetworks because of the increase in message
traffic due to the involvement of all the nodes in broadcast processing. Therefore the
second method, which is based on the idea ofusing a name server, is generally used for
large networks.
2. Bindingagent. Abinding agent isbasically anameserver used tobind aclient to
aserver byproviding theclient withthelocation information ofthedesired server.Inthis
method, a binding agent maintains a binding table, which is a mapping of a server's
interface name to its locations. All servers register themselves with the binding agent as
apart of their initialization process. Toregister with thebinding agent, a server gives the
binder its identification information and a handle used to locate it.The handle is system
dependent and might be an Ethernet address, an IP address, an X.500 address, a process
identifiercontaininganodenumberandportnumber,orsomething else.Aservercanalso
deregister with the binding agent when it is no longer prepared to offer service. The
binding agentcanalsopolltheserversperiodically,automatically deregisteringanyserver
that fails to respond.
Tolocate aserver,aclient contacts the binding agent. Iftheserver isregistered with
the binding agent, it returns the handle (location information) of the server to the client.
The method is illustrated in Figure 4.10.
The binding agent's location is known to all nodes. This is accomplished by using
either a fixed address for the binding agent that is known to all nodes or a broadcast
message to locate the binding agent when a node is booted. In either case, when the
binding agent isrelocated, amessageissenttoallnodesinforming thenewlocationofthe
binding agent.
Abinding agent interface usuallyhasthreeprimitives: (a)registerisusedbyaserver
to register itself with the binding agent, (b) deregister is'used by a server to deregister
itselfwith the binding agent, and (c) lookup is used by a client to locate a server.
The binding agent mechanism for locating servers has several advantages. First, the
method can support multiple servers having the same interface type so that any of the
available servers may beused toservice aclient'srequest. This helps toachieve adegree
offaulttolerance. Second, sinceallbindingsaredone bythebindingagent, whenmultiple

Sec. 4.13 • Client-Server Binding 195
CD
Theserverregisters itselfwiththebindingagent.
®
Theclientrequests thebindingagentfortheserver's location.
®
Thebindingagentreturnstheserver'slocation informationtotheclient.
@
Theclientcallstheserver.
Fig. 4.10 The binding agent mechanism for locating aserver incase ofRPC.
servers provide the same service, the clients can be spread evenly over the servers to
balance the load. Third, the binding mechanism can be extended to allow servers to
specify a list of users who may use its service, in which case the binding agent would
refuse to bind those clients to the servers who are not authorized to use its service.
However, the binding agent mechanism has drawbacks. The overhead involved in
bindingclientsto serversislarge and becomessignificantwhen many clientprocessesare
shortlived. Moreover,in additiontoany functional requirements,abindingagentmustbe
robust against failures and should not become a performance bottleneck. Distributing the
binding function among several binding agents and replicating information among them
can satisfy both these criteria. Unfortunately, replication often involvesextraoverheadof
keeping the multiple replicas consistent. Therefore, the functionality offered by many
binding agents is lower than might be hoped for.
4.13.3 Binding Time
A client may be boundto a serveratcompile time, at link time, or atcall time [Goscinski
1991].
Binding at Compile Time
Inthis method, the clientand servermodulesare programmedas ifthey were intended to
be linked together. For example, the server's network address can be compiled into the
clientcode by the programmerand then it can be found by looking upthe server's name
in a file.

196 Chap. 4 • Remote Procedure Calls
The methodisextremely inflexible in the sense that iftheservermoves or theserver
is replicated or the interface changes, all client programs using the serverwill have to be
found and recompiled. However, the method is useful in certain limited cases. For
example,itmay beused inanapplicationwhose configurationisexpectedtoremainstatic
for a fairly long time.
Binding at Link Time
In this method, a server process exports its service by registering itself with the
binding agent as part of its initialization process. A client then makes an import
request to the binding agent for the service before making a call. The binding agent
binds the client and the server by returning to the client the server's handle (details
that are necessary for making a call to the server). Calls can take place once the
client has received the server's handle. The server's handle is cached by the client to
avoid contacting the binding agent for subsequent calls to be made to the same
server. Due to the overhead involved in contacting the binding agent, this method is
suitable for those situations in which a client calls a server several times once it is
bound to it.
Binding at Call Time
Inthis method,aclientisboundtoaserveratthe time when itcalls the serverfor the first
time during its execution. A commonly used approach for binding at call time is the
indirect call method, As shown inFigure4.11, inthis method, when aclientcalls aserver
for the first time, it passes the server's interface name and the arguments ofthe RPC call
to the binding agent. The binding agent looks up the location ofthe target server in its
bindingtable, and on behalfoftheclientitsends anRPC call messagetothe target server,
including in it the arguments received from the client. When the target serverreturns the
results to the binding agent, the binding agent returns this result to the client along with
the target server's handle so that the client can subsequently call the target server
directly.
4.13.4 ChangingIIndlngs
The flexibility provided by a system to change bindings dynamically is very useful
from a reliability point of view. Binding is a connection establishment between a
client and a server. The client or server of a connection may wish to change the
binding at some instance of time due to some change in the system state. For
example, a client willing to get a request serviced by anyone of the multiple servers
for that service may beprogrammed to change a binding to another server ofthe same
type when a call to the already connected server fails. Similarly, the server of a
binding may want to alter the binding and connect the client to another server in
situations such as when the service needs to move to another node or a new version
of the server is installed. When a binding is altered by the concerned server, it is

Sec. 4.13• Client-ServerBinding 197
CD
Theclientprocesspasses theserver's interface nameand
theargumentsoftheAPe calltothebindingagent.
®
ThebindingagentsendsanRPecall messagetotheserver,
includinginittheargumentsreceivedfromtheclient.
®
Theserver returnstheresultofrequest processing tothe
bindingagent.
o
a~ent
Thebinding returnsthisresulttotheclientalong
withtheservershandle.
®
Subsequentcallsaresentdirectlyfromtheclientprocess
totheserver process.
Fig. 4.11 Illustrating binding atcall time by the method of indirect call.
important to ensure that any state data held by the server is no longer needed or can
be duplicated in the replacement server. For example, when a file server has to be
replaced with a new one, either it must be replaced when no files are open or the state
of all the open files must be transferred from the old server to the new one as a part
of the.replacement process.
4.13.5 Multlpl. Simultan80uS Ilndlngs
In a system, a service may be provided by multiple servers. We have seen that, in
general, a client is bound to a single server of the several servers of the same type.
However, there may be situations when it is advantageous for a client to be bound
simultaneously to all or multiple servers of the same type. Logically, a binding of this
sort gives rise to multicast communication because when a call is made, all the servers
bound to the client for that service will receiveand process the call. For example, a
client may wish to update multiple copies of a file that is replicated at several nodes.
For this, the client can be bound simultaneously to file servers of all those nodes where
a replica of the file is located.

198 Chap. 4 • Remote Procedure Calls
4.14 EXCEPTION HANDUNG
We saw in Figure 4.4 that when a remote procedure cannot be executed successfully,
the server reports an error in the reply message. An RPC also fails when a client
cannot contact the server of the RPC. An RPC system must have an effective
exception-handling mechanism for reporting such failures to clients. One approach to
do this is to define an exception condition for each possible error type and have the
corresponding exception raised when an error of that type occurs, causing the
exception-handling procedure to be called and automatically executed in the client's
environment. This approach can be used with those programming languages that
provide language constructs for exception handling. Some such programming lan
guages are ADA, CLU [Liskov et al. 1981], and Modula-3 [Nelson 1991, Harbinson
1992]. In C language, signal handlers can be used for the purpose of exception
handling.
However, not every language has an exception-handling mechanism. For exam
ple, Pascal does not have such a mechanism. RPC systems designed for use with
such languages generalJy use the method provided in conventional operating systems
for exception handling. One such method is to return a well-known value to the
process, making a system call to indicate failure and to report the type of error by
storing a suitable value in a variable in the environment of the calling program. For
example, in UNIX the value -1 is used to indicate failure, and the type of error is
reported in the global variable errno. In an RPC, a return value indicating an error is
used both for errors due to failure to communicate with the server and errors
reported in the reply message from the server. The details of the type of error is
reported by storing a suitable value in a global variable in the client program. This
approach suffers from two main drawbacks. First, it requires the client to test every
return value. Second, it is not general enough because a return value used to indicate
failure may be a perfectly legal value to be returned by a procedure. For example, if
the value -1 is used to indicate failure, this value is also the return value of a
procedure call with arguments -5 and 4 to a procedure for getting the sum of two
numbers.
4.15 SECURITY
Some implementations of RPC include facilities for client and server authentication as
well as for providing encryption-based security for calls. For example, in [Birrell and
Nelson 1984], callers are given a guarantee ofthe identity of the callee, and vice versa,
by using the authentication service of Grapevine [Birrell et al. 1982]. For full end-to
end encryption ofcalls and results, the federal data encryption standard [DES 1977] is
used in [Birrell and Nelson 1984]. The encryption techniques provide protection from
eavesdropping (and conceal patterns ofdata) and detect attempts at modification, replay,
or creation of calls.

Sec.4.16 • SomeSpecialTypes ofRPCs 199
InotherimplementationsofRPC thatdonotincludesecurity facilities,the arguments
and results ofRPC are readablebyanyonemonitoringcommunicationsbetweenthecaller
and the callee. Therefore in this case, ifsecurity is desired, the user must implement his
or her own authentication and data encryption mechanisms. When designing an
application, the user should consider the following security issues related with the
communication of messages:
• Is the authentication of the server by the client required?
• Is the authentication of the client by the server required when the result is
returned?
• Is it all right ifthe arguments and results of the RPC are accessible to users other
than the caller and the callee?
These and other security issues are described in detail in Chapter 11.
4.16 SOME SPECIAL TYPES OF RPCs
4.16.1 Callback RPC
Inthe usual RPC protocol,thecallerandcalleeprocesseshave aclient-serverrelationship.
Unlike this, the callback RPC facilitates a peer-to-peerparadigm among the participating
processes. It allows a process to be both a client and a server.
Callback RPC facility is very useful in certain distributed applications. For
example, remotely processed interactive applications that need user input from time to
time or under special conditions for further processing require this type of facility. As
shown in Figure 4.12, in such applications, the client process makes an RPC to the
concerned server process, and during procedure execution for the client, the server
process makes a callback RPC to the client process. The client process takes necessary
action based on the server's request and returns a reply for the callback RPC to the
server process. On receiving this reply, the server resumes the execution of the
procedure and finally returns the result of the initial call to the client. Note that the
server may make several callbacks to the client before returning the result of the
initial call to the client process.
The ability for aservertocall itsclientback is very important, and care isneededin
the design ofRPC protocolstoensurethat it ispossible. In particular, toprovidecallback
RPC facility, the following are necessary:
• Providing the server with the client's handle
• Making the client process wait for the callback RPC
• Handling callback deadlocks
Commonly used methods to handle these issues are described below.

200 Chap. 4 • Remote Procedure Calls
Client Server
I
I
I
I
I
I
I
I
I
I
I Startprocedure
execution
Stopprocedure
executiontemporarily
Processcallback
requestandsend
reply
Resumeprocedure
execution
Procedureexecution
ends
Fig.4.12 ThecallbackRPC.
Providing the Server with the Client's Handle
The server must have the client's handle to call the client back. The client's handle
uniquely identifies the client process and provides enough information to the server
for making a call to it. Typically, the client process uses a transient program number
for the callback service and exports the callback service by registering its program
number with the binding agent. The program number is then sent as a part of the RPC
request to the server. To make a callback RPC, the server initiates a normal RPC
request to the client using the given program number. Instead of having the client just
send the server the program number, it could also send its handle, such as the port
number. The client's handle could then be used by the server to directly communicate

Sec.4.16 • Some Special Types ofRPCs 201
with the client and would save an RPC to the binding agent to get the client's
handle.
Making the Client Process Wait for the Callback RPC
The client process must be waiting for the callback so that it can process the incoming
RPC request from the serverand also toensurethat acallbackRPC from the serverisnot
mistaken to be the reply of the RPC call made by the client process. To wait for the
callback, a client process normally makes a call to a svc-routine. The svc-routine waits
until it receives a request and then dispatches the request to the appropriate procedure.
Handling Callback Deadlocks
In callback RPC, since a process may play the role ofeitheraclientor aserver, callback
deadlocks can occur. For example, consider the most simple case in which a process PI
makes anRPC callto aprocessP2andwaits for areply from Pz-Inthe meantime,process
Pzmakes an RPC call to another process P3 and waits for a reply from P 3•Inthe mean
time, process P3 makes an RPC call to process P and waits for a reply from Pl' But PI
J
cannotprocess P3'SrequestuntilitsrequesttoPzhasbeen satisfied,andP
2
cannotprocess
PI'Srequest until its request to P has been satisfied, and P3 cannot process P 'srequest
3 2
until its request to PI has been satisfied. As shown in Figure 4.13, a situation now exists
where PI is waiting for a reply from P2' which is waiting for a reply from P 3, which is
waiting for a reply from PI. The result is that none of the three processes can have their
requestsatisfied,and hence allthree willcontinueto waitindefinitely. Ineffect, acallback
deadlock has occurred due to the interdependencies ofthe three processes.
While using a callback RPC, care must be taken to handle callback deadlock
situations. Various methods for handling deadlocks are described in Chapter 6.
Fig.4.13 Anexampleofacallbackdeadlock P 1 iswaitingforR 21 (replyfrom P2toP t)
incaseofcallbackRPC
P2iswaitingforR
32
(replyfromP
3
toP2)
P3iswaitingforR (replyfromP to P3)
mechanism. 13 1

202 Chap. 4 • Remote Procedure Calls
4.16.1 Broadcast RPC
The RPC...based IPC is normally of theone-to-one type, involving a single client process
and a single server process. However, we have seen in the previous chapter that for
performance reasons several highly parallel distributed applications require the commu
nication system to provide the facility of broadcast and multicast communication. The
RPC-based IPC mechanisms normally support broadcast RPC facility for such
applications. In broadcast RPC, a client's request is broadcast on the network and is
processed byalltheserversthathave theconcerned procedure forprocessing thatrequest.
The client waits for and receives numerous replies.
A broadcast RPC mechanism may use one of the following two methods for
broadcasting a client's request:
1. The client has to use a special broadcast primitive to indicate that the request
message has to be broadcasted. The request is sent to the binding agent, which forwards
therequest toalltheserversregistered withit.Notethatinthismethod, sinceallbroadcast
RPC messages are sent to the binding agent, only services that register themselves with
their binding agent are accessible via the broadcast RPC mechanism.
2. The second method is to declare broadcast ports.Anetwork port ofeach node is
connected to a broadcast port. A network port of a node is a queuing point on that node
for broadcast messages. The client of the broadcast RPC first obtains a binding for a
broadcast portand then broadcasts the RPC message bysending the message tothisport.
Notethatthesameprimitive maybeusedforbothunicastandbroadcast RPCs. Moreover,
unlike the first method, this method also has the flexibility of being used for multicast
RPC in which the RPC message issent only to asubset ofthe available servers. Forthis,
theportdeclaration mechanism shouldhavetheflexibility toassociate onlyasubsetofthe
available servers to a newly declared multicast port.
Since a broadcast RPC message is sent to all the nodes of a network, a reply is
expected from each node.As already described intheprevious chapter, depending on the
degree of reliability desired, theclient process may wait for zero, one, m-out-of-n, or all
the replies. In some implementations, servers that support broadcast RPC typically
respond onlywhentherequest issuccessfully processedandaresilentinthefaceoferrors.
Such systems normally use some type of timeout-based retransmission mechanism for
improving the reliability of the broadcast RPC protocol. For example, in SunOS, the
broadcast RPC protocol transmits the broadcast and waits for 4 seconds before
retransmittingtherequest. Itthenwaitsfor6seconds beforeretransmitting therequestand
continues to increment the amount of time to wait by 2 seconds until the timeout period
becomes greater than 14seconds.Therefore, inthe worstcase,therequestisbroadcast six
times and the total wait time is 54 seconds (4+6+8+10+12+ 14). In SunOS, the
broadcast RPCuses unreliable,packet-basedprotocol forbroadcasting therequest, andso
the routine retransmits the broadcast requests by default. Increasing the amount of time
betweenretransmissionsisknownasaback-offalgorithm.Theuseofaback-offalgorithm
for timeout-based retransmissions helps inreducing the loadon thephysical network and
computers involved.

Sec.4.17 • RPCinHeterogeneous Environments 203
4.16.3 latch-Mode RPe
Batch-mode RPCisusedtoqueueseparateRPCrequests inatransmission bufferonthe
client side and then send them overthe network in one batch to the server. This helps in
the following two ways:
1. It reduces the overhead involved in sending each RPC request independently to
the server and waiting for a response for each request.
2. Applicationsrequiringhighercall rates (50-100remotecallsper second)may not
be feasible with most RPC implementations. Such applications can be
accommodated with the use ofbatch-mode RPC.
However,batch-modeRPC can beusedonly with thoseapplicationsinwhich aclient
has many RPC requests to send to aserver and the client does not need any reply for a
sequenceofrequests. Therefore, the requests are queued on the clientside, and the entire
queue ofrequests is flushed to the server when one ofthe following conditions becomes
true:
1. Apredetermined interval elapses.
2. A predetermined number ofrequests have been queued.
3. The amount of batched data exceeds the buffer size.
4. A call is made to one of the server's procedures for which a result is expected.
From aprogramming standpoint, the semanticsofsuchacall (nonqueueingRPC
request) shouldbe such that the servercan distinguish itfrom the queuedrequests
and send a reply for it to the client.
The flushing out ofqueued requests in cases 1, 2, and 3 happens independent ofa
nonqueuing RPC request and is not noticeable by the client.
Obviously, the queued messages should be sent reliably. Hence, a batch-mode RPC
mechanism requires reliable transports such as TCP. Moreover, although the batch-mode
optimization retains syntactic transparency, it may produce obscuretiming-relatedeffects
where other clients are accessing the server simultaneously.
4.17 RPe INHETEROGENEOUS ENVIRONMENTS
Heterogeneity is an important issue in the design ofany distributed application because
typically the more portablean application, the better. Whendesigning an RPC system for
a heterogeneous environment, the three common types of heterogeneity that need to be
considered are as follows:
1. Data representation. Machines having different architectures may use different
data representations. Forexample, integers may be represented with the most significant
byte at the low-byte address in one machine architecture and at the high-byte address in
another machine architecture. Similarly, integers may be represented in I's complement

204 Chap.4 • Remote Procedure Calls
notation in one machine architecture and in 2's complementnotation in anothermachine
architecture. Floating-pointrepresentations may also vary betweentwo differentmachine
architectures. Therefore, an RPC system for a heterogeneous environment must be
designedtotakecare ofsuch differencesindata representationsbetweenthearchitectures
ofclient and server machines ofa procedure call.
2. Transportprotocol. For betterportabilityofapplications, an RPC system must be
independent of the underlying network transport protocol. This will allow distributed
applications using the RPC system to berun on different networks that use different
transport protocols.
3. Controlprotocol. For betterportabilityofapplications, an RPC systemmust also
be independent of the underlying network control protocol that defines control
information in each transport packet to track the state ofa call.
The most commonly used approach to deal with these types ofheterogeneity while
designingan RPC system for aheterogeneousenvironmentistodelay thechoicesofdata
representation, transport protocol, and control protocol until bind time. In conventional
RPC systems, all these decisions are made when the RPC system isdesigned. That is, the
binding mechanism of an RPC system for a heterogeneous environment is considerably
richerin information than the binding mechanism used byaconventional RPC system. It
includes mechanisms for determining which data conversion software (if any conversion
is needed), which transportprotocol, and which control protocol should be used between
a specific client and server and returns the correct procedures to the stubs as result
parameters of the binding call. These binding mechanism details are transparent to the
users. That is,applicationprograms never directly access the componentstructuresof the
bindingmechanism; they deal with bindingsonly asatomic types and acquireanddiscard
them via the calls ofthe RPC system.
Some RPC systems designed for heterogeneous environments are the HCS
(Heterogeneous ComputerSystems) RPC (called HRPC) [Bershad et al. 1987], the DeE
SRC (SystemResearchCenter)Firefly RPC [Schroederand Burrows 1990],Matchmaker
[Jones et aI. 1985], and Horus [Gibbons 1987].
4.18 UGHTWEIGHT RPe
The LightweightRemote Procedure Call(LRPC) was introducedbyBershadet aI.[1990]
and integrated into the Taos operating system of the DEC SRC Firefly microprocessor
workstation. The description below isbased on the material in their paper[Bershadet al.
1990J.
As mentioned in Chapter 1,based on the size of the kernel, operating systems may
be broadly classified into two categories-e-monolithic-kemel operating systems and
microkernel operating systems. Monolithic-kernel operating systems have a large,
monolithickernel thatisinsulated from userprogramsbysimple hardwareboundaries.On
the otherhand, in microkernel operating systems, a small kernel provides only primitive
operations and most of the services are provided by user-level servers. The servers are

Sec.4.18 • Lightweight RPC 205
usually implementedas processes and can be programmed separately. Each server forms
acomponentofthe operatingsystem and usually hasitsown address space.Ascompared
to the monolithic-kernel approach, in this approach services are provided less efficiently
because the various componentsof theoperatingsystem have to use some form ofIPC to
communicate with each other. The advantages of this approach include simplicity and
flexibility. Due to modular structure, microkemel operating systems are simple and easy
to design, implement, and maintain.
In the microkemel approach, when different components of the operating system
have their own address spaces, the address space of each component is said to form a
domain, and messages are used for all interdomain communication. In this case, the
communication traffic in operating systems are of two types [Bershad et al. 1990]:
1. Cross-domain, which involves communication between domains on the same
machine
2. Cross-machine, which involves communication between domains located on
separate machines
The LRPC is a communication facility designed and optimized for cross-domain
communications.
Although conventional RPC systems can be used for both cross-domain and cross
machine communications, Bershad et al. observed that the use of conventional RPC
systems for cross-domain communications, which dominate cross-machine communica
tions, incurs an unnecessarily high cost. This cost leads system designers to coalesce
weaklyrelatedcomponentsofmicrokernel operating systems intoasingledomain,trading
safety and performance. Therefore, the basic advantages of using the microkernel
approach are notfully exploited. Based onthese observations, Bershad etal.designed the
LRPC facility for cross-domain communications, which has better performance than
conventional RPC systems. Nonetheless, LRPC is safe and transparent and represents a
viable communication alternative for microkernel operating systems.
To achieve better performance than conventional RPC systems, the four techniques
described below are used by LRPC.
4.18.1 Simple ControlTransf.r
Whenever possible, LRPC usesacontrol transfermechanismthatissimplerthanthatused
inconventionalRPCsystems.Forexample, itusesaspecialthreadsschedulingmechanism,
called handoffscheduling (details of the threads and handoffscheduling mechanism are
giveninChapter8),fordirectcontextswitchfromtheclientthreadtotheserverthreadofan
LRPC. In this mechanism, when aclient calls a server's procedure, itprovides the server
withanargumentstackanditsownthreadofexecution. Thecallcausesatraptothekernel.
The kernel validates the caller, creates a call linkage, and dispatches the client's thread
directly tothe server domain, causing the server tostartexecutingimmediately. When the
calledprocedurecompletes,controlandresultsreturnthroughthekernelbacktothepointof
theclient'scall.Incontrasttothis,inconventionalRPCimplementations,contextswitching
between the client and server threads of an RPC is slow because theclient thread and the
serverthreadarefixedintheirowndomains, signalingoneanother atarendezvous, andthe

206 Chap.4 • Remote Procedure Calls
schedulermustmanipulatesystemdatastructurestoblocktheclient'sthreadandthenselect
oneoftheserver'sthreads forexecution.
4.18.2 Simpl. DataTransfer
InanRPC, argumentsandresultsneed tobepassedbetween theclientandserverdomains
inthe form ofmessages. AscomparedtotraditionalRPCsystems,LRPCreducesthecost
ofdatatransferbyperformingfewer copiesofthedata during itstransferfrom onedomain
to another. For example, let usconsiderthe path taken by a procedure's argumentduring
a traditional cross-domain RPC. As shown in Figure 4.14(a), an argument in this case
normally has to be copied four times:
Fourthcopy
, I, ~
.
Messagebuffer
Serverstack
Client'sdomain Server'sdomain
Messagebuffer
Kernel'sdomain
(a)
Asinglecopy
Clientstack Shared-argumentstack
accessiblebyboththe
clientandserver
Client'sdomain Globalsharedvirtualmemory Server'sdomain
(b)
Fig.4.14 Datatransfer mechanismsin traditional cross-domainRPCand LRPC.
(a) The path taken byaprocedure'sargumentduring atraditional
cross-domainRPC involves fourcopy operations. (b)The path tak.enbya
procedure'sargument duringLRPC involvesasingle-copyoperation.

Sec.4.18 • Lightweight RPC 207
1. From the client's stack to the RPC message
2. From the message in the client domain to the message in the kernel domain
3. From the message in the kernel domain to the message in the server domain
4. From the message in the server domain to the server's stack
To simplify this data transfer operation, LRPC uses a shared-argument stack that is
accessible to both the client and the server. Therefore, as shown in Figure 4.14(b), the
same argument in an LRPC can be copied only once-from the client's stack to the
shared-argument stack. The server uses the argument from the argument stack. Pairwise
allocation of argument stacks enables LRPC to provide a private channel between the
client and server and also allows the copying of parameters and results as many times as
are necessary to ensure correct and safe operation.
4.18.3 Simple Stubs
Thedistinctionbetween cross-domainandcross-machinecalls isusually made transparent
to the stubs by lower levels of the RPC system. This results inan interface and execution
path that are general but infrequently needed.
The use of a simple model of control and data transfer in LRPC facilitates the
generation of highly optimized stubs. Every procedure has a call stub in the client's
domainandanentry stub inthe server'sdomain. Athree-layeredcommunicationprotocol
is defined for each procedure in an LRPC interface:
1. End to end, described by the calling conventions of the programming language
and architecture
2. Stub to stub, implemented by the stubs themselves
3. Domain to domain, implemented by the kernel
To reduce the cost of interlayer crossings, LRPC stubs blur the boundaries between the
protocol layers. For example, at the time of transfer of control, the kernel associates
execution stacks with the initial call frame expected by the called server's procedure
and directly invokes the corresponding procedure's entry in the server's domain. No
intermediate message examination or dispatching is done, and the server stub starts
executing the procedure by directly branching to the procedure's first instruction. Notice
that with this arrangement a simple LRPC needs only one formal procedure call (into
the client stub) and two returns (one out of the server procedure and one out of the
client stub).
4.18.4 Design forConcurrency
When the node oftheclient andserver processesofanLRPC has multiple processorswith
a shared memory, special mechanisms are used to achieve higher call throughput and
lower call latency than ispossibleon asingle-processornode. Throughputisincreasedby
avoiding needless lock contention by minimizing the use of shared-datastructures on the

208 Chap. 4 • Remote Procedure Cans
critical domain transfer path. On the other hand, latency is reduced by reducing context
switching overhead by caching domains on idle processors. This is basically a
generalization of the idea of decreasing operating system latency by caching recently
blocked threads on idle processors to reduce wake-up latency. Instead of threads, LRPC
caches domains so that any thread that needs torun in thecontext of an idle domain can
do so quickly, notjust the thread that ran there most recently.
Basedontheperformance evaluation madebyBershadetale [1990],itwasfoundthat
LRPC achieves a factor-of-three performance improvement over more traditional
approaches. Thus LRPC reduces the cost of cross-domain .communication to nearly the
lower bound imposed by conventional hardware.
4.19 OPTIMIZATIONS FOR 8EntR PERFORMANCE
As with any software design, performance is an issue in the design of a distributed
application. The description of LRPC shows some optimizations that may beadopted for
better performance ofdistributed applications using RPC. Some other optimizations that
may also have significant payoff when adopted for designing RPC-based distributed
applications are described below.
4.19.1 Concurr.nt Acc.ss toMultlpl. S.rv.rs
Although one of the benefits of RPC is its synchronization property, many distributed
applicationscan benefit from concurrentaccess to multiple servers. One ofthe following
three approaches may be used for providing this facility:
1. The use of threads (described in Chapter 8) in the implementation of a client
process where each thread can independently make remote procedure calls to different
servers.Thismethodrequires thattheaddressing intheunderlying protocol isrichenough
to provide correct routing of responses.
2. Another method is the use of the early reply approach [Wilbur and Bacarisse
1987].As shown inFigure 4.15,inthis method acall issplitinto twoseparate RPCcalls,
onepassing theparameters totheserverand theother requesting theresult. Inreply tothe
first call, the server returns a tag that is sent back with the second call to match the call
withthecorrectresult.Theclientdecides thetimedelay betweenthetwocalls andcarries
out other activities during this period, possibly making several other RPC calls. A
drawback of this method is that the server must hold the result of a call until the client
makes a request for it. Therefore, if the request for results is delayed, it may cause
congestion or unnecessary overhead at the server.
3. The third approach, known as the call buffering approach, was proposed by
Gimson [1985].Inthismethod,clients andserversdonotinteractdirectly witheachother.
Theyinteract indirectly viaacallbufferserver.TomakeanRPCcall,aclient sendsitscall
request to the call buffer server, where the request parameters together with the nameof
the server and the client are buffered. The client can then perform other activities untilit

Sec.4.19 • Optimizations forBetterPerformance 209
Client Server
Callprocedure(parameter)
Return (tag)
T Reply(tag)
Execute procedure
Carryoutother Store(result)
activities
~
Requestresult (tag)
Return (result)
Reply(result)
14'ig.4.15 Theearlyreplyapproachforprovidingthefacilityofconcurrentaccessto
multipleservers.
needs the result of the RPC call. When the client reaches a state in which it needs the
result, itperiodically polls thecall buffer server tosee ifthe result ofthecall isavailable,
and if so, it recovers the result. On the server side, when a server is free, it periodically
polls thecall buffer server toseeifthere isanycall for it.Ifso,itrecovers thecall request,
executesit,and makes acall back tothecall buffer servertoreturn theresult ofexecution
to the call buffer server. The method is illustrated in Figure 4.16.
A variant of this approach is used in the Mercury communication system developed
at MIT [Liskov and Shrira 1988] for supporting asynchronous RPCs. The Mercury
communication system has a new data type calledpromise that iscreatedduring an RPC
call and isgiven atype correspondingtothose ofthe results and exceptionsof the remote
procedure. When theresults arrive, they arestored intheappropriatepromise, from where
the caller claims the results at atime suitable to it.Therefore, after making acall, acaller
can continue with other work and subsequently pick up the results ofthe call from the
appropriate promise.
Apromiseisinone of two states-blockedor ready. Itisinablocked state from the
time of creation to the time the results of the call arrive, whereupon it enters the ready
state. A promise in the ready state is immutable.
Two operations (ready and claim) are provided to allow a caller to check the
status of the promise for the call and to claim the results of the call from it. The ready
operation is used to test the status (blocked/ready) of the promise. It returns true or
false according to whether the promise is ready or blocked. The claim operation is

210 Chap. 4 • Remote Procedure Calls
Client C~~~~Ver Server
:Checkforawaitingrequest
I
Reply(norequest) Pollingfora
waitingrequest
T I
Reply(tag) : Checkforawaitingrequest
I
Carryoutother
activities
Execute
procedure
-L
Pollingfor
result
Checkforresult(tag)
Continuestopollfor
anawaitingrequest
Fig.4.16 Thecall buffering approach forproviding thefacilityofconcurrent access
tomultiple servers.
used to obtain the results of the call from the promise. The claim operation blocks the
caller until the promise is ready, whereupon it returns the results of the call.
Therefore, if the caller wants to continue with other work until the promise becomes
ready, it can periodically check the status of the promise by using the ready
operation.
4.19.2 Serving Multlpl. Requests Simultaneously
The following types ofdelays are commonly encountered in RPC systems:
1. Delay caused while a server waits for a resource that is temporarily unavailable.
Forexample,during thecourseofacallexecution,aserver might have towaitfor
accessing a shared file that is currently locked elsewhere.

Sec.4.19 • Optimizations forBetterPerformance 211
2. A delay can occur when a server calls a remote function that involves a
considerable amount of computation to complete or involves a considerable
transmission delay.
For better performance, good RPC implementations must have mechanisms toallow
theserver toaccept and process other requests, instead of being idle while waiting for the
completion of some operation. This requires that a server be designed in such a manner
that it can service multiple requests simultaneously. One method to achieve this is to use
the approach of a multiple-threaded server with dynamic threads creation facility for
server implementation (details of this approach are given in Chapter 8).
4.19.3 Reducing Per-Call Workload of Servers
Numerous client requests canquickly affectaserver'sperformancewhentheserverhasto
doalotofprocessingforeachrequest.Thus,toimprovetheoverallperformanceofanRPC
system,itisimportanttokeeptherequestsshortandtheamountofworkrequiredbyaserver
foreachrequestlow.Onewayofaccomplishingthisimprovementistousestatelessservers
and let the clients keep track of the progression of their requests sent to the servers. This
approach sounds reasonable because, inmost cases, theclient portion of anapplication is
really incharge oftheflowofinformation betweenaclient andaserver.
4.19.4 Reply Caching of Idempotent Remote Procedures
The use of a reply cache to achieve exactly-once semantics in nonidempotent remote
procedureshasalready beendescribed. However,areplycachecanalsobeassociated with
idempotent remote procedures for improving a server's performance when it is heavily
loaded. When client requests to a server arrive at a rate faster than the server can process
the requests, a backlog develops, and eventually client requests start timing out and the
clients resend therequests, making theproblem worse. Insuchasituation, thereplycache
helps because the server has toprocess arequest only once. Ifaclient resends itsrequest,
the server just sends the cached reply.
4.19.5 ProperSelection of Timeout Values
Todeal with failure problems, timeout-based retransmissions are necessary indistributed
applications. An important issue here is how to choose the timeout value. A "too small"
timeout value wiJI cause timers to expire too often, resulting in unnecessary
retransmissions. On the other hand, a "too large" timeout value will cause a needlessly
long delay in theevent that a message isactually lost. In RPC systems, servers are likely
to take varying amounts oftime to service individual requests, depending on factors such
as server load, network routing, and network congestion. If clients continue to retry
sending those requests forwhichreplieshave notyetbeenreceived, theserverloading and
network congestion problem will become worse. To prevent this situation, proper
selection of timeout values is important. One method to handle this issue is to use some
sort of back-offstrategy of exponentially increasing timeout values.

212 Chap. 4 • Remote Procedure Calls
4.19.6 Pro,., Design of RPeProtocol Sp.clflcQtlon
For better performance, the protocol specification of an RPC system must be properly
designed so as to minimize the amount ofdata that has to be sent over the network and
the frequency at which it is sent. Reducing the amount ofdata to betransferred helps in
two ways: It requires less time toencode and decode the data and itrequires less time to
transmitthe data over the network. SeveralexistingRPC systems use TCPIIPor UDPIIP
as the basic protocol because they are easy to use and fit in well with existing UNIX
systems and networks such as the Internet. This makes it straightforward to write clients
and servers that run on UNIX systems and standard networks. However, the use of a
standard general-purpose protocol for RPC generally leads to poor performance because
general-purposeprotocolshave many features todeal with differentproblemsindifferent
situations. For example, packets in the IP suite (to which TCP/IP and UDPIIP belong)
have in total 13 header fields, of which only 3 are useful for an RPC-the source and
destinationaddressesand the packetlength. However, several ofthese headerfields, such
as those dealing with fragmentation and checksum, have to be filled in by the senderand
verified by the receiver to make them legal IP packets. Some ofthese fields, such as the
checksum field, are time consuming to compute. Therefore, for better performance, an
RPC system should use a specialized RPC protocol. Ofcourse, a new protocol for this
purpose has to bedesigned from scratch, implemented, tested, and embedded in existing
systems, so it requires considerably more work.
4.20 CASE STUDIES: SUN APC, DeE APC
Many RPC systems have been built and are in use today. Notable ones include the Cedar
RPC system [Birrell and Nelson 1984], Courier in the Xerox NS family of protocols
[Xerox Corporation 1981], the Eden system [Almes et at 1985], the CMU Spice system
[Jones et al. 1985], Sun RPC [Sun Microsystems 1985], Argus [Liskov and Scheifler
1983], Arjuna [Shrivastava et aI. 1991], the research system built at HP Laboratories
[Gibbons 1987],NobelNet'sEZRPC [Smith 1994],Open SoftwareFoundation's(OSF's)
DeE RPC [Rosenberry et al. 1992], which is a descendent of Apollo's Network
Computing Architecture (NCA), and the HRPC system developed at the University of
Washington [Bershad etal. 1987].Ofthese, thebestknown UNIX RPC system isthe Sun
RPC. Therefore,theSun RPC willbedescribedinthis section asacase study.Inaddition,
due tothe policy used inthisbook todescribeDeEcomponentsascase studies, the DCE
RPC will also be briefly described.
4.10.1 Sun RPe
Stub Generation
Sun RPC uses the automatic stub generation approach, although users have the
flexibility of writing the stubs manually. An application's interface definition is written
in an IDL called RPC Language (RPCL). RPCL is an extension of the Sun XDR

Sec.4.20 • CaseStudies: SUD RPC, DeERPC 213
language that was originally designed for specifying external data representations. As
an example, the interface definition of the stateless file service, described in Section
4.8.1, is given in Figure 4.17. As shown in the figure, an interface definition contains
a program number (which is 0 x 20000000 in our example) and a version number of
the service (which is 1 in our example), the procedures supported by the service (in
our example READ and WRITE), the input and output parameters along with their
types for each procedure, and the supporting type definitions. The three values program
number (STATELESS_FS_PROG), version number (STATELESS_FS_VERS), and a
procedure number (READ or WRITE) uniquely identify a remote procedure. The
READ and WRITE procedures are given numbers 1 and 2, respectively. The number
ois reserved for a null procedure that is automatically generated and is intended to be
used to test whether a server is available. Interface definition file names have an
extension .x. (for example, StatelessFS.x).
/*Interfacedefinitionforastatelessfileservice(StatelessFS)
infileStatelessFS.x*/
constFILE_NAME_SIZE=16
=
constBUFFER_SIZE 1024
typedefstringFileName<FILE_NAME_SIZE>;
typedeflongPosition;
typedeflongNbytes;
structData{
tongn:
charbuffer[BUFFER_SIZE];
};
structreadargs{
FileName filename;
Position position;
Nbytes n;
};
structwriteargs{
FileName filename;
Position position;
Data data;
};
programSTATELESS_FS_PROG{
versionSTATELESS_FS_VERS{
Data READ(readargs)=1;
Nbytes WRITE (writeargs)=2;
}=
1;
}=0x2oooo000;
Fig. 4.17 InterfacedefinitionforastatelessfileservicewritteninRPCLofSunRPC.

214 Chap. 4 • Remote Procedure Calls
The IDL compiler is called rpcgen in Sun RPC. From an interface definition file,
rpcgen generates the following:
1. A header file that contains definitions of common constants and types defined in
the interface definition file. Italso contains external declarations for all XDR marshaling
andunmarshalingprocedures thatareautomatically generated.Thenameoftheheaderfile
is formed by taking the base name of the input file to rpcgen and adding a .h suffix (for
example, StatelessFS.h). This file is manually included inclient and server program files
and automatically included in client stub, server stub, and XDR filters files using
#include.
2. AnXDR filters filethat contains XDR marshaling and unmarshalingprocedures.
These procedures are used bytheclient and server stub procedures. The name of this file
is formed by taking the base name of the input file to rpcgen and adding a _xdr.c suffix
(for example, StatelessFS_xdr.c).
3. A client stub file that contains one stub procedure for each procedure defined in
the interface definition file. A client stub procedure name is the name of the procedure
given in the interface definition, converted to lowercase and with an underscore and the
version number appended. For instance, in our example, the client stub procedure names
for READ and WRITE procedures will be read_l and write_l, respectively. The name of
theclient stubfileisformedbytakingthebasenameoftheinputfiletorpcgen andadding
a _clnt.c suffix (for example, StatelessFS_clnt.c).
4. Aserver stubfilethatcontains themain routine, thedispatchroutine, andonestub
procedure for each procedure defined in the interface definition file plus a null
procedure.
The main routine creates the transport handles and registers the service. The default
istoregister theprogram onboththeUDPandTCPtransports. However, ausercan select
which transport to use with a command..line option to rpcgen.
The dispatch routine dispatches incoming remote procedure calls to the appropriate
procedure. The name used for the dispatch routine is formed by mapping the program
name to lowercase characters and appending an underscore followed by the version
number (for example, statelessfsprogFy.
The name of the server stub file is formed bytaking the base name of the input file
to rpcgen and adding a _svC.csuffix (for example, StatelessFS_svc.c).
Now using the files generated by rpcgen, an RPC application is created in the
following manner:
1. The application programmer manually writes the client program and server
program fortheapplication. Theskeletonsofthese twoprograms forourexample
application of stateless file service are given Figures 4.18 and 4.19, respectively.
Notice that the remote procedure names used in.these two programs are those of
the stub procedures (read_l and write_J).

Sec.4.20 • Case Studies: Sun RPC,DCE RPC 215
r
Askeletonofclientsourceprogramforthestatelessfileserviceinfileclient.c*1
#include<stdio.h>
#include-erpc/rpc.rc-
#include"StatelessFS.h"
main(argc,argv)
intargc;
char**argv;
CLIENT *client_.handle;
char *server_host_name="paris";
readargs read_args;
writeargs write_args;
Data *read_result;
Nbytes *write_result;
client_handle=clnt_create(server_host_name,STATELESS_FS_PROG,
STATELESS_FS_VERS,"udp");
/*Getaclienthandle.Createssocket*/
==
if(client_handle NULL){
ctnt_pcreateerror(server_host_name);
retum(1);1*Cannotcontactserver*/
};
/*PrepareparametersandmakeanRPCtoreadprocedure*/
=
read_args.filename "example";
read_args.position=0;
read_args.n=500:
=
read_result read_1 (&read_args,client_handle);
/*PrepareparametersandmakeanRPCtowriteprocedure*1
write_args.filename="example";
=
write_args.position 501;
write_args.data.n=100;
r Statementsforputting100bytesofdatain&write_args.data.buffer*/
write_result=write_l (&write_args,client_handle);
clnt_destroy(client_handle);
/*Destroytheclienthandlewhendone.Closessocket*1
Fig. 4.18 Askeleton ofclient source program forthestateless fileserviceofFigure4.17.

216 Chap. 4 • Remote Procedure Calls
r Askeletonofserversourceprogramforthestatelessfifeserviceinfileserver.c*/
#include-cstdio.n»
#include<rpclrpc.h>
#include"StatelessFS.h"
/*READPROCEDURE*/
Data *read_1 (args)/*Inputparametersasasingleargument*/
readargs *args;
{
staticDataresult;/*Mustbedeclaredasstatic*/
/*Statementsforreadingargs.nbytesfromthefileargs.filenamestarting
frompositionargs.position,andforputtingthedatareadin&result.buffer
andtheactualnumberofbytesreadinresult.n*/
return(&result);/*Returntheresultasasingleargument*/
}
r WRITEPROCEDURE*/
r
Nbytes *write_1 (args) Inputparametersasasingleargument*/
writeargs *args;
{
staticNbytesresult;/*Mustbedeclaredasstatic*/
r Statementsforwritingargs.data.nbytesofdatafromthebuffer
&args.data.bufferintothefUeargs.filenamestartingatposition
args.position*/
/*Statementforputtingtheactualnumberofbyteswritteninresult*/
return(&result);
Fig. 4.19 Askeleton of server source program forthestateless file service of Figure 4.17.
2. The client program file is compiled to get aclient object file.
3. The server program file is compiled to get a server object file.
4. Theclient stubfileandtheXDRfilters filearecompiled togetaclient stubobject
file.
5. The server stub file and the XDRfilters file are compiled to get a server stub
object file.
6. The clientobjectfile, the client stub object file, and the client-side RPCRuntime
library are linked together to get the client executable file.
7. The server object file, theserver stubobject file,and theserver-sideRPCRuntirne
library are linked together to get the server executable file.
The entire process is summarized in Figure 4.20.

Sec.4.20 • Case Studies: Sun RPC, DCE RPC 217
Thisfileismanuallyincludedin
Onlythesethreefilesaremanuallywritten
clientandserverprogramfiles,
andeditedbyanapplicationprogrammer
andautomaticallyincludedin
clientstub,serverstub,andXDR
filtersfilesusing#include
Client-side
APC Runtime
library
Client Server
executablefile executablefile
Fig.4.20 The steps increatingan RPC application inSun RPC.
Procedure Arguments
InSun RPC,aremoteprocedurecanacceptonly oneargumentandreturnonlyoneresult.
Therefore, procedures requiring multiple parameters as input or as output must include
themascomponents ofasinglestructure.This isthereason whythestructuresData(used
as a single output argument to the READ procedure), readargs (used as a single input
argument to the READprocedure), and writeargs (used as a single input argument to the
WRITEprocedure) have been defined in our example of Figures 4.17-4.19. If a remote
procedure does nottake anargument, aNULL pointer muststillbepassedasanargument
to theremote procedure. Therefore, aSun RPC call always has two arguments-the first
isa pointer to the single argument of theremote procedure and the second isa pointer to
a client handle (see the calls for read_l and write_l in Fig. 4.18). On the other hand, a

218 Chap. 4 • Remote Procedure Calls
return argument of a procedure is a pointer to the single result. The returned result must
be declared as a static variable in the server program because otherwise the value of the
returned result becomes undefined when the procedure returns (see the return argument
result in Fig. 4.19).
MarshalingArguments andResults
Wehave seenthatSunRPCallowsarbitrary datastructurestobepassed asarguments and
results. Since significant data representation differences can exist between the client
computer and the server computer, these data structures are converted to eXternal Data
Representation(XDR)and backusingmarshaling procedures. The marshaling procedures
to be used are specified bythe user and may beeither built-inprocedures supplied inthe
RPCRuntime library or user-defined procedures defined in terms of the built-in
procedures. The RPCRuntime library has procedures for marshaling integers of all sizes,
characters, strings, reals, and enumerated types.
Since XDR encoding and decoding always occur, even between a client and server
of the same architecture, unnecessary overhead is added to the network service for those
applications in which XDR encoding and decoding are not needed. In such cases, user
defined marshaling procedures can be utilized. That is, users can write their own
marshaling procedures verifying that the architectures of the client and the server
machines arethesameand,ifso,usethedatawithoutconversion. Iftheyarenotthesame,
the correct XDR procedures can be invoked.
CallSemantics
Sun RPC supports at-least-once semantics. After sending a request message, the
RPCRuntime library waits for a timeout period for the server to reply before
retransmitting the request. The number of retries is the total time to wait divided by the
timeoutperiod.Thetotaltimetowaitandthetimeoutperiod havedefault valuesof25and
5 seconds, respectively. These default values can be set to different values by the users.
Eventually, if no reply is received from the server within the total time to wait, the
RPCRuntime library returns a timeout error.
Client-Server Binding
Sun RPC does not have anetworkwide binding service forclient-serverbinding. Instead,
each node has a local binding agent called portmapper that maintains a database of
mapping of all local services (as already mentioned, each service is identified by its
program numberand version number) and their port numbers. The portmapperruns at a
well-known port number on every node.
When a server starts up, it registers its program number, version number, and port
number with the local portmapper. When a client wants to do an RPC, it must first find
out the portnumber of the server that supports the remote procedure. For this, the client
makes a remote request to the portmapper at the server's host, specifying the program
number and version number (see clnt_create part of Fig. 4.18). This means that a client

Sec.4.20 • Case Studies: Sun RPC, DeE RPC 219
must specify the host name of the server when itimports aservice interface. Ineffect, this
means that Sun RPC has no location transparency.
The procedure clnt_create isusedby aclient toimport aserviceinterface. It returns
a client handle that contains the necessary information for communicating with the
corresponding server port, such as the socket descriptor and socket address. The client
handle is used by the client to directly communicate with the server when making
subsequent RPCs to procedures of the service interface (see RPCs made to read_l and
write_l procedures in Fig. 4.18).
Exception Handling
The RPCRuntime library of Sun RPC has several procedures for processing detected
errors. The server-side error-handling procedures typically send a reply message back to
the client side, indicating the detected error. However, the client-side error-handling
procedures provide the flexibility tochoosetheerror-reportingmechanism. That is,errors
may bereported tousers eitherbyprinting errormessagestostderrorbyreturning strings
containing error messages to clients.
Security
Sun RPC supports the following three types of authentication (often referred to as
flavors):
1. No authentication.This isthedefault type. Inthis case, noattempt ismade bythe
server to check a client's authenticity before executing the requested procedure.
Consequently, clients do not pass any authentication parameters in request messages.
2. UNIX-style authentication. This style is used to restrict access to a service to a
certainsetof users. In this case, theuid andgidofthe user running theclient programare
passed inevery request message, and based on this authentication information, the server
decides whether to execute the requested procedure or not.
3. DES-style authentication. Data Encryption Standard (DES) is an encryption
technique described in Chapter 11.In DES-style authentication, each user has a globally
unique namecallednetname.The netnameoftheuserrunning theclient programispassed
inencrypted form inevery request message. On the serverside, theencryptednetname is
first decrypted and then the server uses the information in netname to decide whether to
execute the requested procedure or not.
The DES-styleauthenticationisrecommendedforusers whoneedmore security than
UNIX-style authentication. RPCs using DES-style authentication are also referred to as
secure RPC.
Clients have the flexibility to select any oftheabove three authentication flavors for
an RPC. The type ofauthentication can be specified when aclient handle iscreated. It is
possible to use a different authentication mechanism for different remote procedures
within a distributed application by setting the authentication type to the flavor desired
before doing the RPC.

220 Chap. 4 • Remote Procedure Calls
TheauthenticationmechanismofSunRPC isopen ended inthesense thatinaddition
to the three authentication types mentioned above users are free to invent and use new
authentication types.
Special Types of RPCs
Sun RPC provides support for asynchronous RPC, callback RPC, broadcast RPC, and
batch-mode RPC.
Asynchronous RPC is accomplished by setting the timeout value ofan RPC to zero
and writing the server such that no reply is generated for the request.
To facilitate callback RPC, the client registers the callback service using a transient
program number with the local portmapper. The program number is then sent as part of
theRPC requesttothe server.The serverinitiates anormal RPCrequesttotheclientusing
the given program number when it is ready to do the callback RPC.
A broadcastRPC isdirected tothe portmapperofallnodes. Each node'sportmapper
then passes itontothelocal servicewith thegiven programname. The clientpicks upany
replies one by one.
Batch-modeRPCisaccomplishedbybatchingofclient callsthat require noreply and
then sending them in a pipeline to the server over TCPII~
Critiques of SUD RPC
In spite of its popularity, some ofthe criticisms normally made against Sun RPC are as
follows:
1. SunRPC lacks locationtransparencybecauseaclient hastospecify thehost name
ofthe server when it imports a service interface.
2. The interface definition language of Sun RPC does not allow a general
specificationofprocedureargumentsandresults. Itallows only asingle argument
and asingle result. This requirementforces multipleargumentsorreturn values to
be packaged as a single structure.
3. Sun RPC is not transport independent and the transport protocol is limited to
either UDP or TCP. However, a transport-independent version of Sun RPC,
knownasTI-RPC(transport-independentRPC), has been developedbySun-Soft,
Inc. TI-RPC provides a simple and consistent way in which transports can be
dynamically selected depending upon user preference and the availability ofthe
transport. Details ofTI-RPC can be found in [Khanna 1994].
4. In UDP, Sun RPC messages are limited to 8 kilobytes in length.
5. Sun RPC supportsonly at-least-oncecall semantics, which may not beacceptable
for some applications.
6. Sun RPC does not have a networkwide client-server binding service.
7. Wesaw inSection4.18 that threads can be used inthe implementationofaclient
or aserverprocessfor betterperformanceofanRPC-basedapplication. SunRPC
does notincludeany integratedfacility forthreads intheclient orserver, although
Sun OS has a separate threads package.

Sec.4.20 • Case Studies: Sun RPC,DeE RPC 221
4.10.2 DeE APC
The DeE RPC isone ofthe most fundamental componentsofDeE because itisthebasis
for allcommunication inDeE. It isderived from the Network ComputingSystem (NCS)
developed by Apollo (now part of Hewlett-Packard).
The DeE RPCalso uses the automatic stub generation approach. An application's
interface definition is written in IDL. As an example, the interface definition of the
stateless file service of Figure 4.17 is rewritten inFigure 4.21 inIDL. Notice that, unlike
Sun RPC, DeE RPC IDL allows a completely general specification of procedure
arguments and results. As shown in the figure, each interface is uniquely identified by a
universally unique identifier (VVID) that is a 128-bit binary number represented in the
IDL file as anASCII string inhexadecimal. The uniqueness of each VVID isensured by
incorporating in it the timestamp and the location of creation. A UUID as well as a
template fortheinterfacedefinitionisproducedby usingtheuuidgenutility.Therefore, to
createtheinterface definitionfileforaservice, thefirststepistocalltheuuidgenprogram.
The automatically generated template file is then manually edited todefine the constants
and the procedure interfaces of the service.
When theIDL fileiscomplete, itiscompiledusing theIDLcompiler togenerate the
client andserver stubsandaheader file.Theclient andserver programs arethenmanually
written for anapplication. Finally,the same stepsasthatofFigure 4.20 areusedtogetthe
client and server executable files.
[uuid(b20a1705-3c26-12d8-8ea3-04163aOdcefz)
version(1.0»
interfacestateless_fs
(
constlongFILE_NAME_SIZE=
16
=
constlongBUFFER_SIZE 1024
typedefcharFileName[FILE_NAME_SIZE];
typedefcharBuffer[BUFFER_SIZE);
voidread(
[in]FileName filename;
[in]long position;
[in,out]long nbytes;
[out]Buffer buffer;
);
voidwrite(
[in]FileName filename;
(in]long position;
(in,out]long nbytes;
[in]Buffer buffer;
);
Fig. 4.21 Interfacedefinition ofthestateless fileserviceof
Figure4.17 writtenintheIDLofDeE RPC.

222 Chap. 4 • Remote Procedure Calls
The default call semantics of a remote procedure in DCE RPC is at-most-once
semantics. However, for procedures that are of idempotent nature, this rather strict call
semantics is not necessary. Therefore, DCE RPC provides the flexibility to application
programmers to indicate as part of a procedure's IDL definition that it is idempotent. In
this case, error recovery isdone viaasimple retransmission strategy rather thanthe more
complex protocol used to implement at-most-once semantics.
The DCE RPC has a networkwide binding service for client-server binding that is
based on its directory service.(the details of the DCE directory service are given in
Chapter 10). For the description here, it is sufficient to know that every cell in a DeE
system hasacomponent called Cell Directory Service (CDS), which controls the naming
environment used within a cell. Moreover, on each DCE server node runs a daemon
process called rpcd(RPCdaemon) that maintains adatabase of(server,endpoint) entries.
An endpoint is a process address (such as the TCP/IP port number) of a server on its
machine.
When an application server initializes, it asks the operating system for an endpoint.
It then registers this endpoint with its local rpcd. At the time of initialization, the server
also registers its host address with the CDS of its cell.
When a client makes its first RPC involving the server, the client stub first gets the
server'shostaddressbyinteracting withtheserver(s)oftheCDS,makingarequesttofind
it a host running an instance of the server. It then interacts with the rpcd (an rpcd has a
well-known endpoint onevery host) of the server's host toget theendpoint ofthe server.
TheRPC can takeplace once the server'sendpoint isknown. Note thatthislookup isnot
needed on subsequent RPCs made to the same server.
The steps described above are used for client-serverbinding when theclient and the
server belong to the same cell. Aclient can also do an RPC with a server that belongs to
another cell. In this case, the process of getting the server's host address also involves
Global Directory Service (GDS), which controls the global naming environment outside
(between) cells (for details see Chapter 10).
The DCE RPC also provides broadcast facility. To use this facility, a remote
procedure has to be given the broadcast attribute in its defining IDL file. When a
procedure with this attribute is called, the request is sent to all servers of the requested
interface. All the servers receiving the request respond, but only the first response is
returned to the caller; the others are discarded by the RPCRuntime library.
4.11 SUMMARY
Remote Procedure Call (RPC) isa special case of the general message-passing model of
IPCthathasbecome awidelyaccepted IPCmechanism indistributed computing systems.
Its popularity is due to its simple call syntax, familiar procedure call semantics, ease of
use,generality,efficiency,andspecificationofawell-defined interface.Idealtransparency
of RPC means that remote procedure calls are indistinguishable from local procedure
calls. However, this is usually only partially achievable.
In the implementation of an RPC mechanism, five pieces ofprograms are involved:
the client, the client stub, the RPCRuntime, the server stub, and the server.The purpose

Sec.4.21 • Summary 223
oftheclient andserver stubsistomanipulatethedatacontainedinacallorreplymessage
sothat itis suitable for transmissionover t.he network or foruseby the receiving process.
On the other hand, the RPCRuntime provides network services in a transparent
manner.
The two types of messages involved in the implementation of an RPC system are
call messages and reply messages. Call messages are sent by the client to the server
for requesting the execution of a remote procedure, and reply messages are sent by the
server to the client for returning the result of remote procedure execution. The process
of encoding and decoding of the data of these RPC messages is known as
marshaling.
Servers of an RPC-based application may either be stateful or stateless. Moreover,
depending onthetimeduration for whichanRPC serversurvives, servers maybeofthree
types-instance-per-call servers, instance-per-transaction/session servers, and persistent
servers. The choice of a particulartype of server depends on the needs ofthe application
being designed.
The twochoices of parameter-passingsemantics inthedesign ofanRPC mechanism
are call-by-value and call-by-reference. Most RPC mechanisms use the call-by-value
semantics because the client and server processes exist in different address spaces.
Thecallsemantics ofanRPCmechanismdetermines howoftentheremoteprocedure
maybeexecuted under faultconditions. Thedifferent types ofcall semantics usedinRPC
mechanisms arepossibly ormaybe,lastone, lastofmany,atleastonce,andexactly once.
Of these, the exactly-once call semantics is the strongest and most desirable.
Based ontheir IPC needs, different systems useoneofthefollowing communication
protocols for RPC: the request (R) protocol, the request/reply (RR) protocol, and the
request/reply/acknowledge-reply (RRA) protocol. In addition to these, special commu
nication protocols are used for handling complicated RPCs that invoJve long-duration
calls or large gaps between calls or whose arguments and/or results are too large to fit in
a single-datagram packet.
Client-server binding is necessary for a remote procedure call to take place. The
general model used for binding is that servers export operations to register their
willingness toprovide service andclients import operations whenthey needsome service.
A client may be bound to a server at compile time, at link time, or at call time.
Some special types of RPCs operate in a manner different from the usual RPC
protocol. For example, asynchronous RPC provides a one-way message facility from
client toserver,callback RPCfacilitates apeer-to-peerparadigminstead ofaclient-server
relationship among the participating processes, broadcast RPC provides the facility of
broadcastand multicast communication instead ofone-to-one communication, and batch
mode RPC allows the batching of client requests, which is a type of asynchronous mode
of communication, instead of the usual synchronous mode of communication.
Unlike the conventional RPC systems, in which most of the implementation
decisions are made when the RPC system is designed, the choices of transport protocol,
data representation, and control protocol are delayed until bind time in an RPC system
designed for a heterogeneous environment. For this, the binding facility used by such an
RPC system is made considerably richer in information than the binding used by
conventional RPC systems.

224 Chap. 4 • Remote Procedure Calls
Bershad et al. [1990] proposed the use of Lightweight Remote Procedure Call
(LRPC), which is a communication facility designed and optimized for cross-domain
communicationsinmicrokemeloperatingsystems. Forachievingbetterperformancethan
conventional RPC systems, LRPC uses the following four techniques: simple control
transfer, simple data transfer, simple stubs, and design for concurrency.
Some optimizations that may be used to improve the performance of distributed
applications that use an RPC facility are concurrent access to multiple servers, serving
multiplerequests simultaneously, reducingpercall workloadofservers, reply caching of
idempotent remote procedures, proper selection oftimeout values, and proper design of
RPC protocol specification.
EXERCISES
4.1. What wastheprimary motivation behind thedevelopmentofthe RPC facility? Howdoes an
RPCfacility make thejobofdistributedapplications programmers simpler?
4.2. What are the main similarities and differences between the RPC model and the ordinary
procedurecall model?
4.3. Intheconventionalprocedurecallmodel, thecallerandthecallee proceduresoften useglobal
variables tocommunicatewitheach other.Explain whysuchglobal variables are not used in
the RPCmodel.
4.4. InRPC,thecalled proceduremaybeonthesamecomputerasthecalling procedureoritmay
beonadifferentcomputer. Explain whythetermremoteprocedurecallisusedevenwhenthe
called procedure ison the same computeras thecalling procedure.
4.5. What are the main issues in designing a transparent RPC mechanism? Is it possible to
achieve complete transparency of an RPC mechanism? If no, explain why. If yes, explain
how.
4.6. Achieving complete transparency of an RPC mechanism that allows the calJer and callee
processes to be on different computers is nearly impossible due to the involvement of the
network inmessagecommunicationbetween thetwoprocesses. SupposeanRPC mechanism
isto bedesigned in which the callerand callee processes are always on the same computer.
Isitpossible toachievecompletetransparencyofthisRPCmechanism?Givereasons foryour
answer.
4.7. What isa"stub"?Howarestubsgenerated?Explain howtheuseofstubs helps inmakingan
RPC mechanism transparent.
4.8. A server isdesigned to perform simple integerarithmeticoperations (addition, substraction,
multiplication, and division). Clients interact with this server by using an RPC mechanism.
Describethecontents of thecall and reply messages of this RPC application, explainingthe
purpose of each component. In case of an error, such as division by zero or arithmetic
overflow, the server must suitably inform the client about the typeof error.
4.9. Writemarshalingproceduresforboth taggedanduntaggedrepresentationsformarshaling the
message contents ofthe RPC application of Exercise 4.8.
4.10. Auser-defined programobjectisastructureconsistingofthefollowing basicdatatypesinthat
order: a Boolean, an integer, a long integer, and a fixed-length character string of eight
characters.Writemarshalingproceduresforbothtagged anduntagged representationsforthis
programobject.Assume thattheRPCsoftware provides marshaling ofthebasicdatatypes.

Chap. 4 • Exercises 225
4.11.The callerprocessofanRPCmust wait for areply from thecalleeprocessaftermakingacall.
Explain how this can actually be done.
4.12. Differentiatebetweenstatefuland statelessservers.Whydo somedistributedapplicationsuse
stateless servers in spite of the fact that stateful servers provide an easier programming
paradigm and are typically more efficient than stateless servers?
4.13. Suggest a suitable server creation semantics for each of the following types of
applications:
(a) Aserviceisneededonly once inawhile,and the sessionfor whichaclientinteractswith
theserverofthis serviceinvolvestheexchangeofasinglecall and asinglereply message
between the client and server processes.
(b) Aserviceisneededonly once inawhile,and the sessionfor whichaclientinteractswith
the server of this service normally involves the exchange of several call and reply
messages between the client and serverprocesses.
(c) Aservercan service the requests of multiple clients.
4.14. A server is to be shared by multiple clients. Describe a scheme for designing the remote
procedures offered by the server so that interleaved or concurrent requests from different
clients do not interfere with each other.
4.15. Why do most RPC systems support call-by-value semantics for parameterpassing?
4.16. Discuss the similarities and differences between the following parameter-passing semantics
that may be used in an object-based system:
(a) Call-by-object-reference
(b) Call-by-move
(c) CalJ-by-visit
4.17. Explain why RPC semantics is normally different from the conventional procedure call
semantics. Clarify the differences among may-be, last-one, last-of-many, at-least-once, and
exactly-once call semantics. Explain how each of these may be implemented.
4.18. What isanorphancall?How areorphancalls handledinthe implementationofthefollowing
types of call semantics:
(a) Last-one call semantics
(b) Last-of-many call semantics
(c) At-least-once call semantics
4.19. Suggestwhethermay-be, last-one, last-of-many,at-least-once,orexactly-oncecall semantics
should be used for each ofthe following applications (give reasons for youranswer):
(a) For making a request to a time serverto get the current time.
(b) For making arequest to anode's resource manager to get the current status of resource
availability of its node.
(c) For periodically broadcasting the total number of currentjobs at its node by aprocess
manager in a system in which the process managers ofall nodes mutually cooperate to
share the overall system load.
(d) For making a request to a computation server to compute the value of an equation.
(e) For making a request to a booking server to get the current status of availability of
seats.
(f) For making a request to a booking serverto reserve a seat.
(g) For making a request to a file server to position the read-write pointer of a file to a
specified position.
(h) For making a request to a file serverto append a record to an existing file.

226 Chap.4 • RemoteProcedure Calls
(i) Formakingarequesttoanameservertogetthelocationofanamedobjectinasystem
thatdoesnotsupportobjectmobility.
G) Formakingarequesttoanameservertogetthelocationofanamedobjectinasystem
thatsupportsobjectmobility.
4.20. ExplainwhymostRPCsystemsdonotuseacknowledgment messages. Differentiateamong
R,RR,andRRAprotocolsforRPCs.Giveanexampleofanapplicationinwhicheachtype
ofprotocolmaybethemostsuitableonetouse.
4.21. SupposeittakestimeT(Tisverylarge)foraservertoprocessanRPCrequest.Eventhough
aclientmakingtheRPCrequestknowsthatitwillreceivethereplyforitsrequestfromthe
serveronlyaftertimeT,itwillunnecessarily keepwaitingforthereplyfromtheserverfor
thisentiredurationinsituationswheretherequestmessagedoesnotreachtheserverdueto
failureofthecommunicationlinkbetweentheclientandtheserverortheservercrasheswhile
processingtheclient'srequest.Deviseamechanism toavoidthissituation.Thatis,itshould
bepossibleforaclienttodetectanexceptionconditionandtotakecorrectiveactionasearly
aspossible.
4.22. Supposeyouhaveto designan RPCmechanism for interaction betweenclientsanda file
server,frequently requiringtransferoflargevolumeofdatainbetweenthem.However, the
underlyingnetworkhasalimitationofmaximumpacketsizeof4kilobytes.Supposethetime
totransfera4..kilobytepacketis4ms,andthetimetodoa nullRPC(i.e.,0databytes)is
0.5ms.IftheaverageamountofdatatransferredforeachRPCrequestis16kilobytes, which
ofthefollowingtwomethodswillyouprefertouseinyourdesign:
(a) UsingseveralphysicalRPCsforonelogicalRPC,eachphysicalRPCtransferringasingle
packetofdata
(b) UsingasingleRPCwiththedatatransferredasmultidatagram messages
4.23. What arethemainadvantagesofanRPCsystem thatallows thebinding between aclient and
a server to change dynamically?What are the main issues involved in providing this
flexibility?Describea mechanism tohandleeachoftheissuesmentionedbyyou.
4.24. Aserverisnormallydesignedtoservicemultipleclientsandisoftenboundsimultaneously
tomultipleclients.Doesasituationeverarisewhenaclientshouldbesimultaneouslybound
tomultipleservers?Ifno,explainwhy. Ifyes,givetwoexamplesofsuchasituation.
4.25. Discusstherelativeadvantagesanddisadvantagesofbindingaclientandaserveratcompile
time,atlinktime,andatcaJltime.
4.26. Giventheinterfacenameof a server,discusstherelativeadvantagesanddisadvantages of
usingthebroadcastmethodandthemethodofusinganameserverforlocatingtheserver.
4.27. WhatiscallbackRPCfacility?Giveanexampleofanapplication wherethisfacilitymaybe
useful.Whatare the main issues involvedin supportingthis facilityin an RPC system?
Describeamechanism tohandleeachoftheseissues.
4.28. Giveanexampleofanapplicationwhereeachofthefollowing facilitiesmaybeuseful:
(a) BroadcastRPCfacility
(b) MulticastRPCfacility
Describeamechanism toimplementeachofthesefacilities.
4.29. Givethecharacteristicsofapplicationsforwhichthebatch-modeRPCfacilitymaybeuseful.
Whatarethemainproblemsinusingthisfacility?
4.30. Findoutthedetailsoftheclient..serverbindingmechanismoftheHRPCsystem[Bershadet
a1. 1987],andexplainhowthechoicesoftransportprotocol,datarepresentation, andcontrol
protocolaredelayeduntilbindtimeinthissystem.

Chap.4 • Bibliography 227
4.31. WhatwastheprimarymotivationbehindthedevelopmentoftheLightweightRPC(LRPC)
system[Bershadetal. I990]?DescribesomeofthetechniquesusedintheLRPCsystemthat
makesitmoreefficientthanconventionalRPCsystems.
4.32. Inaclient-servermodelthatisimplementedbyusingasimpleRPCmechanism,aftermaking
anRPCrequest,aclientkeepswaitinguntilareplyisreceivedfromtheserverforitsrequest.
It would be more efficient to allow the client to perform other jobs while the server is
processingits request (especiallywhentherequestprocessing time islong).Describethree
mechanismsthat maybe usedinthiscase toallowaclient toperformotherjobs whilethe
serverisprocessingits request.
4.33. Aclient-servermodelistobeimplementedbyusinganRPCmechanism.Ithasbeenrealized
thatasharedserverisanexpensiveresourceofanRPCmechanismbecauseithastoservice
requestsfrommanyclients.Suggestsomeguidelinesthatmaybeusedfordesigningashared
serverfor improvingtheoverallperformance of thecorrespondingRPCmechanism.
BIBLIOGRAPHY
[Almes 1986] Almes, G. T., "The Impact of Language and System on Remote Procedure Call
Design,"In:Proceedingsofthe6thInternationalConferenceonDistributedComputing Systems,
IEEEPress,Piscataway,NJ,pp.414-421 (May 1986).
[Almesetal,1985]Almes,G.T.,Black~A.P.,Lazowska,E.D.~andNoe,1.D.,HTheEdenSystem:
ATechnicalReview," IEEE TransactionsonSoftware Engineering,Vol.SE-Il~No.1~pp.43-59
(1985).
[Baconand Hamilton 1987]Bacon,J.M.,andHamilton,K.G.,"DistributedComputingwithRPC:
The Cambridge Approach," Technical Report No. 117~ Computer Laboratory, University of
Cambridge,England(1987).
[Bal et al. 1987] Bal, H. E., Renesse, R., and Tanenbaum, A. S., "Implementing Distributed
AlgorithmsUsingRemoteProcedureCalls," In:Proceedings oftheAFIPSNational Computer
Conference, Chicago, IL,pp.499-506 (June 1987).
[Balet al, 1989]Bal, H.E.)Steiner,J. G.~ andTanenbaum,A.S., "ProgrammingLanguagesfor
Distributed Computing Systems," ACM Computing Surveys, Vol. 21, No.3, pp. 261-322
(1989).
[Bershadetal, 1987]Bershad,B.N.,Ching,D.T.,Lazowska,E.D.~Sanislo,J.,andSchwartz,M.~
"A Remote Procedure Call Facility for Interconnecting Heterogeneous Computer Systems,"
IEEETransactions on Software Engineering, Vol. SE-13,No.8, pp.880-894 (1987).
[Bershad et al. 1990] Bershad, B. N., Anderson, T. E., Lazowska, E. D., and Levy, H. M.,
"LightweightRemoteProcedureCall,"ACMTransactions on ComputerSystems, Vol. 8,No.1,
pp. 37-55 (1990).©ACM,Inc., 1990.
[Birrell 1985] Birrell, A. D., "Secure Communication Using Remote Procedure Calls," ACM
Transactions on ComputerSystems, Vol. 3~ No. I~ pp. 1-14 (1985).
[Birrell and Nelson1984]Birrell,A.D.,andNelson,B.,"ImplementingRemoteProcedureCalls,"
ACM Transactions on ComputerSystems, Vol. 2,No.1, pp. 39-59 (1984).
[Birrellet al. 1982]Birrell,A.D.,Levin,R.,Needham,R.M.,andSchroeder,M.D.,"Grapevine:
An Exercise in Distributed Computing," Communications of the ACM~ Vol. 25, No.4, pp.
260-274 (1982).

228 Chap.4 • RemoteProcedureCalls
[Black et al. 1986] Black,A., Hutchinson, N., Jul, E., and Levy, H., "Object Structure in the
EmeraldSystem,"In:ProceedingsofthelstACMConferenceonObject...OrientedProgramming
Systems, Languages, andApplications(OOPSLA...1986), pp.78-86 (1986).
[Black et al. 1987]Black,A.,Hutchinson,N.,Jul,E.,Levy,H.,andCarter,L.,"Distributionand
AbstractTypesinEmerald,"IEEETransactionsonSoftwareEngineering, Vol.SE-13,No.I, pp.
65-76 (1987).
[Corbin 1991]Corbin,1.R.,TheArt ofDistributedApplications, Springer-Verlag, NewYork, NY
(1991).
[Davison et al, 1992]Davison,A., Drake, K., Roberts,W.,and Slater,M., Distributed Window
Systems, A Practical Guide toXlI and OpenWindows,Addison-Wesley, Reading,MA(1992).
[DES 1977] DATA ENCRYPTION STANDARD, FIPS Publication 46, National Bureau of
Standards,U.S.DepartmentofCommerce,Washington DC (January 1977).
[Gibbons 1987JGibbons, P B., "A Stub Generator for Multi-language RPC in Heterogeneous
Environment," IEEE Transactions on Software Engineering, Vol. SE-J3, No. I, pp. 77-87
(1987).
[Gifford and Glasser 1988] Gifford, D., and Glasser, N., "Remote Pipes and Procedures for
EfficientDistributedCommunication," ACMTransactionson Computer Systems, Vol. 6, No.3,
pp.258-283 (1988).
[Gimson 1985] Girnson, R., "Call Buffering Service," TechnicalReport No. 19, Programming
ResearchGroup,OxfordUniversity, Oxford,England(1985).
[Goscinski 1991]Goscinski,A.,"DistributedOperatingSystems,TheLogicalDesign," Addison
Wesley, Reading,MA(1991).
[Hamilton 1984] Hamilton, K. G., "A Remote Procedure Call System," Ph.D. Dissertation,
TechnicalReportNo.70, ComputerLaboratory,UniversityofCambridge,England(December
1984).
[Harbinson 1992]Harbinson,S.~, Modula-S, Prentice-Hall,EnglewoodCliffs,NJ(1992).
[Hiltunen and Schlichting 1995] Hiltunen, M. A., and Schlichting, R. D., "Constructing a
Configurable Group RPC Service," In: Proceedings ofthe 15th International Conference on
DistributedComputing Systems, IEEEPress,Piscataway,NJ(May 1995).
[Hutchinsonetal. 1989]Hutchinson,N.C.,Peterson,L.L.,Abbott,M.B.,andO'Malley,S.,"RPC
in the x-Kernel: Evaluating New Design Techniques," In: Proceedings of the 12th ACM
Symposium on Operating Systems Principles, pp.91-101 (1989).
[Joneset al, 1985]Jones,M.B.,Rashid,R.F.,andThompson,M.R.,"Matchmaker:AnInterface
SpecificationLanguageforDistributedProcessing,"In:Proceedingsofthe12thACMSymposium
on Principles ofProgramming Languages, pp.225-235 (1985).
[Karger 1989]Karger,P.A.,"UsingRegisterstoOptimizeCross-DomainCallPerformance,"In:
Proceedings ofthe 3rd Conference onArchitecturalSupportfor Programming Languages and
Operating Systems, pp. 194-204 (April 1989).
[Khanna 1994] Khanna, R. (Ed.), Distributed Computing: Implementation and Management
Strategies, Prentice-Hall,EnglewoodCliffs,NJ(1994).
[KimandPurtilo1995]Kim,T.H.,andPurtilo,J.M.,"Configuration-LevelOptimizationofRPC
Based Distributed Programs," In: Proceedings of the 15th International Conference on
DistributedComputing Systems, IEEEPress,Piscataway,NJ(May 1995).
[Lin and Gannon 1985]Lin,K.1., andGannon,J. D., "Atomic RemoteProcedureCall," IEEE
Transactions on SoftwareEngineering, Vol. SE-ll, No. 10,pp. 1126-1135(1985).

Chap. 4 • Bibliography 229
[Liskov et al, 1981] Liskov, B., Moss, E., Schaffert, C., Sheifler, R., and Snyder, A., "CLU
Reference Manual," In: Lecture Notes in Computer Science 114, Springer-Verlag, Berlin
(1981).
[Liskov and ScheiDer 1983] Liskov, B., and Scheifler, R., "Guardians and Actions: Linguistic
Supportfor Robust, DistributedPrograms,"ACMTransactions onProgrammingLanguagesand
Systems, Vol.5, No.3, pp. 381-404 (1983).
[Liskov and Shrira 1988] Liskov, B., and Shrira, L., "Promises: Linguistic Support for Efficient
Asynchronous Procedure Calls in Distributed Systems," In: Proceedings of the ACM
SIGPLAN'88 Conference on Programming Language Design and Implementation, Association
for Computing Machinery, New York;NY,pp. 260-267 (June 1988).
[LockhartJr. 1994] Lockhart,Jr.,H.W.,OSFDeE: Guide toDevelopingDistributedApplications,
IEEE ComputerSociety Press, Los Alamitos, CA (J994).
[Nelson 1991]Nelson, G. (Ed.), Systems Programming with Modula-T,Prentice-Hall, Englewood
Cliffs, NJ (1991).
[Panzieriand Srivastava1988]Panzieri,F.,and Srivastava,S. K.,"Rajdoot:ARemoteProcedure
Call Mechanism with Orphan Detection and Killing," IEEE Transactions on Software
Engineering, Vol.SE-14, pp. 30-37 (1988).
[Rosenberry et al, 1992J Rosenberry, W., Kenney, D., and Fisher, G., OSF DISTRIBUTED
COMPUTING ENVIRONM't~1: Understanding DeE', O'Reilly & Associates, Sebastopol, CA
(1992).
[Schroeder and Burrows 1990] Schroeder, M.D., and Burrows, M., "Performance of Firefly
RPC," A('MTransactions on Computer Systems, Vol.8, No.1, pp. 1-17 (1990).
[Shirley et al, 1994]Shirley, J., Hu, \V., and Magid, D., Guide to Writing D('EApplications, 2nd
ed., O'Reilly & Associates, Sebastopol, CA (1994).
[Shrivastavaetal,1991] Shrivastava,S.,Dixon, G.N.,andParrington,G.D.,"AnOverviewofthe
Arjuna Programming System," JEE~'Software (January 1991).
[Smith 1994]Smith, B., "Client/Server Made Easy," BYTE (March 1994).
[Spector 1982]Spector, A. Z.,"Performing Remote Operations Efficiently on a Local Computer
Network," Communications ofthe ACM, Vol.25, No.4, pp. 246-259 (1982).
[Srivastavaand Panzieri1982] Srivastava,S.K.,andPanzieri,F.,"TheDesign ofReliableRemote
Procedure Call Mechanism," IEEE Transactions on Computers, Vol.C-31, No. 7 (1982).
[Sun Mlcrosystems 1985]"RemoteProcedureCall ProtocolSpecification,"Networking ontheSun
Workstation, Sun Microsystems, Mountain View,CA (1985).
[Sun Microsystems 1990] Network Programming, Sun Microsystems, Mountain View, CA
(1990).
[Tanenbaumand VanRenesse 1988]Tanenbaum,A. S., and VanRenesse, R., "A Critique of the
Remote Procedure Call Paradigm," In: Proceedings ofthe E'UTECO'88 Conference, Vienna,
Austria, North-Holland, Amsterdam, pp. 775-783 (April 1988).
['"fay and Ananda 1990] Tay,B. H., and Ananda, A. L., "A Survey of Remote Procedure Calls,"
Operating Systems Review, Vol.24, pp. 68-79 (1990).
[Walker et al. 1990] Walker, E. F.,Floyd, R., and Neves, P.,"Asynchronous Remote Operation
Execution in Distributed Systems," In: Proceedings ofthe 10th International Conference on
DistributedComputing Systems, IEEE Press, Piscataway, NJ, pp. 253-259 (1990).
[Wilbur and Bacarisse 1987]Wilbur, S., and Bacarisse, B., "Building Distributed Systems with
Remote Procedure Call," Software Engineering Journal, pp. 148-159 (September 1987).
[Xerox Corporation 1981J "Courier: The Remote Procedure Call Protocol," Xerox System
Integration Standard XSIS-038112,Stanford, CT (1981).

230 Chap. 4 • Remote Procedure Calls
POINTERS TO818UOGRAPHIES ONTHE INTERNET
I could not find a bibliography dedicated only to Remote Procedure Calls. However, the
following bibliographies contain references on this topic:
ftp:ftp.cs.umanitoba.calpub/bibliographies/OsIIMMD_IV.html
ftp:ftp.cs.umanitoba.ca/pub/bibliographies/Os/os.html
ftp:ftp.cs.umanitoba.calpub/bibliographieslMisc/misc.l.htmI
ftp:ftp.cs.umanitoba.ca/pub/bibliographieslParalleUJPDC.html

5
CHAPTER
Distributed Shared
Memory
5.1 INTRODUOION
In Chapter 3, it was mentioned that the two basic paradigms for interprocess
communication are as follows:
• Shared-memory paradigm
• Message-passing paradigm
Message-passing systems (described in Chapter 3), or systems supporting Remote
Procedure Calls (RPCs) (described in Chapter 4), adhere to the message-passing
paradigm. This paradigm consists of two basic primitives for interprocess
communication:
Send(recipient, data)
Receive (data)
The sending process generates the data to be shared and sends it to the
recipient(s) with which it wants to communicate. The recipient(s) receive the data.
231

232 Chap.5 • Distributed SharedMemory
This functionality is sometimes hidden in language-level constructs. For example, RPC
provides automatic message generation and reception according to a procedural
specification. However, the basic communication paradigm remains the same because
the communicating processes directly interact with each other for exchanging the
shared data.
In contrast to the message-passing paradigm, the shared-memory paradigm
provides to processes in a system with a shared address space. Processes use this
address space in the same way they use normal local memory. That is, processes access
data in the shared address space through the following two basic primitives, of course,
with some variations in the syntax and semantics in different implementations:
data = Read (address)
Write (address, data)
Read returns the data item referenced by address, and write sets the contents
referenced by address to the value of data.
We saw in Chapter 1 that the two major kinds of multiple-instruction, multiple
data-stream (MIMD) multiprocessors that have become popular and gained commer
cial acceptance are tightly coupled shared-memory multiprocessors and loosely cou
pled distributed-memory multiprocessors. The use of a shared-memory paradigm for
interprocess communication is natural for distributed processes running on tightly
coupled shared-memory multiprocessors. However, for loosely coupled distributed
memory systems, no physically shared memory is available to support the shared
memory paradigm for interprocess communication. Therefore, until recently, the
interprocess communication mechanism in loosely coupled distributed-memory multi
processors was limited only to the message-passing paradigm. But some recent loosely
coupled distributed-memory systems have implemented a software layer on top of the
message-passing communication system to provide a shared-memory abstraction to the
programmers. The shared-memory abstraction gives these systems the illusion of
physically shared memory and allows programmers to use the shared-memory
paradigm. The software layer, which is used for providing the shared-memory
abstraction, can be implemented either in an operating system kernel or in runtime
library routines with proper system kernel support. The term Distributed Shared
Memory (DSM) refers to the shared-memory paradigm applied to loosely coupled
distributed-memory systems [Stumm and Zhou 1990].
As shown in Figure 5.1, DSM provides a virtual address space shared among
processes on loosely coupled processors. That is, DSM is basically an abstraction
that integrates the local memory of different machines in a network environment into
a single logical entity shared by cooperating processes executing on multiple sites.
The shared memory itself exists only virtually. Application programs can use it in
the same way as a traditional virtual memory, except, of course, that processes using
it can run on different machines in parallel. Due to the virtual existence of the
shared memory, DSM is sometimes also referred to as Distributed Shared Virtual
Memory (DSVM).

Sec.5.2 • General Architecture ofDSMSystems 233
Distributedsharedmemory
(existsonlyvirtually)
Communication Network
Fig. 5.1 Distributedshared memory (DSM).
5.2 GENERAL ARCHITEOURE OFDSM SYSTEMS
The DSM systems normally have an architecture of the form shown in Figure 5.1. Each
node of the system consists of one or more CPUs and a memory unit. The-nodes are
connected by a high-speed communication network. A simple message-passing system
allows processes on different nodes to exchange messages with each other.
The DSM abstraction presents a large shared-memory space to the processors ofall
nodes. In contrastto the shared physical memory in tightlycoupled parallel architectures,
the shared memory ofDSM exists only virtually. A software memory-mapping manager
routine ineach node maps the local memory onto the shared virtual memory. To facilitate
the mapping operation, the shared-memory space is partitioned into blocks.
Data cachingisawell-known solution toaddress memory access latency.The idea of
datacachingisusedinDSM systemstoreducenetworklatency.Thatis,themainmemoryof
individualnodes isusedtocachepiecesoftheshared-memoryspace.Thememory-mapping
managerofeach node viewsitslocal memoryasabigcacheoftheshared-memoryspacefor
itsassociatedprocessors.The basic unitofcachingisamemoryblock.
When a process on a node accesses some data from a memory block of the shared
memory space, the local memory-mapping manager takes charge of its request. If the
memory blockcontainingthe accessed data isresident in the local memory, the request is
satisfied by supplying the accessed data from the local memory. Otherwise, a network
block fault is generated and the control is passed to the operating system. The operating

234 Chap.5 • Distributed SharedMemory
system then sends a message to the node on which the desired memory block is located
to get the block. The missing block is migrated from the remote node to the client
process's node and the operating system maps itinto the application's address space. The
faulting instruction isthenrestartedand can now complete. Therefore, the scenarioisthat
data blocks keep migrating from one node to another on demand but no communication
is visible to the user processes. That is, to the user processes, the system looks like a
tightly coupledshared-memorymultiprocessorssystem inwhich multipleprocessesfreely
read and write the shared-memory at will, Copies of data cached in local memory
eliminate network traffic for a memory access on cache hit, that is, access to an address
whose data is stored in the cache. Therefore, network traffic is significantly reduced if
applications show a high degree oflocality ofdata accesses.
Variations ofthis general approach are used in different implementations depending
on whether the DSM system allows replication and/or migration of shared-memory data
blocks. These variations are described in Section 5.6.1.
5.3 DESIGN AND IMPlEMENTAnON ISSUES OF DSM
Important issues involved in the design and implementation of DSM systems are as
folJows:
1. Granularity. Granularity refers to the block size of a DSM system, that is, to the
unitofsharing and theunitofdata transferacross the network when anetwork block fault
occurs. Possibleunits areafew words, apage, orafew pages. Selectingproperblock size
isanimportantpartofthedesign ofaDSM system because block sizeisusually ameasure
ofthe granularity ofparallelismexplored and the amount of network traffic generated by
network block faults.
2. Structureofshared-memory space. Structurerefers tothelayout oftheshared data
in memory. The structure of the shared-memory space of a DSM system is normally
dependent on the type of applications that the DSM system is intended to support.
3. Memory coherence and access synchronization. In a DSM system that allows
replication of shared data items, copies of shared data items may simultaneously be
available in the main memories ofa number ofnodes. In this case, the main problem is
to solve the memory coherence problem that deals with the consistency of a piece of
shared data lying in the main memories oftwo or more nodes. This problem is similarto
that which arises with conventional caches [Smith 1982], in particular with multicache
schemes for shared-memory multiprocessors [Frank 1984, Goodman 1983, Katz et al.
1985. Yen et al. 1985]. Since different memory coherence protocols make different
assumptions and trade-offs, the choice is usually dependent on the pattern of memory
access. The terms coherence and consistency are used interchangeably in the literature.
In a DSM system, concurrent accesses to shared data may be generated. Therefore,
amemorycoherenceprotocol alone isnot sufficienttomaintain theconsistencyof shared
data. In addition, synchronization primitives, such as semaphores, event count, and lock,
are needed to synchronize concurrent accesses to shared data.

Sec.5.4 • Granularity 235
4. Datu locationand access.To share data in a DSMsystem, it should be possible
to locate and retrieve the data accessed bya user process. Therefore, aDSMsystem must
implement some form of data block locating mechanism inorder to service network data
block faults to meet the requirement of the memory coherence semantics being used.
5. Replacement strategy. If the local memory of a node is full, a cache miss at that
node implies not only a fetch of the accessed data block from a remote node but also a
replacement. That is, adata block of the local memory must be replaced by the new data
block. Therefore, a cache replacement strategy is also necessary in the design of a DSM
system.
6. Thrashing. In a DSM system, data blocks migrate between nodes on demand.
Therefore, if two nodes compete for write access to a single data item, the
corresponding data block may be transferred back and forth at such a high rate that no
real work can get done. A DSM system must use a policy to avoid this situation
(usually known as thrashing).
7. Heterogeneity. The DSM systems built for homogeneous systems need not
address the heterogeneity issue. However, if die underlying system environment is
heterogeneous, the DSM system must bedesigned to take care of heterogeneity so that it
functions properly with machines having different architectures.
These design andimplementationissuesofDSMsystems aredescribedinsubsequent
sections.
5.4 GRANULARITY
One of the most visible parameters to be chosen in the design of a DSM system is the
block size. Several criteria for choosing this granularity parameter are described below.
Just as with paged main memory, there are a number of trade-offs and nosingle criterion
dominates.
5.4.1 Factors Influencing 810ck SizeSelection
In a typical loosely coupled multiprocessor system, sending large packets of data (for
example, 4 kilobytes) is not much more expensive than sending small ones (for example,
256 bytes) [Liand Hudak 1989J.This isusually due tothe typical software protocols and
overhead of the virtual memory layer of the operating system. This fact favors relatively
large block sizes. However, other factors that influence the choice of block size are
described below [Nitzberg and Virginia Lo 1991].
1. Paging overhead. Because shared-memory programs provide locality of refer
ence, a process is likely to access a large region of its shared address space in a small
amount of time. Therefore, paging overhead is less for large block sizes as compared to
the paging overhead for small block sizes.

236 Chap.5 • Distributed SharedMemory
2. Directory size. Another factor affecting the choice of block size is the need to
keep directory information about the blocks in the system. Obviously, the larger the
block size, the smaller the directory. This ultimately results in reduced directory
management overhead for larger block sizes.
3. Thrashing.The problemofthrashingmay occurwhen data items inthe same data
blockare being updatedbymultiplenodes atthe same time, causinglarge numbersofdata
blocktransfersamongthenodes withoutmuch progressintheexecutionoftheapplication.
Whileathrashingproblemmayoccurwithanyblock size,itismore likely withlargerblock
sizes,asdifferentregionsinthesameblockmaybeupdatedbyprocessesondifferentnodes,
causingdatablock transfers,thatarenotnecessarywithsmallerblocksizes.
4. False sharing. False sharing occurs when two different processes access two
unrelated variables that reside in the same datablock (Fig. 5.2). In such a situation, even
thoughtheoriginal variablesare notshared,thedatablock appearstobesharedbythetwo
processes. The larger is the block size, the higher is the probability of false sharing, due
to the fact that the same data block may contain different data structures that are used
independently. Notice that false sharing ofa block may lead to a thrashing problem.
ProcessP1 accesses .....-----f p
data inthisarea 1
ProcessP accesses ..._----1:
2
data inthisarea P2
Adata block Fig.5.2 False sharing.
5.4.1 Using Page 51z8 as810ck Size
The relativeadvantagesand disadvantagesofsmall and large block sizes make itdifficult
for aDSM designertodecideon aproperblocksize. Therefore,asuitablecompromisein
granularity, adopted by several existing DSM systems, is to use the typical page size ofa
conventional virtual memory implementation as the block size ofa DSM system. Using
page size as the blocksize ofa DSM systemhas the following advantages [Li and Hudak
1989]:
1. It allows the use ofexisting page-fault schemes (i.e., hardware mechanisms) to
triggera DSM page fault. Thus memory coherence problems can beresolved in
page-fault handlers.

Sec.5.5 • Structure ofShared-Memory Space 237
2. It allows the access right control (needed for each shared entity) to be readily
integrated into the functionality ofthe memory management unit ofthe system.
3. As long as a page can fit into a packet, page sizes do not impose undue
communication overhead at the time ofnetwork page fault.
4. Experience has shown that a page size is a suitable data entity unit with respect
to memory contention.
5.5 STRUauRE OF SHARED-MEMORY SPACE
Structure defines the abstract view of the shared-memory space to be presented to the
applicationprogrammersofaDSM system. Forexample,the shared-memoryspace ofone
DSM system may appear to its programmers as a storage for words, while the
programmersof anotherDSM systemmay view its shared-memory space as a storagefor
dataobjects.The structure and granularity of a DSM system are closely related [Nitzberg
and Virginia Lo 1991]. The three commonly used approaches for structuring the shared
memory space of a DSM system are [Nitzberg and Virginia Lo 1991] as follows:
1. No structuring. Most DSM systems do not structure their shared-memory space.
In these systems, the shared-memory space is simply a linear array of words. An
advantage of the use of unstructured shared-memory space is that it is convenient to
chooseany suitablepage size as the unit of sharingand afixed grain size may be used for
all applications. Therefore, it is simple and easy to design such a DSM system. It also
allows applications to impose whatever data structures they want on the shared memory.
IVY [Li and Hudak 1989Jand Mether [Minnich and Farber 1989] use this approach.
2. Structuring by data type. In this method, the shared-memory space is structured
eitheras a collection of objects (as in Clouds [Dasgupta et a1. 1991] and Orca [Bal et a1.
1992]) or as a collection of variables in the source language (as in Munin [Bennett et al.
1990] and Midway [Bershad et al. 1993]). The granularity in such DSM systems is an
object or a variable. But since the sizes of the objects and data types vary greatly, these
DSM systems use variable grain size to match the size of the object/variable being
accessed by the application. The use of variable grain size complicates the design and
implementation ofthese DSM systems.
3. Structuring as adatabase. Anothermethod isto structurethe sharedmemory like
a database. For example, Linda [Carriero and Gelernter 1989] takes this approach. Its
shared-memory space is ordered as an associative memory (a memory addressed by
content rather than by name or address) called a tuple space~ which is a collection of
immutable tuples with typed data items in their fields. A set of primitives that can be
addedtoany base language(such asCand FORTRAN)are providedto place tuples inthe
tuple space and to read or extract them from tuple space. To perform updates, old data
items in the DSM are replaced by new data items. Processes select tuples by specifying
the number of their fields and their values or types. Although this structure allows the
location of data to be separated from its value, it requires programmers to use special

238 Chap. 5 • Distributed Shared Memory
access functions to interact with the shared-memory space. Therefore, access to shared
data is nontransparent. In most other systems, access to shared data is transparent.
5.6 CONSISTENCY MODELS
Consistency requirements vary from application to application [Bennett et a1. 1990,
Cheriton 1986, Garcia-Molina and Wiederhold 1982]. A consistency model basically
refers to the degree of consistency that has to be maintained for the shared-memory data
for the memory to workcorrectlyfor acertain set ofapplications. It isdefined as a setof
rules that applications must obey ifthey want the DSM system to.provide the degree of
consistency guaranteed by the consistency model. Several consistency models have been
proposed in the literature. Ofthese, the main ones are described below.
It may benoted here that theinvestigationof newconsistencymodels iscurrently an
active area ofresearch. The basic idea is to invent a consistency model that can allow
consistency requirements to be relaxed to a greater degree than existing consistency
models, with the relaxation done in such a way that a set of applications can function
correctly. This helps in improving the performance of these applications because better
concurrency can be achieved by relaxing the consistency requirement. However,
applications that depend on a stronger consistency model may not perform correctly if
executed in a system that supports only a weaker consistency model. This is because ifa
system supports the stronger consistency model, then the weaker consistency model is
automatically supported but the converse is not true.
Strict Consistency Model
The strict consistencymodel isthe strongestform of memory coherence, having the most
stringent consistency requirement. A shared-memory system is said to support the strict
consistencymodel ifthe.valuereturned byareadoperationonamemory address isalways
the same as the value written by the most recent write operation to that address,
irrespective of the locations of the processes performing the read and write operations.
That is, all writes instantaneously become visible to all processes.
Implementationofthe strict consistency model requires the existence of an absolute
global time so that memoryread/write operations can be correctly ordered to make the
meaning of "most recent" clear. However, as explained in Chapter 6, absolute
synchronization of clocks of all the nodes of a distributed system is not possible.
Therefore, the existence of an absolute global time in a distributed system is also not
possible. Consequently, implementationof thestrict consistency model foraDSM system
is practically impossible.
Sequential Consistency Model
The sequential consistency model was proposed by Lamport [1979]. A shared-memory
system is said to support the sequential consistency model if all processes see the same
orderofallmemory access operationsontheshared memory.Theexact order inwhich the

Sec.5.6 • Consistency Models 239
memory access operations are interleaved does not matter. That is, ifthe three operations
read ('1), write (WI)' read ('2) are performed on a memory address in that order, any of
the orderings ('1, WI' '2), ('" '2, WI), (w., '" '2), (W., '2' '1), (r2' '1, WI), (r2' WI' '1)
of the three operations is acceptable provided all processes see the same ordering. If one
process sees one of the orderings of the three operations and another process sees a
differentone, thememory isnotasequentiallyconsistentmemory. Note here that theonly
acceptable ordering for a strictly consistent memory is (rl' WI, '2)'
The consistencyrequirementofthe sequentialconsistency model isweakerthan that
of the strict consistency model because the sequential consistency model does not
guarantee that a read operation on a particular memory address always returns the same
value aswritten bythe most recent writeoperationtothataddress. Asaconsequence,with
a sequentially consistent memory, running a program twice may not give the same result
in the absence of explicit synchronization operations. This problem does not exist in a
strictly consistent memory.
A DSMsystem supporting the sequentialconsistency model can be implemented by
ensuring that no memory operation is started until all the previous ones have been
completed. A sequentially consistent memory provides one-copy/single-copy semantics
becauseallthe processes sharingamemory locationalways seeexactly thesame contents
stored in it. This is the most intuitively expected semantics for memory coherence.
Therefore, sequential consistency is acceptable by most applications.
Causal Consistency Model
The causal consistency model, proposed by Hutto and Ahamad [1990], relaxes the
requirement of the sequential consistency model for better concurrency. Unlike the
sequential consistency model, in the causal consistency model, all processes see only
those memory referenceoperationsinthesame (correct)orderthat arepotentiallycausally
related. Memory referenceoperationsthat are not potentiallycausally related may beseen
by different processes in different orders. A memory reference operation (read/write) is
said to be potentially causally related to another memory reference operation if the first
one might have been influenced in any way by the second one. For example, if aprocess
performs aread operationfollowed by awrite operation, the write operationispotentially
causally related to the read operation because the computation of the value written may
have depended in some way on the value obtained by the read operation. On the other
hand, a write operation performed by one process is not causally related to a write
operation performed by another process if the first process has not read either the value
written by the second process or any memory variable that was directly or indirectly
derived from the value written by the second process.
A shared memory system is said to support the causal consistency model ifall write
operations that are potentially causally related are seen by all processes in the same
(correct) order. Write operations that are not potentially causally related may be seen by
different processes in different orders. Note that "correct order" means that if a write
operation (w2) is causally related to another write operation (Wi), the acceptable order is
(WI' w2) becausethe value written by W2 might have been influenced insome way by the
value written by WI' Therefore, (w2' WI) is not an acceptable order.

240 Chap. 5 • Distributed SharedMemory
Obviously, in the implementation ofa shared-memory system supporting the causal
consistency model, there is a need to keep track ofwhich memory reference operation is
dependenton which other memory referenceoperations.This can bedone byconstructing
and maintaining a dependency graph for the memory access operations.
Pipelined Random-Access Memory Consistency Model
The pipelined random-access memory (PRAM) consistency model, proposed by Lipton
and Sandberg [1988], provides a weaker consistency semantics than the consistency
models described so far. It only ensures that all write operations performed by a single
processare seen byallotherprocesses inthe order inwhich they were performedas ifall
the write operations performed by a single process are in a pipeline. Write operations
performed by different processes may be seen by different processes in different orders.
For example, if WIl and Wl2 are two writeoperations performed by a process PI in that
order, and w21 and W22 are two write operations performed by a process P2 in that order,
aprocess P 3 may see them in the order [(WII, WI2), (W21' W2Z)] and another process P4
may see them in the order [(wz., WZ2), (WIJ' w,z)].
The PRAM consistency model is simple and easy to implement and also has good
performance. It can be implemented by simply sequencing the write operations
performed at each node independently of the write operations performed on other
nodes. It leads to better performance than the previous models because a process need
not wait for a write operation performed by it to complete before starting the next one
since all write operations of a single process are pipelined. Notice that in sequential
consistency all processes agree on the same order of memory reference operations, but
in PRAM consistency all processes do not agree on the same order of memory
reference operations. Therefore, for the example given above, either [(WIl' WIZ), (WZlt
W22)] or [(W21, W22), (wJ" WIZ)] is an acceptable ordering for sequential consistency
but not both. That is, unlike PRAM consistency, both processes P and P must agree
3 4
on the same order.
Processor Consistency Model
The processor consistency model, proposed by Goodman [1989], is very similar to the
PRAM consistency model with an additional restriction ofmemory coherence. That is, a
processor consistent memory is both coherent and adheres to the PRAM consistency
model. Memorycoherencemeans that for any memory locationallprocessesagree on the
same orderofall write operationstothatlocation. Ineffect, processorconsistencyensures
that all write operations performed on the same memory location (no matter by which
processthey are performed)are seen byallprocessesinthe same order. This requirement
is in addition to the requirementimposed by the PRAM consistencymodel. Therefore, in
theexamplegiven for PRAM consistency, if WIZ and W22 are write operations for writing
tothesame memorylocationx,allprocessesmust seethem inthesame order-wlZbefore
W22 or W22 before W12. This means that for processorconsistency both processes P3 and
P4must seethe write operationsinthe same order, which may beeither[(WII' WIZ), (WZl'
W22)] or [(W21' W22),(Wll' WJ2)]. Notice that, forthisexample,processorconsistencyand

Sec.5.6 • Consistency Models 241
sequential consistency lead to the same final result, but this may not be true for other
cases.
Weak Consistency Model
The weak consistency model, proposed by Dubois et a1. [1988], is designed to take
advantage of the following two characteristics common to many applications:
1. It is not necessary to show the change in memory done by every write operation
to other processes. The results of several write operations can be combined and sent to
other processesonly when they need it.Forexample, when aprocess executesinacritical
section, other processes are not supposed to see the changes made by the process to the
memory until the process exits from the critical section. In this case, all changes made to
the memory by the process while itis in itscritical section need be made visible to other
processes only at the time when the process exits from the critical section.
2. Isolated accesses to shared variables are rare. That is, in many applications, a
process makes several accesses toasetof shared variables and then noaccess atalltothe
variables in this set for a long time.
Both characteristics imply that better performance can be achieved ifconsistency is
enforced on a group of memory reference operations rather than on individual memory
reference operations. This is exactly the basic idea behind the weak consistency model.
The main problem in implementing this idea is determining how the system can
know that it istime to show the changes performed by a process to other processes since
this time is different for different applications. Since there is no way for the system to
know this on its own, the programmers are asked to tell this to the system for their
applications. For this, a DSM system that supports the weak consistency model uses a
special variable called a synchronization variable. The operations on it are used to
synchronize memory. That is, when a synchronization variable is accessed by a process,
the entire (shared) memory is synchronized by making all the changes to the memory
made byall processes visible toall other processes. Note that memory synchronization in
aDSM system will involve propagatingmemory updates done atanode toallother nodes
having a copy ofthe same memory addresses.
For supporting weak consistency, the following requirements must be met:
1. All accesses to synchronization variables must obey sequential consistency
semantics.
2. All previous write operations must be completedeverywhere before an access to
a synchronization variable is allowed.
3. All previous accesses to synchronization variables must be completed before
access to a nonsynchronization variable is allowed.
Note that the weak consistency model provides better performance at the cost of
putting extra burden on the programmers.

242 Chap.5 • DistributedSharedMemory
Release Consistency Model
Wesaw that in the weak consistency model the entire (shared) memory is synchronized
when a synchronization variable is accessed by a process, and memory synchronization
basically involves the following operations:
1. All changes made to the memory by the process are propagated to other nodes.
2. All changes made to the memory by other processes are propagated from other
nodes to the process's node.
A closer observation shows that this is not really necessary because the first
operation need only be performed when the process exits from a critical section and
the second operation need only be performed when the process enters a critical section.
Since a single synchronization variable is used in the weak consistency model, the
system cannot know whether a process accessing a synchronization variable is entering
a critical section or exiting from a critical section. Therefore, both the first and second
operations are performed on every access to a synchronization variable by a process.
For better performance, the release consistency model [Gharachorloo et al. 1990]
provides a mechanism to clearly tell the system whether a process is entering a critical
section or exiting from a critical section so that the system can decide and perform only
either the first or the second operation when a synchronization variable is accessed by
a process. This is achieved by using two synchronization variables (called acquire and
release) instead of a single synchronization variable. Acquire is used by a process to
tell the system that it is about to enter a critical section, so that the system performs
only the second operation when this variable is accessed. On the other hand, release
is used by a process to tell the system that it has just exited a critical section, so that
the system performs only the first operation when this variable is accessed. Pro
grammers are responsible for putting acquire and release at suitable places in their
programs.
Release consistency may also be realized by using the synchronization mechanism
based on barriers instead of critical sections. A barrier defines the end of a phase of
executionofagroup ofconcurrentlyexecuting processes.Allprocesses inthegroup must
complete their execution uptoa barrier before any process isalJowedtoproceed withits
execution following the barrier. That is, when a process of a group encounters a barrier
during its execution, it blocks until all other processes in the group complete their
executions up to the barrier. When the last process completes its execution up to the
barrier, all shared variables are synchronized and then all processes resume with their
executions. Therefore, acquire isdeparture fromabarrier,andrelease iscompletionofthe
execution of the last process up to a barrier.
A barrier can be implemented by using a centralized barrier server. When a barrier
iscreated, itisgiven acount ofthe number ofprocesses thatmust bewaiting on itbefore
they can all be released. Each process of a group of concurrently executing processes
sends a message to the barrier server when it arrives at a barrier and then blocks until a
reply isreceived from thebarrier server.The barrier serverdoes notsend anyreplies until
all processes in the group have arrived at the barrier.

Sec.5.6 • Consistency Models 243
For supporting release consistency, the following requirements must be met:
1. All accesses to acquire and release synchronization variables obey processor
consistency semantics.
2. All previous acquires performed by a process must be completed successfully
before the process is allowed to perform a data access operation on the
memory.
3. All previous data access operations performed by a process must be completed
successfully before a release access done by the process is allowed.
Note that acquires and releases of locks of differentcritical regions (or barriers) can
occur independentlyofeach other.Gharachorlooetal.[1990] showed thatifprocessesuse
appropriate synchronization accesses properly, a release consistent DSM system will
produce the same results for an application as that if the application was executed on a
sequentially consistent DSM system.
Avariationof release consistency islazy release consistency,proposedbyKeleher et
al. [1992], and is more efficient than the conventional release consistency. In the
conventional approach, when a process does a release access, the contents of all the
modified memory locations at the process's node are sent to all other nodes that have a
copy of the same memory locations in their local cache. However,.inthe lazy approach,
the modifications are not sent to other nodes at the time of release. Rather, these
modifications are sent to other nodes only on demand. That is, when a process does an
acquire access, all modifications of other nodes are acquired by the process's node.
Therefore, the modifications that were to besent tothis node atthe time of release access
will be received by the node now (exactly when it is needed there). Lazy release
consistency hasbetterperformancebecause inthis method nonetwork traffic isgenerated
at all until an acquire access is done by a process at some other node.
Discussion of Consistency Models
Inthedescriptionabove, wesawsomeofthemainconsistencymodels. Several others have
beenproposedintheliterature. Aniceoverviewoftheconsistencymodelscan befound in
[Mosberger 1993]. It is difficult to grade the consistency models based on performance
because quite different results are usually obtained fordifferentapplications. That is, one
application may have good performance forone model, butanotherapplication may have
good performance for some other model. Therefore, in the.design of a DSM system, the
choice ofaconsistencymodelusually dependsonseveralother factors, suchashoweasy is
ittoimplement,howclose isitssemanticstointuition, howmuchconcurrencydoesitallow,
andhoweasy isittouse(does itimpose extra burden ontheprogrammers).
Among the consistency models described above, strict consistency is never used in
the design of a DSM system because its implementation is practically impossible. The
most commonly used model inDSM systems isthe sequentialconsistency model because
it can be implemented, it supports the most intuitively expected semantics for memory
coherence, and it does not impose any extra burden on the programmers. Another

244 Chap. 5 • Distributed SharedMemory
important reason for its popularity is that a sequentially consistent DSM system allows
existing multiprocessor programs to be run on multicomputer architectures without
modification. This is because programs written for multiprocessors normally assume that
memory is sequentially consistent. However, it is veryrestrictive and hence suffers from
the drawback of low concurrency. Therefore, several DSM systems are designed to use
other consistency models that are weaker than sequential consistency.
Causal consistency, PRAM consistency, processor consistency, weak consistency,
and release consistency are the main choices in the weaker category. The main problem
with the useofcausal consistency, PRAM consistency, and processor consistency models
in the design of a DSM system is that they do not support the intuitively expected
semantics for memory coherence because different processes mayseedifferent sequences
of operations. Therefore, with these three models it becomes the responsibility of the
programmers to avoid doing things that work only if the memory is sequentially
consistent. In this respect, these models impose extra burden on the programmers.
Weak consistency and release consistency models, which use explicit synchroniza
tion variables, seem more promising for use in DSM design because they provide better
concurrency and also support the intuitively expected semantics. The only problem with
these consistency models is that they require theprogrammers to use the synchronization
variables properly. This imposes some burden on the programmers.
Note that one of the main reasons for taking all the trouble to implement aD5M
system is to support the shared-memory paradigm to make programming of distributed
applicationssimpler thaninthemessage-passingparadigm.Therefore, someDSM system
designers areof theopinion that noextra burden should beimposed on theprogrammers.
Designers having this view prefer to use the sequential consistency model.
5.6.1 Implam.ntlng Sequential Conslst.ncy Model
Wesaw above that the most commonly used consistency model in DSM systems is the
sequential consistency model. Hence, a description of the commonly used protocols for
implementing sequentially consistent D5M systems is presented below. A protocol for
implementing a release-consistent DSM system will be presented in the next section.
Protocols for implementing the sequential consistency model in a DSM system
depend toa great extent on whether the DSM system allows replication and/or migration
of shared-memory data blocks. The designer of a DSM system may choose from among
the following replication and migration strategies [Stumm and Zhou 1990]:
1. Nonreplicated, nonmigrating blocks (NRNMBs)
2. Nonreplicated, migrating blocks (NRMBs)
3. Replicated, migrating blocks (RMBs)
4. Replicated, nonmigrating blocks (RNMBs)
Theprotocols thatmay beusedforeachofthesecategories aredescribed below.The
data-locatingmechanisms suitable foreach category have also beendescribed. Several of
the ideas presented below were first proposed and used in the IVY system [Li 1988].

Sec.5.6 • Consistency Models 245
Nonreplicated, Nonmigrating Blocks
This is the simplest strategy for implementing a sequentially consistent DSM system.
In this strategy, each block of the shared memory has a single copy whose location is
always fixed. All access requests to a block from any node are sent to the owner node
ofthe block, which has the only copy ofthe block. On receiving a request from a client
node, the memory management unit (MMU) and operating system software of the
owner node perform the access request on the block and return a response to the client
(Fig. 5.3).
Clientnode Ownernodeoftheblock
(sends request and (receivesrequest, performsdata
receives response) access, andsends response)
Request
Response
Fig.5.3 Nonreplicated, nonmigrating blocks (NRNMB)strategy.
Enforcingsequentialconsistencyistrivial inthiscase becauseanodehaving ashared
block can merely perform all the access requests (on the block) in the order it receives
them.
Although the method is simple and easy to implement, it suffers from the following
drawbacks:
• Serializing data access creates a bottleneck.
• Parallelism, which is a major advantage of DSM, is not possible with this
method.
Data Locating in the NRNMB Strategy. The NRNMB strategy has the
following characteristics:
1. There is a single copy ofeach block in the entire system.
2. The location of a block never changes.
Based on these characteristics, the best approach for locating a block in this case is
to use asimplemappingfunction to map ablockto anode. When a fault occurs,the fault
handlerofthe faulting node uses the mappingfunction to get the locationofthe accessed
block and forwards the access request to that node.

246 Chap.5 • Distributed SharedMemory
Nonreplicated, Migrating Blocks
In this strategy each block of the shared memory has a single copy in the entire system.
However, each access to a block causes the block to migrate from its current node to the
nodefrom where itisaccessed. Therefore, unliketheprevious strategy inwhich theowner
node of a block always remains fixed, in this strategy theowner node of ablock changes
as soon as theblock ismigrated toa new node (Fig.5.4). When a block ismigrated away,
it is removed from any local address space it has been mapped into. Notice that in this
strategy only the processes executing on one node can read or write a given data item at
anyone time. Therefore the method ensures sequential consistency.
Clientnode Ownernode
(becomesnewownernode of (ownstheblock before
block afteritsmigration) itsmigration)
Blockrequest
0 = = 0
Blockmigration
Fig.5.4 Nonreplicated, migrating blocks (NRMB) strategy.
The method has the following advantages [Stumm and Zhou 1990]:
1. Nocommunicationcosts are incurred whenaprocess accesses data currentlyheld
locally.
2. It allows the applications to take advantage of data access locality. If an
application exhibits high locality of reference, the cost of data migration is
amortized over multiple accesses.
However, the method suffers from the following drawbacks:
1. It is prone to thrashing problem. That is, a block may keep migrating frequently
from one node to another, resulting in few memory accesses between migrations
and thereby poor performance.
2. The advantage of parallelism cannot be availed in this method also.
Data Locating in the NRMB Strategy. In the NRMB strategy, although there
is a single copy of each block, the location of a block keeps changing dynamically.
Therefore, one ofthe following methods may beused in this strategy to locate a block:
1. Broadcasting. In this method, each node maintains an owned blocks table that
contains anentry for each block for which the node is thecurrent owner (Fig. 5.5). When

Sec.5.6 • Consistency Models 247
Nodeboundary Nodeboundary
Node1 NodeI NodeAt
Blockaddress Blockaddress Blockaddress
(changesdynamically) (changesdynamically) (changesdynamically)
Containsanentryfor Containsanentryfor Containsanentryfor
eachblockforwhich eachblockforwhich eachblockforwhich
thisnodeisthecurrent thisnodeisthecurrent thisnode;sthecurrent
owner owner owner
Ownedblockstable Ownedblockstable Ownedblockstable
Fig.5.5 Structure and locationsof owned blocks table in the broadcasting
data-locating mechanism for NRMB strategy.
afault occurs, the fault handlerof the faulting node broadcasts aread/writerequest on the
network. The node currently having the requested block then responds to the broadcast
request by sending the block to the requesting node.
A major disadvantage of the broadcasting algorithm is that it does not scale well.
When a request is broadcast, notjust the node that has the requested block but all nodes
must process the broadcastrequest. This makes the communication subsystemapotential
bottleneck. The network latency of a broadcast may also require accesses to take a long
time to complete.
2. Centralized-server algorithm. In this method, a centralized server maintains a
block table that contains the location information for all blocks in the shared-memory
space (Fig. 5.6). The location and identity of the centralized server is well known to all
nodes.
When afault occurs,the fault handlerofthefaulting node (N) sends arequest forthe
accessed block to the centralized server. The centralized server extracts the location
informationofthe requested block from the block table, forwards therequest tothatnode,
andchangesthe location informationinthecorrespondingentry oftheblock table tonode
N. On receiving the request, the current owner transfers the block to node N, which
becomes the new owner of the block.
The centralized-servermethod suffers from twodrawbacks: (a)thecentralizedserver
serializes location queries, reducing parallelism, and (b) the failure of the centralized
server will cause the DSM system to stop functioning.
3. Fixeddistributed-serveralgorithm.The fixed distributed-serverscheme isadirect
extensionof thecentralized-serverscheme. Itovercomes the problems of the centralized
server schemeby distributing the role of thecentralized server.Therefore, inthis scheme,
there is a block manager on several nodes, and each block manager is given a

248 Chap. 5 • Distributed SharedMemory
Nodeboundary Nodeboundary
Node1 NodeI NodeIf
Block Owner
address node
(remains (changes
fixed) dynamically)
Containsanentryfor
eachblockintheshared-
memoryspace
Blocktable
Centralizedserver Fig.5.6 Structure and location of block table
inthecentralized-serverdata
locating mechanism for NRMB
strategy.
predeterminedsubset of data blocks to manage (Fig. 5.7).The mapping from data blocks
to block managers and their corresponding nodes is described by a mapping function.
Wheneverafaultoccurs, the mapping function isused by thefault handlerofthe faulting
node tofindout thenode whose block managerismanaging thecurrentlyaccessed block.
Then arequest for theblock issent totheblock managerofthat node.The block manager
handles therequest exactly inthesame manner asthatdescribedforthecentralized-server
algorithm.
Nodeboundary Nodeboundary
Node1 NodeI NodeIf
Block Owner Block Owner Block Owner
address node address node address node
(remains (changes (remains (changes (remains (changes
fixed) dynamically) fixed) dynamically) fixed) dynamically)
Containsentriesfora Containsentriesfora Containsentriesfora
fixedsubsetofallblocks fixedsubsetofallblocks fixedsubsetofallblocks
intheshared-memory intheshared-memory intheshared-memory
space space space
Blocktable Blocktable Blocktable
Blockmanager Blockmanager Blockmanager
Flg.5.7 Structure and locations of block tableinthefixed distributed-server
data-locatingmechanism for NRMB strategy.

Sec. 5.6 • Consistency Models 249
4. Dynamic distributed-server algorithm. This scheme does not use any block
manager and attempts to keep track of the ownership information of all blocks in each
node. For this, each node hasablock table thatcontains theownershipinformationfor all
blocks in the shared-memory space (Fig. 5.8). However, the information containedin the
ownership field is not necessarily correct at all times, but if incorrect, it at least provides
the beginning of a sequence of nodes to be traversed to reach the true owner node of a
block. Therefore, this field gives the node a hint on the location of the owner of a block
and hence is called the probable owner.
Nodeboundary Nodeboundary
Node1 NodeI NodeM
Block Probable Block Probable Block Probable
address node address node address node
(remains (changes (remains (changes (remains (changes
fixed) dynamically) fixed) dynamically) fixed) dynamically)
Containsanentryfor Containsanentryfor Containsanentryfor
eachblockintheshared- eachblockintheshared- eachblockintheshared-
memoryspace memoryspace memoryspace
Blocktable Blocktable Blocktable
(f'ig.5.8 Structureand locationsof block table in the dynamic distributed-server
data-locatingmechanismforNRMBstrategy.
Whenafaultoccurs, thefaulting node(N)extracts fromitslocal blocktablethenode
information stored inthe probable owner field of theentry for the accessed block. It then
sends a request for the block to that node. If that node is the true owner of the block, it
transfers theblock tonode Nand updates the location informationoftheblock initslocal
block table tonode N.Otherwise, itlooks up its local block table, forwards the request to
the node indicated in the probable-ownerfield of the entry for the block, and updates the
value ofthis field to nodeN. When node N receives the block, itbecomes the new owner
of the block.
Replicated, Migrating Blocks
A major disadvantage of the nonreplication strategies is lack of parallelismbecause only
the processes on one node can access data contained in a block at any given time. To
increase parallelism, virtually all DSM systems replicate blocks. With replicated blocks,
readoperationscanbecarried outinparallelatmultiple nodes byaccessing thelocalcopy
of the data. Therefore, the average cost of read operations is reduced because no

250 Chap.5 • Distributed SharedMemory
communication overhead is involved if a replica of the data exists at the local node.
However, replication tends to increase the cost ofwrite operations because for a write to
a block all its replicas must be invalidated or updated to maintain consistency.
Nevertheless,iftheread/writeratio islarge, theextra expenseforthe write operationsmay
be more than offset by the lower average cost ofthe read operations.
Replication complicates the memory coherence protocol due to the requirement of
keeping multiple copies ofa block consistent. The two basic protocols that may be used
for ensuring sequential consistency in this case are as follows:
1. Write-invalidate. In this scheme, aJl copies of a piece of data except one are
invalidated before a write can beperformed on it. Therefore, when a write fault occurs
at a node, its fault handler copies the accessed block from one of the block's current
nodes to its own node, invalidates all othercopies ofthe block by sending an invalidate
message containing the block address to the nodes having a copy ofthe block, changes
the access ofthe local copy ofthe block to write, and returns to the faulting instruction
(Fig. 5.9). After returning, the node "owns" that block and can proceed with the write
operation and other read/write operations until the block ownership is relinquished to
some othernode. Notice that in this method, after invalidation ofa block, only the node
that performs the write operation on the block holds the modified version ofthe block.
Nodeshavingvalidcopies
ofthedatabrockbefore
writeoperation
ellenlnode
(hasthevalidcop~
ofthedatablockafter
writeoperation)
Fig.5.9 Write-invalidatememory coherenceapproach for replicated. migrating
blocks (RMB) strategy.

Sec. 5.6 • Consistency Models 251
If one of the nodes that had a copy of the block before invalidation tries to perform
a memory access operation (read/write) on the block after invalidation, a cache miss
will occur and the fault handler of that node will have to fetch the block again from
a node having a valid copy of the block. Therefore the scheme achieves sequential
consistency.
2. Write-update. In this scheme, a write operation is carried out by updating all
copies of the data on which the write is performed. Therefore, when a write fault occurs
atanode, the fault handlercopiestheaccessedblock from one ofthe block'scurrentnodes
to its own node, updates all copies of the block by performing the write operation on the
local copy of the block and sending the address of the modified memory location and its
new value to the nodes having a copy of the block, and then returns to the faulting
instruction(Fig. 5.10). The write operationcompletesonly after all thecopiesoftheblock
have been successfully updated. Notice that in this method, after a write operation
completes, all the nodes that had a copy of the block before the write also have a valid
copy of the block after the write. In this method, sequential consistency can be achieved
by using a mechanism to totally order the write operations of all the nodes so that all
processesagree on the orderof writes. One methodto do this isto use aglobal sequencer
tosequencethe write operationsofall nodes. In this method, the intendedmodificationof
each write operation is first sent to the global sequencer. The sequencer assigns the next
Nodeshavingvalidcopies
ofthedatablockbothbeforeand
afterwriteoperation
Clientnode
(alsohasavalidcopy
ofthedatablockafter
writeoperation)
Fig. S.10 Write-update memory coherenceapproach for replicated, migrating
blocks (RMB) strategy.

252 Chap.5 • Distributed SharedMemory
sequence number to the modification and multicasts the modification with this sequence
number to all the nodes where a replica of the data block to be modified is located (Fig.
5.11).The writeoperations areprocessed ateach nodeinsequence number order.That is,
when a new modification arrives at a node, its sequence number is verified as the next
expected one. If the verification fails, either a modification was missed or a modification
was received out of order, in which case the node requests the sequencer for a
retransmissionofthemissing modification. Obviously,alogofrecent writerequests must
be maintained somewhere in this method. Note that the set of reads that take place
between any two consecutive writes is well defined, and theirordering is immaterial to
sequential consistency.
OthernodeshaVing
areplicaofthe
datablock
Sequenced
modification
0
Sequenced
Clientnode modification
(hasareplica
ofthedatablock)
Fig.5.11 Globalsequencingmechanism tosequencethewriteoperationsofallnodes.
The write-update approach is very expensive for use with loosely coupled
distributed-memorysystems because itrequiresanetwork access onevery writeoperation
and updates all copies of the modified block. On the other hand, in the write-invalidate
approach, updates are only propagated when data are read, and several updates can take
place (because many programs exhibit locality of reference) before communication is
necessary. Therefore, most DSM systems use the write-invalidate protocol. In the basic
implementation approach of the write-invalidate protocol, there is a status tag associated
witheach block. The status tag indicates whether the block is valid, whether it is shared,
and whether it is read-only or writable. With this status information, read and write
requests are carried out in the following manner [Nitzberg and Virginia Lo 1991]:

Sec.5.6 • Consistency Models 253
Read Request
1. Ifthereisalocalblockcontainingthedataandifitisvalid, therequestissatisfied
by accessing the local copy of the data.
2. Otherwise, the fault handler of the requesting node generates a read fault and
obtains a copy of the block from a node having a valid copy of the block. If the
block waswritable onanother node, thisreadrequest willcause ittobecome read
only.The read request isnow satisfied byaccessingthedata from thelocal block,
which remains valid until an invalidate request is received.
Write Request
1. If there is a local block containing the data and if it is valid and writable, the
request is immediately satisfied by accessing the local copy of the data.
2. Otherwise, the fault handler of the requesting node generates a write fault and
obtains a valid copy of the block and changes its status to writable. A write fault
for a block causes the invalidation of all other copies of the block. When the
invalidation of all other copies of the block completes, the block is valid locally
and writable, and the original write request may now be performed.
Data Locating in the RMB Strategy. The following data-locating.issues are
involved in the write-invalidate protocol used with the RMB strategy:
1. Locating the owner of a block. An ownerof a block is the node that owns the
block, namely, the most recent node to have write access to it.
2. Keeping track of the nodes that currently have a valid copy of the block.
One of the following algorithms may be used to address these two issues:
1. Broadcasting. In this method, each node has an owned blocks table (Fig. 5.12).
This table of a node has an entry for each block for which the node is the owner. Each
entry of this table has a copy-set field that contains a list of nodes that currently have
a valid copy of the corresponding block.
When a read fault occurs, the faulting node (N) sends a broadcast read request
on the network to find the owner of the required block. The owner of the block
responds by adding node N to the block's copy-set field in its owned blocks table
and sending a copy of the block to node N. Similarly, when a write fault occurs, the
faulting node sends a broadcast write request on the network for the required block.
On receiving this request, the owner of the block relinquishes its ownership to node
N and sends the block and its copy set to node N. When node N receives the block
and the copy set, it sends an invalidation message to all nodes in the copy set. Node
N now becomes the new owner of the block, and therefore an entry is made for the
block in its local-owned blocks table. 'The copy-set field of the entry is initialized to
indicate that there are no other copies of the block since all the copies were
invalidated.

254 Chap.5 • Distributed Shared Memory
Nodeboundary Nodeboundary
Node1 NodeI NodeM
Block Copy-set Block Copy-set Block ~opy-set
address (changes address (changes address (changes
(changes dynamically (changes dynamically) (changes ~ynamically)
dynamically) dynamically) dynamically)
Containsanentryfor Containsanentryfor Containsanentryfor
eachblockforwhich eachblockforwhich eachblockforwhich
thisnodeistheowner thisnodeistheowner thisnodeistheowner
Ownedblockstable Ownedblockstable Ownedblockstable
Fig. 5.12 Structure and locationsofowned blocks table inthe broadcasting
data-locating mechanismfor RMB strategy.
The method of broadcasting suffers from the disadvantages mentioned during
the description of the data-locating mechanisms for the NRMB strategy.
2. Centralized-server algorithm. This method is similar to the centralized-server
algorithm of the NRMB strategy. However, in this case, each entry of the block table,
managed by the centralized server, has an owner-node field that indicates the current
owner node of the block and a copy-set field that contains a list of nodes having a valid
copy of the block (Fig. 5.13).
When aread/writefault occurs, thefaulting node (N) sends aread/writefault request
fortheaccessedblocktothecentralizedserver.Forareadfault,thecentralizedserveradds
nodeNto the block's copy set and returns the owner node information to nodeN.Onthe
other hand, for a write fault, it returns both the copy set and owner node information to
node N and then initializes thecopy-set field tocontain only node N. Node N then sends
a request for the block to the owner node. On receiving this request, the owner node
returns a copy of the block to node N. In a write fault, node N also sends an invalidate
message to all nodes in the copy set. Node N can then perform the read/write
operation.
The disadvantages of the centralized-serveralgorithm have already been mentioned
during description ofthe data-locating mechanisms for the NRMB strategy.
3. Fixed distributed-server algorithm. This scheme is a direct extension of the
centralized-serverscheme. In this scheme, therole of the centralized serverisdistributed
to several distributed servers. Therefore, in this scheme, there is a block manager on
several nodes (Fig. 5.14). Each block manager manages apredeterminedsubset ofblocks,
and a mapping function is used to map a block to a particular block manager and its
correspondingnode.When afaultoccurs, themapping function isusedtofindthelocation
of the block manager that is managing the currently requested block. Then a request for

Sec. 5.6 • Consistency Models 255
Nodeboundary Nodeboundary
Node1 Node I NodeIf
Block
OwnernodeCopy-set
address
(changes (changes
(remains
dynamically)dynamically~
fixed)
Containsanentryforeachblock
intheshared-memoryspace
Blocktable
Fig.5.13 Structureand locationof block table inthe centralized-serverdata-locating
mechanismfor RMB strategy.
the accessed block is sent to the block manager of that node. A request is handled in
exactly the same manner as that described for the centralized-server algorithm.
The advantagesand limitationsofthefixed distributed-serveralgorithmhave already
been mentioned during the description of the data-locating mechanisms for the NRMB
strategy.
Nodeboundary Nodeboundary
Node1 Node; NodeM
Block Block Block
OwnernodeCopy-set OwnernodeCopy-set Ownernode~opy-set
address address address
(remains (changes (changes (remains (changes (changes (remains (changes Kchanges
dynamicaUy)dynamicafly~ dynamicaUy}dynamicaffy~ dynamically)~ynamica"y)
fixed) fixed) fixed)
Containsentriesforafixed Containsentriesforafixed Containsentriesforafixed
subsetofallblocksinthe subsetofallblocksinthe subsetofallblocksinthe
shared-memoryspace shared-memoryspace shared-memoryspace
Blocktable Blocktable Blocktable
Blockmanager Blockmanager Blockmanager
Fig.5.]4 Structureand locationsof blocktable in the fixed distributed-server
data-locating mechanism for RMB strategy.

256 Chap. 5 • Distributed Shared Memory
4. Dynamic distributed-server algorithm. This scheme works in a similar manner
as the dynamic distributed-server algorithm of the NRMB strategy. Each node has a
block table that contains an entry for all blocks in the shared-memory space (Fig. 5.15).
Each entry of the table has a probable-owner field that gives the node a hint on the
location of the owner of the corresponding block. In addition, if a node is the true
owner of a block, the entry for the block in the block table of the node also contains
a copy-set field that provides a list of nodes having a valid copy of the block.
Nodeboundary Nodeboundary
Node1 Node; NodeM
Block Probable Block Probable Block Probable
address owner Copy-set .ddress owner Copy-set address owner Copy-set
(changes (changes (changes
(remains (changes (remains (changes (remains (changes
dynamically) dynamically) dynamically)
fiXed) dynamically) ~ixed) dynamically) fixed) dynamically)
Anentryhas Anentryhas Anentryhas
Containsan avaluein Containsan avaluein Containsan avaluein
entryforeach thisfieldonly entryforeach thisfieldonly entryforeach thisfieldonly
blockinthe ifthisnodeis blockinthe ifthisnodeis blockinthe ifthisnodeis
shared-memory thetrueowner shared-memory thetrueowner shared-memory thetrueowner
space ofthe space oftha space ofthe
corresponding corresponding corresponding
block block block
Blocktable Blocktable Blocktable
Fig.5.15 Structureand locations ofblock table in thedynamicdistributed-server
data-locating mechanism forRMB strategy.
Whenafaultoccurs, thefaulthandlerofthefaulting node(N) extracts the probable
owner node information for the accessed block from its local block table and sends a
request for the block tothat node.Ifthat node isnotthe true owner of the block, it looks
up its local block table and forwards the request to the node indicated in the probable...
owner field oftheentry forthe block. On theother hand, ifthe node is the trueownerof
the block, it proceeds with the request as follows. For a read fault, it adds node N in the
copy-set field of the entry corresponding to the block and sends a copy of the block to
nodeN, which then performs the read operation. For a'write fault, it sends acopy of the
blockand itscopy-set information to nodeNand deletes thecopy-set information ofthat
block from the local block table, On receiving the block and copy-set information, node
Nsendsaninvalidation request toallnodesinthecopy set.Afterthis,itbecomes thenew
owner of the block, updates its local block table, and proceeds withperforming thewrite
operation.

Sec. 5.6 • Consistency Models 257
Toreduce the length ofthe chain of nodes to be traversed to reach the true owner of
ablock, the probable-ownerfield of a block ina node's block table isupdated asfollows
[Li and Hudak 1989]:
(a) Whenever the node receives an invalidation request
(b) Whenever the node relinquishes ownership, that is, on a write fault
(c) Whenever the node forwards a fault request
In the first two cases, the probable-owner field is changed to the new owner ofthe
block. In the third case, the probable-ownerfield ischanged to the original faulting node
(N).This isbecause, iftherequest isforwrite, thefaulting node(N) isgoingtobethenew
owner. If the request is for read, we know that after the request is satisfied, the faulting
node (N) will have the correct ownership information. In either case, it is a good idea to
change the probable-owner field of the block to node N.
It has been proved in [Li and Hudak 1989] that a fault on anynode for a block
eventually reaches the true owner of the block, and ifthere are altogetherMnodes, it will
take at most M - 1messages to locate the true owner of a block.
Some refinements of the data-locating algorithms described above may be found in
[Li and Hudak 1989].
Replicated, Nonmigrating Blocks
In this strategy, a shared-memory block may be replicated at multiple nodes of the
system, but the location of each replica is fixed. A read or write access to a memory
address is carried out by sending the access request to one of the nodes having a
replica of the block containing the memory address. All replicas of a block are kept
consistent by updating them all in case of a write access. A protocol similar to the
write-update protocol is used for this purpose. Sequential consistency is ensured by
using a global sequencer to sequence the write operations of all nodes (Fig. 5.11).
Data Locating in the RNMB Strategy. The RNMB strategy has the following
characteristics:
1. The replica locations of a block never change.
2. All replicas of a data block are kept consistent.
3. Only areadrequest can bedirectly senttooneofthenodes having areplica ofthe
block containing the memory address on which the read request isperformedand
all write requests have to be first sent to the sequencer.
Based on these characteristics, the best approach of data locating for handling read!
write operations in this case is to have a block table at each node and a sequence table
with the sequencer (Fig. 5.16). The block table of a node has an entry for each block
in the shared memory. Each entry maps a block to one of its replica locations. The
sequence table also has an entry for each block.in the shared-memory space. Each entry
of the sequence table has three fields-a field containing the block address, a replica-

258 Chap. 5 • Distributed Shared Memory
Nodeboundary Nodeboundary
Node1 Node; NodeM
Block Replica Block Replica Block Replica
address node address node address node
(remains (remains (remains (remains (remains (remains
fixed) fixed) fixed) fixed) fixed) fixed)
Containsanentry Containsanentry Containsanentry
foreachblockin foreachblockin foreachblockin
theshared-memory theshared-memory theshared-memory
space space space
Blocktable Blocktable Blocktable
Block Replica Sequence
address set number(is
f.remains lremainsIncremented
IXed) ixed) byonefor
eve':Y.new
modification
intheblock)
Containsanentry
foreachblockin
theshared-memory
space
Sequencetable
Centralizedsequencer
Fig. 5.16 Structure and locations of block tableand sequence table inthecentralized
sequencerdata-locating mechanism forRNMB strategy.
set field containing a list of nodes having a replica of the block, and a sequence number
field that is incremented by I for every new modification performed on the block.
For performing a read operation on a block, the replica location of the block is
extracted from the local block table and the read request is directly sent to that node.
A write operation on a block is sent to the sequencer. The sequencer assigns the next
sequence number to the requested modification. It then multicasts the modification
with this sequence number to all the nodes listed in the replica-set field of the entry
for the block. The write operations are performed at each node in sequence" number
order.
Note that, to prevent all read operations on a block getting serviced at the same
replica node, as far as practicable, the block table of different nodes should have
different replica locations entered in theentry corresponding to the block. This will help
in evenly distributing the read operations on the same block emerging from different
nodes.

Sec.5.6 • Consistency Models 259
5.6.2 Munin:AReleaseConsistentDSMSystem
Wesaw inthe discussionofconsistency models that inadditionto sequentialconsistency,
release consistencyisalsopromisingandattractiveforuseinDSMsystems.Therefore,asa
case study ofa release consistent DSM system, a description of the Munin system is
presentedbelow [Bennettetal. 1990,Carter etal. 1991,Carteretal. 1994].
StructureofShared-MemorySpace
The shared-memory space of Munin is structured as a collection of shared variables
(includes program data structures). The shared variables are declared with the keyword
sharedsothe compilercan recognize them. Aprogrammercan annotate ashared variable
with one of the standard annotation types (annotation types for shared variables are
described later). Each shared variable, bydefault, isplaced by the compileron a separate
page that isthe unit of data transferacross the networkby the MMU hardware. However,
programmerscanspecify thatmultiple shared variables having thesameannotationtypebe
placed inthesame page. Placing ofvariables ofdifferentannotationtypes inthesamepage
isnot allowed because theconsistencyprotocol used forapage dependsonthe annotation
typeofvariables containedinthepage.Obviously, variables ofsizelargerthanthesizeofa
pageoccupy multiple pages.The shared variables aredeclaredwiththekeyword sharedso
thecompilercanrecognizethem.
ImplementationofReleaseConsistency
In the description ofrelease consistency, wesaw that for release consistency applications
must be modeled around critical sections. Therefore a DSM system that supports release
consistency must have mechanisms and programming language constructs for critical
sections. Munin provides two such synchronization mechanisms-a locking mechanism
andabarriermechanism.
The locking mechanism uses lock synchronization variables with acquirel.ock and
releaseLockprimitivesforaccessingthese variables.Theacquirel.ockprimitivewithalock
variable as its parameter is executed by a process to enter a critical section, and the
releaseLock primitive with the same lock variable as its parameter is executed by the
process toexit from thecritical section. Toensurerelease consistency, writeoperationson
shared variables must be performed only within critical sections, but read operations on
shared variables maybeperformedeitherwithin oroutsideacritical section. Modifications
madetoashared variable withinacritical section aresenttoothernodeshaving areplicaof
theshared variable only whentheprocess making theupdate exitsfromthecritical section.
Ifprogramsof anapplicationare properly structuredasdescribedabove, the DSM system
willappear tobesequentiallyconsistent.
Whenaprocessmakesanacquirel.ockrequestforacquiringalockvariable,thesystem
first checks if the lock variable is available on the local node. If not, the probable-owner
mechanism is used to find the location of the current ownerof the lock variable, and the
requestissenttothat node.Whetherthelockisonthelocalorremote node,ifitisfree,itis
grantedto the requestingprocess. Otherwise,the requestingprocessisadded to theend of

Chap. 5 • Distributed SharedMemory
the queue of processeswaitingto acquirethe lock variable. Whenthe lock variableis
releasedbyitscurrentowner,itisgiventothenextprocessinthewaitingqueue.
Thebarriermechanism usesbarriersynchronization variableswitha waitAtBarrier
primitiveforaccessingthesevariables. Barriersareimplementedbyusingthecentralized
barrierservermechanism.
In a network page fault, the probable-owner-based dynamic distributed-server
algorithm(alreadydescribedduringthedescriptionofRMBstrategy)isusedinMuninto
locateapagecontainingtheaccessedsharedvariable.Amechanismsimilartothecopy-set
mechanism(alsodescribedduringthedescriptionofRMB strategy)isusedtokeeptrackof
allthereplicalocationsofapage.
Annotationsfor SharedVariables
ThereleaseconsistencyofMuninallowsapplicationstohavebetterperformancethanina
sequentiallyconsistentDSMsystem.Forfurtherperformanceimprovement,Munindefines
severalstandardannotationsforsharedvariablesandusesadifferentconsistencyprotocol
for each type that is most suitablefor that type.That is, consistencyprotocolsin this
approachareappliedatthegranularityofindividualdataitems.Thestandardannotations
andtheconsistencyprotocolforvariablesofeachtypeareasfollows[Bennetteta1. 1990]:
I. Read-only. Shared-datavariablesannotatedasread-onlyareimmutabledataitems.
Thesevariablesareread butneverwrittenafterinitialization. Therefore,thequestionof
consistencycontroldoesnotarise.Asthesevariablesarenevermodified,theiraverageread
costcanbereduceddrasticallybyfreelyreplicatingthemonallnodesfromwheretheyare
accessed.Therefore,whenareferencetosuchavariablecausesanetworkpagefault,the
pagehavingthevariableiscopiedtothefaultingnodefromoneofthenodesalreadyhaving
acopyofthepage.Read-onlyvariablesareprotectedbytheMMUhardware,andanattempt
towritetosuchavariablecausesafatalerror.
2. Migratory. Shared variables that are accessed in phases, where each phase
correspondsto a seriesof accessesby a singleprocess,maybeannotatedas migratory
variables.Thelockingmechanism isusedtokeepmigratoryvariablesconsistent.Thatis,
migratoryvariablesareprotectedbylocksynchronization variables and are usedwithin
criticalsections.
Toaccessamigratoryvariable,aprocessfirstacquiresalockforthevariable,usesthe
variableforsometime,andthenreleasesthelockwhenithasfinishedusingit.Atatime,the
systemallowsonlyasingleprocesstoacquirealockforamigratoryvariable.Ifanetwork
pagefaultoccurswhenaprocessattemptstoacquirea lockforamigratoryvariable, the
pageismigratedtothefaultingnodefromthenodethatisitscurrentowner.TheNRMB
strategyisusedinthiscase.Thatis,pagesmigratefromonenodetoanotheronademand
basis, but pages are not replicated. Therefore, only one copy of a page containinga
migratoryvariableexistsinthesystem.
Migratoryvariablesarehandledefficientlybyintegratingtheirmovementwiththatof
thelockassociatedwiththem.Thatis,thelockandthevariablearesenttogetherinasingle
messageto thelocationofthenextprocessthatisgiventhelockforaccessingit.

Sec.5.6 • Consistency Models 261
3. Write-shared. A programmer may use this annotation with a shared variable to
indicatetothe systemthat the variable isupdatedconcurrentlyby multipleprocesses, but
the processes do not update the same parts of the variable. For example, in a matrix,
different processes can concurrently update different row/column elements, with each
process updating only the elements of one row/column. Munin avoids the false sharing
problem of write-shared variables by allowing them to be concurrently updated by
multiple processes.
A write-shared variable isreplicated on all nodes where aprocesssharing is located.
That is, when access to such a variable causes a network page fault to occur, the page
having the variable is copied to the faulting node from one of its current nodes. If the
access is awrite access, the system first makes acopy of the page (called twinpage)and
then updates theoriginalpage.The process may perform several writes tothepage before
releasing it.When the page is released, the system performs a word-by-wordcomparison
of the original page and the twin page and sends the differences to all nodes having a
replica of the page.
When a node receives the differences of a modified page, the system checks if the
local copy of the page wasalso modified. Ifnot, the local copy of the page is updated by
incorporatingthereceiveddifferencesinit.Ontheother hand, ifthelocalcopy ofthepage
was also modified, the local copy, its twin, and the received differences are compared
word by word.Ifthesame wordhasnotbeen modifiedinboth thelocalandremotecopies
of the page, the words of the original local page are updated by using the differences
received from the remote node. On the other hand, if the comparison indicates that the
same word was updatedinboththe local and remote copies of the page, aconflictoccurs,
resulting in a runtime error.
4. Producer-consumer. Shared variables that are written (produced) by only one
process and read (consumed) bya fixed set of other processes may be annotated to be of
producer-consumer type. Munin uses an "eager object movement" mechanism for this
type of variable. In this mechanism, a variable is moved from the producer's node to the
nodesoftheconsumersinadvance ofwhenthey arerequiredsothatnonetwork page fault
occurs when a consumer accesses the variable. Moreover, the write-update protocol is
used to update existing replicas of the variable whenever the producer updates the
variable. If desired, the producer may send several updates togetherby using the locking
mechanism. In this case, the procedure acquires a synchronization lock, makes several
updates on the variable, and then releases the lock when the variable or the updates are
sent to the nodes of consumer processes.
5. ResuLt. Result variables are just the opposite of producer-consumer variables in
the sense that they are written by multiple processes but read by only one process.
Different processeswritetodifferent partsofthe variablethatdonotconflict. The variable
isread only when all itsparts have been written. Forexample, inanapplicationthere may
be different "worker" processes to generate and fill the elements ofeach row/column of
amatrix, andonce the matrix iscomplete,it may beused bya"master"process forfurther
processing.
Munin uses a special write-update protocol for result variables in which updates are
sent only to the node having the master process and not to all replica locations of the

262 Chap. 5 • Distributed Shared Memory
variable (each worker process node has a replica of the variable). Since writes to the
variablebydifferentprocessesdo notconflict,all workerprocessesare allowedtoperform
write operations concurrently. Moreover, since result variables are not read until all parts
have beenwritten,aworkerprocessreleasesthe variableonly when ithas finished writing
all partsthat itis supposedtowrite, whenall updatesare sent togetherinasinglemessage
to the master process node.
6. Reduction. Shared variables that must be atomically modified may be anno
tated to be of reduction type. For example, in a parallel computation application, a
global minimum must be atomically fetched and modified if it is greater than the
local minimum. In Munin, a reduction variable is always modified by being locked
(acquire lock), read, updated, and unlocked (release lock). For better performance, a
reduction variable is stored at a fixed owner that receives updates to the variable
from other processes, synchronizes the updates received from different processes,
performs the updates on the variable, and propagates the updated variable to its
replica locations.
7. Conventional. Shared variables that are not annotated as one ofthe above types
are conventional variables. The already described release consistency protocol ofMunin
is used to maintain the consistency of replicated conventional variables. The write
invalidation protocol is used in this case to ensure that no process ever reads a stale
version of a conventional variable. The page containing a conventional variable is
dynamicallymovedtothe locationofaprocessthat wants toperformawrite operationon
the variable.
Experience with Munin has shown that read-only, migratory, and write-shared annotation
types are very useful because variables ofthese types are frequently used, but producer
consumer,result,and reductionannotationtypes areoflittle usebecausevariablesofthese
types are less frequently used.
5.7 REPLACEMENT STRATEGY
In DSMsystemsthat allow shared-memory blockstobedynamically migrated/replicated,
the following issues must beaddressed when the available space for caching shared data
fills up at a node:
1. Which block should be replaced to make space for a newly required block?
2. Where should the replaced block be placed?
5.7.1 Which Ilockto lleplac.
The problem ofreplacement has been studied extensively for paged main memories and
shared-memory multiprocessor systems. The usual classification of replacement algo
rithms group them into the following categories [Smith 1982]:

Sec.5.7 • Replacement Strategy 263
1. Usage based versus non-usage based. Usage-based algorithms keep track of the
history ofusage ofa cache line (or page) and use this information to make replacement
decisions. That is, the reuse of a cache line normally improves the replacement status of
that line. Least recently used (LRU)is an example ofthis type ofalgorithm. Conversely,
non-usage-basedalgorithmsdo nottake therecord ofuseofcachelines intoaccountwhen
doing replacement. First in, first out (FIFO) and Rand (randomor pseudorandom) belong
to this class.
2. Fixedspace versus variable space. Fixed-spacealgorithms assume that the cache
size is fixed while variable-space algorithms are based on the assumption that the cache
sizecan bechanged dynamically dependingon the need.Therefore, replacementinfixed
space algorithms simply involvesthe selectionofaspecificcacheline. Ontheotherhand,
in a variable-space algorithm, a fetch does not imply a replacement, and a swap-out can
take place without a corresponding fetch.
Variable-space algorithms are not suitable for a DSM system because each node's
memory that acts as cache for the virtualJy shared memory is fixed in size. Moreover, as
compared to non-usage-based algorithms, usage-based algorithms are more suitable for
DSM systems because they allow to take advantage of the data access locality feature.
However, unlike most caching systems, which use a simple LRUpolicy for replacement,
most DSM systemsdifferentiatethe status ofdata items and use apriority mechanism. As
an example, the replacement policy used by the DSM system of IVY[Li 1986, 1988] is
presentedhere. Inthe DSM system ofIVY,each memory block ofanode isclassifiedinto
one of the folJowing five types:
1. Unused. A free memory block that is not currently being used.
2. Nil. A block that has been invalidated.
3. Read-only. A block for which the node has only read access right.
4. Read-owned.Ablock for which the node has only read access right butisalso the
owner of the block.
5. Writable. A block for which the node has write access permission. Obviously, the
node is the owner of the block because IVY uses the write-invalidate protocol.
Based on this classification of blocks, the following replacement priority is used:
1. Both unused and nil blocks have the highest replacement priority. That is, they
will be replaced first if a block is needed. It is obvious for an unused block to have the
highest replacement priority. A nil block also has the same replacement priority because
it is no longeruseful and future access to the block would causeanetwork fault tooccur.
Notice thatanilblockmaybearecentlyreferencedblock, andthisisexactly whyasimple
LRU policy is not adequate.
2. The read-only blocks have the next replacement priority. This is because a copy
of a read-only block is available with its owner, and therefore it is possible to simply
discardthat block. When the node again requires that blockin the future, the block has to
be brought from its owner node at that time.

264 Chap.5 • Distributed SharedMemory
3. Read-ownedand writable blocks for which replica(s) exist on some other node(s)
have the next replacementpriority because it is sufficient topass ownershiptoone of the
replica nodes. The block itselfneed not be sent, resulting in a smaller message.
4. Read-owned and writable blocks for which only this node has a copy have the
lowest replacement priority because replacementof such a block involves transferof the
block'sownershipaswellastheblock from thecurrent nodetosomeothernode.AnLRU
policy isused toselect ablockforreplacementwhenalltheblocks inthelocalcache have
the same priority.
5.7.2 Wiler. to 'lac. a "-placed Block
Once amemory block hasbeen selected forreplacement, itshould beensured thatifthere
is some useful information in the block, it should not be lost. For example, simply
discardingablockhaving unused, nil,orread-only statusdoes not leadtoany lossofdata.
Similarly, discardingaread-owned or a writable block for which replica(s) exist on some
othernode(s) isalso harmless. However, discardinga read-owned or a writable block for
which there isnoreplica onanyother node mayleadtolossofusefuldata.Therefore, care
must be taken to store them somewhere before discarding. The two commonly used
approaches for storing a useful block at the time of its replacement are as follows:
I. Usingsecondarystore.Inthismethod, theblockissimply transferredontoalocal
disk. The advantageofthis method is that itdoes not waste any memory space, and ifthe
node wants toaccess the same block again, itcan get the block locally without aneed for
network access.
2. Using the memory space ofother nodes. Sometimes it may be faster to transfer a
block over the network than to transfer it to a local disk. Therefore, another method for
storing auseful block istokeep track offree memory space atallnodes inthesystem and
to simply transferthe replaced block to the memory of a node with available space. This
method requires each node to maintain a table of free memory space in all other nodes.
This table may be updated byhaving each node piggyback itsmemory status information
during normal traffic.
5.8 THRASHING
Thrashing is said to occur when the system spends a large amount of time transferring
shared data blocks from one node toanother, comparedtothe time spent doing the useful
work of executing application processes. It is a serious performance problem with DSM
systems that allow data blocks tomigrate from one node toanother.Thrashingmayoccur
in the following situations:
1. When interleaved data accesses made byprocesses on two or more nodes causes
adata block tomove back and forth fromone node toanother inquick succession
(a ping-pong effect)

Sec.5.8 • Thrashing 265
2. When blocks with read-only permissions are repeatedly invalidated soon after
they are replicated
Such situations indicate poor (node) locality in references. If not properly handled,
thrashing degrades system performance considerably. Therefore, steps must be taken to
solve this problem.The following methods may be used to solve the thrashing problemin
DSM systems:
I. Providingapplication-controlledlocks. Lockingdata to preventother nodes from
accessing that data for a short period of time can reduce thrashing. An application
controlled lock can be associated with each data block to implement this method.
2. Nailing a block to a node for a minimum amount oftime. Another method to
reduce thrashing is to disallow a block to be taken away from a node until a minimum
amount of time t elapses after its allocation to that node. The time t can either be fixed
statically or be tuned dynamically on the basis of access patterns. For example, Mirage
[Fleisch and Popek 1989J employs this method to reduce thrashing and dynamically
determines the minimum amountoftime for which a block will be available at anode on
the basis of access patterns.
The maindrawbackofthisschemeisthatit isvery difficulttochoosetheappropriate
value for the time t. If the value is fixed statically, it isliable to be inappropriatein many
cases. For example, if a process accesses a block for writing to it only once, other
processes will be prevented from accessing the block until time t elapses. On the other
hand, if a process accesses a block for performing several write operations on it, time t
may elapse before the process has finished using the block and the system may grant
permission to another process for accessing the block. Therefore, tuning the value of t
dynamically is the preferred approach. In this case, the value of t for a block can be
decidedbased on pastaccesspatternsofthe block. The MMU'sreferencebits may beused
for this purpose. Another factor that may be used for deciding the value.oft for a block
is the length of the queue of processes waiting for their turn to access the block.
3. Tailoring the coherence algorithm to the shared-data usage patterns. Thrashing
can also be minimized by using different coherence protocols for shared data having
different characteristics. For example, the coherence protocol used in Munin for write
shared variables avoids the false sharing problem, which ultimately results in the
avoidance of thrashing.
Notice from the description above that complete transparency ofdistributed shared
memory is compromised somewhat while trying to minimize thrashing. This is because
most of the approaches described above require the programmer's assistance. For
example, in the method of application-controlled locks, the use of locks needs to be
directed toward a particular shared-memory algorithm and hence the shared-memory
abstraction can no longer be transparent. Moreover, the application must be aware of the
shared data it is accessing and its shared access patterns. Similarly, Munin requires
programmers to annotate shared variables with standard annotation types, which makes
the shared-memory abstraction nontransparent.

266 Chap.5 • Distributed SharedMemory
5.9 OTHER APPROACHES TODSM
Depending on the manner in which data caching (placement and migration of data) is
managed, there are three main approaches for designing a DSM system:
1. Data caching managed by the operating system
2. Data caching managed by the MMU hardware
3. Data caching managed by the language runtime system
This being a book on operating systems, in this chapter we mainly concentrate on
the first approach. Systems such as IVY [Li 1986] and Mirage [Fleish and Popek
1989] fall in this category. In these systems, each node has its own memory and
access to a word in another node's memory causes a trap to the operating system. The
operating system then fetches and acquires the page containing the accessed word
from the remote node's memory by exchanging messages with that machine. There
fore, in these systems, the placement and migration of data are handled by the
operating system. For completion, the other two approaches of designing a DSM
system are briefly described below.
The second approach is to manage caching by the MMU. This approach is used
in multiprocessors having hardware caches. In these systems, the DSM implementation
is done either entirely or mostly in hardware. For example, if the multiprocessors are
interconnected by a single bus, their caches are kept consistent by snooping on the
bus. In this case, the DSM is implemented entirely in hardware. The DEC Firefly
workstation belongs to this category. On the other hand, if the multiprocessors are
interconnected by switches, directories are normally used in addition to hardware
caching to keep track of which CPUs have which cache blocks. Algorithms used to
keep cached data consistent are stored mainly in MMU microcode. Therefore, in this
case, the DSM is implemented mostly in hardware. Stanford's Dash [Lenoski el al.
1992] and MIT's Alewife [Agarwal et al. 1991, Kranz et al. 1993] systems belong to
this category. Notice that in these systems, when a remote access is detected, a
message is sent to the remote memory by the cache controller or MMU (not by the
operating system software).
The third approach is to manage caching by the language runtime system. In
these systems, the DSM is structured not as a raw linear memory of bytes from 0 to
total size of the combined memory of all machines, but as a collection of program
ming language constructs, which may be shared variables and data structures (in
conventional programming languages) or shared objects (in object-oriented program
ming languages). In these systems, the placement and migration of shared variables/
objects are handled by the language runtime system in cooperation with the operating
system. That is, when a variable/object is accessed by a process, it is the responsibility
of the runtime system and the operating system to successfully perform the requested
access operation on the variable/object independently of its current location. An
advantage of this approach is that programming languages may be provided with
features to allow programmers to specify the usage pattern of shared variables/objects

Sec.5,]0 • Heterogeneous DSM 267
for their applications, and the system can support several consistency protocols and
use the one most suitable for a shared variable/object. Therefore, these systems can
allow consistency protocols to be applied at the granularity of individual data items
and can rely on' weaker consistency models than sequential consistency model for
better concurrency. However, a drawback of this approach is that it imposes extra
burden on programmers. Munin [Bennett et al. 1990] and Midway [Bershad et at
1993] are examples of systems that structure their DSM as a collection of shared
variables and data structures. On the other hand, Orca [Bal et al. 1992] and Linda
[Carriero and Gelernter 1989] are examples of systems that structure their DSM as a
collection of shared objects.
5.10 HETEROGENEOUS DSM
Computers of different architectures normally have different characteristic features. For
example, supercomputersand multiprocessors are goodat compute-intensiveapplications
while personal computers and workstations usually have good user interfaces. A
heterogeneouscomputingenvironmentallowstheapplicationstoexploitthe bestofallthe
characteristic features ofseveral different types ofcomputers. Therefore, heterogeneity is
often desirable in distributed systems,
Heterogeneous DSM is a mechanism that provides the shared-memory paradigm in
a heterogeneous environment and allows memory sharing among machines ofdifferent
architectures. At first glance, sharing memory among machines ofdifferent architectures
seems almost impossible. However, based on the measurements made on their
experimental prototype heterogeneous DSM, called Mermaid, Zhou et al. [1990, 1992]
have concluded that heterogeneous DSM is not only feasible but can also be comparable
in performance to its homogeneous counterpart (at least for some applications). The two
main issues in building a DSM system on a network ofheterogeneous machines are data
conversion and selection ofblock size. These issues are described below. The following
description is based on the material presented in [Zhou et al. 1990,1992J.
5.1O.1 Data Conversion
Machines of different architectures may use different byte orderings and floating-point
representations. When data is transferred from a node ofone type to a node ofanother
type, it must be converted before it is accessed on the destination node. However, the
unit of data transfer in a DSM system is normally a block. Therefore, when a block
is migrated between two nodes of different types, the contents of the block must be
converted. But the conversion itself has to be based on the types of data stored in the
block. It is not possible for the DSM system to convert a block without knowing the
type of application-level data contained in the block and the actual block layout.
Therefore, it becomes necessary to take assistance from application programmers, who
know the layout of the memory being used by their application programs. Two
approaches proposed in the literature for data conversion in a heterogeneous DSM are
described below.

268 Chap.5 • Distributed SharedMemory
Structuring the DSM System as a Collection of Source
Language Objects
In this method, the DSM system isstructuredas acollectionofvariablesor objectsin the
source language so that the unit ofdata migration is an objectinsteadofa block. A DSM
compilerisusedthataddsconversionroutinesfortranslatingvariables/objectsinthesource
languageamongvariousmachinearchitectures.Foreach accesstoashared-memoryobject
from a remote node, a check is made by the DSM systemifthe machineofthe node that
holdstheobjectandtherequestingnodearecompatible.Ifnot,asuitableconversionroutine
isusedtotranslatetheobjectbeforemigratingittothe requestingnode.
This methodofdata conversion isused in theAgorashared-memorysystem[Bisiani
et a1. 1987]. In Agora, each machine marks messages with a machine tag. A message
contains an objectbeing migrated from one node to another. Messages between identical
or compatiblemachinesdo not requiretranslation. When translationisnecessary, the type
ofinformationassociated with the destinationcontextis used to do aone-pass translation
ofthe object. Dependingonthe field tagand the machinerequirementsfordataalignment,
the translation process may involve such operations as field-by-fieJd swapping ofbytes,
inserting/removing gaps where necessary, and shuffling ofbits.
Structuring a DSM system as acollectionofsource language objects may be useful
from the data conversion point of view but is generally not recommended due to
performance reasons. This is because objects in conventional programming languages
typicallyare scalars(single-memory words), arrays, and structures. None ofthese types is
suitableas a sharedentityofa DSM system. For eachsharedentity, accessrightsmustbe
issued for every entity and the migration ofan entity involves communication overhead.
Therefore, the choice ofscalardata types as a unit ofsharing would make a system very
inefficient. On the other hand, arrays may easily be too large to be treated as units of
sharing and data migration. Large data objects lead to false sharing and thrashing, and
their migration often requires fragmentation and reassembly operations to be carried out
on them due to the limited packet size oftransport protocols.
Allowing Only One Type of Data in a Block
This mechanism is used in Mermaid [Zhou et al. 1990, 1992], which uses a page size as
its blocksize. Therefore,apagecan containonly one typeofdata. In Mermaid,the DSM
page (block) table entry keeps additional information identifying the type of data
maintainedin that pageand the amountofdata allocated to the page. Whenevera page is
movedbetweentwo machinesofdifferentarchitectures, a routineconverts the data in the
pageto the appropriateformat. Mermaidrequiresthe applicationprogrammersto provide
the conversion routine for each user-defined data type in the application. Mermaid
designers pointed out that the cost ofconverting a page is small compared to the overall
cost ofpage migration.
Zhou et ale [1990] identified the following limitations ofthis approach:
1. Allowingapagetocontaindataofonly one type may lead to wastageofmemory
due to fragmentation, resulting in increased paging activity.

Sec.5.10 • Heterogeneous DSM 269
2. The mechanism requires that the compilers used on different types of machines
must be compatiblein the sense that inthe compiler-generatedcode the size of each data
type and the order of the fields within compound structures must be the same on each
machine. If incompatible compilers are used for an application to generate code for two
different machines such thatthe sizeoftheapplication-leveldata structures differs for the
two machines, the mapping between pages onthe two machines would notbeone toone.
That is, it may not be possible for a structure to fit on a page in its entirety after the
conversion or,conversely, some data from the following page may beneeded inorder to
complete the conversion of the current page [Zhou et al. 1990]. This complicates the
conversion process.
3. Another problem associated with this method is that entire pages are converted
even though only a small.portion may be accessed before it is transferred away.
Sometimesthis problem may bemore severe inthefirst method, whichconverts anentire
object. Forexample, alargearray mayoccupy several pages ofmemory,and migration of
this object would convert the data inall the occupied pages even when only the first few
array elements may be accessed before the object is transferred away.
4. The mechanism isnotfullytransparent because itrequires theuserstoprovide the
conversion routines foruser-defineddata typesandatablespecifying themapping ofdata
types to conversion routines. The transparency problem may be solved by automatically
generating the conversion routines and the mapping table by using apreprocessoron the
user program [Zhou et al. 1990].
Anotherserious problem associated with the data conversion issue inheterogeneous
DSM systems is that of the accuracy of floating-point values in numerical applications.
Since an application has no control over how often a data is migrated or converted,
numerical accuracy offloating-pointdata maybelostifthedata isconverted several times
and the results may become numerically questionable.
5.10.2 Block Size Salaction
Recall that in a homogeneous DSM system the block size is usually the same size as
a native virtual memory (VM) page, so that the MMU hardware can be used to trigger
a DSM block fault. However, in a heterogeneous environment, the virtual memory
page size may be different for machines of different types. Therefore, block size
selection becomes a complicated task in such a situation. Zhou et al. [1990, 1992]
proposed the use of one of the following algorithms for block size selection in a
heterogeneous DSM system:
1. Largestpage size algorithm. In this method, the DSM block size is taken as the
largest VM page size of all machines. Since VM page sizes are normally powers of 2,
multiple smallerVMpages fitexactly inone DSM block. Ifapagefault occurs on anode
withasmaller page size, itwillreceive ablock ofmultiple.pages that includes thedesired
page. This algorithm suffers from the same false sharing and thrashing problems
associated with large-sized blocks.

270 Chap.5 • Distributed SharedMemory
2. Smallestpage sizealgorithm. In this method, the DSM block size is taken as the
smallestVMpage sizeofall machines. Ifapage fault occurs onanode with alarger page
size, multiple blocks (whose total size is equal to the page size of the faulting node) are
movedtosatisfy the page fault.Althoughthis algorithmreducesdatacontention,itsuffers
from the increased communication and block table management overheads associated
with small-sized blocks.
3. Intermediatepage sizealgorithm.Tobalance between the problems oflarge- and
small-sized blocks, a heterogeneous DSM system may select to choose a block size
somewhere in between the largest VMpage size and the smallest VMpage size of all
machines.
5.11 ADVANTAGES OFDSM
DistributedSharedMemoryisahigh-level mechanismforinterprocesscommunicationin
loosely coupled distributed systems. It is receiving increased attention because of the
advantages it has over the message-passing mechanisms. These advantages are discussed
below.
5.11.1 Simpl.rAbstraction
Bynow it is widely recognized that directly programming loosely coupled distributed
memory machines using message-passing models is tedious and error prone. The main
reason is that the message-passing models force programmers to be conscious of data
movement between processes at all times, since processes must explicitly use
communication primitives and channels or ports. To alleviate this burden, RPC was
introduced to provide a procedure call interface. However, even in RPC, since the
procedurecall isperformedin an address space different from that ofthecaller's address
space, it is difficult for the callerto pass context-related data or complex data structures;
that is, parameters must be passed by value. In the message-passing model, the
programming task is further complicated by the fact that data structures passed between
processes in the form of messages must bepacked and unpacked. The shared-memory
programming paradigm shields the application programmers from many such low-level
concerns.Therefore, theprimary advantageofDSM isthe simplerabstraction itprovides
to the application programmers ofloosely coupled distributed-memory machines.
5.11.1 ktt.r Portability of Distributed Application
Programs
The access protocol used in case of DSM is consistent with the way sequential
applications access data. This allows for a more natural transition from sequential to
distributed applications. In principle, distributed application programs written for a
shared-memory multiprocessor system can be executed on a distributed shared-memory
system without change. Therefore, it is easier to port an existing distributed application

Sec.5.11 • Advantages ofDSM 271
program to adistributed-memory system with DSM facility than to adistributed-memory
system without this facility.
5.11.3 letter Performance of Some Applications
The layerofsoftwarethat provides DSM abstractionisimplementedon topofamessage
passing system and uses the services of the underlying message-passing communication
system.Therefore,inprinciple,theperformanceofapplicationsthatuseDSMisexpectedto
beworse than ifthey usemessage-passingdirectly. However, this isnotalways true, and it
has been found that some applications using DSM can even outperform their message
passing counterparts.This ispossibleforthree reasons [StummandZhou 1990]:
1. Locality ofdata. The computation model of DSM is to make the data more
accessible by moving it around. DSM algorithms normally move data between nodes in
large blocks. Therefore, in those applications that exhibit a reasonable degree oflocality
in their data accesses, communication overhead is amortized over multiple memory
accesses. This ultimately results in reduced overall communication cost for such
applications.
2. On-demanddata movement. The computation model ofDSM also facilitates 00
demand movementofdata asthey are being accessed. On theotherhand, there are several
distributed applications that execute inphases, where each computation phase ispreceded
by a data-exchange phase. The time needed for the data-exchange phase is often dictated
bythe throughputofexistingcommunicationbottlenecks.Therefore, insuch applications,
the on-demand data movement facility provided by DSM eliminates the data-exchange
phase, spreads the communication load over a longer period of time, and allows for a
greater degree ofconcurrency.
3. Largermemoryspace. With DSM facility, the total memory size isthe sum ofthe
memory sizes ofall the nodes inthe system, Thus, paging and swapping activities, which
involve disk access, arc greatly reduced.
5.11.4 Flexible Communication Environment
The message-passing paradigm requires recipient identification and coexistence of the
sender and receiver processes. That is, the sender process of a piece of data must
know the names of its receiver processes (except in multicast communication), and the
receivers of the data must exist at the time the data is sent and in a state that they can
(or eventually can) receive the data. Otherwise, the data is undeliverable. In contrast,
the shared-memory paradigm of DSM provides a more flexible communication
environment in which the sender process need not specify the identity of the receiver
processes of the data. It simply places the data in the shared memory and the receivers
access it directly from the shared memory. Therefore, the coexistence of the sender
and receiver processes is also not necessary in the shared-memory paradigm. In fact,
the lifetime of the shared data is independent of the lifetime of any of its receiver
processes.

272 Chap. 5 • Distributed Shared Memory
5.11.5 Eas. of 'roc.ss Migration
Migration of a process from one node to another in a distributed system (described in
Chapter 8) has been shown to be tedious and time consuming due to the requirement of
transferring the migrant process's address space from its old node to its new node.
However, thecomputationmodelofDSMprovides thefacilityofon-demand migration of
databetween processors.Thisfacilityallowsthemigrantprocesstoleaveitsaddressspace
on its old node at the time of migration and fetch the required pages from its new node
on demand at the·time of accessing. Hence in a distributed system with DSM facility,
process migration isassimpleasdetaching theprocesscontrolblock(PCB)ofthemigrant
process from the processor of theold node and attaching ittotheready queue of the new
node's processor. A PCB is a data block or a record associated with each process that
containsuseful information such as process state,CPU registers, scheduling information,
memory management information, I/O status information, and so on. This approach
provides a very natural and efficient form of process migration between processors in a
distributed system.
5.11 SUMMARY
Programming of applications for loosely coupled distributed-memory machines with the
message-passing paradigm is a difficult and error-prone task. The Distributed Shared
Memory (DSM) facility simplifies this programming task by providing a higher level
abstractionthatallowsprogrammers towriteprograms withtheshared-memoryparadigm,
which is consistent with the way sequential applications access data.
Important issues involved in the design and implementation of a DSM system are
granularity ofdata sharing, structure oftheshared-memory space, memory coherence and
access synchronization, data location and access, replacement strategy, handling of
thrashing, and heterogeneity.
Granularity refers to block size, which is the unit of data sharing and data transfer
across the network. Both large- and small-sized blocks have their own advantages and
limitations. Several DSM systems choose the virtual memory page size as block size so
that the MMU hardware can be used to trigger a DSM block fault.
Thestructure oftheshared-memory spaceofaDSMsystemdefines theabstract view
to be presented to application programmers of that system. The three commonly used
methods for structuring the shared-memory space of a DSM system are no structuring,
structuring by data type, and structuring as adatabase. The structure and granularity of a
DSM system are closely related.
Memory coherence isanimportantissueinthedesignofaDSMsystem.Itdealswith
the consistency of a data block lying in the main memories of two or more nodes of the
system. Several consistency models have been proposed in the literature to handle this
issue. The main ones described in this chapter are strict consistency, sequential
consistency, causal consistency, PRAM consistency, processor consistency, weak
consistency, and release consistency. Of these, sequential consistency and release
consistency are appropriate for a large number of applications.

Sec.5.12 • Summary 273
ProtocolsforimplementingthesequentialconsistencymodelinaDSMsystemdepend
onthe following four replicationand migrationstrategiesusedbythe DSM system:
1. Nonreplicated, nonmigrating blocks (NRNMBs)
2. Nonreplicated, migrating blocks (NRMBs)
3. Replicated, migrating blocks (RMBs)
4. Replicated, nonmigrating blocks (RNMBs)
Protocols for implementing sequential consistency for each ofthese strategies have
been described in this chapter. The data-locating mechanisms suitable for each case have
also been described. In addition, the Munin system has been presented as an example of
a release consistent DSM system.
Replacement strategy deals with the selection of a block to be replaced when the
availablespace forcachingshared data fills upandtheplacementofareplacedblock. The
usual classifications of replacement algorithms are usage-based versus non-usage-based
and fixed-space versus variable-space. DSM systems use fixed-space, usage-based
replacement algorithms. At the time of its replacement, a useful block may be placed in
either the secondary storage of the local node or the memory space of some other
node.
Thrashing is a serious performance problem with DSM systems that allow data
blocks to migrate from one node to another. Methods that may be used to solve the
thrashing problem in DSM systems are providing application-controlled locks, nailing a
block to a node for a minimum amount of time, and tailoring the coherence algorithm to
the shared-data usage patterns.
Depending on the manner in which data caching is managed, there are three main
approaches to designing a DSM system: data caching managed by the operating system,
data caching managedby the MMU hardware, anddata caching managedbythe language
runtime system.
Heterogeneous DSM is a mechanism that provides the shared-memory paradigm
in a heterogeneous environment and allows memory sharing among machines with
different architectures. The two main issues in building this facility are data conversion
and selection of block size. Two approaches proposed in the literature for data
conversion in a heterogenous DSM are structuring the DSM system as a collection of
source language objects and allowing only one type of data in a DSM block. Three
algorithms that may be used for block size selection in a heterogeneous DSM system
are largest page size algorithm, smallest page size algorithm, and intermediate page size
algorithm.
Research has shown that DSM systems are viable, and they have severaladvantages
overthe message-passing systems. However, they are still far from mature. Most existing
DSM systems are very small experimental or prototype systems consisting ofonly a few
nodes. The performance results to date are also preliminary and based on a small group
of applications or a synthetic workload. Nevertheless, research has proved that DSM
effectively supports parallel processing and promises to be a fruitful and exciting area of
research for the coming decade.

274 Chap. 5 • Distributed Shared Memory
EXERCISES
5.1. The distributed shared-memory abstraction is implemented by using the services of the
underlyingmessage-passingcommunicationsystem. Therefore.inprinciple,theperformance
of applications that use DSM is expected to beworse than if they use message passing
directly. Inspite ofthis fact, why dosome distributedoperatingsystem designerssupportthe
DSM abstractionintheirsystems?Arethereanyapplicationsthatcanhavebetterperformance
inasystem with DSM facility than inasystem thathasonly message-passingfacility? Ifyes,
give the types of such applications. If no, explain why.
5.2. Discuss the relative advantages and disadvantages of using large block size and small block
size inthedesign ofablock-basedDSM system. Whydo most DSM system designersprefer
to use the typical page size used in a conventional virtual-memory implementation as the
block size of the DSM system?
5.3. It is often said that the structure of the shared-memory space and the granularity of data
sharing in a DSM system are closely related. Explain why.
5.4. What isfalse sharing?When isitlikely tooccur?Canthisproblem leadtoanyotherproblem
in a DSM system? Give reasons for your answer.
5.5. What should bedone tominimizethefalse sharing problem?Can thisproblembecompletely
eliminated? What other problems may occur if one tries to completely eliminate the false
sharing problem?
5.6. Discussthe relative advantagesand disadvantagesof usingthe NRNMB. NRMB, RMB, and
RNMB strategies in the design of a DSM system.
5.7. Discuss the relative advantages and disadvantages of the various data-locating mechanisms
that may beused in a DSM system that uses the NRMB strategy.
5.8. AsequentiallyconsistentDSM system usestheRMBstrategy.Itemploysthewrite-invalidate
approachforupdatingdata blocks and thecentralized-serveralgorithmforlocating theowner
ofablock and keeping track ofthe nodes thatcurrentlyhave avalidcopy oftheblock. Write
pseudocode for the implementation of the memory coherence schemeofthis DSM system.
5.9. Why isaglobal sequencerneeded inasequentiallyconsistentDSM system thatemploysthe
write-update protocol?
5.10. Most DSM systems in which caching is managed by the operating system use the write
invalidate scheme for consistency instead of the write-update scheme. Explain why.
5.11. Differentiate between weak consistency and release consistency. Which of the two will you
preferto use in the design of a-DSMsystem?Give reasons for your answer.
5.12. A programmer is writing an application for a release-consistent DSM system. However the
applicationneeds,sequentialconsistencytoproducecorrect results. What precautionsmustthe
programmertake?
S.13. Differentiate between PRAM consistency and processorconsistency.
5.14. Give the relative advantages and disadvantages of sequential and release consistency
models.
5.15. What iscausal consistency?Give anexampleofan applicationfor which causal consistency
is the most suitableconsistency model.
5.16. Proposea suitablereplacementalgorithmfor a DSM system whose shared-memory space is
structured as objects. One of the goals in this case may be to minimize memory
fragmentation.
5.17. Why does the simple LRU policy often used for replacing cache lines in a buffer cache not
work well as a replacementpolicy for replacing blocks in a DSM system?

Chap. 5 • Bibliography 275
5.18. Tohandle the issue of wheretoplace a replaced block,the DSMsystem of Memnet [Delp
1988]usestheconcept of"home memory," inwhicheachblockhasahome memory.When
replacement of ablock requires that the block betransferred to some other node's memory,
theblock istransferred tothenodewhose memoryisthehomememory fortheblock.What
aretheadvantagesanddisadvantagesofthisapproachascomparedtotheonepresentedinthis
chapter?
5.19. What are the main causes of thrashing in a DSM system? What are the commonly used
methods to solve the trashing problem in aDSM system?
5.20. Complete transparency ofa DSMsystem iscompromised somewhat whena methodis used
tominimizethrashing.Therefore, thedesignerofaDSMsystemisoftheopinionthatinstead
of using a method to minimize thrashing, a method should be used by which the system
automatically detects andresolyesthis problem. Proposea methodbywhichthesystemcan
detect thrashing and a method to resolve itonce ithas beendetected.
5.21. A distributed system has DSM facility. The process-scheduling mechanism of this system
selectsanother process torun whenafaultoccursfor thecurrently runningprocess,andthe
CPUisutilizedwhiletheblockisbeingfetched.Twosystemengineersarguingabouthowto
better utilize the CPUs of this system have the followingopinions:
(a) The firstone says that if a large number of processes are scheduled for execution at a
node, theavailable memory space ofthe nodecan bedistributed amongthese processes
sothatalmostalwaystherewillbeareadyprocesstorunwhenapagefaultoccurs.Thus,
CPU utilization can be kept high.
(b) The second one says that ifonly a few processesarescheduled forexecution atanode,
theavailablememoryspaceofthenodecanbeallocatedtoeachofthefewprocesses,and
each process will produce fewer pagefaults.Thus,CPU utilizationcan be kept high.
Whose argument iscorrect? Give reasons foryour answer.
5.22. Whatare the three mainapproaches fordesigning a DSM system?
5.23. What are some of the issues involved in building a DSM system on a network of
heterogeneous machines? Suggest suitable methods for handling these issues.
5.24. Are DSM systems suitable for both LAN and WAN environments? Give reasons for your
answer.
5.25. Suggest some programming practices that will reduce network block faults in a DSM
system.
5.26. Writepseudocode descriptions for handling a block fault in each of the following types of
DSM systems:
(a) A DSM system that usesthe NRNMBstrategy
(b) A DSM system that uses the NRMB strategy
(c) A DSM system that uses the RMB strategy
(d) ADSM system that uses the RNMB strategy
Youcan makeany assumptions that you feel necessary,but statethe assumptions made.
BIBLIOGRAPHY
[AdveandHill 1990]Adve,S.,andHill,M; "WeakOrdering:ANewDefinition,"In:Proceedings
ofthe 17th International Symposium on Computer Architecture, Association for Computing
Machinery, NewYork,NY,pp.2-14 (1990).

276 Chap. 5 • Distributed Shared Memory
[Agarwaletal.1991]Agarwal,A.,Chaiken,D.,D'Souza,G.,Johnson, K.,Kranz,D.,Kubiatowicz,
J.,Kurihara,K.,Lim, B.,Maa, G., Nussbaum,D.,Parkin, M.,andYeung,D.,"The MITAlewife
Machine:ALarge-ScaleDistributedMemoryMultiprocessor,"In:Proceedingsofthe Workshop
on Scalable Shared Memory Multiprocessors, Kluwer Academic, Norwell, MA (1991).
[Bal 1991] Bal, H. E., Programming Distributed Systems, Prentice-Hall, London, England
(1991).
[8aletal.1992]Bal,H.E.,Kaashoek,M.F.,andTanenbaum,A.S.,"Orca: ALanguageforParallel
ProgrammingofDistributedSystems:'IEEETransactionsonSoftwareEngineering, Vol.SE-18,
pp. 190-205 (1992).
[Baldoniet al. 1995] Baldoni, R., Mostefaoui, A., and Raynal, M., "Efficient Causally Ordered
Communications for Multimedia Real Time Applications," In: Proceedings of the 4th
International Symposium on High Performance Distributed Computing, IEEE, New York
(1995).
[Bennett et al, 1990J Bennett, J., Carter, J., and Zwaenepoel, W., "Munin: Distributed Shared
Memory BasedonType-SpecificMemory Coherence," In: Proceedingsofthe 1990Conference
on Principles and Practice ofParallel Programming, Association for Computing Machinery,
New York,pp. 168-176(1990).
[Bershad et al, 1993]Bershad, B. N., Zekauskas, M. 1, and Sawdon, W. A., "The Midway
Distributed Shared Memory System," In: Proceedings ofthe IEEE COMPCON Conference,
IEEE, New York,pp. 528-537 (1993).
[Bisiani and Ravishankar 1990J Bisiani, R., and Ravishankar, M., "Plus: A Distributed Shared
Memory System," In: Proceedings of the 17th International Symposium on Computer
Architecture, Association for Computing Machinery, New York,NY,pp. 115-124 (1990).
[Bisianiet al. 1987] Bisiani, R., Alleva, F.,Correrini, F., Forin, A., Lecouat, F., and Lerner, R.,
"Heterogeneous Parallel Processing: The Agora Shared Memory," Technical Report No. CMU
CS-87-112, ComputerScience Department, Carnegie-Mellon University (March 1987).
[Bisianietal.1989] Bisiani,R.,Nowatzyk,A.,andRavishankar,M.,"CoherentSharedMemory on
a Distributed Memory Machine," In: Proceedings ofthe International Conference on Parallel
Processing, IEEE, New York, pp. 133-141 (August 1989).
[Campine et al. 1990] Campine, G. A., Geer, Jr., D. E, and Ruh, W. N., "Project Athena as a
Distributed ComputerSystem," IEEE Computer,Vol.23, pp. 40-51 (1990).
[Carrieroand Gelernter 1989] Carriero, N., and Gelernter, D., "LindainContext," Communica
tions oftheACM, Vol.32, No.4, pp. 444-458 (1989).
[Carrieroetal, 1986] Carriero, N.,Gelernter, D., and Leichter, 1,"DistributedData Structures in
Linda," In: Proceedings ofthe ACM Symposium on Principles ofProgramming Languages,
Association for Computing Machinery, New York(1986).
[Carter et al, 1991]Carter, 1. B., Bennett, 1 K., and Zwaenepoel, W., "Implementation and
Performance of Munin," In: Proceedings of the 13th Symposium on Operating Systems
Principles, Association for Computing Machinery, New York,NY,pp. 152-164 (1991).
[Carteret al. 1994]Carter, J.B., Bennett, 1 K., and Zwaenepoel, W.,"Techniques for Reducing
Consistency-Related Communication in Distributed Shared Memory Systems," ACM Transac
tions on Computer Systems, Vol.12(1994).
[Chaseetal. 1989] Chase, J. S.,Amador, F.G., Lazowska, E. D., Levy, H.M., and Littlefield, R.
J., "The Amber System: Parallel Programming on a Network of Multiprocessors," In:
Proceedingsofthe12thSymposiumonOperatingSystemsPrinciples,AssociationforComputing
Machinery, New York, NY,pp. 147-158 (1989).

Chap.5 • Bibliography 277
[Cheong and Veidenbaum 1988] Cheong, H., and Veidenbaum, A. V., "A Cache Coherence
Scheme with Fast Selective Invalidation," In: Proceedings ofthe 15thInternationalSymposium
on ComputerArchitecture,Association for Computing Machinery, NewYork,NY,pp.299-307
(1988).
[Cheriton1986]Cheriton,D.R.,"Problem-OrientedSharedMemory:ADecentralizedApproach to
DistributedSystemDesign," In:Proceedings ofthe6th InternationalConference onDistributed
Computing Systems, IEEE, New York,pp. 190-197 (May 1986).
[Cheriton et al. 1991] Cheriton, D. R., Goosen, H.A., and Boyle, P.D., "Paradigm: A Highly
Scalable Shared-Memory Multicomputer Architecture," IEEE Computer, Vol. 24, No.2, pp.
33-46(1991).
[Cox andFowler1989JCox,A.L.,andFowler,R.1.,"The ImplementationofaCoherentMemory
Abstraction onaNUMA Multiprocessor: Experiences withPLATINUM,"In:Proceedings ofthe
12thSymposium on Operating Systems Principles, Association for ComputingMachinery, New
York,NY,pp. 32-34 (December 1989).
[Dasguptaet al. 1991] Dasgupta, P.,LeBlanc, R.L,Ahmad, Jr.,M.,and Ramachandran, U.~"The
Clouds Distributed Operating System," IEEEComputer, Vol.24, No. 11,pp. 34-44 (1991).
[Delp 1988] Delp, G. S.•"The Architecture and Implementation of MemNet: An Experiment on
High-Speed Memory Mapped Network Interface," Ph.D. Dissertation, DepartmentofComputer
Science, University of Delaware (1988).
[Delpetal, 1991]Delp,G.S.,Farber,D.J.,Minnich, R.G.,Smith,1.M.,andTam,M.C.,"Memory
as a Network Abstraction," IEEE-'Network, Vol.5, pp.34-41 (1991).
[Duboiset al, 1986] Dubois, M., Scheurich, C., and Briggs, F.A., "Memory Access Buffering in
Multiprocessors," In: Proceedings ofthe J3th Annual Symposium on Computer Architecture,
Association for Computing Machinery, New York,NY,pp. 343-442 (1986).
[Duboisetal, 1988]Dubois, M.,Scheurich, C., andBriggs,F.A.,"Synchronization,Coherence,and
Event Ordering in Multiprocessors," IEEE Computer, Vol.21, No.2, pp. 9-21 (1988).
[Fekete et al. 1995]Fekete, A, Kaashoek, F.,and Lynch, N.~ "Providing Sequentially-Consistent
Shared Objects Using Group and Point-to-Point Communication," In: Proceedings ofthe J5th
International Conference on Distributed Computing Systems, IEEE, New York (May-June
1995).
[Fleisch 1987] Fleisch, B. D., "Distributed Shared Memory in a Loosely Coupled Distributed
System," In: Proceedings ofthe 1987ACMSIGCOMM Workshop,Association for Computing
Machinery, New York,NY (1987).
[FleischandPopek1989]Fleisch, B.D.,andPopek,G.J.,"Mirage: ACoherent Distributed Shared
Memory Design,"In:Proceedings ofthe 12thACMSymposium onOperating System Principles,
Association for Computing Machinery, New York,NY,pp. 211-223 (December 1989).
[Forinet al. 1989] Forin, A.,Barrera, L, Young,M.,and Rashid, R.,"Design, Implementation,and
Performance Evaluation of a Distributed Shared Memory Server for Mach," In: Proceedingsof
the 1989 Winter Usenix Conference (January 1989).
[Frank 1984] Frank, S. J., "Tightly Coupled Multiprocessor System Speeds Memory-Access
Times," Electronics, pp. 164-169 (January 1984).
[Garcia Molina and Wiederhold 1982] Garcia-Molina, H., and Wiederhold, G., "Read-Only
Transactions inaDistributed Database,"ACMTransactions onDatabase Systems, Vol.7, No.2,
Association for Computing Machinery, New York,NY,pp. 209-234 (1982).

278 Chap. 5 • Distributed Shared Memory
[Gharachorlooetal.1990] Gharachorloo, K.,Lenoski, D.,Laudon, J.,Gibbons, P, Gupta, A.,and
Hennessy, J., "Memory Consistency and Event Ordering in Scalable Shared-Memory Multi
processors," In: Proceedings of the 17th Annual Symposium on Computer Architecture,
Association for Computing Machinery, New York,NY,pp. 15-26(1990).
[Gharachorloo et al. 1991] Gharachorloo, K., Gupta, A., and Hennessy, J., "Performance
Evaluation of Memory Consistency Models for Shared-Memory Multiprocessors," In: Proceed
ingsofthe4th International ConferenceonArchitectural Supportfor Programming Languages
and Operating Systems, IEEE Computer Society Press, Los Alamitos, CA, pp. 245-257
(1991).
[Ghose 1995] Ghose, K., "SNOW: Hardware Supported Distributed Shared Memory over a
Network of Workstations," In: Proceedings ofthe 24th Annual International Conference on
Parallel Processing, IEEE, New York(August 1995).
[Goodman 1983] Goodman, J. R., "UsingCache Memory to Reduce Processor-MemoryTraffic,"
In: Proceedings of the 10th Annual Symposium on Computer Architecture, Association for
Computing Machinery, New York,NY,pp. 124-131 (June 1983).
[Goodman 1989] Goodman, 1. R., "Cache Consistency and Sequential Consistency," Technical
Report No. 61, IEEE ScalableCoherent Interface Working Group, IEEE, New York(1989).
[Goodmanetal,1989]Goodman,1.R.,Vernon,M.K.,andWoest,P.1.,"EfficientSynchronization
Primitives for Large-Scale Cache-Coherent Multiprocessors," In: Proceedings of the 3rd
International ConferenceonArchitectural Supportfor ProgrammingLanguages and Operating
Systems, IEEE ComputerSociety Press, Los Alamitos, CA, pp. 64-73 (1989).
[Hartyand Cheriton1992] Harty,K.,andCheriton,D.,"Application-ControlledPhysical Memory
Using External Page-Cache Management," In: Proceedingsofthe5th International Conference
onArchitectural Supportfor Programming Languages and Operating Systems, Association for
Computing Machinery, New York,NY,pp. 187-]99 (1992).
[HuttoandAhamad 1990]Hutto, P.W.,andAhamad,M.,"Slow Memory: Weakening Consistency
to Enhance Concurrency in Distributed Shared Memories," In: Proceedings of the 10th
International Conference on Distributed Computing Systems, IEEE, New York, pp. 302-311
(1990).
[Johnson et al, 1995] Johnson, D., Lilja, D., and Riedl, J., "A Circulating Active Barrier
Synchronization Mechanism," In: Proceedings ofthe 24thAnnual International Conference on
Parallel Processing, IEEE, New York(August ]995).
[Katz et al, 1985]Katz, R. H., Eggers, S. 1., Wood, D. A., Perkins, C. L., and Sheldon, R. G.,
"Implementing aCache Consistency Protocol," In: Proceedingsofthe 12thAnnualSymposium
on ComputerArchitecture,Association for Computing Machinery, New York,NY,pp. 276-283
(June 1985).
[Keleheretal, 1992] Keleher, P.,Cox,A.L.,andZwaenepoel,W.,"Lazy Release Consistency,"In:
Proceedings ofthe 19th International Symposium on Computer Architecture, Association for
Computing Machinery, New York,NY,pp. ]3-21 (1992).
[Kessler and Livny 1989] Kessler, R. E., and Livny, M., "An Analysis of Distributed Shared
Memory Algorithms," In: Proceedings of the 9th International Conference on Distributed
Computing Systems, IEEE, New York,pp. 98-104 (June 1989).
[Kranz et al, 1993] Kranz, D., Johnson, K., Agarwal, A., Kubiatowicz, J. J., and Lim, B.,
"IntegratingMessage Passing and Shared Memory: Early Experiences," In: Proceedings ofthe
4thSymposiumonPrinciplesandPracticeofParallelProgramming,AssociationforComputing
Machinery, New York,NY,pp. 54-63 (May 1993).

Chap.5 • Bibliography 279
[Lamport1979]Lamport, L., "How to Make aMultiprocessorComputerThatCorrectly Executes
Multiprocess Programs," IEEE Transactions on Computers, Vol. C-28, IEEE, New York, pp.
690-69] (1979).
[Lenoski and Weber 1995) Lenoski, D. E., and Weber, W. D., Scalable Shared-Memory
Multiprocessing, Morgan Kaufmann, San Francisco, CA (1995).
[Lenoski et al, 1992] Lenoski, D., Laudon, J., Gharachorloo, K., Weber, W. D., Gupta, A.,
Hennessy, J., Horowitz, M., and Lam, M, S., "The Stanford Dash Multiprocessor," IEEE
Computer, Vol.25, No.3, pp. 63-79 (1992).
(Lenoski et al, 1993J Lenoski, D., Laudon, 1., Joe, T., Nakahira, D., Steves, L., Gupta, A., and
Hennessy, J., "TheDASH Prototype: Logic Overhead and Performance," IEEE Transactions on
Parallel and DistributedSystems, Vol.4, No.1, pp. 41-61 (1993).
[Li 1986] Li, K., "Shared Virtual Memory on Loosely Coupled Multiprocessors," Ph.D.
Dissertation,Technical Report No. YALE/DCS/RR-492, DepartmentofComputerScience,Yale
University (September 1986).
[Li1988]Li, K.,"IVY: ASharedVirtual MemorySystemfor ParallelComputing,"In:Proceedings
ofthe International Conference on Parallel Processing. IEEE, New York, pp. 94-101 (August
1988).
[Li and Hudak 1989] Li, K., and Hudak. P., "Memory Coherence in Shared Virtual Memory
Systems,"ACM Transactions on Computing Systems, Vol.7, No.4, pp. 321-359 (1989).
[Li and Schaefer19891Li, K.,and Schaefer, R.,"A Hypercube Shared Virtual Memory System,"
In: Proceedings ofthe International Conference on Parallel Processing, Pennsylvania State
University Press, pp. 125-132 (]989).
(Lilja1993JLilja,D.1.,"CacheCoherenceinLarge-ScaleShared-MemoryMultiprocessors: Issues
and Comparisons,"AC~MComputing Surveys, Vol. 25,pp. 303-338 (1993).
[Lipton and Sandberg 1988] Lipton, R. 1., and Sandberg, J. S., "Pram: A Scalable Shared
Memory,"Technical Report No. CS-TR-]80-88, Princeton University (1988).
[Liskov 19881Liskov, B.,"Distributed Programming inArgus," Communications oftheACM, Vol.
31, No.3, pp. 300-313 (J988).
[MinnichandFarber1989]Minnich,R.G.,and Farber, D.J.,"The MetherSystem:ADistributed
Shared Memory for SunOS 4.0," In: Proceedings ofthe /989 Summer Usenix Conference, pp.
5]-60 (1989).
[Minnich and Farber 19901Minnich, R. (]., and Farber, D.1., "Reducing Host Load, Network
Load, and Latency in a Distributed Shared Memory," In: Proceedings ofthe 10thInternational
Conference on DistributedComputing Systems, IEEE, New York (June)990).
[Mosberger1993]Mosberger,D., "MemoryConsistencyModels,"TechnicalReportNo.TR93/11,
Department ofComputer Science, University ofArizona (1993).
[Nitzberg and Virginia Lo 1991]Nitzberg, N.,and Virginia Lo, "Distributed Shared Memory: A
Survey of Issues and Algorithms," IEEE Computer, Vol.24,No. 11,pp. 52-60 (1991).
[Oguchi et al, 1995] Oguchi, M., Aida, H., and Saito, T., "A Proposal for a DSMArchitecture
Suitable for a Widely Distributed Environment and Its Evaluation," In: Proceedings ofthe 4th
International Symposium on High Performance Distributed Computing, IEEE, New York
(August 1995).
[RamachandranandKhalidi1989JRamachandran.U.,and Khalidi,M.Y.A.,HAnImplementation
ofDistributed Shared Memory," First Workshop on Experiences with Building Distributedand
Multiprocessor Systems, Usenix Association, Berkeley, CA, pp. 21-38 (1989).

280 Chap. 5 • Distributed Shared Memory
[Sane et al, 1990] Sane, A., MacGregor, K., and Campbell, R., "Distributed Virtual Memory
Consistency Protocols: Design and Performance," Second IEEE Workshop on Experimental
Distributed Systems, IEEE, New York, pp. 91-96 (October 1990).
[Scheurich and Dubois 1988] Scheurich, C., and Dubois, M., "Dynamic Page Migration in
Multiprocessors with Distributed Global Memory," In: Proceedings ofthe 8th International
Conference on Distributed Computing Systems, IEEEComputer Society Press, Los Alamitos,
CA, pp. 162-169(June 1988).
[Shrivastavaetal.1991]Shrivastava,S.,Dixon, G.N.,andParrington,G.D.,"An Overviewofthe
Arjuna Distributed Programming-System," IEEE Software, pp. 66-73 (January 1991).
[SinghalandShivaratri1994]Singhal,M.,andShivaratri,N.G.,AdvancedConceptsinOperating
Systems, McGraw Hill, New York (1994).
[Sinha et al. 1991] Sinha, P.K., Ashihara, H., Shimizu, K., and Maekawa, M., "Flexible User
Definable Memory Coherence Scheme in Distributed Shared Memory of GALAXY," In:
Proceedings of the 2nd European Distributed Memory Computing Conference (EDMCC2),
Springer-Verlag, New York, pp. 52-61 (April 1991).
[Smith 1982] Smith, A. J., "Cache Memories," ACM Computing Surveys, Vol. 14, No.3, pp.
437-530, New York, NY (1982).
[StummandZhou1990]Stumm,M.,andZhou,S.,"AlgorithmsImplementingDistributedShared
Memory," IEEE Computer,Vol.23, No.5, New York, NY,pp. 54-64 (1990).
[18m andUsu 1990]Tam, V.,and Hsu,M.,"FastRecovery inDistributedSharedVirtual Memory
Systems," In: Proceedings of the 10th International Conference on Distributed Computing
Systems, IEEE, New York,pp.38-45 (May-June 1990).
(18m etal, 1990]Tam, M.C.,Smith,J. M.,andFarber,D.J., "ATaxonomy-BasedComparisonof
Several Distributed Shared Memory Systems," Operating Systems Review, Vol.24, pp.40-67
(1990).
[Tartalja and Milutinovic 1996]Tartalja, I., and Milutinovic, V.(Eds.), The Cache Coherence
ProbleminShared-MemoryMultiprocessors: SoftwareSolutions, IEEE ComputerSocietyPress,
Los Alamitos, CA (1996).
[Theel and Fleisch 1995] Theel, O. E., and Fleisch, B. D., "Design and Analysis of Highly
Available and Scalable Coherence Protocols for Distributed Shared Memory Systems Using
StochasticModeling,"In: Proceedingsofthe24thAnnual International Conferenceon Parallel
Processing, IEEE,New York(August 1995).
[Wu and Fuchs 1989] Wu, K. L., and Fuchs, W. K., "Recoverable Distributed Shared Virtual
Memory: MemoryCoherenceand StorageStructures,"In:Proceedingsofthe19thInternational
Symposium onFault-Tolerant Computing, pp. 520-527 (June 1989).
[Yenetal,1985]Yen,D.W. L.,Yen,W.C.,and Fu, K.,"DataCoherenceProbleminaMulticache
System," IEEE Transactions on Computers, Vol. C-34, No.1, pp. 56-65, New York, NY
(1985).
[Zhou et al. 1990] Zhou, S., Stumm, M., and McInerney, T., "Extending Distributed Shared
MemorytoHeterogeneousEnvironments,"In:Proceedingsofthe10thInternationalConference
on DistributedComputing Systems, IEEE, New York,pp. 30-37 (May-June 1990).
[Zhou et al. 1992] Zhou, S., Stumm, M., Li, K., and Wortman, D., "Heterogeneous Distributed
SharedMemory," IEEE Transactionson Paralleland Distributed Systems, Vol. 13,No.5, New
York, NY,pp. 540-554 (1992).

Chap. 5 • Pointers toBibliographies ontheInternet 281
POINTERS TO 818UOGRAPHIES ON THE INTERNET
Bibliography containing references on DistributedShared Memory can be found at:
ftp:ftp.cs.umanitoba.calpublbibliographieslParallelJdsm.html
Bibliography containing references on Distributed Memory Systems can be found at:
ftp:ftp.cs.umanitoba.calpublbibliographies/ParalleUdistmem.htrnl
Bibliography containing references on Cache Memories and Related Topics can be
found at:
ftp:ftp.cs.umanitoba.calpublbibliographieslMisc/cache.html
Bibliography containing references on Single Address Space Operating Systems (SASOS)
andRelated Topics can be found at:
ftp:ftp.cs.umanitoba.calpublbibliographies/Os/sasos.html

6
CHAPTER
SynchronizQtion
6.1 INTRODUOION
A distributed system consists of a collection of distinct processes that are spatially
separated and run concurrently. In systems with multiple concurrent processes, it is
economical toshare thesystem resources (hardwareor software) among theconcurrently
executingprocesses.Insuch asituation, sharing may becooperativeor competitive.That
is,sincethenumberofavailable resources inacomputingsystemisrestricted,oneprocess
must necessarily influence the action of other concurrently executing processes as it
competes for resources. Forexample, for a resource (such as a tape drive) that cannot be
usedsimultaneouslybymultiple processes, aprocess willing touseitmust waitifanother
process is using it. At times, concurrent processes must cooperate either to achieve the
desired performance of the computing system or due to the nature of the computation
beingperformed.Typicalexamples ofprocess cooperationinvolve twoprocesses thatbear
a producer-consumer or client-server relationship to each other. For instance, a client
process and a file server process must cooperate when performing fileaccess operations.
Both cooperative and competitive sharing require adherence to certain rules of behavior
that guarantee that correct interaction occurs.The rules for enforcing correct interaction
are implemented in the form of synchronization mechanisms. This chapter presents
282

Sec.6.2 • ClockSynchronization 283
synchronization mechanisms that are suitable for distributed systems. In particular, the
following synchronization-related issues are described:
• Clock synchronization
• Event ordering
• Mutual exclusion
• Deadlock
• Election algorithms
6.2 ClOCK SYNCHRONIZATION
Every computer needs a timer mechanism (called a computer clock) to keep track of
current time and also for various accounting purposes such as calculating the time spent
by a process in CPU utilization, disk I/(), and so on, so that the corresponding user can
be charged properly. In a distributed system, an application may have processes that
concurrently run on rnultiple nodes of the system. For correct results, several such
distributed applications require that the clocks of the nodes are synchronized with each
other. For example, for a distributed on-line reservation system to be fair, the only
remaining seat booked almost simultaneously from two different nodes should be offered
totheclient whobooked first,even ifthetime differencebetweenthetwobookingsisvery
small. Itmay not be possibleto guarantee this iftheclocks of the nodes ofthe system are
notsynchronized. Inadistributed system, synchronizedclocks alsoenableone tomeasure
thedurationofdistributedactivities that start on one node and terminateonanothernode,
for instance, calculating the time taken to transmita message from one node to anotherat
any arbitrary time. It is difficult to get the correct result in this case if the clocks of the
sender and receiver nodes are not synchronized. There are several other applications of
synchronized clocks in distributed systems. Some good examples of such applications
may be found in [Liskov 1993].
The discussion above shows that it is the job of a distributed operating system
designer to devise and use suitable algorithms for properly synchronizing the clocks of a
distributed system. This section presents adescription of such algorithms. However, fora
better understanding ofthese algorithms, we will first discuss how computer clocks are
implemented and what arc the main issues in synchronizing the clocks of a distributed
system.
6.2.1 How Computer Clocks AreImplemented
Acomputerclock usually consistsofthree components-aquartz crystal that oscillatesat
a well-definedfrequency, acounterregister, and aconstantregister. The constantregister
is used to store a constant value that is decided based on the frequency of oscillation of
the quartz crystal. The counter register is used to keep track of the oscillations of the
quartz crystal. That is, the value in the counter register is decremented by 1 for each
oscillation ofthe quartz crystal. When the value ofthe counterregister becomes zero, an

284 Chap. 6 • Synchronization
interrupt is generated and its value is reinitialized to the value in the constant register.
Each interrupt is called a clock tick.
To make the computerclock function as an ordinaryclock used by us in our day-to
day life, the following things are done:
1. The value in the constant register is chosen so that 60 clock ticks occur in a
second.
2. Thecomputerclockis synchronized with realtime (external clock). Forthis, two
more values are stored in the system-a fixed starting date and time and the
numberofticks. For example, in UNIX, time begins at 0000on January 1, 1970.
Atthe time ofinitial booting,the systemasks theoperatortoenterthe currentdate
and time. The system converts the entered value to the numberofticks after the
fixed starting date and time. At every clock tick, the interrupt service routine
increments the value ofthe number ofticks to keep the clock running.
6.2.1 Drifting orClocks
Aclockalwaysruns ataconstantrate becauseitsquartzcrystaloscillatesatawell-defined
frequency. However, due to differences in the crystals, the rates at which two clocks run
are normally different from each other. The difference in the oscillation period between
two clocks might be extremely small, but the difference accumulated over many
oscillationsleads toanobservabledifferencein thetimesofthetwo clocks,no matterhow
accurately they were initialized to the same value. Therefore, with the passage of time, a
computer clock drifts from the real-time clock that was used for its initial setting. For
clocks based on aquartz crystal, the drift rate is approximately 10-6 , giving a difference
of 1 second every 1,000,000 seconds, or 11.6 days [Coulouris et al. 1994]. Hence a
computer clock must be periodically resynchronized with the real-time clock to keep it
nonfaulty. Even nonfaulty clocks do not always maintain perfect time. A clock is
considerednonfaultyifthereisaboundonthe amountofdriftfrom real time forany given
finite time interval.
More precisely, let us suppose that when the real time is t, the time value of aclock
p is Cp(t). Ifall clocks in the world were perfectly synchronized, we would have Cp(t)=
tfor allp and all t.That is, if Cdenotes the time value ofaclock, in the ideal case dCldt
should be 1. Therefore, if the maximum drift rate allowable is p,a clock is said to be
nonfaulty if the following condition holds for it:
dC
l-p~-~l+p
dt
As shown in Figure 6.1, after synchronization with a perfect clock, slow and fast
clocks drift in oppositedirections from the perfectclock. This is becausefor slow clocks
dCldt < 1and for fast clocks dCldt > 1.
A distributed system consists ofseveral nodes, each with its own clock, running at
its own speed. Because of the nonzero drift rates of all clocks, the set of clocks of a
distributedsystemdo notremainwell synchronizedwithoutsomeperiodicresynchroniza-

Sec.6.2 • Clock Synchronization 285
Perfectclock
dC - 1
Fastclockregion dt -
Q) dC>1
E dt
.~
.x:
o
o
[5 Slowclockregion
dC<1
dt
Fig. 6.1 Slow,perfect, andfastclocks. Realtime
tion. This means that the nodes of a distributed system must periodically resynchronize
their local clocks to maintain a global time base across the entire system. Recall from
Figure 6.1 that slow and fast clocks drift in opposite directions from the perfect clock.
Therefore, of two clocks, if one is slow and one is fast, at a time at after they were
synchronized, the maximum deviation between the time value of the two clocks will be
2pat. Hence, to guarantee that no two clocks in a set of clocks ever differ by more than
0,theclocks inthesetmust beresynchronizedperiodically, withthetimeinterval between
two synchronizations being less than or equal to 0/2p.Therefore, unlike a centralized
system inwhich only thecomputerclock has tobesynchronizedwith thereal-time clock,
a distributed system requires the following types of clock synchronization:
1. Synchronization ofthe computer clocks with real-time (or external) clocks. This
type of synchronization is mainly required for real-time applications. That is, external
clock synchronization allows the system to exchange information about the timing of
events with other systems and users.
Anexternal time source that isoften used as areference for synchronizingcomputer
clocks with real time is the Coordinated Universal Time (UTC). The UTC is an
international standard. Many standard bodies disseminate UTe signals by radio,
telephone, and satellite. For instance, the WWV radio station in the United States and the
Geostationary Operational Environmental Satellites (GEOS) are two such standard
bodies. Commercial devices (called timeproviders) are available to receive and interpret
these signals. Computers equipped with time provider devices can synchronize their
clocks with these timing signals.
2. Mutual(orinternal) synchronization oftheclocksofdifferentnodesofthesystem.
This type of synchronization is mainly required for those applications that require a
consistent view of time across all nodes of a distributed system as well as for the
measurement of the duration of distributed activities that terminate on a node different
from the one on which they start.
Note that externally synchronizedclocks are also internally synchronized. However,
the converse is not true because with the passage of time internally synchronized clocks
may drift arbitrarily far from external time.

286 Chap.6 • Synchronization
6.1.3 Clock SynchronlzQtlon Issues
We have seen that no two clocks can be perfectly synchronized. Therefore, in
practice, two clocks are said to be synchronized at a particular instance of time if the
difference in time values of the two clocks is less than some specified constant 8.
The difference in time values of two clocks is called clock skew. Therefore, a set of
clocks are said to be synchronized if the clock skew of any two clocks in this set is
less than 8.
Clock synchronization requires each node to read the other nodes' clock values.
The actual mechanism used by a node to read other clocks differs from one
algorithm to another. However, regardless of the actual reading mechanism, a node
can obtain only an approximate view of its clock skew with respect to other nodes'
clocks in the system. Errors occur mainly because of unpredictable communication
delays during message passing used to deliver a clock signal or a clock message
from one node to another. A minimum value of the unpredictable communication
delays between two nodes can be computed by counting the time needed to prepare,
transmit, and receive an empty message in the absence of transmission errors and
any other system load. However, in general, it is rather impossible to calculate the
upper bound of this value because it depends on the amount of communication and
computation going on in parallel in the system, on the possibility that transmission
errors will cause messages to be transmitted several times, and on other random
events, such as page faults, process switches, or the establishment of new commu
nication routes.
An important issue in clock synchronization is that time must never run
backward because this could cause serious problems, such as the repetition of certain
operations that may be hazardous in certain cases. Notice that during synchronization
a fast clock has to be slowed down. However, if the time of a fast clock is
readjusted to the actual time all at once, it may lead to running the time backward
for that clock. Therefore, clock synchronization algorithms are normally designed to
gradually introduce such a change in the fast running clock instead of readjusting it
to the correct time all at once. One way to do this is to make the interrupt routine
more intelligent. When an intelligent interrupt routine is instructed by the clock
synchronization algorithm to slow down its clock, it readjusts the amount of time to
be added to the clock time for each interrupt. For example, suppose that if 8msec is
added to the clock time on each interrupt in the normal situation, when slowing
down, the interrupt routine only adds 7msec on each interrupt until the correction
has been made. Although not necessary, for smooth readjustment, the intelligent
interrupt routine may also advance its clock forward, if it is found to be slow, by
adding 9msec on each interrupt, instead of readjusting it to the correct time all at
once.
6.1.4 Clock SynchronlzQtlon Algorithms
Clock synchronization algorithms may be broadly classified as centralized and
distributed.

Sec.6.2 • ClockSynchronization 287
Centralized Algorithms
In centralized clock synchronization algorithms one node has a real-time receiver. This
node isusually calledthe time servernode, and theclock time ofthis node is regarded as
correctand used as the reference time. The goal of the algorithm is to keep the clocksof
all other nodes synchronized with the clock time of the time server node. Depending on
the role ofthetime server node, centralizedclocksynchronizationalgorithmsare again of
two types-passive time server and active time server.
Passive Time Server Centralized Algorithm. In this method, each node
periodically(with the interval between two periods being lessthanorequal to8/2p)sends
a message ("time=?") to the time server. When the time server receives the message, it
quickly responds with a message ("time=T"), where Tis the current time in the clock of
=
the time server node. Let us assume that when the client node sends the "time ?"
message, itsclock time is To,and when itreceives the "time=T" message, its clocktime
is T). Since To and T) are measured using the same clock, in the absence of any other
information, the best estimate of the time required for the propagation of the message
"time=T" from the time server node to the client's node is (T 1- To)/2.Therefore, when
the reply is received at the client's node, its clock is readjusted to'T+(T 1- To)/2.
Since there may beunpredictable variation inthe message propagationtime between
two nodes, (T)- To)/2 is not a very good estimate of the time to be added to T for
calculating the currenttime of the client's node clock. Several proposalshave been made
to improve this estimated value. Two such methods are described below. The first one
assumes the availability of some additional information and the second one assumes that
no additional information is available:
1. In this method, it is assumed that the approximate time taken by the time server
=
to handle the interrupt and process a "time ?" request message is known. Let this time
be equal to 1.Then a better estimate of the time taken for propagation of the message
"time=J'" from the tirne server node to the client's node would be (T)- To-/)/2.
Therefore, in this method, when the reply is received at the client's node, its clock is
readjusted to T+(T)- To- I)/2.
2. This method was proposed by Cristian [1989]. In this method, several
measurements of T)- To are made, and those measurements for which T)- Toexceeds
some threshold value are considered to be unreliable and discarded. The average ofthe
remaining measurementsisthen calculated, and halfofthecalculatedvalue isused asthe
value to be added to T. Alternatively, the measurement for which the value of T 1- Tois
minimum is considered to be the 1110staccurate one, and halfofthis value is used as the
value to be added to T. One limitation of this approach is the need to restrict the number
of measurements forestimatingthe value to beadded toT,since these aredirectly related
to the message traffic generated and the overhead imposed by the algorithm.
Active Time Server Centralized Algorithm. In the passive time server
approach, thetimeserveronlyresponds torequests fortime fromother nodes.Ontheother
hand, in the active time server approach, the time server periodically broadcasts its clock
time ("time=T"). Theother nodes receive thebroadcastmessage andusetheclock timein

288 Chap. 6 • Synchronization
the message for correcting their own clocks. Each node has a priori knowledge of the
approximatetime(To)requiredforthepropagationofthemessage"time=T"fromthetime
severnodetoitsownnode.Therefore, whenthebroadcastmessageisreceivedatanode,the
node'sclockisreadjustedtothetimeT+To.Amajordrawbackofthismethodisthatitisnot
fault tolerant. If the broadcast message reaches too late at a node due to some
communicationfault,theclockofthatnodewillbereadjustedtoanincorrectvalue.Another
drawback of this approach is that it requires broadcast facility to besupported by the
network.
Another active time server algorithm that overcomes the drawbacks of the above
algorithmistheBerkeleyalgorithm.ItwasproposedbyGusellaandZatti[1989]forinternal
synchronization ofclocks of a group of computers running the Berkeley UNIX. In this
algorithm,thetimeserverperiodicallysendsamessage ("time=?")toallthecomputersin
thegroup. Onreceiving thismessage, eachcomputersendsbackitsclock valuetothetime
server. The time server has a priori knowledge of the approximate time required for the
propagationofamessagefromeachnodetoitsownnode.Basedonthisknowledge,itfirst
readjuststheclock valuesofthereplymessages.Itthentakesefault-tolerantaverageofthe
clock valuesofallthecomputers(including itsown).Totakethefault-tolerantaverage, the
timeserverchooses asubsetofallclock valuesthatdonotdifferfromoneanotherbymore
than a specifiedamount, and theaverage is taken only for theclock values in this subset.
Thisapproacheliminatesreadings fromunreliableclocks whoseclock valuescould havea
significantadverse effectifanordinary average wastaken.
The calculated average is the current time to which all the clocks should be
readjusted. The time server readjusts its own clock to this value. However, instead of
sending thecalculatedcurrenttime backtotheother computers, the time server sends the
amount by which each individual computer's clock requires adjustment. This can be a
positive or a negative value and iscalculatedbasedon the knowledge the time server has
about the approximate time required for thepropagation of a message from each node to
its own node.
Centralized clock synchronization algorithms suffer from two major drawbacks:
I. They are subject to single-point failure. If the time server node fails, the clock
synchronization operation cannot be performed. This makes the system unreli
able. Ideally, a distributed system should be more reliable than its individual
nodes. If one goes down, the rest should continue to function correctly.
2. From a scalability point of view it is generally not acceptable to get all the time
requests serviced by a single time server. In a large system, such a solution puts
a heavy burden on that one process.
Distributed algorithms overcome these drawbacks.
Distributed Algorithms
Recall that externally synchronized clocks are also internally synchronized. That is, if
each node's clock is independently synchronized with real time, all the clocks of the
system remain mutually synchronized. Therefore, asimple method forclock synchroniza-

Sec.6.2 • ClockSynchronization 289
tion may betoequip each nodeofthesystem withareal-time receiversothateach node's
clock can be independently synchronized with real time. Multiple real-time clocks (one
for each node) are normally used for this purpose.
Theoretically, internal synchronization of clocks is not required in this approach.
However, in practice, due to the inherent inaccuracy of real-time clocks, different real
time clocks produce different time. Therefore, internal synchronization is normally
performed for better accuracy. One of the following two approaches is usually used for
internal synchronization in this case.
GlobalAveragingDistributedAlgorithms. In thisapproach, theclock process
at each node broadcasts its local clock time in the form of a special "resync" message
when its local time equals To+iRfor some integer i, where To is a fixed time inthe past
agreed upon byall nodes andR isasystem parameterthat depends on such factors asthe
total number of nodes in the system, the maximum allowable drift rate, and so on. That
is, a resync message is broadcast from each node at the beginning of every fixed-length
resynchronization interval. However, since the clocks of different nodes run at slightly
different rates, these broadcasts will not happen simultaneously from all nodes.
After broadcasting the clock value, the clock process of a node waits for time T,
where Tisaparametertobedetermined bythealgorithm. During this waiting period, the
clock process collects the resync messages broadcast by other nodes. For each resync
message, theclock processrecords thetime,according toitsownclock, whenthemessage
wasrcceived.Attheend ofthe waiting period, theclock process estimates the skew ofits
clock withrespect toeach oftheother nodeson thebasisofthe times atwhich itreceived
resync messages. Itthencomputes afault-tolerantaverage oftheestimated skewsanduses
it to correct the local clock before the start of the next resynchronization interval.
The global averaging algorithms differ mainly in the manner in which the fault
tolerant average ofthe estimated skews iscalculated. Twocommonly used algorithms are
described here:
1. The simplestalgorithm istotake theaverage of theestimatedskews and use itas
the correction for the local clock. However, to limit the impact of faulty clocks on the
average value, the estimated skew with respect to each node is compared against a
threshold, and skews greater than the threshold are set to zero before computing the
average of the estimated skews.
2. In another algorithm, each node limits the impact of faulty clocks by first
discarding the m highest and mlowest estimated skews and then calculating the average
oftheremaining skews, which isthen used asthecorrectionforthelocalclock.The value
of m is usually decided based on the total number of clocks (nodes).
Localized Averaging Distributed Algorithms. The global averaging algo
rithms donotscale wellbecause theyrequire thenetwork tosupport broadcast facility and
also becauseofthelargeamount ofmessage trafficgenerated. Therefore, theyaresuitable
forsmall networks, especially forthose thathave fullyconnectedtopology (in whicheach
node has a direct communication link to every other node). The localized averaging
algorithms attempt to overcome these drawbacks of the global averaging algorithms. In

290 Chap. 6 • Synchronization
this approach, the nodes of a distributed system are logically arranged in some kind of
pattern, such as aring oragrid. Periodically, each node exchanges itsclock time with its
neighbors in the ring, grid, or other structure and then sets its clock time to the average
ofits own clock time and the clock times of its neighbors.
6.2.5 (ase Study: Distributed nme Service
Twopopularservices forsynchronizingclocks and forproviding timing informationover
awide variety ofinterconnectednetworks are theDistributed TimeService (DTS) and the
Network Time Protocol (NTP). DTS is a component of DCE (Distributed Computing
Environment)thatisusedtosynchronizeclocks ofanetwork ofcomputersrunning DCE,
and NTP isused intheInternet forclock synchronization. DTSisbriefly describedbelow
as a case study of clock synchronization. Details of NTP can be found in [Mills 1991].
InaDeEsystem, each node isconfigured aseither aDTS clientoraDTS server. On
each DTS client node runs adaemon process called aDTS clerk. Tosynchronizeits local
clock, each DTS clerk makes requests to the DTS servers on the same LAN for timing
information. The DTS servers provide timing information to DTS clerks or to other DTS
servers upon request. Tomake them publicly known, each DTS server exports itsname to
a LAN profile.
DTS does not define time as a single value. Instead, time isexpressed as an interval
containing the correcttime. By using intervals instead of values, DTS provides the users
with a clear idea ofhow far offthe clock might be from reference time.
A DTS clerk synchronizes its local clock in the following manner. It keeps track of
thedrift rate ofitslocal clock, and whenitdiscovers that the timeerror of thelocal clock
has exceeded the allowable limit, it initiates resynchronization by doing an RPC with all
the DTS servers on its LAN requesting for the time. Each DTS server that receives this
message returns areply containinga time interval based on the server's own clock. From
the received replies, the DTS clerk computes its new value of time in the following
manner(see Fig. 6.2 foranexample). Atfirst, time intervals thatdo not intersect with the
majority oftimeintervals areconsideredtobefaultyanddiscarded. Forinstance, inFigure
6.2, the value supplied by DTS server 3isdiscarded. Then the largest intersectionfalling
within the remaining intervals is computed. The DTS clerk then resets its clock value to
the midpoint of this interval. However, instead ofresetting the clock to the calculated
value all at once, an intelligent interrupt routine is used to gradually introduce such a
change in the clock time.
Note thatduetotheuseofthemethodofintersection forcomputingnewclock value,
itisrecommendedthateach LANinaDCEsystemshould have atleastthree DTSservers
to provide time information.
In addition to DTS clerks synchronizing their clocks with DTS servers, the DTS
servers ofa LAN also communicate among themselves periodically to keep their clocks
mutually synchronized. They also use the algorithm of Figure 6.2 to compute the new
clock value.
So far we have seen how clocks of nodes belonging to the same LAN are
synchronized. However, a DCE system.may have several interconnected LANs. In this
case, aneedarises tosynchronizetheclocksofallnodesinthenetwork. Forthis,oneDTS

Sec.6.2 • ClockSynchronization 291
Timeintervals -----.~ Time
suppliedby
DTSserver1
DTSserver2
DTSserver3
Discarded
interval
DTSserver4
Largestintersectionfalling
withintheremainingintervalsI
I
Midpointofthisinterval
isthenewclockvalue
Fig.6.2 ComputationofnewclockvalueinDTSfromobtainedtimeintervals.
server of each LAN is designated.a global server. Although not necessary for external
synchronization, it is recommended that each global server be equipped with a time
provider device to receive UTe signals. The global servers of all LANs communicate
among themselves periodically to keep their clocks mutually synchronized. Since the
global server of a LAN is also a DTS server, its clock value is automatically used to
synchronize the clocks of other nodes in the LAN. Inthis manner, DTS synchronizes the
clocks of all nodes in the network.
DTS is transparent to DeE users in the sense that users cannot access DTS directly.
However, DTS application programming interface (API) provides a rich set of library
procedures to allow DTS applications to perform time-related activities to control their
executions. In particular, there are library procedures to get the current time, to convert
between binary andASCII representationsoftime,tomanipulate binary timeinformation,
to compare two times, to perform arithmetic operations on times, to get time zone
information, and soon. Inaddition, there isanadministrativeinterface toDTSthat allows
a system administrator to perform administrative operations such as configuring and
dynamically reconfiguring the number of DTS clients and DTS servers on a LAN,

292 Chap. 6 • Synchronization
changing a DTS server into a global server when the global server of a LAN fails, and
setting the maximum inaccuracy and error tolerance to decide how frequently
resynchronization should take place.
6.3 EVENT ORDERING
Keeping the clocks in a distributed system synchronized to within 5 or 10msec is an
expensiveandnontrivial task.Lamport [1978]observed thatformostapplications itisnot
necessary to keeptheclocks in a distributed system synchronized. Rather, it is sufficient
toensure that all events that occur in adistributed system be totally ordered in a manner
that is consistent with an observed behavior.
For partial ordering of events, Lamport defined a new relation called happened
before and introduced the concept of logical clocks for ordering of events based on the
happened-before relation. He then gave a distributed algorithm extending his idea of
partial ordering toaconsistent total ordering ofalltheevents inadistributed system. His
idea is presented below.
6.3.1 HOPfMn.d-kfor. Ralotlon
The happened-before relation (denoted by ~) on a set of events satisfies the following
conditions:
1. If a and b are events in the same process and a occurs before b, then a ~ b.
2. If a is the event of sending a message by one process and b is the event of the
receipt ofthesamemessage byanotherprocess, thena ~b.This condition holds
by the law of causality because a receiver cannot receive a message until the
sender sends it, and the time taken to propagate a message from its sender to its
receiver is always positive.
3. Ifa~bandb~c,thena~c.Thatis,happened-before isatransitiverelation.
Notice that inaphysically meaningful system, anevent cannot happen before itself,
that is, a ~ a is not true for any event a. This implies that happened-before is an
irreflexive partial ordering on the set of all events in the system.
Intermsofthehappened-beforerelation,twoeventsaandbaresaidtobeconcurrent
if they are not related by the happened-before relation. That is, neither a ~ b nor b --7 a
istrue.This ispossible ifthetwoeventsoccurindifferentprocesses thatdonotexchange
messages either directly or indirectly via other processes. Notice that this definition of
concurrency simply means that nothing can be said about whenthe twoevents happened
or which one happened first. That is, two events are concurrent if neither can causally
affecttheother.Duetothisreason, thehappened-before relation issometimes alsoknown
as the relation of causal ordering.
A space-timediagram (such as the one shown in Fig. 6.3) isoften used toillustrate
theconcepts ofthe happened-beforerelation andconcurrent events. In thisdiagram, each

Sec.6.3 • Event Ordering 293
ProcessP1 Process P2 ProcessP
3
Fig.6.3 Space-time diagram forthreeprocesses.
vertical line denotes a process, each dot on a vertical line denotes an event in the
corresponding process,and each wavy line denotes a message transfer fromone process
to another in the direction of the arrow.
From this space-time diagram it iseasy to seethat fortwo events a andb,a ~ b is
trueifandonly ifthereexists apath fromatobbymovingforwardintime along process
and message lines in the direction of the arrows. For example, some of the events of
Figure 6.3 that are related by the happened-before relation are
elO ~ ell e20 ~ e24 ell ~ e23 e2l ~ el3
e30 ~ e24 (since e30 -) e22 and e22 ~ e24)
ell ~ e32 (since eII ~ e23' e23 ~ e24' and e24 ~ e32)
On the other hand, two events a and b are concurrent if and only if no path exists
either from a to b or frOITI b to a. For example, some of theconcurrent events ofFigure
6.3 are
6.3.2 logical Clocks Concept
To determine that an event a happened before an event b, either a common clock or
a set of perfectly synchronized clocks is needed. Wehave seen that neither of these is
available in a distributed system. Therefore, in a distributed system the happened-before
relation must be defined without the use of globally synchronized physical clocks.

294 Chap. 6 • Synchronization
Lamport [1978] provided a solution for this problem by introducing the concept of
logical clocks.
The logical clocks conceptis a way toassociate a timestamp(which may be simply
a number independent ofany clock time) with each system event so that events that are
related to each other by the happened-before relation (directly or indirectly) can be
properly ordered in that sequence. Under this concept, each process P;has a clock C;
associated with itthat assigns a numberC,{a) toany event ainthat process. The clock of
each processiscalledalogical clockbecausenoassumption ismade about the relation of
the numbers C;(a) to physical time. In fact, the logical clocks may beimplemented by
counters with no actual timing mechanism. With each process having its own clock, the
entire system ofclocks isrepresented by the function C,which assigns toany event bthe
number C(b), where C(b)=Cj(b) if b is an event in process Pj.
The logical clocks ofa system can be considered to be correct if the events of the
system that are related to each other by the happened-before relation can be properly
orderedusing theseclocks. Therefore,thetimestampsassignedtotheevents bythesystem
of logical clocks must satisfy the following clock condition:
For any two events a and b. if a ~ b. then C(a) <C(b).
Note that wecannotexpect the converse condition to hold as well, since that would
imply thatanytwoconcurrentevents mustoccuratthesame time, whichisnotnecessarily
true for all concurrent events.
6.3.3 Impl.m.ntatlonof logical Clocks
From the definition ofthe happened-before relation, it follows that the clock condition
mentioned above is satisfied if the following conditions hold:
Cl: Ifaand bare two events within the same process Pi and aoccurs before b,then
Ci(a) < Cj(b).
C2:Ifaisthesending ofamessagebyprocess Piandbisthereceipt ofthat message
by process Pj, then C;(a) < Cj(b).
Inadditiontotheseconditions, which arenecessarytosatisfy theclock condition,the
following condition is necessary for the correct functioning ofthe system:
C3: A clock C, associated with a process Pi must always go forward, never
backward. That is, corrections to time of a logical clock must always be made by
adding a positive value to the clock, never by subtracting value.
Obviously, any algorithm used for implementinga set of logical clocks must satisfy
all these three conditions. The algorithm proposed by Lamport is given below.
To meet conditions CI, C2, and C3, Lamport's algorithm uses the following
implementation rules:

Sec. 6.3 • Event Ordering 295
IRl: Each process Piincrements C,between any two successive events.
IR2:Ifevent ais the sending of a message mbyprocess Pi' the messagemcontains
=
atimestampT Cia),anduponreceivingthemessage maprocessP setsC,greater
m j
than or equal to its present value but greater than Tm:
Rule IR1ensuresthatconditionC1issatisfiedandruleIR2ensures thatconditionC2
issatisfied. Both IR1and IR2ensure that conditionC3 isalso satisfied. Hence the simple
implementation rules IR1and IR2 guarantee a correct system oflogical clocks.
The implementation of logical clocks can best be illustrated with an example. How
a system of logical clocks can be implemented either by using counters with no actual
timing mechanism or by using physical clocks is shown below.
Implementation of Logical Clocks by Using Counters
As shown in Figure 6.4, two processes PI and P2 each have a counter C I and C 2,
respectively. The counters act as logical clocks. At the beginning, the counters are
initialized to zero and a process increments its counterby 1wheneveran event occurs in
that process. If the event is sending of a message (e.g., events e04 and e14), the process
includes the incremented value of the counter in the message. On the other hand, if the
event isreceiving of a message (e.g., events e)3and e~g), instead of simply incrementing
the counterby 1,a check is made to see if the incremented counter value is less than or
equal to the timestamp in the received message. If so, the counter value is correctedand
C1=8 eoa
C1=7 eo7
C1=6 Bos
e13 C2=-35
since 3islessthan
C1=5 Bos
Q) timestamp4
E
t=
C1=4 904
C1=3 e03
C1=2 Bo2
C1=1 eo1
C1=O --"'--------.........- C2=0
Process Pt Process P2
Fig. 6.4 Example illustrating theimplementationof logicalclocks by using counters.

296 Chap.6 • Synchronization
setto 1plusthetimestamp inthe received message (e.g.,inevent eJ3). Ifnot,thecounter
value is left as it is (e.g., in event e08).
Implementation of Logical Clocks by UsingPhysical
Clocks
The implementation of the example of Figure 6.4 by using physical clocks instead of
counters isshown inFigure6.5. In thiscase, each processhasaphysicalclock associated
withit.Eachclockrunsataconstantrate. However,theratesatwhichdifferentclocks run
aredifferent. Forinstance, intheexample ofFigure 6.5,whentheclockofprocess PI has
ticked 10times, the clock of process P2 has ticked only 8 times.
Physicalclocktimesif Physicalclocktimes
nocorrectionsweremade aftercorrections(ifany)
e08 101
110 88 93
907 100 80 85 614
806 90 72 77
90s 80 64 69
CD 70 56 61 913
E
i=
Bo4 60 ------------- 48
..
50 -------------- 40
....
803 40 -------------- 32 812
802 --3-0 -------------- 24
20 -------------- 16 ----911
eo1
10 -------------- 8
0 0
ProcessP1 ProcessP2
Fig.6.5 Example illustrating theimplementationoflogical clocks byusingphysical
clocks.
To satisfy condition CI, the only requirement is that the physical clock of a
process must tick at least once between any two events in that process. This is usually
not a problem because a computer clock is normally designed to click several times
between two events that happen in quick succession. To satisfy condition C2, for a

Sec.6.4 • Mutual Exclusion 297
message-sending event (e.g., events e04 and eI4), the process sending the message
includes its current physical time in the message. And for a message-receiving event
(e.g., events el3 and e~g), a check is made to see if the current time in the receiver's
clock is less than or equal to the time included in the message. If so, the receiver's
physical clock is corrected by fast forwarding its clock to be 1 more than the time
included in the message (e.g., in event e13). If not, the receiver's clock is left as it is
(e.g., in event e~g).
6.3.4 Total Or.rlng of Ev.nts
We have seen how a system of clocks satisfying the clock condition can be used to
order the events of a system based on the happened-before relationship among the
events. We simply need to order the events by the times at which they occur.
However, recall that the happened-before relation is only a partial ordering on the set
of all events in the system. With this event-ordering scheme, it is possible that two
events a and b that are not related by the happened-before relation (either directly or
indirectly) may have the same timestamps associated with them. For instance, if
events a and b happen respectively in processes PI and P2' when the clocks of both
processes show exactly the same time (say 100), both events will have a timestamp of
100. In this situation, nothing can be said about the order of the two events.
Therefore, for total ordering on the set of all system events, an additional requirement
is desirable: No two events ever occur at exactly the same time. To fulfill this
requirement, Lamport proposed the use of any arbitrary total ordering of the processes.
For example, process identity numbers may be used to break ties and to create a total
ordering of events. For instance, in the situation described above, the timestamps
associated with events a and b will be 100.001 and 100.002, respectively, where the
process identity numbers of processes PI and.P2are 001 and 002, respectively. Using
this method, we now have a way to assign a unique timestamp to each event in a
distributed system to provide a total ordering of all events in the system.
6.4 MUTUAL EXCLUSION
There are several resources in a system that must not be usedsimultaneously by multiple
processes if program operation is to be correct. For example, a file must not be
simultaneously updated by multiple processes. Similarly, useof unit record peripherals
such as tape drives or printers must be restricted to a single process at atime. Therefore,
exclusive access to such a. shared resource by a process must be ensured. This
exclusiveness of access is called mutual exclusion between processes. The sections of a
programthat need exclusiveaccess toshared resources are referredtoascritical sections.
For mutual exclusion, means are introduced to prevent processes from executing
concurrently within their associated critical sections.
An algorithm for implementing mutual exclusion must satisfy the following
requirements:

298 Chap.6 • Synchronization
1. Mutual exclusion. Given a shared resource accessed by multiple concurrent
processes, at any time only one process should access the resource. That is, a
process thathas been granted theresource mustrelease itbefore itcan begranted
to another process.
2. No starvation. Ifevery process that is granted the resource eventually releases it,
every request must beeventually granted.
In single-processor systems, mutual exclusion is implemented using semaphores,
monitors, and similar constructs. The three basicapproaches used bydifferent algorithms
forimplementing mutual exclusion indistributed systems are described below.Interested
readers who wanttoexplore further onthistopic mayrefer to[Agarwal andAbbadi 1991,
BulgannawarandVaidya1995,Rayna11991,Sanders 1987,Suzukiand Kasami 1985].To
simplifyourdescription,weassume thateachprocessresides atadifferent node.
6.4.1 C.ntrallzM Approach
In this approach, one of the processes in the system is elected as the coordinator
(algorithms for electing a coordinatorare described later in this chapter) and coordinates
the entry to the critical sections. Each process that wants to enter a critical section must
first seek permission from the coordinator. If noother process iscurrently in that critical
section, the coordinator can immediately grant permission to the requesting process.
However, if two or more processes concurrently ask for permission to enter the same
critical section, the coordinator grants permission to only one process at a time in
accordance with some scheduling algorithm. After executing a critical section, when a
process exits thecritical section, itmustnotify thecoordinatorsothat thecoordinatorcan
grantpermissiontoanotherprocess (ifany)thathasalso asked forpermissiontoenter the
same critical section.
An algorithm for mutual exclusion that uses the centralized approach is described
here with the help of an example. As shown in Figure 6.6, let us suppose that there is a
coordinator process (Pc:) and three other processes PI' P 2, and P 3 in the system. Also
assume that the'requests are granted in the first-come, first-served order for which the
coordinator maintains a request queue. Suppose PI wants to enter a critical section for
which it sends a request message to Pc.On receiving the request message, Pcchecks to
seewhether someotherprocess iscurrently inthatcritical section. Since noother process
is in the critical section, P;immediately sends back a replymessage granting permission
to Pl. When the reply arrives, PI enters the critical section.
Now suppose that while PI is in the critical section P2 asks for permission to enter
the same critical section by sending a request message to Pc.Since PI is already in the
critical section, P2 cannot be granted permission. The exact method used to deny
permissionvaries fromone algorithm toanother.Forouralgorithm, letusassume thatthe
coordinator does not return any reply and the process that made the request remains
blockeduntilitreceives thereplyfromthecoordinator.Therefore, P,doesnotsendareply
to P2immediately and enters its request in the request queue.
Again suppose that while PI is still in the critical section P3 also sends a request
message to P; asking for permission to enter the same critical section. Obviously, P
3

Sec.6.4 • Mutual Exclusion 299
Qen)
«S
Q)
Q)
a:
e
I
Initialstatus
L.....-
~ --.l~
Statusafter@
__
~
Statusafter @
~ ~~ St~usafterQD
____---'1
Statusafter<V
Fig.6.6 Example illustrating the centralized Statusof
approach for mutual exclusion. requestqueue
cannot begranted permission, so no reply is sent immediately to P3 byPC' and its request
is queued in the request queue.
Now supposePI exitsthe critical sectionand sendsareleasemessagetoP; releasing
its exclusive access to the critical section. On receiving the release message, P; takes the
first request from the queue of deferred requests and sends a reply message to the
corresponding process, granting it permission to enter the critical section. Therefore, in
this case, P; sends a reply message to Pz.
On receiving the reply message, P2enters the critical section, and when it exits the
critical section, it sends arelease message to Pc.AgainP;takes the first request from the
requestqueue (in this case request ofP 3) and sends a reply messagetothe corresponding
process (P3)' On receiving the reply message, P3enters the critical section, and when it
exits the critical section, it sends a release message to Pc.Now since there are no more
requests, P, keeps waiting for the next request message.
This algorithm ensures mutual exclusion because, at a time, the coordinator allows
only one process to entera critical section. The algorithm also ensures that no starvation
will occur because of the use of first-come, first-served scheduling policy. The main
advantages of this algorithm is that it is simple to implement and requires only three
messages per critical section entry: a request, a reply, and a release. However, it suffers
from the usual drawbacks of centralized schernes. That is, a single coordinatoris subject

300 Chap. 6 • Synchronization
to a single point of failure and can become a performance bottleneck in a large system.
Furthermore, for failure handling, means must be provided to detect a failure of the
coordinator, toelectaunique newcoordinator, and toreconstruct itsrequest queue before
the computation can be resumed.
6.4.1 Dlstribut.d Approach
Inthedistributedapproach, thedecision making formutualexclusion isdistributedacross
theentiresystem.Thatis,allprocessesthatwanttoenterthesamecriticalsectioncooperate
witheach other before reaching adecision on whichprocess willenter thecritical section
next.Thefirstsuchalgorithm waspresented byLamport[1978]basedonhisevent-ordering
scheme described in Section·6.3. Later, Ricart and Agrawala [1981] proposed a more
efficientalgorithm thatalsorequires therebeatotalorderingofallevents inthesystem.As
an example of a distributed algorithm for mutual exclusion, Ricart and Agrawala's
algorithm isdescribed below.Inthefollowingdescription weassume thatLamport'sevent
ordering schemeisusedtogenerateauniquetimestamp foreachevent inthesystem.
When a process wants to enter a critical section, it sends a request message to all
other processes. The message contains the following information:
1. The process identifier of the process
2. The name of the critical section that the process wants to enter
3. A unique timestamp generated by the process for the request message
On receiving a request message, a process either immediately sends back a reply
message to the sender or defers sending a reply based on the following rules:
1. Ifthereceiverprocess isitselfcurrently executing inthecritical section, itsimply
queues the request message and defers sending a reply.
2. If the receiver process is currently not executing in the critical section but is
waiting for its turn to enter the critical section, it compares the timestamp in the
received request message with the timestamp in its own request message that it
has sent to other processes. If the timestamp of the received request message is
lower,itmeans that thesenderprocess madearequest before the recciverprocess
to enter the critical section. Therefore, the receiver process immediately sends
back a reply message to the sender. On the other hand, if the receiver process's
own request message has a lower timestamp, the receiver queues the received
request message and defers sending a reply message.
3. If the receiver process neither is in the critical section nor is waiting for its turn
to enter the critical section, it immediately sends back a reply message.
A process that sends out a request message keeps waiting for reply messages from
otherprocesses. Itenters thecritical section assoonasithasreceived reply messages from
allprocesses. After it finishes executing in thecritical section, itsends reply messages to
all processes in its queue and deletes them from its queue.

Sec.6.4 • Mutual Exclusion 301
To illustrate how the algorithm works, let us consider the example of Figure 6.7.
There are four processes PI, P2' P3' and P4' While process P4 is in a critical section,
processes PI and P wanttoenter the same critical section.Toget permission from other
2
processes, processes PI and P send request messages with timestamps 6 and 4
2
respectively to other processes (Fig. 6.7(a)).
Nowlet usconsider the situation inFigure 6.7(b). Sinceprocess P4 isalready inthe
critical section, it defers sending a reply message to PI and P2and enters them in its
queue. Process P3 is currently not interested in the critical section, so it sends a reply
TS=6
Fig.6.7 Example illustrating thedistributed algorithm formutual exclusion: (a) status
when processes PI andP2send request messages toother processes while
process P4isalready inthecritical section; (b) status whileprocess P4is
still incritical section; (c) status after process P4exits critical section; (d)
status after process P exits critical section.
2

302 Chap.6 • Synchronization
message to both PI and Pz. Process Pzdefers sending a reply message to PI and enters
P 1 in its queue because the timestamp (4) in its own request message is less than the
timestamp (6) in PI'S request message. On the other hand, PI immediately replies to P 2
becausethetimestamp(6)initsrequestmessage isfound tobegreaterthan thetimestamp
(4) ofP2'S request message.
Next consider the situation in Figure 6.7{c). When process P exits the critical
4
section, it sends areply messagetoall processes initsqueue (in this case toprocessesPI
and P2) and deletes them from its queue. Now since process P2 has received a reply
message from allother processes(PI' P 3, and P4), itenters thecritical section. However,
process PI continues to wait since it has not yet received a reply message from process
P2•
Finally, whenprocess P exits thecritical section, itsendsareply messagetoPI (Fig.
2
6.7(d». Now since process PI has received a reply message from all other processes, it
enters the critical section.
The algorithm guarantees mutual exclusion because a process can enter its critical
section only after getting permission from allother processes, and inthecase ofaconflict
only one ofthe conflicting processes can get permission from all other processes. The
algorithm also ensures freedom from starvation since entry to the critical section is
scheduled according to the timestamp ordering. It has also been proved by Ricart and
Agrawala [1981] that the algorithm is free from deadlock. Furthermore, if there are n
processes, thealgorithm requires n-I request messagesand n-l reply messages, giving a
total of2(n-I) messages per critical section entry. However, this algorithm suffers from
the following drawbacks becauseofthe requirement that allprocesses must participate in
a critical section entry request by any process:
1. In a system having n processes, the algorithm is liable to n points of failure
because ifoneoftheprocessesfails, theentire scheme collapses. This isbecause thefailed
process will not reply to request messages that will be falsely interpreted as denial of
permission by the requesting processes, causing all the requesting processes to wait
indefinitely.
Tanenbaum [1995] proposed a simple modification to the algorithm to solve this
problem. In the modified algorithm, instead ofremaining silent by deferring the sending
of the reply message in cases when permission cannot be granted immediately, the
receiver sends a "permission denied" reply message to the requesting process and then
later sends an OK message when the permission can be granted. Therefore, a reply
message(either "permissiondenied" or OK) isimmediatelysent totherequestingprocess
inanycase. Iftherequestingprocessdoes notreceive areply fromaprocess withinafixed
timeoutperiod, iteitherkeeps trying untiltheprocess replies orconcludesthat theprocess
has crashed. When the requesting process receives a "permissiondenied" reply message
from one or more ofthe processes, itblocks until an OK messageis received from all of
them.
2. The algorithm requires that each process know the identity of all the processes
participating in the mutual-exclusion algorithm. This requirement makes implementation
ofthealgorithmcomplexbecauseeachprocess ofagroup needstodynamicallykeeptrack

Sec.6.4 • Mutual Exclusion 303
ofthe processes entering or leaving the group. That is, when a process joins a group, it
must receive the names of all the other processes in the group, and the name of the new
process must be distributed to all the other processes in the group. Similarly, when a
process leaves the group or crashes, all members ofthat group must be informed so that
they can delete it from their membership list. Updating of the membership list is
particularlydifficultwhenrequestand reply messagesare alreadybeingexchangedamong
the processes of the group. Therefore, the algorithm is suitable only for groups whose
member processes are fixed and do not change dynamically.
3. In this algorithm, a process willing to entera critical section can do so only after
communicating with all other processes and getting permission from them. Therefore,
assuming that the networkcan handle only one message at a time, the waiting time from
the moment the process makes a request to enter a critical region until it actually enters
the critical section is the time for exchanging 2(n-1) messages in a system having n
processes. This waiting time may be large if there are too many processes in the system.
Therefore, the algorithm is suitable only for a small group ofcooperating processes.
Some improvements to this algorithm have been proposed in the literature. For
instance, a simple improvement is possible by using the idea of majority consensus
rather than the consensus of all other processes for critical section entry [Tanenbaum
1995]. That is, in an algorithm that uses the idea of majority consensus, a process can
enter a critical section as soon as it has collected permission from a majority of the
other processes, rather than from all of them. Note that in this algorithm a process can
grant permission for a critical section entry to only a single process at a time. Two other
possible improvements to the algorithm can be found in [Carvalho and Roucairol 1983,
Maekawa et al. 1987).
6.4.3 Token-Passing Approach
In this method, mutual exclusion is achieved by using a single token that is circulated
among the processes in the system. A token is a special typeofmessage that entitles its
holder to enter a critical section. For fairness, the processes in the system are logically
organized in a ring structure, and the token is circulated from one process to another
around the ring always in the same direction (clockwise or anticlockwise).
The algorithm works as follows. When a process receives the token, it checks if it
wants to enter a critical section and acts as follows:
• If it wants to enteracritical section, itkeeps the token, enters the critical section,
and exits from the critical section afterfinishing its work in the critical section. It
then passes the token along the ring to its neighborprocess. Note that the process
can enteronly one critical section when it receives the token. If it wants to enter
another critical section, it must wait until it gets the token again.
• If itdoes not want to enteracriticalsection, itjustpassesthe token alongthe ring
to its neighborprocess.Therefore" ifnone ofthe processesisinterestedinentering
a critical section, the token simply keeps circulating around the ring.

304 Chap. 6 • Synchronization
Mutualexclusionisguaranteed bythealgorithmbecauseatanyinstanceoftimeonly
one process can be in a critical section, since there is only a single token. Furthermore,
sincetheringisunidirectionalandaprocessispermittedtoenteronlyonecritical section
each time it gets the token, starvation cannot occur. In this algorithm the number of
messages percriticalsectionentry may varyfrom 1(wheneveryprocess always wantsto
enter acritical section) toanunbounded value (when noprocess wants to enter acritical
section). Moreover, for a total of n processes in the system, the waiting time from the
momentaprocess wantstoenteracriticalsectionuntilitsactualentry may varyfromthe
timeneeded toexchange 0 ton-l token-passing messages. Zero token-passing messages
are needed when the process receives the token just when it wants to enter the critical
section, whereas n-I messages are needed when the process wants to enter the critical
sectionjust after it has passed the token to its neighbor process.
The algorithm, however, requires the handling of the following types of failures:
1. Processfailure. A process failure in the system causes the logical ring to break.
In such a situation, a new logical ring must be established to ensure the continued
circulation ofthetokenamongother processes.Thisrequiresdetection ofafailedprocess
anddynamic reconfigurationofthelogicalring whenafailedprocess isdetectedor when
a failed process recovers after failure.
Detection of a failed process can be easily done by making it a rule that a
process receiving the token from its neighbor always sends an acknowledgment
message to its neighbor. With this rule, a process detects that its neighbor has failed
when it sends the token to it but does not receive the acknowledgment message within
a fixed timeout period. On the other hand, dynamic reconfiguration of the logical ring
can be done by maintaining the current ring configuration with each process. When a
process detects that its neighbor has failed, it removes the failed process from the
group by skipping it and passing the token to the process after it (actually to the next
alive process in the sequence). When a process becomes alive after recovery, it simply
informs the neighbor previous to it in the ring so that it gets the token during the next
round of circulation.
2. Lost token. If the token is lost, a new token must be generated. Therefore, the
algorithm must also have mechanisms todetect andregenerate a lost token. One method
to solve this problem is to designate one of the processes on the ring as a "monitor"
process. The monitor process periodically circulates a "who has the token?" message on
thering.This.messagerotatesaround thering fromoneprocess toanother.Allprocesses
simplypass thismessage totheir neighbor process, except theprocess that has the token
when it receives this message. This process writes its identifier in a special field of the
message before passing it to its neighbor. When the message returns to the monitor
processafteronecomplete round, itchecks thespecialfieldofthemessage. Ifthereisno
entry in this field, it concludes that the token has been lost, generates a new token, and
circulates it around the ring.
There are two problems associated with this method-the monitor process may
itself fail and the "who has the token?" message may itself get lost. Both problems may
be solved byusing more than one monitor processes. Each monitor process independ
ently checks the availability of the token on the ring. However, when a monitor process

Sec.6.5 • Deadlock 305
detects that the token is lost, it holds an election with other monitor processes to decide
which monitor process will generate and circulate a new token (election algorithms are
described later in this chapter). An election is needed to prevent the generation of
multiple tokens that may happen when each monitor process independently detects that
the token is lost, and each one generates a new token.
6.5 DEADLOCK
We saw in the previous section that there are several resources in a system for which
the resource allocation policy must ensure exclusive access by a process. Since a system
consists of a finite number of units of each resource type (for example, three printers,
six tape drives, four disk drives, two CPUs, etc.), multiple concurrent processes
normally have to compete to use a resource. In this situation, the sequence of events
required to use a resource by a process is as follows:
I. Request. The process first makes a request for the resource. If the requested
resource is not available, possibly because it is being used by another process, the
requesting process must wait until the requested resource is allocated to it by the
system. Note that if the system has multiple units of the requested resource type, the
allocation of any unit of the type will satisfy the request. Also note that a process may
request as many units of a resource as it requires with the restriction that the number
of units requested may not exceed the total number of available units of the
resource.
2. Allocate. The system allocates the resource to the requesting process as soon
as possible. It maintains a table in which it records whether each resource is free or
allocated and, if it is allocated, to which process. If the requested resource is currently
allocated to another process, the requesting process is added to a queue of processes
waiting for this resource. Once the system allocates the resource to the requesting
process, that process can exclusively use the resource by operating on it.
3. Release.After theprocess hasfinished using theallocated resource, itreleases the
resource tothe system. The system table records are updated atthe timeof allocationand
release to reflect the current status of availability of resources.
The request and release ofresourcesare system calls, such as requestand releasefor
devices, open and closefor files, and allocateandfree for memory space. Notice that of
the three operations, allocate isthe only operation that the system can control. The other
two operations are initiated by a process.
With the above-mentioned pattern of request, allocation, and release of resources,
if the total request made by multiple concurrent processes for resources of a certain
type exceeds the amount available, some strategy is needed to order the assignment of
resources in time. Care must be taken that the strategy applied cannot cause a
deadlock, that is, a situation in which competing processes prevent their mutual
progress even though no single one requests more resources than are available. It may

306 Chap.6 • Synchronization
happen that some ofthe processes that entered the waiting state (because the requested
resources were not available at the time of request) will never again change state,
because the resources they have requested are held by other waiting processes. This
situation is calJed deadlock, and the processes involved are said to be deadlocked.
Hence, deadlock is the state of permanent blocking of a set of processes each of
which is waiting for an event that only another process in the set can cause. All the
processes in the set block permanently because all the processes are waiting and hence
none of them will ever cause any of the events that could wake up any of the other
members of the set.
A deadlock situation can be best explained with the help of an example. Suppose
that a system has two tape drives T and T and the resource allocation strategy is
I 2
such that a requested resource is immediately allocated to the requester if the resource
is free. Also suppose that two concurrent processes PI and P make requests for the
2
tape drives in the following order:
1. PI requests for one tape drive and the system allocates T I to it.
2. P requests for one tape drive and the system allocates T to it.
2 2
3. PI requests for one more tape drive and enters a waiting state because no tape
drive is presently available.
4. P requests for one more tape drive and it also enters a waiting state because no
2
tape drive is presently available.
From now on, PI and P 2 will wait for each other indefinitely, since PI will not
release T I until it gets T 2 to carry out its designated task, that is, not until P2 has
released T 2, whereas Pz will not release T 2 until it gets T I. Therefore, the two
processes are in a state of deadlock. Note that the requests made by the two processes
are totally legal because each is requesting for only two tape drives, which is the total
number of tape drives available in the system. However, the deadlock problem occurs
because the total requests of both processes exceed the total number of units for the
tape drive and the resource allocation policy is such that it immediately allocates a
resource on request if the resource is free.
In the context of deadlocks, the term "resource" applies not only to physical
objects (such as tape and disk drives, printers, CPU cycles, and memory space) but
also to logical objects (such as a locked record in a database, files, tables, semaphores,
and monitors). However, these resources should permit only exclusive use by a single
process at a time and should be nonpreemptable. A nonpreemptable resource is one
that cannot be taken away from a process to which it was allocated until the process
voluntarily releases it. If taken away, it has ill effects on the computation already
performed by the process. For example, a printer is a nonpreemptable resource
because taking the printer away from a process that has started printing but has not
yet completed its printing job and giving it to another process may produce printed
output that contains a mixture of the output of the two processes. This is certainly
unacceptable.

Sec. 6.5 • Deadlock 307
6.5.1 Necessary Conditions forDeadlock
Coffman et al. [1971] stated that the following conditions are necessary for a deadlock
situation to occur in a system:
1. Mutual-exclusion condition. If a resource is held by a process, any otherprocess
requesting for that resource must wait until the resource has been released.
2. Hold-and-wait condition. Processes are allowed to request for new resources
without releasing the resources that they are currently holding.
3. No-preemption condition.A resourcethathas been allocatedto aprocessbecomes
available for allocation to another process only after it has been voluntarily
released by the process holding it.
4. Circular-wait condition. Two or more processes must form a circular chain in
which each process is waiting for a resource that is held by the next member of
the chain.
All four conditions must hold simultaneously in a system for adeadlock to occur. If
anyone of them is absent, no deadlock can occur. Notice that the four conditions are not
completely independent because the circular-wait condition implies the hold-and-wait
condition. Although these four conditions are somewhat interrelated, it is quite useful to
consider them separately to devise methods for deadlock prevention.
6.5.2 Deadlock Modeling
Deadlockscan bemodeled usingdirectedgraphs. Beforepresentingagraphical model for
deadlocks, some terminology from graph theory is needed:
1. Directed graph. A directed graph is a pair (N, E), where N is a nonempty set of
nodes and E is a set of directed edges. A directed edge is an ordered pair (a, b),
where a and b are nodes in N.
2. Path. A path isasequenceof nodes (a, b, c, ...,i,j) of adirected graph such that
(a, b), (b, c), ...,(i,j) are directed edges. Obviously, apath contains at least two
nodes.
3. Cycle. A cycle is a path whose first and last nodes are the same.
4. Reachable set. The reachable set of a node a is the set of all nodes b such that a
pathexists from a to b.
5. Knot. Aknot.isanonempty set Kof nodes such thatthe reachablesetofeach node
in Kis exactly the set K.A knot always contains one or more cycles.
An exampleof adirectedgraph isshown in Figure6.8. The graph has aset of nodes
{a, b,c, d,e,f} and a set of directed edges {(a, b), (b, c), (c, d), (d,e), (e,j), if, a), (e,
b)}. It has two cycles (a, b,c, d,e,f, a) and (b, C, d, e, b). It also has a knot {a, b, C, d,
e,11 that contains the two cycles of the graph.

308 Chap. 6 • Synchronization
e
Fig.6.8 Adirected graph.
For deadlock modeling, adirected graph, called a resourceallocation graph, is used
inwhich boththesetofnodesandthesetofedges arepartitionedintotwo types, resulting
in the following graph elements:
1. Process nodes. A process node represents a process of the system. In a resource
allocation graph, it is normally shown as a circle, with the name of the process written
inside the circle (nodes PI' Pz,and P3 ofFig. 6.9).
2. Resource nodes. A resource node represents a resource of the system. In a
resource allocation graph, it is normally shown as a rectangle with the name of the
resource written inside the rectangle. Since a resource type R, may have more than one
unit in the system, each such unit is represented as a bullet within the rectangle. For
instance, intheresource allocation graphofFigure6.9,therearetwounitsofresourceR J,
one unit ofR 2, and three units ofR 3•
3. Assignmentedges.Anassignment edge isadirected edge from aresource node to
a process node. It signifies that the resource iscurrently held by the process. In multiple
units of a resource type, the tail of an assignment edge touches one ofthe bullets in the
rectangle toindicate thatonly one unitof theresource isheld bythat process. Edges (R 1,
PI),(R I,P3),and(R 2,P 2) arethethreeassignment edges intheresource allocation graph
of Figure 6.9.
4. Requestedges.Arequestedgeisadirected edgefromaprocess node toaresource
node. It signifies that the process made a request for a unit of the resource type and is
currently waiting for thatresource. Edges (PI'R 2) and (P2,RI)arethetwo request edges
in the resource allocation graph of Figure 6.9.

Sec.6.S • Deadlock 309
G
AprocessnamedPi'
I: I
R AresourceR having3unitsinthesystem.
j j
~
ProcessPiholdingaunitofresourceRj'
~
ProcessPirequestingforaunitofresourceRj'
Fig.6.9 Resourceallocationgraph.
Constructing a Resource Allocation Graph
Aresource allocation graph provides an overall view of the processes holding or waiting
for the various resources in the system. Therefore, the graph changes dynamically as the
processes inthe system request fororrelease resources or the system allocates aresource
to a process. That is, when a process Pirequests for a unit of resource type R j, a request
edge (Pi' R j) is inserted in the resource allocation graph. When this request can be
fulfilled, a unit of resource R, is allocated to Pi and the request edge (Pi' R j) is
instantaneouslytransformedtoanassignmentedge (R j,Pi). Later,whenPireleases R j,the
assignment edge (R j, Pi) is deleted from the graph.
Note that in many systems the resource allocation graph is not constructed in the
above-mentioned manner. Rather it is used as a tool for making resource allocation
decisions that do not lead to deadlock. In these systems, for the available resources,
different request/release sequences are simulated step by step, and after every step, the
graph is checked for deadlock. Later, in the resource allocation strategy, only those

310 Chap. 6 • Synchronization
sequences are allowed that do not lead to deadlock. Therefore, in these systems, the
resourceallocationgraph isbasically usedtoformulate adeadlock freeresource allocation
strategy.
Necessary and Sufficient Conditions for Deadlock
Inaresourceallocation graph, acycle isanecessary condition foradeadlocktoexist. That
is, if the graph has no cycles, then it represents a state that is free from deadlock. On the
other hand, ifthegraph containsacycle, adeadlock mayexist.Therefore, thepresenceof
acycle in ageneral resource allocation graph isanecessary but not a sufficientcondition
for the existence of deadlock. For instance, the resource allocation graph of Figure 6.9
contains a cycle (Ph Rz, P Z, R h P,) but does not represent a deadlock state. This is
becausewhen P3completes using R)and releases it,R can beallocated toPz.With both
I
R and R allocated to it, P2can now complete itsjob after which it will release both R
1 2 I
and R 2.As soon as R
2
is released, itcan beallocated to PI' Therefore, all processes can
finish their job one by one.
The sufficient condition for deadlock is different for the following different cases:
1. Acycle inthe graph is both anecessary and asufficient condition fordeadlockif
all theresourcetypes requested bytheprocesses forming thecycle have only asingle unit
each.
For example, the resource allocation graph of Figure 6.10 shows a deadlock state in
which processes PI and Pzare deadlocked. Notice that in this graph, although there are
three units ofresource R 3, itisnot involved inthecycle (PI,Rz,Pz,RI,PI)' BothRI and
Rzthat are involved in the cycle have only one unit each. Therefore, the cycle represents
a deadlock state.
2. A cycle in the graph is a necessary but not a sufficient condition for deadlock if
one or more oftheresource types requested bytheprocesses forming thecycle have more
than one unit. In this case, a knot is a sufficient condition for deadlock.
Fig.6.10 Acy.clerepresenting adeadlock.

Sec. 6.5 • Deadlock 311
Wehavealready seenthatthecycle(PI' R 2, P 2, R}, PI) inthegraph ofFigure 6.9
doesnotrepresentadeadlock.ThisisbecauseresourcetypeRI hastwounitsandthereare
noknotsinthegraph.NowsupposethatinthesamegraphP requestsforR andarequest
3 2
edge (P3' Rz) is added to the graph. The modified graph is shown in Figure 6.11. This
graph has two cycles (PI' R 2, P 2, «; PI) and (P3,R 2, P 2, R}, P3) and a knot {PI' Pz,
P 3, R., R 2}. Since the graph contains a knot, it represents a deadlock state in which
processes p., Pz,and P are deadlocked.
3
Fig.6.11 Aknot representing adeadlock.
In terms of the resource allocationgraph, the necessary and sufficientconditions for
deadlock can be summarized as follows:
• A cycle is a necessary condition for deadlock.
• If there is only a single unit of each resource t.ypeinvolved in the cycle, a cycle
is both a necessary and a sufficient condition for a deadlock to exist.
• Ifone or more ofthe resource types involvedinthecycle have morethan one unit,
a knot is a sufficient condition for a deadlock to exist.
Wait-for Graph
When all the resource types have only a single unit each, a simplified form of resource
allocation graph is normally used. The simplified graph is obtained from the original
resource allocation graph by removing the resource nodes and collapsing the appropriate
edges. This simplificat.ion is based on the observation that a resource can always be
identified by its current owner (process holding it). Figure 6.12 shows an example of a
resource allocation graph and its simplified form.
The simplified graph is commonly known as a wait-for graph (Wf-'G) because it
clearly shows which processes are waiting for which otherprocesses. For instance, in the
WFG ofFigure6.12(b),processesPI and P3 arc waiting for P2 and process Pzis waiting
for PJ•Since WFG isconstructed only when each resource type has only a single unit, a
cycle is both a necessary and sufficient condition for deadlock in a WFG.

312 Chap. 6 • Synchronization
.....----Simplifiedto
--+
(a) (b)
Fig.6.12 Aconversion fromaresource allocation graph toaWFG:
(0) resource allocation graph; (b) corresponding WFG.
6.5.3 Handling Deadlocks InDistributed Systems
In principle, deadlocks in distributed systems are similar to deadlocks in centralized
systems. Therefore, the description of deadlocks presented above holds good both for
centralized and distributed systems. However, handling of deadlocks in distributed
systems ismorecomplex thanincentralized systemsbecausetheresources, theprocesses,
and other relevant information are scattered on different nodes of the system.
Three commonly used strategies to handle deadlocks are as follows:
1. Avoidance. Resources are carefully allocated to avoid deadlocks.
2. Prevention. Constraints are imposed on the ways in which processes request
resources in order to prevent deadlocks.
3. Detectionandrecovery.Deadlocks areallowedtooccurandadetection algorithm
is used to detect them. After a deadlock is detected, it is resolved by certain
means.
Althoughthethirdstrategyisthemostcommonly usedoneindistributed systems,for
completion, the other two strategies will also bebriefly described.
At this point, it may also be noted that some people prefer to make a distinction
between two kinds of distributed deadlocks-resource deadlocks and communication
deadlocks. As already described, a resource deadlock occurs when two or more
processes wait permanently for resources held by each other. On the other hand, a
communication deadlock occurs among a set of processes when they are blocked
waiting for messages from other processes in the set in order to start execution but
there are no messages in transit between them. When there are no messages in transit
between any pair of processes in the set, none of the processes will ever receive a

Sec.6.5 • Deadlock 313
message. This implies that all processes in the set are deadlocked. Communication
deadlocks can be easily modeled by using WFGs to indicate which processes are
waiting to receive messages from which other processes. Hence, the detection of
communication deadlocks can be done in the same manner as that for systems having
only one unit of each resource type.
Deadlock Avoidance
Deadlock avoidance methods use some advance knowledge of the resource usage of
processes to predict the future state of the system for avoiding allocations that can
eventually leadtoadeadlock. Deadlockavoidance algorithmsareusually inthefollowing
steps:
1. When a process requests for a resource, even if the resource is available for
allocation, it is not immediately allocated to the process. Rather, the system
simply assumes that the request is granted.
2. Withtheassumptionmade instep 1andadvance knowledgeoftheresource usage
of processes, the system performs some analysis to decide whether granting the
process's request is safe or unsafe.
3. The resource is allocated to the process only when the analysis of step 2 shows
that it is safe to do so; otherwise the request is deferred.
Since the algorithms for deadlock avoidance are based on the concept of safe and
unsafe states, itisimportanttolookatthenotionofsafety inresource allocation. Asystem
is said to be in a safe state if it is not in a deadlock state and there exists some ordering
of the processes in which the resource requests of the processes can be granted to run all
of them to completion. For a particular safe state there may be many such process
orderings. Any ordering of the processes that can guarantee the completion of all the
processes iscalled asafe sequence. The formation of asafe sequence isbased onthe idea
of satisfying the condition that, for any process Piin a safe sequence, the resources that
Pican still request can besatisfied by thecurrently available resources plus the resources
held by all the processes lying before Piin the safe sequence. This condition guarantees
that process Pican be run to completion because if the resources that Pineeds are not
immediately available, Pican wait until all other processes in the sequence lying before
Pihave finished. When they have finished, Pican obtain all its needed resources and run
to completion. A system state is said to be unsafe if no safe sequence exists for that
state.
The concept of safe and unsafe states can be best illustrated with the help of an
example. Let usassume that inasystem there are atotal of8unitsofaparticularresource
type forwhich three processes PI' Pz,andP3arecompeting. Suppose themaximumunits
of theresource required byPJ, Pz,and P3 are4, 5, and 6, respectively. Also suppose that
currently each of the three processes is holding 2 units of the resource. Therefore, in the
current state ofthe system, 2units oftheresource are free.The current stateofthe system
can be modeled as shown in Figure 6.13(a).

314 Chap.6 • Synchronization
P1 P2 P
3
I
Max 4 5 6
I
Holds 2 2 2
=
(a)Free 2
P1 P2 P
3
I
Max 4 5 6
I
Holds 4 2 2
=
(b)Free 0 ,
P
1
P2 P
3
I
Max - 5 6
I
Holds 0 2 2
/ ~
P1 P2 P
3
P
1
P
2
P
3
I I
Max - 5 6 Max - 5 6
I
f Holds 0 5 2 Holds 0 2 6
= =
(d)Free 1 (~Free 0
,
"
P P2 P3 P P2 P
1 1 3
I Max - - 6 t Max - 5 -
I a I a
Holds 0 2 Holds 0 2
(e)Free=6 (g)Free=6
Fig.6.13 Demonstrationthat thestate in(a) isasafe state and has two safe sequences.
Now let us tryto find out whether the state of Figure 6.13(a) is safe or unsafe. The
analysis performed in Figure 6.13 shows that this state is safe because there exists a
sequence of allocations that allows all processes to complete. In fact, as shown in the
figure, for this state there are two safe sequences, (Ph P2' P3) and (Ph P3' P 2). Let us
see the schedulingof theresource units for the first of these two safe sequences. Starting
from the state of Figure 6.13(a), the scheduler could simply run PI exclusively, until it
asked forandgottwomoreunitsoftheresource thatarecurrently free,leading tothestate
of Figure 6.13(b). When PI completes and releases the resources held by it, we get the
state of Figure 6.13(c). Then the scheduler chooses to run P2' eventually leading to the
state of Figure 6.13(d). When P2completes and releases the resources held by it, the
system enters thestateofFigure 6.13(e). Now withtheavailable resources, P3 can berun
to completion. The initial state of Figure 6.13(a) is a.safe state because the system, by
careful scheduling, canavoiddeadlock.Thisexample alsoshowsthatforaparticularstate
there may be more than one safe sequence.

Sec. 6.5 • Deadlock 315
If resource allocation is not done cautiously, the system may move from a safe state
to an unsafe state. For instance, let us considerthe example shown in Figure 6.14. Figure
6.14(a) is the same initial state as that of Figure 6.l3(a). This time, suppose process P2
requests for one additional unit of the resource and the same is allocated to it by the
system. The resulting system state is shown inFigure 6.14(b). The system is no longer in
a safe state because we only have one unit ofthe resource free, which is not sufficientto
run any ofthe three processes tocompletion. Therefore, there is no safe sequencefor the
state ofFigure6.14(b). Thus thedecision toallocate one unit ofthe resource toP2moved
the system from a safe state to an unsafe state.
P1 P P3
2
I
Max 4 5 6
I Holds 2 2 2
=
(a)Free 2
,
P 1 P2 P3
I Max 4 5 6
Fig.6.14 Demonstration that an allocation I Holds 2 3 2
[nay move the system from asafe
(b)Free=1
toan unsafe state.
It is important to note the following remarks about safe and unsafe states:
1. The initial state inwhich noresources are yetallocatedand allareavailable(free)
is always a safe state.
2. From a safe state, the system can guarantee that all processes can be run to
completion.
3. An unsafe state is not a deadlock state, but it may lead to a deadlock state. That
is, from an unsafe state, the system cannotguaranteethat all processes can berun
to completion.
Deadlock avoidance algorithms basically perform resource allocation in such a
manner as to ensure that the system will always remain in a safe state. Since the initial
state of a system is always a safe state, whenever a process requests a resource that is
currently available, the system checks to find out if the allocation of the resource to the
process will change the state of the system from safe to unsafe. If no, the request is
immediately granted; otherwise it is deferred.
Although theoretically attractive, deadlock avoidance algorithms are rarely used in
practice due to the following reasons:
1. The algorithms work on the assumption that advance knowledge of the resource
requirements of the various processes is available. However, in practice, processes rarely
know in advance what their maximum resource needs will be. Modernoperating systems

316 Chap. 6 • Synchronization
are attempting to provide more and more user-friendly interfaces, and as a result, it is
becoming common to have users who do not have the slightest idea about what their
resource needs are.
2. The algorithms also assume that the number of processes that compete for a
particular resource is fixed and known in advance. However, in practice, the number of
processes is not fixed but dynamically varies as new users log in and log out.
3. The algorithms alsoassume that the number of unitsofaparticularresource type
is always fixed and known in advance. However, in practice, the actual number of units
available maychange dynamically duetothesudden breakdown andrepairofoneormore
units.
4. The manner in which these algorithms work restricts resource allocation too
severely andconsequently degrades thesystemperformance considerably. This isbecause
thealgorithms firstconsider theworst possible case andthen guarantee that the system is
deadlock freeevenintheworstsituation.This worstsituation mayarisebutwouldbevery
unlikely. Thus many safe requests could be turned down.
The practical limitations ofdeadlock avoidance algorithms become more severe ina
distributed system because the collection of information needed for making resource
allocation decisions at one point is difficult and inefficient. Therefore, the deadlock
avoidance strategy is never used in distributed operating systems.
Deadlock Prevention
This approach is based on the idea of designing the system in such a way that deadlocks
become impossible. It differs from avoidance and detection in that no runtime testing of
potential allocations need be performed.
We saw that mutual-exclusion, hold-and-wait, no-preemption, and circular-wait are
the four necessary conditions for a deadlock to occur in a system. Therefore, if we can
somehow ensure that atleast one of these conditions is never satisfied, deadlocks willbe
impossible. Based onthis idea,there are three important deadlock-prevention methods
collective requests, ordered requests, and preemption. The first one denies the hold-and
waitcondition, thesecondonedenies thecircular-waitcondition, and thethirdone denies
the no-preemption condition.
The mutual-exclusion condition can also be denied for some nonsharable resources
bydevising an alternative wayof using them. For instance, thefollowing example, taken
from [Tanenbaum 1992], illustrates how this can be done for a printer. A printer is a
nonsharableresource. However,byspooling printeroutput, severalprocesses cangenerate
output at the same time.The spooled outputs are.transferred one by one to theprinter by
the printer daemon. Therefore, the printer daemon is the only process that actually
requests forthephysical printer.Since thedaemon neverrequests foranyother resources,
deadlock for the printer becomes structurally impossible. Unfortunately, in general, it is
not possible to prevent deadlocks by denying the mutual-exclusion condition because
some resources areintrinsically nonsharable and itisnotpossible to devise analternative

Sec.6.5 • Deadlock 317
way of using them. Therefore, denial of the mutual-exclusion condition for deadlock
prevention is rarely used. The other three methods that are more commonly used are
described below.
Collective Requests. This method denies the hold-and-wait condition by
ensuring that whenever aprocess requests aresource, itdoes notholdanyotherresources.
One of the following resource allocation policies may beused to ensure this:
1. A process must request all of its resources before it begins execution. If all the
neededresources areavailable, theyareallocated totheprocesssothattheprocesscanrun
to completion. If one or more of the requested resources are not available, none will be
allocated and the process would just wait.
2. Instead of requesting all its resources before its execution starts, a process may
request resources during its execution if it obeys the rule that it requests resources only
when it holds no other resources. If the process is holding some resources, itcan adhere
to this rule by first releasing all of them and then re-requesting all the necessary
resources.
The second policy has the following advantages over the first one:
1. Inpractice, manyprocesses donotknowhowmanyresources theywillneeduntil
they have started running. For such cases, the second approach is more useful.
2. A longprocess may require some resources only toward theend of itsexecution.
Inthefirstpolicy,theprocess willunnecessarily holdtheseresourcesfortheentire
duration of its execution. In the second policy, however, the process can request
for these resources only when it needs them.
Thecollectiverequests methodofdeadlock prevention issimpleandeffectivebuthas
the following problems:
1. It generally has low resource utilization because a process may hold many
resources but may not actually use several of them for fairly long periods.
2. It may cause starvation of a process that needs many resources, but whenever it
makes a request for the needed resources, one or more of the resources is not
available.
3. The method also raises an accounting question. When a process holds resources
forextended periods during which theyare not needed, itisnotclear whoshould
pay the charge for the idled resources.
OrderedRequests. Inthismethod,eachresource typeisassignedauniqueglobal
number toimpose atotalordering ofallresource types. Now aresource allocation policy
is used according to which a process can request a resource at any time, but the process
shouldnotrequest aresource withanumberlowerthanthenumberofanyoftheresources
that it isalready holding. That is,if.a process holds aresource type whose number is i,it
may request aresource type having the numberj only ifj»i. Iftheprocess needs several

318 Chap.6 • Synchronization
units of thesameresource type, itmust issue asingle request forall the units.Ithas been
proven thatwiththisruletheresource allocation graphcanneverhavecycles (denying the
circular-wait condition), and hence deadlock is impossible.
Note that thisalgorithm does notrequire thataprocess must acquire allitsresources
in strictly increasing sequence. For instance, a process holding two resources having
numbers 3and 7 may release the resource having number 7 before requesting a resource
having number 5. This is allowed because when the process requests for the resource
having number 5, it is not holding any resource having number larger than 5.
The ordering of resources is decided according to the natural usage pattern of the
resources. For example, since the tapedrive is usually needed before theprinter, it would
bereasonable toassign alower number tothetapedrive than totheprinter.However, the
natural ordering is not always the same for all jobs. Therefore, a job that matches the
decided ordering can be expected to use resources efficiently but others would waste
resources. Another difficulty is that once the ordering is decided, it will stay for a long
timebecause theordering iscoded intoprograms. Reordering willrequire reprogramming
of several jobs. However, reordering may become inevitable when new resources are
added to the system. Despite these difficulties, the method of ordered requests is one of
the most efficient methods for handling deadlocks.
Preemption. Apreemptable resource is one whose state'can be easily saved and
restored later. Such aresource can be temporarily taken away from the process to which
itiscurrently allocated withoutcausing any harm tothecomputation performed sofar by
theprocess. The CPUregisters andmainmemory areexamples ofpreemptableresources.
If the resources are preemptable, deadlocks can be prevented by using either of the
following resource allocation policies that deny the no-preemption condition:
1. When a process requests for a resource that is not currently available, all the
resources held by the process are taken away (preempted) from it and the process is
blocked. The process is unblocked when the resource requested by it and the resources
preempted from it become available and can be allocated to it.
2. When a process requests a resource that is not currently available, the system
checks iftherequested resource iscurrently held byaprocess thatis blocked, waitingfor
some other resource. If so, the requested resource is taken away (preempted) from the
waiting process and given to the requesting process. Otherwise, the requesting process is
blocked and waits for the requested resource tobecome available. Some ofthe resources
that this process is already holding may be taken away (preempted) from it while it is
blocked, waiting for the allocation of the requested resource. The process is unblocked
when the resource requested by it and any other resource preempted from it become
available and can be allocated to it.
In general, the applicability of this method for deadlock prevention is extremely
limited because it works only for preemptable resources. However, the availability of
atomic transactions and global timestamps makes this method an attractive approach for
deadlock prevention in distributed and database transaction processing systems. The
transaction mechanism allowsatransaction (process) tobeaborted (killed) withoutanyill

Sec.6.5 • Deadlock 319
effect (transactions are described in Chapter 9). This makes it possible to preempt
resources from processes holding them without any harm.
In the transaction-based deadlock prevention method, each transaction is assigned a
unique priority numberbythesystem, and when twoormore transactionscompeteforthe
same resource, their priority numbers are used to break the tie. For example, Lamport's
algorithm may be used to generate systemwide globally unique timestamps, and each
transaction may be assigned a unique timestamp when it is created. A transaction's
timestampmayserveasitspriority number; atransactionhaving lower valueoftimestamp
may have higher priority because it is older.
Rosenkrantzetat [1978] proposedthefollowing deadlockpreventionschemesbased
on this idea:
1. Wait-die scheme. In this scheme, if a transaction 1'; requests a resource that is
currentlyheldbyanothertransaction1),T,isblocked(waits) ifitstimestampislower than
that of 1); otherwise it is aborted (dies). For example, suppose that of the three
transactions T 1, T 2,and T 3, 1'1 istheoldest (has the lowest timestampvalue) and T 3 isthe
youngest(hasthehighesttimestampvalue). Now ifT)requests aresourcethatiscurrently
held by T 2, T 1 will be blocked and will wait until the resource is voluntarily released by
T 2• On the other hand, if T 3 requests a resource held by T 2, T 3 win be aborted.
2. Wait-wound scheme. In this scheme, if a transaction T, requests a resource
currentlyheld byanothertransactionT, T,isblocked(waits) ifitstimestampislargerthan
that of 1);otherwise T,is aborted (wounded by T;). Once again considering the same
exampleoftransactions T), T 2, and T), ifT I requests aresource held byT 2, the resource
will bepreemptedbyaborting T and will begiven to1').Ontheother hand, ifT requests
2 3
a resource held by T 2, T 3 will be blocked and will wait until the resource is voluntarily
released by T 2.
Notice that both schemesfavor older transactions, which isquitejustifiedbecause, in
general, an older transaction has run for a longer period and has used more system
resourcesthan ayoungertransaction. However, themannerinwhichthetwoschemes treat
a younger transaction is worth noticing. In the wait-die scheme, a younger transaction is
aborted when it requests for a resource held by an older transaction. The aborted
transaction will be restarted after a predetermined time and will be aborted again if the
older transaction is still holding the resource. This cycle may be repeated several times
before the younger transaction actually gets the resource. This problem of the wait-die
scheme can besolved byusing animplementationmechanism thatensuresthatanaborted
transaction is restarted only when its requested resource becomes available.
On theother hand, inthe wait-wound scheme, when ayoungertransaction isaborted
(wounded) byanolder transaction, itwill berestartedafter apredeterminedtime, and this
time it will be blocked (will wait) if its preempted resource is being held by the older
transaction. Therefore, the implementation of the wait-wound scheme is simplerthan the
wait-die scheme. Furthermore, to avoid starvation, it is important that in the
implementation of both schemes, a transaction should not be assigned a new timestamp
when it is restarted after being aborted (a younger transaction will become olderas time
passes and will not be aborted again and again).

320 Chap. 6 • Synchronization
Deadlock Detection
In this approach for deadlock handling, the system does not make any attempt to prevent
deadlocks and allows processes to request resources and to wait for each other in an
uncontrolled manner. Rather, it uses an algorithm that keeps examining the state ofthe
system to determine whether a deadlock has occurred. When a deadlock is detected, the
system takes some action to recover from the deadlock. Some methods for deadlock
detection in distributed systems are presented below, and some of the ways to recover
from a deadlock situation are presented in the next section.
In principle, deadlock detection algorithms are the same in both centralized and
distributed systems. It is based on maintenance ofinformation on resource allocation to
variousprocessesintheform ofaresourceallocationgraph and searchingforacyclelknot
in the graph depending on whetherthe system has single/multiple units ofeach'resource
type. However, for simplicity, in the following description weconsideronly the case of a
single unit of each resource type. Therefore, the deadlock detection algorithms get
simplified to maintaining WFG and searching for cycles in the WFG.
The following steps may be followed to construct the WFG for a distributed
system:
1. Construct a separate WFG for each site of the system in the following manner.
Using the convention ofFigure 6.9, construct a resource allocation graph for all
theresourceslocated onthissite.That is,intheresourceallocationgraph ofasite,
aresource node exists for all the local resources and aprocess node exists for all
processes that are eitherholding or waiting for a resource ofthis site immaterial
ofwhether the process is local or nonlocal.
2. Converttheresourceallocationgraphconstructedinstep1toacorrespondingWFG
by removing the resource nodes and collapsing the appropriate edges. It may be
noted that step 1and this step are mentioned here only forclarityofpresentation.
The actual algorithmmaybedesignedtodirectly constructaWFG.
3. Take the union ofthe WFGs ofall sites and construct a single global WFG.
Let us illustrate the procedure with the help ofthe simple example shown in Figure
6.15. Supposethat the system is comprised ofonly two sites (S. and S2)with SJ having
two resourcesR
I
andR
2
and S2 having one resourceR 3•Also supposethat there are three
processes (PI' P 2, P 3) that are competing for the three resources in the following
manner:
• PI is holding R and requesting for R
I 3
• P is holding R and requesting for R.
2 2
• P3 is holding R 3 and requesting for R 2
The corresponding resource allocation graphs for the two sites are shown in Figure
6.15(a). Notice thatprocessesPI andP appearinthegraph ofboththesites because they
3
have requestedfor resourceson both sites. On the otherhand, process P appears only in
2
the graph ofsite SI because both resources requested by it are on SI.

Sec. 6.5 • Deadlock 321
~
P2
Site 51 Site52
(a)
P1
Site51 SiteS
2
(b)
Fig.6.15 Illustrationof the constructionofa
WFO inadistributed system: (a)
resource allocationgraphs ofeach
site; (b) WFGs correspondingto
graphs in(a); (c) global WFG by
taking theunion ofthe two local
WFOsof (b). (c)
Figure 6.15(b) shows the corresponding WFGs for the two sites and Figure 6.15(c)
shows the global WFG obtained by taking the union of the local WFGs of the two sites.
Notice that although the local WFGs of the two sites do not contain any cycle, the global
WFG contains a cycle, implying that the system is in a deadlock state. Therefore, this
example shows that the local WFGs are not sufficient to characterize all deadlocks in a
distributed system and the construction of aglobal WFG by taking the union of all local
WFGs isrequiredtofinallyconclude whether thesystem isinastateofdeadlock ornot.
The main difficulty in implementing deadlock detection in a distributed system is
how to maintain the WFG. Three commonly used techniques for organizing the WFG in
a distributed system are centralized, hierarchical, and distributed. These techniques are
describedbelow.However, before wedescribethem, itmaybenoted that one ofthe most
important features of deadlock detection algorithms iscorrectness, which depends on the
following properties [Knapp 1987]:
I. Progress property. This property states that all deadlocks must be detected in a
finite amount of time.
2. Safetyproperty. Ifadeadlock isdetected, itmustindeedexist. Message delays and
out-of-date WFGs sometimes cause false cycles to be detected, resulting in the
detection of deadlocks that do not actually exist. Such deadlocks are called
phantom deadlocks.

322 Chap. 6 • Synchronization
Centralized Approach for Deadlock Detection. In the centralized deadlock
detectionapproach,thereisalocalcoordinatorateachsitethatmaintainsaWFGforitslocal
resources,andthereisacentralcoordinator(alsoknownasacentralizeddeadlockdetector)
that is responsible for constructing the union of all the individual WFGs. The central
coordinator constructs the global WFG from information received from the local
coordinatorsofallthesites.Inthisapproach, deadlock detection isperformedasfollows:
1. Ifacycle exists in thelocal WFG ofany site, itrepresents a local deadlock. Such
deadlocks are detected and resolved locally by the local coordinator of the site.
2. Deadlocks involving resources at two or more sites get reflected as cycles in the
global WFG. Therefore, such deadlocks are detected and resolved by the central
coordinator.
In thecentralizedapproach, thelocal coordinatorssend local state informationtothe
central coordinator in the form of messages. One of the following methods is used to
transfer information from local coordinators to the central coordinator:
1. Continuous transfer. A local coordinator sends a message providing the update
done in the local WFG whenever a new edge isadded to or deleted from it.
2. Periodic transfer. To reduce the number of messages, a local coordinator
periodically(whenanumberofchangeshaveoccurredinitslocalWFG)sendsalist
ofedges addedtoordeleted fromitsWFGsincetheprevious message wassent.
3. Transfer-on-request.A local coordinatorsends a list of edges added to or deleted
from its WFG since the previous message is sent only when the central
coordinatormakesarequest forit.Inthiscase, thecentral coordinatorinvokes the
cycle detection algorithm periodicallyandrequests informationfromeach sitejust
before invoking the algorithm.
Although thecentralizeddeadlockdetectionapproachisconceptuallysimple,itsuffers
from several drawbacks. First, itisvulnerable tofailures ofthecentral coordinator. Hence
special provision for handling such faults have to be made. One approach is to provide a
back..upcentral coordinatorthatduplicates thejobofthecentral coordinator. Second, the
centralizedcoordinatorcanconstituteaperformancebottleneckinlargesystems havingtoo
many sites. Third, the centralized coordinator may detect false deadlocks. Below we
illustrate with a simple example how the algorithm may lead to the detection of false
deadlocksandthenwedescribeamethod toovercomethisthirddrawback.
Let us consider the same system configuration as that of Figure 6.15 and this time
supposethat the three processes (PI' P 2,P 3) competefor the three resources (R 1,R 2,R 3)
in the following manner:
Step 1: PI requests for RI and RI is allocated to it.
Step 2: P2 requests for R and R is allocated to it.
2 2
Step 3: P requests for R and R is allocated to it.
3 3 3
Step 4: P
2
requests for R1and waits for it.

Sec. 6.5 • Deadlock 323
Step 5: P3 requests for Rzand waits for it.
Step6: PI releases R
1
andR, is allocated to Pz
Step 7: PI requests for R
3
and waits for it.
Assuming that the method of continuous transfer is employed by the algorithm, the
following sequence of messages will be sent to the central coordinator:
ml: from site SI to add the edge (RI, PI)
m2:from site SI to add the edge (Rz,Pz)
m-: from site S2to add the edge (R 3, P 3)
m4:from site SI to add the edge (P2' RI)
mi: from site SI to add the edge (P 3, Rz)
m.;from site SI to delete edges (R I, PI) and (Pz,R 1), and add edge (R), P2)
m7:from site 5zto add edge (PI' R3)
The resource allocation graphs maintained by the local coordinators of the two sites
and the central coordinator are shown in Figure 6.16 (for clarity of presentation, the
resource allocation graphs are shown instead of the WFGs). Figure 6.16(a) shows the
graphs after step 5,that is,after message mshas been receivedby thecentral coordinator,
and Figure 6.16(b) shows the graphs after message m-,has been received by the central
coordinator. The graph of the central coordinator in Figure 6.16(b) has no cycles,
indicating thatthesystem isfreefromdeadlocks. However, suppose thatmessage m7from
site Sz isreceived before message m6from site 51 bythecentral coordinator. In thiscase,
the central coordinator's view of the system will be as shown in the resource allocation
graph of Figure 6.16(c). Therefore, the central coordinator will incorrectly conclude that
a deadlock has occurred and may initiate deadlock recovery actions. Although the above
example shows the possibility of detection of phantom deadlocks when the method of
continuous transfer of information is used, phantom deadlocks may even get detected in
the other two methods of information transfer due to incomplete or delayed
information.
One method to avoid the detection of false deadlocks is to use Lamport's
algorithm to append a unique global timestamp with each message. In our above
example, since message m- from site Sz to the central coordinator is caused by the
request from site SI (see step 7), message m- will have a later timestamp than
message m6' Now if the central coordinator receives message m- before m6 and
detects a false deadlock, before taking any action to resolve the deadlock, it first
confirms if the detected deadlock is a real one. For confirmation, it broadcasts a
message asking all sites if any site has a message with timestamp earlier than T for
updation of the global WFG. On receiving this message, if a site has a message with
timestamp earlier than T, it immediately sends it to the central coordinator; otherwise
it simply sends a negative reply. After receiving replies from all the sites, the central
coordinator updates the global WFG (if there are any update messages), and if the
cycle detected before still exists, it concludes that the deadlock is a real one and

324 Chap. 6 • Synchronization
Resourceallocation Resourceallocation Resourceallocation
graphofthelocal graphofthelocal graphmaintainedby
coordinatorofsite8 coordinatorofsite52 thecentralcoordinator
1
(a)
R
Site51 Site52 Centralcoordinator
(b)
Centralcoordinator
(c)
Fig.6.16 Localand globalresource allocation graphs inthecentralized deadlock
detection approach: (a) resource allocationgraphs afterstep5;(b) resource
allocation graphs afterstep7;(c) resource allocationgraphofthecentral
coordinator showing falsedeadlock ifmessagem7isreceived beforem6by
thecentralcoordinator.
initiates recovery actions. Notice that in our above example, in reply to its broadcast
message, the central coordinator will receive message m6 from site S, and a negative
reply from site S2. Therefore, after final updation of the global graph, the central
coordinator's view of the system will change from that of Figure 6.16(c) to that in
Figure 6.16(b). Hence no deadlock resolution action will be initiated.
HierarchicalApproachfor DeadlockDetection. Ithasbeenobservedthatfor
typical applications most WFG cycles are very short. In particular, experimental
measurements have shown that 90% of all deadlock cycles involve only two processes
[Grayetal. 1981].Therefore,thecentralizedapproachseemstobelessattractiveformost
real applications because of the significant time and message overhead involved in

Sec.6.5 • Deadlock 325
assembling all the local WFGs at the central coordinator. Furthermore, to minimize
communicationscost, in geographically distributed systems, deadlock should be detected
by a site located as close as possible to the sites involved in the cycle. But this is not
possibleinthe centralizedapproach.The hierarchicalapproachovercomestheseand other
drawbacks ofthe centralized approach.
The hierarchical deadlock detection approach uses a logical hierarchy (tree) of
deadlock detectors. These deadlock detectors are called controllers. Each controller is
responsiblefor detectingonly thosedeadlocksthat involvethe sites falling withinitsrange
in the hierarchy. Therefore, unlike the centralized approach in which the entire global
WFG is maintained at a single site, in the hierarchical approach it is distributed over a
number of different controllers. Each site has its own local controller that maintains its
own local graph.
In the tree representing the hierarchy ofcontrollers, the WF(i to be maintained by a
particular controller is decided according to the following rules:
1. Each controllerthat forms aleafofthe hierarchy tree maintains the local WFGof
a single site.
2. Each nonleaf controller maintains a WFG that is the union of the WFGs of its
immediate children in the hierarchy tree.
The lowestlevel controllerthat finds acyclein itsWFGdetectsadeadlockand takes
necessaryactionto resolveit.Therefore,aWFG that containsacyclewill neverbepassed
as it is to a higher level controller.
Let usillustratethe methodwith the helpofthe exampleshowninFigure6.17.There
are four sites and sevencontrollers inthe system. ControllersA, B, C, and D maintain the
local WFGs of sites 51' 52, 53' and 54, respectively. They form the leaves of the
controllers' hierarchy tree.ControllerE,being theparent ofcontrollersAandB,maintains
the unionofthe WFGsofcontrollersA and B.Similarly,controllerF maintains the union
of the WFGs of controllers C and D. Finally, controller G maintains the union of the
WFGs of controllers E and F.
Notice from the figure that the deadlockcycle (PI' P3' Pz,PI)that involves sites S,
and Szgets reflected in the WFG ofcontroller E,but the deadlock cycle (P4' Ps, P 6, P7'
P 4) that involves sites S2' S3' and 54gets reflectedonly in the WFGofcontrollerG. This
is because controllerG is the first controlJerin the hierarchy in whose range all the three
sites 8 2, S3' and 8 4 are covered. Also notice that although we have shown the deadlock
cycle (p), P 3, P2' P, in the WFG ofcontroller G to reflect the union ofthe WFGs of
controllersEand F,this will neverhappen in practice. This is because, whencontrollerE
detects the deadlock, it will initiate arecovery action instead ofpassing its WFG as it is
to controller G.
Fully Distributed Approaches for Deadlock Detection. In the fully dis
tributed deadlock detection approach, each site ofthe system shares equal responsibility
for deadlockdetection. Surveysofseveralalgorithmsbasedon this approachcanbefound
in [Knapp 1987, Singhal 1989]. Below we describe two such algorithms. The first one is
based on the construction ofWFGs, and the second one is a probe-based algorithm.

326 Chap.6 • Synchronization
ControllerF
Site8 Site8 Site8 Site54
1 2 3
Controller A Controller B Controller C Controller0
Fig.6.17 Hierarchical deadlock detection approach.
WFG-Based Distributed Algorithm for Deadlock Detection. The descrip
tion below follows from the description of the fully distributed deadlock detection
algorithm presented in (Silberschatz and Galvin 1994]. As in the centralized and
hierarchical approaches, in the WFG-based distributed algorithm, each site maintains its
own local WFG. However, to model waiting situations that involve external (nonlocal)
processes, a slightlymodifiedform ofWFG is used. In this modifiedWFG, anextranode
P is added to the local WFG ofeach site, and this node isconnected to the WFG ofthe
ex
corresponding site in the following manner:
1. An edge (Ph P ex) is added if process Piis waiting for a resource in another site
being held by any process.
2. An edge (Pex' P j) is added if P j is a process ofanother site that is waiting for a
resource currently being held by a process ofthis site.
To illustrate the construction ofthis modified WFG, let us consider the example of
Figure6.18. Inthisexampletherearetwosites, andthelocalWFGsofeachsiteareshownin
Figure6.18(a).ThemodifiedWFGsofthetwositesaftertheadditionofnodePexareshown
inFigure6.18(b).The explanationfortheedgesinvolvingnode P areasfollows:
ex

Sec.6.5 • Deadlock 327
(a)
Site8,
(b)
Fig.6.18 Example illustratingthe
WFG-basedfully distributed
deadlockdetection algorithm:
(u) local WFGs; (b) local WFGs
after additionofnode P ex;
(c) updated local WFG of site 52
after receivingthe deadlock
detection message from site 5I' (c)
I. In the WFG of site Sf, edge (P" j>ex) isadded because process P, is waiting for
a resource in site 52that is held by process P3' and edge (P ex' P3) is added
because process P3 is a process of site 52that is waiting to acquire a resource
currently held by process Pzof site S,.
2. In the WFG of site S2' edge (P 3, P ex) is added because process P3is waiting for
a resource in site 5) that is held by process P2' and edge (Pex' PI) is added
because process PI is a process of site SI that is waiting to acquire a resource
currently held by process P3 of site 52'
Now these modified WFGs are usedfordeadlock detection inthefollowing manner.
Ifa local WFG contains acycle that does not involve node P ex' adeadlock that involves
only local processes of that site has occurred. Such deadlocks can be locally resolved
without the need to consult any other site.
On the other hand, if a local WFG contains a cycle that involves node Pex' there is
apossibility ofadistributed deadlock thatinvolves processes ofmultiple sites.Toconfirm
a distributed deadlock, a distributed deadlock detection algorithm is invoked by the site

328 Chap. 6 • Synchronization
whose WFG contains the cycle involving node P ex• The algorithm works as described
below.
Suppose acycleinvolving nodeP isdetected intheWFGofsiteSj.Thiscycle must
ex
be ofthe form
whichmeans thatprocessP iswaiting foranexternal resource thatbelongs tosomeother
k
site(saySj).Therefore, siteSjsendsadeadlock detection message tositeSj.This message
does notcontain thecomplete WFGofsiteS,butonly thatpartoftheWFG thatforms the
cycle. Forinstance, inourexample ofFigure 6.18, ifsiteSI detects itscycle first,itsends
a message like (P ex' P 3, P 2, P" P ex) to site S2 since PI is waiting for a resource in site
S2·
Onreceiving themessage, siteS,updates itslocalWFGbyadding thoseedgesofthe
cycle thatdonotinvolvenodeP
ex
toitsWFG.Thatis,inourexample, edges (P 3,P2)and
(P 2, PI) will be added to the local WFG of site S2'resulting in the new WFG of Figure
6.18(c).
Now if the newly constructed WFG of siteSjcontains a cycle that does not involve
node P ex' adeadlock exists and an appropriate recovery procedure must be initiated. For
instance, inour example, the newly constructedWFG of siteS2 contains acycle (P J, P 3,
P 2,P J) thatdoesnotinvolvenodePexo Hence, inourexample, thesystemisinadeadlock
state.
On the other hand, if a cycle involving node P is found in the newly constructed
ex
WFGofsiteSj' S,sendsadeadlock detection message totheappropriate site(saySs), and
the whole procedure is repeated by site Sko In this manner, after a finite number of
deadlock detection message transfers from one site to another, either a deadlock is
detected or the computation for deadlock detection halts.
A problem associated with the above algorithm is that two sites may initiate the
deadlock detection algorithm independently for a deadlock that involves the same
processes. For instance, in our example of Figure 6.18, sites Sl and S2 may almost
simultaneouslydetect thecycles (P ex' P 3,P 2,PI' P ex) and(P ex' P J, P 3,P ex) respectively
in their local WFGs, and both may send a deadlock detection message to the other site.
The result will bethatboth sites willupdate theirlocalWFGs andsearch forcycles.After
detecting a deadlock, both may initiate a recovery procedure that may result in killing
more processes than is actually required to resolve the deadlock. Furthermore, this
problem also leads toextra overhead inunnecessary message transfers andduplication of
deadlock detection jobs performed at the two sites.
One way to solve theabove problem is toassign a unique identifier to each process
Pi [denoted as ID(Pi)]. Now when acycle of theform (P ex9 Pi' Pj' ..0'Pi, P ex) is found
in the local WFG of a site, this site initiates the deadlock detection algorithm bysending
a deadlock detection message to the appropriate site only if
Otherwise, this site does not take any action and leaves thejob of initiating thedeadlock
detection algorithm to some other site.

Sec.6.5 • Deadlock 329
Let us apply the modified algorithm to our example of Figure 6.18. Let
Now suppose both sites SI and Sz almost simultaneously detect the cycles (P ex' P 3,
r;
P 1, P e,.) and (P ex' P J, P 3, P ex), respectively, in their local WFGs. Since ID(P 1) <
ID(P3), so siteSIwill initiate the deadlock detection algorithm. On theother hand, since
ID(P 3) > ID(P,), site Sz does not take any action on seeing the cycle in its local WFG.
When site S2receives the deadlock detection message sent to it by site SI' it updates its
localWFGandsearches foracycle in the updatedWFG. Itdetects thecycle (PI,P 3,Pz,
PI) in the graph and then initiates a deadlock recovery procedure.
Probe-Based Distributed Algorithm for Deadlock Detection. The probe
baseddistributed deadlock detection algorithm described below wasproposed byChandy
etal.[1983]andisknownastheChandy-Misra-Hass(oreMH)algorithm.Itisconsidered
tobethebestalgorithm todate fordetecting global deadlocks indistributed systems.The
algorithm allows a process to request for multiple resources at a time.
The algorithm is conceptually simple and works in the following manner.When a
process that requests for a resource (or resources) fails to get the requested resource (or
resources) and times out, itgenerates aspecialprobemessage andsends it totheprocess
(orprocesses) holding the requested resource (or resources). The probe messagecontains
the following fields (assuming that each process in the system is assigned a unique
identifier):
1. The identifier of the process just blocked
2. The identifier of the process sending this message
3. The identifier of the process to whom this message is being sent
Onreceiving aprobe message,therecipient checks toseeifititselfiswaitingforany
resource (orresources). Ifnot,thismeansthattherecipient isusingtheresourcerequested
by theprocess that sent the probe message to it. In thiscase, the recipient simply ignores
the probe message. On the other hand, if the recipient is waiting for any resource (or
resources), itpasses theprobe message to the process (orprocesses) holding the resource
(or resources) for which it is waiting. However, before the probe message is forwarded,
the recipient modifies its fields in the following manner:
1. The first field is left unchanged.
2. The recipient changes the second field to its own process identifier.
3. The third field is changed to the identifier of the process that will be the new
recipient of this message.
Every new recipient of the probe message repeats this procedure. If the probe
messagereturns backtotheoriginal sender(theprocess whoseidentifierisinthefirstfield
of the message), a cycle exists and the system is deadlocked.

330 Chap. 6 • Synchronization
Let us illustrate the algorithm with the help of the simple example shown in Figure
6.19. Notice that this figure depicts the same situation as thatof Figure 6.18(a) but in a
slightly different style. Suppose that process PI gets blocked when it requests for the
resource held by process P3. Therefore PI generates a probe message (PI' PI, P3) and
sends it to P3.When P receives this message, it discovers that it is itself blocked on
3
r;
processes P 2 and Therefore P3forwards the probes (PI' P3'P 2) and (Pit P3'Ps)to
processes P2 and P s, respectively. When P s receives the probe message, it ignores it
because it is not blocked on any other process. However, when P receives the probe
2
message, it discovers that it is itself blocked on processes PI and P4· Therefore P2
forwards the probes (Ph P 2, P J) and (Pit P 2, P 4) to processes PI and P 4, respectively.
Since the probe returns to its original sender (PI)' a cycle exists and the system is
deadlocked.
Fig.6.19 Example illustratingtheeMH
distributeddeadlock detection
Site~
algorithm.
The CMH algorithm is popular, and variants of this algorithm are used in most
distributed locking schemes due to the following attractive features of the algorithm:
1. The algorithm is easy to implement, since each message is of fixed length and
requires few computational steps.
2. The overhead of the algorithm is fairly low.
3. There is no graph constructing and information collecting involved.
4. False deadlocks are not detected by the algorithm.
5. It does not require anyparticular structure among the processes.
Ways for Recovery from Deadlock
Whenasystemchooses tousethedetection andrecoverystrategyforhandlingdeadlocks,
it is not sufficient to simply detect deadlocks. The system must also have some way to
recoverfrom adetecteddeadlock. Oneofthefollowing methods maybeusedinasystem
to recover from a deadlock:
• Asking for operator intervention
• Termination of process(es)
• Rollback ofprocess(es)

Sec.6.5 • Deadlock 331
Askingfor OperatorIntervention. The simplest way is to inform the operator
thatadeadlockhas occurredandtolet theoperatordeal with itmanually.The systemmay
assist the operatorin decision making for recovery by providing him or her with a list of
the processes involved in the deadlock.
This method is not suitable for usc in modern systems because the concept of
an operator continuously monitoring the smooth running of the system from the
console has gradually vanished. Furthermore, although this method may work for a
centralized system, it does not work in a distributed environment because when a
deadlock involving processes of multiple sites is detected, it is not clear which site
should be informed. If all the sites whose processes are involved in the deadlock are
informed, each site's operator may independently take some action for recovery. On
the other hand, if the operator of only a single site is informed, the operator may
favor the process (or processes) of its own site while taking a recovery action.
Furthermore, the operator of one site may not have the right to interfere with a
process of another site for taking recovery action. Therefore, distributed systems
normally use other methods described below in which the system recovers automati
cally from a deadlock.
Termination ofProcessies). The simplest way to automatically recover from a
deadlock is to terminate (kill) one or more processes and to reclaim the resources held
by them, which can then be reallocated. Deadlock recovery algorithms based on this
idea analyze the resource requirements and interdependencies of the processes involved
in a deadlock cycle and then select a set of processes, which, if killed, can break the
cycle.
Rollback of Process(es). Killing a process requires its restart from the very
beginning, which proves to be very expensive, particularly when the process has already
run forasubstantiallylong time.Tobreak adeadlock, itissufficienttoreclaimthe needed
resources from the processes that were selected for being killed. Also notice that to
reclaim aresource from aprocess, itis sufficientto roll back the process toa point where
the resource was not allocated to the process. The method of rollback is based on this
idea.
In this method, processes are checkpointed periodically.That is, aprocess's state (its
memory image and the list ofresources held by it) is written to a file at regular intervals.
Therefore, the file maintains a history of the process's states so that, if required, the
process can be restarted from any of its checkpoints. Now when a deadlock is detected,
the method described in the process termination approach is used to select a set of
processestobekilled. However,this time, insteadoftotal rollback(killing)oftheselected
processes, the processes are rolled back only as far as necessary to break the deadlock.
Thatis,each selectedprocessisrolled back toacheckpointat which the neededresources
can be reclaimed from it.
Although the rollback approach may appear to be less expensive than the process
terminationapproach, this isnot alwaystrue becauseoftheextraoverheadinvolvedinthe
periodic checkpointing of all the processes. If deadlocks are rare in a system, it may be
cheaper to use the process termination approach.

332 Chap. 6 • Synchronization
Issues in Recovery from Deadlock
Twoimportantissuesintherecovery action areselection ofvictimsanduseoftransaction
mechanism. These are described below.
Selection of Victim(s). In any of the recovery approaches described above,
deadlock is broken bykilling or rolling back one or more processes. These processes are
called victims. Notice that even in the operatorintervention approach, recovery involves
killing one or more victims.Therefore, animportantissueinanyrecovery procedure isto
select the victims. Selection ofvictim(s) is normally based on two major factors:
1. Minimization ofrecoverycost.This factor suggests thatthoseprocesses should be
selected as victims whose termination/rollback will incur the minimum recovery cost.
Unfortunately, it is not possible to have a universal cost function, and therefore, each
system should determine its own cost function to select victims. Some of the factors that
may beconsideredfor this purpose are (a) the priority of the processes; (b) the nature of
the processes, such as interactive or batch and possibility of rerun with no ill effects; (c)
the number andtypesofresources held bytheprocesses; (d)thelength of service already
received and the expected length of service further needed by the processes; and (e) the
total number of processes that will beaffected.
2. Prevention ofstarvation. Ifa system only aims at minimization of recovery cost,
it may happen that the same process (probably because its priority is very low) is
repeatedly selected as a victim and may never complete. This situation, known as
starvation, must be somehow prevented inany practical system. One approach to handle
this problem is to raise the priority of the process every time it is victimized. Another
approach is to include the number of times a process is victimized as a parameter in the
cost function.
Use of Transaction Mechanism. After a process is killed or rolled back for
recoveryfromdeadlock,ithastobererun.However,rerunningaprocessmaynotalwaysbe
safe,especially whentheoperations already performed bytheprocess are nonidempotent.
For example, if a process has updated the amount of a bank account by adding a certain
amount toit,reexecution oftheprocess willresult inadding thesameamount once again,
leaving thebalance intheaccount inanincorrect state.Therefore, the useofatransaction
mechanism (which ensures allornoeffect) becomes almost inevitable formost processes
when the system chooses the method of detection and recovery for handling deadlocks.
However, notice thatthetransaction mechanism need not beusedforthose processes that
canbererunwithnoilleffects.Forexample,rerunofacompilationprocesshasnoilleffects
because allitdoesisreadasourcefileandproduceanobjectfile.
6.6 ElEalON AlGORITHMS
Several distributed algorithms require that there be a coordinator process in the entire
system thatperforms sometypeofcoordinationactivity needed forthesmoothrunningof
other processes in the system. Twoexamples of such coordinatorprocesses encountered

Sec.6.6 • Election Algorithms 333
in this chapter are the coordinator in the centralized algorithm for mutual exclusion and
the central coordinator in the centralized deadlock detection algorithm. Since all other
processes in the system have to interact with the coordinator, they all must unanimously
agree on who the coordinator is. Furthermore, ifthe coordinatorprocess fails due to the
failure ofthe siteon which itis located, anewcoordinatorprocess must beelected totake
up the job of the failed coordinator. Election algorithms are meant for electing a
coordinatorprocess from among the currently running processes in such amanner that at
any instance of time there is a single coordinator for all processes in the system.
Election algorithms are based on the following assumptions:
1. Each process in the system has a unique priority number.
2. Whenever an election is held, the process having the highest priority number
among the currently active processes is elected as the coordinator.
3. On recovery, a failed process can take appropriate actions to rejoin the set of
active processes.
Therefore, whenever initiated, anelection algorithm basically finds out whichof the
currently activeprocesses hasthehighest prioritynumber andtheninformsthistoallother
active processes. Different election algorithms differ in the way they do this. Two such
election algorithms are described below. Readers interested in other election algorithms
may refer to [Tel 1994]. For simplicity, in the description of both algorithms we will
assume that there is only one process on each node of the distributed system.
6.6.1 TheBully Algorithm
Thisalgorithm wasproposedbyGarcia-Molina[1982]. Inthisalgorithm itisassumed that
every process knows the priority number of every other process in the system. The
algorithm works as follows.
When a process (say Pi) sends a request message to the coordinator and does not
receive a reply within a fixed timeout period, it assumes that the coordinator has failed.
Itthen initiates anelection bysending anelection message toevery process withahigher
priority number than itself. If Pi does not receive any response to its election message
within a fixed timeout period, itassumes that among the currently active processes it has
the highest priority number. Therefore it takes up thejob of the coordinator and sends a
message (let us call it a coordinator message) to all processes having lower priority
numbers than itself, informing that from now on it is the new coordinator. On the other
hand, ifPireceives aresponse foritselection message, thismeans thatsomeotherprocess
having higher priority number is alive. Therefore Pidoes not take any further action and
justwaits to receive the final result (a coordinatormessage from the new coordinator) of
the election it initiated.
When a process (say P j) receives an election message (obviously from a process
having alower priority number thanitself), itsends-aresponse message (letuscall italive
message) to the sender informing that it is alive and will take over the election activity.
Now P holds anelection ifitisnot already holding one. Inthis way,theelection activity
j
gradually moves on to the process that has the highest priority number among the

334 Chap. 6 • Synchronization
currently active processes and eventually wins the election and becomes the new
coordinator.
As part of the recovery action, this method requires that a failed process (say P k)
mustinitiate anelectiononrecovery.Ifthecurrentcoordinator'spriority number ishigher
than that of Pk»then thecurrent coordinatorwill win the election initiated by Pkand will
continue to be the coordinator. On the other hand, if Pk'spriority number is higher than
that of the current coordinator, it will not receive any response for its election message.
So it wins the election and takes over the coordinator's job from the currently active
coordinator. Therefore, theactive process having thehighest priority number always wins
theelection. Hencethealgorithm iscaJledthe"bully"algorithm. Itmayalsobenotedhere
that if the process having the highest priority number recovers after a failure, itdoes not
initiate an election because it knows from its list of priority numbers that all other
processes in the system have lower priority numbers than that of its own. Therefore, on
recovery, it simply sends a coordinator message to all other processes and bullies the
current coordinator into submission.
Let us now see the working of this algorithm with the help of an example. Suppose
thesystem consists offiveprocesses PI,P2' P3' P 4,andPsandtheirpriority numbers are
1,2,3,4, and 5respectively.Also suppose thatataparticular instance of time the system
is in a state in which P 2 iscrashed, and PI' P 3, P 4, and Ps are active. Starting from this
state, the functioning of the bully algorithm with thechanging system states is illustrated
below.
1. Obviously, Psis the coordinator in the starting state.
2. Suppose P5crashes.
3. Process P
3
sends a request message toPsand does not receive a reply within the
fixed timeout period.
4. Process P
3
assumes that Ps has crashed and initiates an election by sending an
election message to P4 and Ps (recall that an election message is sent only to
processes with higher priority numbers).
5. When P
4
receives P3 's election message, it sends an alive message to P3'
informing thatitisaliveandwilltakeover theelection activity.ProcessPscannot
respond to P 'selection message because it is down.
3
6. Now P4 holds an election by sending an election message to Ps-
7. Process Ps does not respond to P
4
's election message because it is down, and
therefore, P
4
winstheelection andsendsacoordinatormessage toPI'P 2,andP3,
informing them that from now on it is the new coordinator. Obviously, this
message is not received by P because it is currently down.
2
8. Now suppose P recovers from failure and initiates an election by sending an
2
election message toP3' P 4, and Ps.Since P 2 'spriority number is lower than that
of P (current coordinator), P4 will win the election initiated by P and will
4 2
continue to be the coordinator.
9. Finally,supposePsrecovers fromfailure. Since Psistheprocess withthehighest
priority number, itsimply sends acoordinatormessage toPI, P2' P 3, andP 4 and
becomes the new coordinator.

Sec. 6.6 • Election Algorithms 335
6.6.2 ARing Algorithm
The following algorithm is based on the ring-based election algorithms presented in
[Tanenbaum 1995,Silberschatzand Galvin 1994]. In this algorithm it is assumed that all
theprocessesinthe system areorganizedinalogical ring.The ring isunidirectionalinthe
sense that all messages related to the election algorithm are always passed only in one
direction (clockwise/anticlockwise). Every process in the system knows the.structure of
the ring, so that while trying to circulate a message over the ring, if the successorof the
senderprocess isdown, the sender can skip over the successor, or the one after that, until
an active member is located. The algorithm works as follows.
When aprocess (say Pi) sends arequestmessagetothecurrentcoordinatoranddoes
not receive a reply within a fixed timeout period, it assumes that the coordinator has
crashed. Therefore it initiates an election by sending an election message to its successor
(actually to the first successorthat iscurrently active). This message contains the priority
number of process Pi' On receiving the election message, the successorappends its own
priority number tothe message and passes itontothenext active memberinthering.This
member appends its own priority number to the message and forwards it to its own
successor. In this manner, the election message circulates over the ring from one active
process to another and eventually returns back to process Pi. Process Pirecognizes the
message as its own election message by seeing that in the list ofpriority numbers held
within the message the first priority number is its own priority number.
Note that when process Pireceives its own election message, the message contains
the list of priority numbers of all processes that are currently active. Therefore of the
processes in this list, it elects the process having the highest priority number as the new
coordinator. Itthen circulates acoordinator message over the ring to inform all the other
active processes who the new coordinator is.When the coordinator message comes back
to process Piafter completing its one round along the ring, it is removed by process Pi.
At this point all the active processes know who the current coordinator is.
When a process (say P j) recovers after failure, it creates an inquiry message and
sends itto itssuccessor. The message contains the identity of process P j•Ifthe successor
isnotthecurrentcoordinator, itsimply forwards theenquiry message toitsown successor.
In this way,theinquiry message moves forward along the ring until itreaches the current
coordinator. On receiving an inquiry message, the current coordinator sends a reply to
process P, informing that it is the current coordinator.
Notice that in this algorithm two or more processes may almost simultaneously
discover that the coordinator has crashed and then each one may circulate an election
message over the ring.Although this resultsinalittle wasteofnetworkbandwidth, itdoes
not cause any problem because every process that initiated an election will receive the
same list of active processes, and all of them will choose the same process as the new
coordinator.
6.6.3 Discussion of theTwo Election Algorithms
In the bully algorithm, when the process having the lowest priority number detects the
coordinator's failure and initiates an election, in a system having total n processes,

336 Chap. 6 • Synchronization
altogether n-2 elections are performed one after another forthe initiated one.That is, all
the processes, except the active process with the highest priority number and the
coordinator process that has just failed, perform elections by sending messages to all
processes with higher priority numbers. Hence, in the worst case, the bully algorithm
requires O(n2 ) messages. However, when the process having the priority number just
below the failed coordinator detects that the coordinator has failed, it immediately elects
itself as thecoordinator andsends n-2coordinatormessages. Hence, inthebestcase, the
bully algorithm requires only n-2 messages.
On the other hand, in the ring algorithm, irrespective of which process detects the
failure of the coordinator and initiates an election, an election always requires 2(n-l)
messages (assuming that only the coordinator process has failed); n-l messages are
needed for one round rotation of the election message, and another n-l messages are
needed for one round rotation of the coordinator message.
Nextletusconsiderthecomplexity involvedintherecoveryofaprocess. Inthebully
algorithm, a failed process must initiate an election on recovery. Therefore, once again
dependingontheprioritynumberoftheprocessthatinitiatestherecoveryaction,thebully
algorithm requires O(n2 ) messages in the worst case, and n-l messages in the best case.
On the other hand, in thering algorithm, a failed process does not initiate anelection on
recovery but simply searches for the current coordinator. Hence, the ring algorithm
requires only nl2messages on an average for recovery action.
In conclusion, as compared to the bully algorithm, the ring algorithm is more
efficient and easier to implement.
6.7 SUMMARY
Sharing system resources among multiple concurrent processes may be cooperative or
competitive in nature. Both cooperative and competitive sharing require adherence to
certain rules of behavior that guarantee that correct interaction occurs. The rules for
enforcingcorrectinteractionareimplementedintheformofsynchronization mechanisms.
Inthischapter wesawthe synchronization issues indistributed systems and mechanisms
to handle these issues.
For correct functioning of several distributed applications, the clocks of different
nodes ofadistributed system mustbemutually synchronized as wellas with theexternal
world (physical clock). Clock synchronization algorithms used indistributed systemsare
broadly classified into two types-centralized and distributed. In the centralized
approach, there is a time server node and the goal of the algorithm is to keep the clocks
of all other nodes synchronized with the clock time of the time server node. In the
distributed approach,clocksynchronizationisdoneeitherbyglobalaveragingorlocalized
averaging of the clocks of various nodes of the system.
Lamport observed that for most applications clock synchronization is not required,
anditissufficienttoensurethatalleventsthatoccurinadistributed systemcanbetotally
orderedinamannerthatisconsistentwithanobserved behavior.Therefore, hedefinedthe
happened-before relation and introduced the concept of logical clocks for ordering of
events based on the happened-before relation. The happened-before relation, however,is

Chap. 6 • Exercises 337
only a partial ordering on the setof allevents inthe system. Therefore, for total ordering
on the set of all system events, Lamport proposed the use of any arbitrary total ordering
of the processes.
There areseveral resources inasystem for whichexclusiveaccess byaprocess must
be ensured. This exclusiveness of access is called mutual exclusion between processes,
and the sections of a program that need exclusive access to shared resources are referred
to as critical sections. The three basic approaches used by different algorithms for
implementing mutual exclusion in distributed systems are centralized, distributed, and
token passing. In the centralized approach, one of the processes in the system is elected
as the coordinator, which coordinates the entry to the critical sections. In the distributed
approach, all processes that \vant to enter the same critical section cooperate with each
other before reaching a decision on which process will enter the critical section next. In
the token-passing approach, mutual exclusion is achieved by using a single token that is
circulated among the processes in the system.
Deadlock is the state of permanent blocking of a set of processes each of which is
waiting for anevent thatonly another process inthe setcan cause. Inprinciple, deadlocks
indistributed systems are similar todeadlocks incentralizedsystems. However, handling
of deadlocks indistributed systems is more complex than in centralized systems because
the resources, the processes, and other relevant information are scattered on different
nodes of the system.
The three commonly used strategies to handle deadlocks are avoidance, prevention,
anddetection andrecovery.Deadlock avoidance methods use someadvance knowledge of
the resource usage of processes to predict the future state of the system for avoiding
allocations that can eventually lead to a deadlock. Deadlock prevention consists of
carefully designing the system sothat deadlocks become impossible. Inthedetection and
recovery approach, deadlocks are allowed to occur, are detected by the system, and then
are recovered. Of the three approaches, detection and recovery is the recommended
approach for handling deadlocks in distributed systems. The three approaches used for
deadlock detection in distributed systems are centralized, hierarchical, and fully
distributed. Forrecovery fromadetected deadlock, asystem mayuseoneofthefollowing
methods: asking for operator intervention, termination of process(es), or rollback of
process(es).
Several distributedalgorithms requirethatthere beacoordinatorprocess intheentire
system. Election algorithms arc meant for electing acoordinatorprocess from among the
currently running processes. Two election algorithms that were described in this chapter
are the bully algorithm and the ring algorithm.
EXERCISES
6.1. Write pseudocode for an algorithm that decides whether a given set of clocks are
synchronized or not. What input parameters are needed in your algorithm?
6.2. How do clock synchronization issues differ in centralized and distributed computing
systems?

338 Chap. 6 • Synchronization
6.3. Explain why one-timesynchronizationof the clocksofall the nodes of adistributed system
is not sufficient and periodic resynchronization is necessary. How will you determine the
interval for periodic resynchronization?
6.4. Adistributedsystem has three nodes N., N 2, andN 3,each having itsown clock. The clocks
ofnodes N), N 2, and N 3 tick 800,810, and 795times per millisecond. The system uses the
external synchronizationmechanism, in which all three nodes receivethe real time every 30
seconds from an external time source and readjust their clocks. What is the maximum clock
skew that will occur in this system?
6.5. Differentiate between internal synchronization and external synchronization ofclocks in a
distributed system. Externally synchronized clocks are also internally synchronized, but the
converse is not true. Explain why.
6.6. An importantissue inclock synchronizationincomputersystemsisthat time must never run
backward. Give two examplesto show why this issue is important. How can afast clock be
readjusted to take care ofthis issue?
6.7. Indistributedsystems, there may be unpredictablevariation inthe messagepropagationtime
between two nodes. Explain why. How does this problem make the task of synchronizing
clocks in a distributed system difficult? Give two methods that can be used in a clock
synchronization algorithm to handle this problem.
6.8. Give twoexamplestoshow that formostdistributedapplications, what usuallymatters isnot
that all processes agree on exactly what time it is, but rather that they agree on the orderin
which events occur.
6.9. Using the space-timediagram of Figure 6.20, listall pairs of concurrentevents accordingto
the happened-before relation.
}'ig.6.20 Aspace-time diagram.
6.10. Add a message-sendingevent to the space-timediagram of Figure 6.20 that isconcurrentto
eventses, e6' and e7' Now add anon-message-sendingevent that isconcurrenttoeventse),
e2' and e3'
6.11. Explaintheconceptof logical clocks and their importanceindistributedsystems.Aclock of
a computer system must never run backward. Explain how this issue can be handled in an
implementation of the logical clocks concept.

Chap. 6 • Exercises 339
6.12. In the centralized approach to mutual exclusion described in this chapter, the coordinator
grants permission forcritical sectionentry tothefirst processinthe queue. Insome systems,
itmay bedesirabletogrant permissiontosome higher priorityjobsbeforeotherlower priority
jobs.Modifythe algorithmtotake care of thisand show how youraJgorithm satisfiesthe00
starvation property.
6.13. The first general algorithm for implementing mutual exclusion in adistributedenvironment
was developed by Lamport (1978). Find the details of this algorithm and compare its
performance and reliability with that of Ricart and Agrawala's [1981] algorithm.
6.14. Write pseudocode for the majority-consensus-based distributed algorithm for mutual
exclusion in which aprocess willing toenteracritical region sends a request message to all
otherprocessesandentersthecriticalregion assoon asitreceivespermissionfrom amajority
of the processes.
6.15. What isa"deadlock"?What are the four necessaryconditionsfor adeadlocktooccur?Give
suitable examples to prove that if anyone of the four conditions is absent, no deadlock is
possible.
6.16. Prove that thepresenceofacycle inageneral resourceallocationgraph isanecessarybutnot
a sufficient condition for the existence of deadlock.
6.17. Prove that for asystemhavingonly one unitofeach resource type the presenceof acycle in
aresourceallocationgraph isboth anecessary and asufficientconditionfor theexistenceof
deadlock.
6.18. Write the pseudocode of an algorithm that determines whether a given resource allocation
graph contains a deadlock.
6.19. Asystem hasthree types ofresources,R 1,Rz,andR 3,and their numbersofunits are3,2,and
2,respectively. Four processesPI'Pz,P 3,andP
4
arecurrentlycompetingforthese resources
in the following manner:
(a) PI is holding one unit of RI and is requesting for one unit of R 2.
(b) P2 is holding two units of R 2 and is requesting for one unit each of RI and R 3•
(c) P 3 is holding one unit of R J and is requesting for one unit of R 2.
(d) P4 is holding two units of R3and isrequesting for one unit of RI•
Determine which, if any, of the processes are deadlocked in this system state.
6.20. A distributed system uses the following IPC primitives:
(a) send(receiver_process_id, scnderprocessjd, message)
(b) receive (sender_process_id, message)
The primitivesare synchronous inthe sense that the senderblocksifthe receiverisnotready
to receive the message and the receiver blocks until a message is received from the sender.
What isthe minimum numberofcommunicatingprocessesfor acommunicationdeadlockto
occurinthis system?Give reasons for your answerand give anexampleofacommunication
deadlock that involves a minimum number of processes.
6.21. Assume that in the distributed system ofExercise6.20 an IPe takes place only between two
types of processes--clients and servers. That is, one is always a client and the other is a
server.AclientalwaysexecutestheIPCprimitivesin(send,receive)sequence.That is,forall
IPes, aclientfirst executesasendto send arequest messagetoaserverand then executesa
receive toreceivethereply foritsrequest. Ontheotherhand,aserveralways executestheIPC
primitives in (receive,send) sequence. That is, for all IPCs, a serverfirst executes a receive
to receive a request from a client and then, after processing, it executes a send to send the
result of processingtothe client. Isacommunicationdeadlockpossible inthis system?Give
reasons for your answer.

340 Chap. 6 • Synchronization
6.22. Differentiateamongsafe, unsafe,anddeadlock states.Assumethatinasystemthere aretotal
10unitsofaresourceforwhichfourprocessesPI'P 2,P 3,andP
4
arecompeting. Supposethe
maximumunitsoftheresourcerequiredbyPI'P 2,P 3,andP
4
are3,6,5,and4,respectively,and
theyarecurrentlyholding2,1,3,and2unitsoftheresource,respectively.Findoutwhetherthe
currentstateofthesystemissafeorunsafe.Ifitissafe,enumerateaJlthesafesequences.
6.23. There arefour unitsofaresource inasystem.Three processes compete to usethisresource,
each of which needs at most two units. Prove that a deadlock situation will never arise no
matter in which order the processes acquire and release the resource units.
6.24. Prove that an unsafe state is not adeadlock state.
6.25.Give an example to show that if the resources are not cleverly scheduled to the competing
processes, asystem mayenter an unsafe state from asafe state. Now use the same example
to show the following:
(a) The system may enter adeadlock state from the unsafe state.
(b) All competing processes may successfully complete without the system entering into a
deadlock state from the unsafe state.
6.26. Discuss why advance knowledge of the resource usage of processes is essential to avoid
deadlocks. Why is the deadlock avoidance strategy never used in distributed systems for
handling deadlocks?
6.27. Deadlocksmaybeprevented inasystembycarefully designing thesystemsothatatleastone
of the necessary conditions for deadlock is never satisfied. Based on this idea, suggest a
deadlock prevention scheme for each of the following:
(a) That denies the mutual-exclusion condition
(b) That denies the hold-and-wait condition
(c) That denies thecircular-waitcondition
(d) That denies the no-preemption condition
Discuss the practical applicability of each of these schemes.
6.28. Prove that the following resource allocation policies prevent deadlocks:
(a) Ordered requests
(b) Collectiverequests
6.29. Asystem usesthepreemption method fordeadlock prevention. Suppose thesystemcurrently
has fivetransactions T), T 2, T 3, T 4,and T«,theirtimestamp values being11' 12,13, t 4,and Is,
respectively (t) > 12 > 13> 14 > Is). Explain what happens if:
(a) The system uses the wait-die scheme and T
2
requests fora resource held by Ts.
(b) The system uses the wait-die scheme and T 4 requests for a resource held by T J•
(c) The system uses the wait-wound scheme and T 3 requests for a resource held by T 4•
(d) The system uses the wait-wound scheme and Tsrequests for a resource held by T 2•
6.30. What is a phantom deadlock? What might be the reason for phantom deadlocks in a
distributed system? Suppose that in the centralizeddeadlock detection scheme described in
this chapter the transfer-on-request method is used to transfer information from local
coordinatorstothecentralcoordinator. Give anexample toshowthatthealgorithm maystill
detect aphantom deadlock.
6.31. Acentralizeddeadlockdetectionalgorithmthatdoesnotdetectfalsedeadlocks wasdeveloped
by Stuart et al. [1984]. Find out how this algorithm prevents the detection of false
deadlocks.
6.32. Write pseudocode for the probe-based distributed algorithm for deadlock detection in a
distributed system. What are the main advantages of this algorithm over a WFG-based
distributed algorithm?

Chap. 6 • Bibliography 341
6.33. What problemsmayarise when aprocess iskilled/rolledback and then restartedastheresult
ofadeadlock?Suggestsuitablemethodsto handle these problems. What conclusionscan be
drawn from these problems in connection with the proper selection of victims for recovery
from adeadlock state?
6.34. What are the main issues involved in the selection of victims for recovery from a detected
deadlock? Suggesta suitable victim selectionalgorithm. How does your algorithmtake care
of a starvation problem?
6.35. Why are election algorithms normally needed in a distributed system? A LAN-based
distributed system has broadcast facility. Suggest asimple election algorithm for use in this
system.
6.36. Initiation of an election is actually needed only when the current coordinator process fails.
However, this is not the case in the bully algorithm, in which an election is also initiated
wheneverafailedprocess recovers. Isthisreally necessary?Ifyes,explain why.Ifno,suggest
a modification to the bully algorithm inwhich an ejection is initiated only when the current
coordinator fails.
6.37. Inthering-basedelectionalgorithmdescribedinthischapter, a unidirectional ring was used.
Suppose the ring is bidirectional. Can the election algorithm be made more efficient? If no,
explain why.If yes, suggestsuch analgorithm and comparethe number of messages needed
for electing a coordinator in the two algorithms, assuming that there are n processes in the
system.
6.38. In the ring-based election algorithm described in this chapter, two or more processes may
almost simultaneously discover that the coordinator has crashed and then each one may
circulatean election message over the ring.Although this does notcause any problem inthe
election, it results in waste of network bandwidth. Modify the algorithm so that only one
election message circulates completely round the ring and others are detected and killed as
soon as possible.
6.39. What willhappen inabully algorithm forelectingacoordinatorwhen twoormore processes
almost simultaneously discover that the coordinator has crashed?
818UOGRAPHY
[AdelsteinandSinghal1995]Adelstein,F.,andSinghal,M.,"Real-TimeCausal MessageOrdering
in Multimedia Systems," In: Proceedings ofthe J5th International Conference on Distributed
Computing Systems, IEEE, New York(May-June 1995).
[Agarwal and Abbadi 1991] Agarwal, D., and El Abbadi, A., "An Efficient and Fault-Tolerant
Solution of Distributed Mutual Exclusion," ACM Transactions on Computer Systems, Vol.9,
Association for Computing Machinery, New York,pp. 1-20(1991).
[Badal1986] Badal, D.Z.,"The DistributedDeadlockDetectionAlgorithm," ACMTransactionson
ComputerSystems,Vol.4,No.4,AssociationforComputingMachinery, NewYork,pp.320-337
(1986).
[Barborak et al, 1993] Barborak, M., Malek, M., and Dahbura, A., "The Consensus Problem in
Fault-Tolerant Computing," ACM Computing Surveys, Vol. 25, Association for Computing
Machinery, New York,pp. 171-220(1993).
[Bernstein et al, 1987] Bernstein, P. A., Hadzilacos, V.,and Goodman, N., Concurrency and
Recovery inDatabase Systems, Addison-Wesley, Reading, MA, pp. 289-307 (1987).

342 Chap. 6 • Synchronization
(Bracha and Toueg1984] Bracha, G., and Toueg, S., "A DistributedAlgorithm for Generalized
Deadlock Detection." In: Proceedings ofthe 3rdACMSymposium on Principles ofDistributed
Computing, Association for Computing Machinery, New York,pp. 285-301 (1984).
[Bracha and Toueg 1987] Bracha, G., and Toueg, S., "Distributed Deadlock Detection,"
DistributedComputing, Vol.2, pp. 127-138 (1987).
[Bulgannawar and Vaidya1995JBulgannawar, S., and Vaidya, N. H., "Distributed K-Mutual
Exclusion," In: Proceedings ofthe 15th International Conference on Distributed Computing
Systems, IEEE, New York(May-June ]995).
[Carvalho and Roucairol1983] Carvalho, O.S. F.,and Roucairol, G., "On Mutual Exclusion in
ComputerNetworks," CommunicationsoftheACM, Vol.26, No.2,Association for Computing
Machinery, New York, pp. 146-]47 (1983).
[Chandy and Lamport 1985] Chandy, K. M., and Lamport, L., "Distributed Snapshots:
Determining Global States of Distributed Systems,"ACM Transactions on Computer Systems,
Vol.3, No. I, Association for Computing Machinery, New York,pp. 63-75 (1985).
[Chandy and Misra 1982J Chandy, K. M., and Misra, 1.,"A DistributedAlgorithm for Detecting
Resource Deadlocks in Distributed Systems," In: Proceedings of the ACM Symposium on
Principles ofDistributed Computing, Association for Computing Machinery, New York, pp.
157-164 (August ]982).
[Chandyetale1983JChandy, K.M.,Misra, 1.,and Haas, L.M.,"DistributedDeadlockDetection,"
ACMTransactions on ComputerSystems, Vol. I, No.2,Association for Computing Machinery,
New York,pp. 144-156(1983).
[Choudharyetal, 1989]Choudhary,A. N., Kohler, W.H.,Stankovic,1.A., and Towsley, D., "A
Modified Priority Based Probe Algorithm for Distributed Deadlock Detection and Resolution,"
IEEE Transactions on Software Engineering, Vol.SE-15, No. l, pp. 10-17 (1989).
[Cidonetale 1987] Cidon,I.,Jaffe, J. M.,and Sidi, M., "LocalDistributedDeadlockDetection by
Cycle Detection and Clustering," IEEE Transactions on-Software Engineering, Vol.SE-13, No.
I, pp. 3-14 (1987).
[CotTmanet al, 1971] Coffman,Jr., E. G. Elphick, M. 1.,and Shoshani,A., "System Deadlocks,"
ACMComputing Surveys, Vol.3, No.2,Association for Computing Machinery, New York,pp.
67-78 (1971).
[Coulouris et al. 1994] Coulouris, G. F., Dollimore, J., and Kindberg, T., Distributed Systems
Concepts and Design, 2nd ed., Addison-Wesley, Reading, MA (1994).
[Cristian 1989]Cristian,F.,"ProbabilisticClockSynchronization,"DistributedComputing,Vol.3,
pp. 146-158 (1989).
[Cristian 1991]Cristian,F.,"UnderstandingFault-TolerantDistributedSystems,"Communications
oftheACM, Vol.34, Association for Computing Machinery, New York, pp. 56-78 (1991).
[Cristian and Fetzer 1995] Cristian, F., and Fetzer, C., "Fault-Tolerant External Clock
Synchronization," In: Proceedings of the J5th International Conference on Distributed
Computing Systems, IEEE, New York(May-June 1995).
[Drummond and Babaoglu 1993] Drummond, R., and Babaoglu, 0., "Low-Cost Clock
Synchronization," DistributedComputing, Vol.6, pp. 193-203 (1993).
[Duboisetal,1988]Dubois,M.,Scheurich,C.,andBriggs,F.A.,"Synchronization,Coherence,and
EventOrdering in Multiprocessors," JEEEComputer, Vol.21, pp. 9-21 (1988).

Chap. 6 • Bibliography 343
[Elmagarmid 1986] Elmagarmid, A. K., "A Survey of Distributed Deadlock Detection
Algorithms,"ACMSIGMOD, Vol. 15,No.3, pp.37-45 (1986).
[Fidge 1991]Fidge,C., "LogicalTimein DistributedComputing Systems," IEEE Computer, Vol.
24,pp.28-33 (1991).
[Fredrickson and Lynch 1987] Fredrickson, N., and Lynch, N., "Electing a Leader in a
Synchronous Ring," Journal ofthe ACM, Vol.34, pp. 98-115 (1987).
[Garcia-Molina 1982]Garcia-Molina, H.,"Elections in a Distributed Computing System," IEEE
Transactions on Computers, Vol.C-31, No.1, pp. 48-59 (1982).
[Garg 1996] Garg, V. K., PrincipLes ofDistributed Systems, Kluwer Academic, Norwell, MA
(1996).
[Goscinski 1991] Goscinski, A., Distributed Operating Systems, The Logical Design, Addison
Wesley, Reading, MA (1991).
[Gray et al, 1981]Gray, J. N., Homan, P.,Korth, H. F., and Obermarck, R.L., "A Straw Man
Analysis ofthe ProbabilityofWaiting and DeadlockinDatabaseSystems,"Technical Report RJ
3066, IBMResearch Laboratory, San Jose, CA (1981).
[Gusella and Zatti 1989]Gusella, R., and Zatti, S., "The Accuracy of the Clock Synchronization
AchievedbyTEMPO inBerkeley UNIX 4.3BSD,"IEEE TransactionsonSoftware Engineering,
Vol.SE-15, No.7, pp. 847-853 (1989).
[Jefferson 1985]Jefferson, D. R., "Virtual Time," ACMTransactions on ProgrammingLanguages
and Systems, Vol.7, No.3, pp. 404-425 (1985).
[Knapp 1987] Knapp, E., "Deadlock Detection in Distributed Databases," ACM Computing
Surveys, Vol. 19,No.4, pp. 303-328 (1987).
[Kopetz and Ochsenreiter 1987] Kopetz, H., and Ochsenreiter, W., "Clock Synchronization in
Distributed Real-Time Systems," IEEE Transactions on Computers, Vol. C-36, pp. 933-940
(1987).
[Lamport1978]Lamport, L.,"Time, Clocks, andtheOrderingofEvents inaDistributedSystem,"
Communications ofthe ACM, Vol.21, No.7, pp. 558-565 (1978).
[Lamport 1990]Lamport, L.,"ConcurrentReading andWriting ofClocks,"ACMTransactions on
Computer Systems, Vol.8, pp. 305-310 (1990).
[Lamportand Smith 1984]Lamport,L.,andSmith, P.M.,"ByzantineClock Synchronization,"In:
Proceedingsofthe3rdACMSymposiumonPrinciplesofDistributedComputing,Associationfor
Computing Machinery, New York,pp. 68-74 (1984).
[Lamportand Smith1985]Lamport,L.,and Smith, P.M.,"SynchronizingClocks inthePresence
ofFaults,"JournaloftheACM, Vol.32,No. I,AssociationforComputingMachinery, NewYork,
pp. 52-78 (1985).
[Lee and Kim 1995] Lee, S., and Kim, 1. L., 44An Efficient Distributed Deadlock Detection
Algorithm," In: Proceedings ofthe 15th International Conference on Distributed Computing
Systems, IEEE, New York(May-June 1995).
[Liskov 1993] Liskov, B., "Practical Uses of Synchronized Clocks in Distributed Systems,"
DistributedComputing, Vol.6, pp. 211-219 (1993).
[Lockhart 1994]Lockhart, Jr., H. W.,OSF DeE: Guide to Developing DistributedApplications,
IEEE Computer Society Press, Los Alamitos, CA (1994).

344 Chap. 6 • Synchronization
[Maekawa etal.1987JMaekawa,M.,Oldehoeft,A. E., and Oldehoeft,R. R., Operating Systems:
AdvancedConcepts, Benjamin/Cummings, White Plains, NY (1987).
[Mattern 1993]Mattern, F., "Efficient Algorithms for Distributed Snapshots and Global Virtual
Time Approximation," Journal ofParallel and Distributed Computing, Vol. 18, No.4, pp.
423-434 (1993).
[Mills 1991]Mills, D. L., "Internet Time Synchronization: The Network Time Protocol," IEEE
Transactionson Communications, Vol.39, No. ]0, pp. ]482-1493 (1991).
[Misra and Chandy 1982]Misra, J., and Chandy, K. M., "A DistributedGraphAlgorithm: Knot
Detection," ACM Transactions on Programming Languages andSystems, Vol. 4, No.4, pp.
678-686 (1982).
[Mitchell and Merritt 1984J MitchelJ, D. P.,and Merritt, M.1., "A Distributed Algorithm for
DeadlockDetectionand Resolution,"In: Proceedingsofthe3rdACMSymposium onPrinciples
of Distributed Computing, Association for Computing Machinery, New York, pp. 282-284
(1984).
[Natrajan 1986]Natrajan, N.,"A Distributed Scheme for DetectingCommunication Deadlocks,"
IEEE Transactionson Software Engineering, Vol.SE-12, No.4, pp. 531-537 (1986).
[Prattand Nguyen1995]Pratt, G.A.,and Nguyen,1.,"DistributedSynchronousClocking,"IEEE
Transactionson Parallel and Distributed Systems, Vol.6, No.3, pp. 314-328 (1995).
[Ramanathanetal, 1990a]Ramanathan, P.,Kandlur, D. D., and Shin, K.G.,"Hardware-Assisted
SoftwareClockSynchronizationfor HomogeneousDistributedSystems,"IEEE Transactionson
Computers, Vol.C-39, No.4, pp. 514-524 (1990).
[Ramanathanetale1990b]Ramanathan, P.,Shin, K.G.,and Butler, R.W.,"Fault-TolerantClock
Synchronization in Distributed Systems," IEEE Computer, Vol.23, No. 10,pp. 33-42 (1990).
[Raynal 1991]Raynal, M., "A Simple Taxonomy for Distributed Mutual Exclusion Algorithms,"
ACMOperating Systems Review, Vol.25, pp. 47-50 (1991).
[Raynal 1992] Raynal, M., "About Logical Clocks for Distributed Systems," ACM Operating
Systems Review, Vol.26, No. ], pp. 41-48 (1992).
[Ricartand Agrawala 1981]Ricart, G., and Agrawala,A. K., "An OptimalAlgorithm for Mutual
Exclusion in Computer Networks," Communications ofthe ACM, Vol. 24, No. I, pp. 9-17
(1981).
[Roesler and Burkhard 1989]Roesler, M., and Burkhard, W.A., "Resolution of Deadlocks in
Object-Oriented Distributed Systems," IEEE Transactions on Computers, Vol. 38, No.8, pp.
1212-1224 (1989).
[Rosenberry et al, 1992] Rosenberry, W., Kenney, D., and Fisher, G., OSP' DISTRIBUTED
COMPUTING ENVIRONMENT, Understanding DeE, O'Reilly & Associates, Sebastopol, CA
(1992).
[Rosenkrantz et al, 1978]Rosenkrantz, D. 1., Stearns, R. E., and Lewis, P.M., "System Level
Concurrency Control for Distributed Database Systems," ACM Transactions on Database
Systems, Vol.3, No.2, pp. 178-198 (1978).
[Sanders 1987]Sanders,B.A.,"TheInformationStructureofDistributedMutualExclusion,"ACM
Transactions on Computer Systems, Vol.5, pp. 284-299 (1987).
[Shinand Ramanathan1987]Shin, K.G.,and Ramanathan,P, "ClockSynchronizationofaLarge
MultiprocessorSystemin the Presenceof Malicious Faults,"IEEE Transactionson Computers,
Vol.C-36, No.1, pp. 2-12 (1987).
[Shinand Ramanathan1988]Shin, K.G.,andRamanathan,P.,"TransmissionDelaysinHardware
Clock Synchronization," IEEE Transactions on Computers, Vol.C-37, No. 11, pp. 1465-1467
(1988).

Chap. 6 • Pointers to Bibliographies on the Internet 345
[Silberschatzand Galvin 1994] Silberschatz,A., and Galvin, P.B., Operating Systems Concepts,
4th ed., Addison-Wesley, Reading, MA (1994).
[SinghandKurose1994]Singh,S.,and Kurose,J.,"Electing'Good' Leaders,"JournalofParallel
and DistributedComputing, Vol.21, pp. 184-201 (1994).
[Singhal1989]Singhal,M.,"DeadlockDetectioninDistributedSystems," IEEEComputer,Vol.22,
No. 11, pp. 37-48 (1989).
[SinghalandShivaratri1994]Singhal,M.,andShivaratri,N.G.,AdvancedConceptsinOperating
Systems, McGraw-Hill, New York (1994).
[SinhaandNatrajan1985]Sinha,M. K.,and Natrajan,N.,"A PriorityBasedDistributedDeadlock
DetectionAlgorithm," IEEETransactionsonSoftwareEngineering, Vol.SE-ll,No.1,pp.67-80
(1985).
[Srikanth and Teueg 1987] Srikanth, T. K., and Teueg, S., "Optimal Clock Synchronization,"
Journal ofACM, Vol. 34, No.3, pp. 626-645 (1987).
[Stalling 1995] Stalling, W., Operating Systems, 2nd ed., Prentice-Hall, Englewood Cliffs, NJ
(1995).
[Steinmanetal.1995]Steinman,1.S., Lee, C.A.,Wilson, L.F.,and Nicol,D.M.,"GlobalVirtual
Time and Distributed Synchronization," In: Proceedings ofthe 9th Workshop on Parallel and
Distributed Simulation, pp. 139-148 (1995).
[Stuart et al, 1984] Stuart, D., Buckley, G., and Silberschatz, A., HA Centralized Deadlock
DetectionAlgorithm,"Technical Report, University ofTexas at Austin (J984).
[Suzuki and Kasami 1985] Suzuki, I., and Kasami, T., "A Distributed Mutual Exclusion
Algorithm,"ACM Transactions on Computer Systems, Vol.3, No.4, pp. 344-349 (1985).
[Tanenbaum 1992} Tanenbaum, A. S., Modern Operating Systems, Prentice-Hall, Englewood
ClitTs, NJ(1992).
[Tanenbaum 1995]Tanenbaum, A. S.~ Distributed Operating Systems, Prentice-Hall, Englewood
Cliffs, NJ(1995).
[Tel1994]Tel,(J., Introduction toDistributedAlgorithms,CambridgeUniversity Press, New York,
NY (1994).
[Turekand Shasha 1992]Turek,1.,and Shasha,D., "TheManyFacesofConsensusinDistributed
Systems," IEEE Computer, Vol. 25, pp. 8-17 (1992).
[Vasanthavada and Marinos 1988] Vasanthavada, N., and Marinos, P.N., "Synchronization of
Fault-TolerantClocksinthe PresenceofMaliciousFailures," IEEE Transactionson Computers,
Vol.C-37, No.4, pp. 440-448 (1988).
[Wuu and Bernstein 1985] Wuu, G. T., and Bernstein, A. J., "False Deadlock Detection in
Distributed Systems," IEEE Transactions on Software Engineering, Vol. SE-II, No.8, pp.
820-821 (1985).
[Yangand Marsland 1994] Yang, Z., and Marsland, T. A. (Eds.), Global States and Time in
DistributedSystems, IEEEComputerSociety Press, Los Alamitos, CA (1994).
POINTERS TOBIBI.IOGRAPHIES ONTHE INTERNET
Bibliography containing references on Time in DistributedSystems can be found at:
ftp:ftp.cs.umanitoba.calpublbibliographieslDistributed/dist-time.html

346 Chap. 6 • Synchronization
Bibliography containing references on Synchronization ofConcurrent Processes can be
found at:
ftp:ftp.cs.umanitoba.calpublbibliographieslParalleVpar.synch.html
Bibliographiescontainingreferences on othertopics coveredin this chaptercan be found
at:
ftp:ftp.cs.umanitoba.calpublbibliographies/OslIMMD_IV.html
ftp:ftp.cs.umanitoba.ca/publbibliographieslMisc/misc.l.htm1

7
CHAPTER
Resource
Management
7.1 INTRODUOION
Distributed systems are characterized by resource multiplicity and system transparency.
Every distributed system consists of a number of resources interconnected by a
network. Besides providing communication facilities, the network facilitates resource
sharing by migrating a local process and executing it at a remote node of the network.
A process may be migrated because the local node does not have the required
resources or the local node has to be shut down. A process may also be executed
remotely if the expected turnaround time will be better. From a user's point of view,
the set of available resources in a distributed system acts like a single virtual system.
Hence, when a user submits a process for execution, it becomes the responsibility of
the resource manager of the distributed operating system to control the assignment of
resources to processes and to route the processes to suitable nodes of the system
according to these assignments. A resource can be logical, such as a shared file, or
physical, such as a CPU. For our purpose, we will consider a resource to be a
processor of the system and assume that each processor forms a node of the
distributed system. Thus, in this chapter, we will be interchangeably using the terms
node and processor to mean the same thing.
347

348 Chap. 7 • Resource Management
A resource manager schedules the processes in a distributed system to make use of
the system resources in such a manner that resource usage, response time, network
congestion, and scheduling overhead are optimized. A variety of widely differing
techniquesand methodologiesfor schedulingprocessesofadistributedsystemhave been
proposed. These techniques can be broadly classified into three types:
1. Task assignment approach, in which each process submitted by a user for
processingis viewedas acollectionofrelated tasks and these tasks are scheduled
to suitable nodes so as to improve performance
2. Load-balancing approach, in which all the processes submitted by the users are
distributed among the nodes ofthe system so as to equalize the workloadamong
the nodes
3. Load-sharing approach, which simply attempts to conserve the ability of the
system to perform work by assuring that no node is idle while processes wait for
being processed
Of the three approaches, the task assignment approach has limited applicability in
practical situations because it works on the assumption that the characteristics ofall the
processes to be scheduled are known in advance. Furthermore, the schedulingalgorithms
that fall in this category do not normally take care ofthe dynamically changing state of
the system. Therefore, this approach will be covered very brieflyjust to give an idea of
how it works. Before presenting a description ofeach ofthese techniques, the desirable
features ofa good global scheduling algorithm are presented.
7.2 DESIMILEFEATURES OFAGOODGLOBAL
SCHEDUUNG ALGORITHM
7.2.1 No A PrIori Knowledge about the Process8s
A good process scheduling algorithm should operate with absolutely no a pnon
knowledge about the processes to be executed. Scheduling algorithms that operate based
on the information about the characteristics and resource requirements ofthe processes
normally pose an extra burden upon the users who must specify this information while
submitting theirprocesses for execution.
7.1.1 DynamicIn Nature
It is intendedthat a good process-schedulingalgorithm should be able to take care ofthe
dynamicallychangingload (or status)ofthe various nodesofthe system. Thatis, process
assignmentdecisions should be based on the current load ofthe system and not on some
fixed static policy. For this, sometimes it is also recommended that the scheduling
algorithm shouldpossess the flexibility to migrate a process more than once because the
initial decision ofplacing a process on a particular node may have to be changed after

Sec.7.2 • Desirable FeaturesofaGoodGlobalSchedulingAlgorithm 349
some time to adapt to the new system load. This feature mayalso require that the system
support preemptive process migration facility in which a process can bemigrated from
one node to another during the course of its execution.
7.2.3 Quick Decision-Making Capability
Agood process-schedulingalgorithm must makequick decisions about theassignment of
processes to processors. This is an extremely important aspect of the algorithms and
makes many potential solutions unsuitable. For example, an algorithm that models the
system by a mathematical program and solves iton-line isunsuitable because itdoes not
meet this requirement. Heuristic methods requiring less computational effort while
providing near-optimal results are therefore normally preferable to exhaustive (optimal)
solution methods.
7.2.4 Balanced System Performance andScheduling
Overhead
Several global scheduling algorithms collect global state information and use this
information in making process assignment decisions. A common intuition is that greater
amounts of information describing global system state allow more intelligent process
assignment decisions to be made that have a positive affect on the system as a whole. In
a distributed environment, however, information regarding the state of the system is
typically gathered at a higher cost than in a centralized system. The general observation
is that, as overhead is increased in an attempt to obtain more information regarding the
global state of the system, the usefulness of that information isdecreased due to both the
aging of the information being gathered and the low scheduling frequency as a result of
thecostofgathering and processing thatinformation. Hence algorithms thatprovide near
optimal system performance with a minimum of global state information gathering
overhead arc desirable.
7.2.5 Stability
Ascheduling algorithmissaidtobeunstable ifitcanenter astateinwhichallthenodesof
the system are spending all of their time migrating processes without accomplishing any
useful work inan attempt to properly schedulethe processes forbetter performance. This
form of fruitless migration of processes is known as processor thrashing. Processor
thrashingcanoccurinsituations whereeachnodeofthesystemhasthepowerofscheduling
its own processes and scheduling decisions either are made independently of decisions
made by other processors or are based on relatively old data due to transmission delay
betweennodes.Forexample, itmayhappenthatnodesnlandn2bothobservethatnoden3is
idle and then both offload a portion of their work to node n3 without being aware of the
offloading decision made by the other. Now if node n3 becomes overloaded due to the
processes received from both nodes nl and n2' then it may again start transferring its
processes toother nodes.Thisentirecyclemayberepeatedagainandagain,resulting inan
unstable state.This iscertainly notdesirable foragoodschedulingalgorithm.

350 Chap. 7 • Resource Management
Processorthrashing can also occur ifprocesses intransit toalightly loaded node are
not taken into account. In thiscase, several processes'may be migrated to the same node,
possiblyproducing instabilities. Forexample, suppose ataparticularinstance oftimenode
nJ is very busy and node n2isthe least busy node. Also suppose that node n. isactivated
every 2seconds andonanaverage ittakes 20seconds foraprocess toreach noden2from
node nl'Then node n. could conceivably send at least 10processes tonode n2before the
first one was received. This could result in an unstable situation. A simple method to
overcomethisproblem istokeeptrackofwhichnodehasbeensentprocessesrecently and
use this information in an effort to mitigate the problem and to minimize process
movement. More sophisticated techniques to deal with stability are possible, but they
require the retention of more past data and hence are expensive.
7.2.6 Scalability
Aschedulingalgorithm should becapableofhandling smallas wellaslargenetworks. An
algorithm that makes scheduling decisions by first inquiring the workload from all the
nodes and then selecting the most lightly loaded node as the candidate for receiving the
process(es) has poor scalability factor. Such an algorithm may work fine for small
networks but gets crippled when applied to large networks. This is because the inquirer
receives a very large number of replies almost simultaneously, and the time required to
process thereplymessages formaking ahostselection isnormally toolong.Moreover, the
N2 (N is the total number of nodes in the system) nature ofthe algorithm creates heavy
network traffic and quickly consumesnetwork bandwidth. Asimple approach tomake an
algorithm scalable is toprobe only m of N nodes for selectingahost. The value of m can
be dynamically adjusted depending upon the value of N.
7.1.7 Fault Tol.raftC8
Agood schedulingalgorithm should notbedisabled bythecrash ofone ormore nodesof
the system. At any instance of time, it should continue functioning for nodes that are up
at that time. Moreover, if the nodes are partitioned into two or more groups due to link
failures, the algorithm should be capable of functioning properly for the nodes within a
group. Algorithms that have decentralized decision-making capability and consider only
available nodes in their decision-making approach have better fault tolerance
capability.
7.1.8 Fairness or Sarvle.
While the averagequality ofservice provided isclearly an importantperformance index,
how fairly service is allocated is also a common concern. For example, two users
simultaneouslyinitiating equivalentprocessesexpect toreceive about the samequality of
service. Several researchers think that global scheduling policies that blindly attempt to
balance theaverage loadonallthenodesofthesystem arenotgoodfromthepointofview
offairness ofservice.This isbecause inany load-balancingscheme, heavily loaded nodes
willobtain allthebenefits whilelightly loaded nodes willsufferpoorerresponse timethan

Sec.7.3 • TaskAssignmentApproach 351
that in a stand-alone configuration. What is desirable is a fair strategy that will improve
response time to the former without unduly affecting the latter. For this, the concept of
load balancinghas to be replacedby the conceptofloadsharing,thatis, anode will share
some ofits resources as long as its users are not significantly affected.
7.3 TASK ASSIGNMENT APPROACH
7.3.1 The BasicIdea
In this approach, aprocess isconsideredto be composedofmultiple tasks and the goal is
to find an optimal assignment policy for the tasks of an individual process. Typical
assumptions found in task assignment work are as follows:
• A process has already been split into pieces called tasks. This split occurs along
natural boundaries, so that eachtask will have integrity in itselfand data transfers
among the tasks will be minimized.
• The amountofcomputationrequiredby eachtask and the speedofeach processor
are known.
• The costofprocessing each task on every node ofthe system is known. Thiscost
is usually derived basedon the informationaboutthe speedofeachprocessorand
the amount ofcomputation required by each task.
• The interprocesscommunication(fPC)costsbetweeneverypair oftasks isknown.
The IPCcost is considered zero (negligible) for tasks assigned to the same node.
They are usually estimated by an analysis ofthe static program ofa process. For
example, during the execution ofthe process, if two tasks communicate n times
and if the average time for each intertask communication is t, the intertask
communication cost for the two tasks is n X t.
• Other constraints, such as resource requirements of the tasks and the available
resources at each node, precedence relationships among the tasks, and so on, are
also known.
• Reassignment of the tasks is generally not possible.
With these assumptions, the task assignmentalgorithms seek to assign the tasks ofa
processtothe nodes ofthe distributedsysteminsuch amannersoastoachievegoals such
as the following:
• Minimization ofIPe costs
• Quick turnaround time for the complete process
• A high degree of parallelism
• Efficient utilization ofsystem resources in general
Thesegoalsoftenconflictwith eachother. Forexample,whileminimizingIPetends
toassignallthe tasks ofaprocesstoasinglenode, efficientutilizationofsystemresources

352 Chap. 7 • Resource Management
tries to distribute the tasks evenly among the nodes. Similarly, while quick turnaround
time and a high degree of parallelism encourage parallel execution of the tasks, the
precedencerelationshipamong the tasks limits their parallelexecution.Also notice that in
case of m tasks and q nodes, there are mq possible assignments of tasks to nodes. In
practice,however, theactual numberofpossibleassignmentsoftasks tonodes maybeless
than mqdue tothe restriction that certaintasks cannotbeassignedtocertainnodes due to
their specific resource requirements; the required resource(s) may not be available on all
the nodes ofthe system.
Toillustrate with an example, let usconsiderthe assignment problem ofFigure 7.1.
This isthe same problemdiscussedbyStone[1977]. Itinvolvesonly two task assignment
parameters-thetaskexecutioncost and the intertaskcommunicationcost. This system is
made up of six tasks {tI, t2, t3, t4, Is, t6} and two nodes {nI' n2}. The intertask
communication costs (c;j) and the execution costs (Xab) ofthe tasks are given in tabular
form in Figure7.1(a) and (b), respectively. An infinite cost for a particulartask againsta
particularnodeinFigure 7.1(b) indicatesthat thetaskcannotbeexecutedon thatnodedue
to the task's requirement ofspecific resources that are not available on that node. Thus,
task t2 cannotbeexecuted on node nzand task t6 cannotbe executed on node nI. In this
model ofa distributed computing system, there is no parallelism or multitasking oftask
execution within a program. Thus the total cost ofprocess executionconsists ofthe total
executioncost ofthe tasks on their assignednodes plus theintertaskcommunicationcosts
between tasks assigned to different nodes.
Figure 7.1(c) shows a serial assing.nment ofthe tasks to the two nodes in which the
first three tasks are assigned to node and the remaining three are assigned to node n2.
Observe that this assignment is aimed at minimizing the total executioncost. But if both
the executioncosts and the communicationcosts are taken into account, the total cost for
this assignment comes out to be 58. Figure 7.1(d) shows an optimal assignment of the
tasks to the two nodes that minimizes total execution and communication costs. In this
case, although the execution cost is more than that ofthe previous assignment, the total
assignment cost is only 38.
7.3.2 Finding anOptimal Asslgnm.nt
Theproblemoffinding anassignmentoftasks tonodes thatminimizestotalexecutionand
communication costs was elegantly analyzed using a network flow model and network
flow algorithms by Stone [1977, 1978] and a numberofotherresearchers [Lo 1988, Wu
andLiu 1980,Bokhari 1979].Inthisapproach,anoptimalassignmentisfound bycreating
astatic assignmentgraph, asshown inFigure 7.2. Inthis graph, nodes nI and n2represent
the two nodes (processors)ofthe distributed system and nodes t. through 16 representthe
tasks of the process. The weights of the edges joining pairs of task nodes represent
intertask communication costs. The weight on the edge joining a task node to node n.
represents the execution cost of that task on node nzand vice versa.
A cutset in this graph is defined to be a set ofedges such that when these edges are
removed, the nodes of the graph are partitioned into two disjoint subsets such that the
nodes in one subset are reachable from nI and the nodes in the otherare reachable from
n2. Each task node is reachable from eithernI or n2. No proper subset ofacutset is also

Sec.7.3 • TaskAssignmentApproach 353
Execution costs
Intertask communicationcosts Nodes
Tasks
t 1 t 2 t 3 t 4 t 5 t 6 n, n 2
'1 0 6 4 0 0 12 t 5 10
1
'2 6 0 8 12 3 0 t 2 2 00
'3 4 8 0 0 11 0 1 4 4
3
1 0 12 0 0 5 0 '4 6 3
4
1 5 0 3 11 5 0 0 '5 5 2
te 12 0 0 0 0 0 4; 00 4
(a) (b)
Serialassignment Optimal assignment
Task Node Task Node
1 n, 1 n
1 1 1
1 n t n
2 1 2 1
1 n 1 n
3 1 3 1
n 1 n
4 2 4 1
1'
5
n
2
1s n
1
Ie n '6 n
2 2
(c) (dJ
Serialassignmentexecutioncost(x)= X11 +X21 +X31 +X42 +XS2 +X62
=5+2+4+3+2+4=20
=
Serialassignmentcommunicationcost(c) C14 +C15 +C16 +C24 +C25 +~6+C34 +~s+C36
=
=0+0+12+12+3+0+0+11+0 38
Serialassignment totalcost=x+C=20+38=58
Optimalassignmentexecution cost(x)=X11 +X21 +X31 +X41 +XS1 +x
62
=5+2+4+6+5+4=26
Optimal assignmentcommunicationcost (c)=C16 +C26 +~+C46 +CS6
=12+0+0+0+0=12
Optimal assignmenttotalcost
=x+C =
26+12
=
38
Fig.7.1 Atask assignmentproblem example. (a) intertask communicationcosts;
(b) execution costs of the tasks on the two nodes; (c) serial assignment;
(d) optimal assignment.

354 Chap. 7 • Resource Management
Minimumcostcut
I.'ig.7.2 Assignmentgraphfortheassignment problem ofFigure7.1withminimum
costcut.
acutset, that is, acutset isaminimal set.Each cutset corresponds inaone-to-onemanner
to a task assignment.
The weight of a cutset is the sum of the weights of the edges in the cutset. It
representsthecost ofthecorrespondingtaskassignmentsince the weight ofacutset sums
up the execution and communication costs for that assignment. An optimal assignment
may be obtained by finding a minimum-weight cutset. This may be done by the use of
network flow algorithms, which are among the class of algorithms with relatively low
computationalcomplexity. The bold line inFigure 7.2 indicates aminimum-weightcutset
thatcorrespondstotheoptimal assignmentofFigure 7.1(d). Notethatiftask t1isassigned
to node n2' then theedge tonoden2iscut, butthisedge carries thecost ofexecutingtask
t) on node nl. Similarly, other edges cut between task t) and other nodes of the graph
represent actual communication costs incurred by this assignment for communication
between task tI and the other corresponding nodes.
In a two-processor system, an optimal assignment can be found in polynomial time
by utilizing Max Flow/Min Cut algorithms [Shen and Tsai 1985]. However, for an
arbitrary numberofprocessors, theproblem isknown tobeNonpolynomial(NP) hard.An
NPhard problemiscomputationallyintractablebecause itcannot be solved inpolynomial
time.Thus, formoregeneral cases, several researchershave turned toheuristic algorithms
that are computationally efficient but may yield suboptimal assignments. Readers
interested in some of these heuristic algorithms may refer to [Arora and Rana 1980, Ere
1982,Lo 1988].

Sec.7.4 • Load-BalancingApproach 355
It may be noted that in the model described above, the tasks of a process were
assigned tothe various nodes ofthe system. This modelmay begeneralizedtothegeneral
task assignment problem in which several processes are to be assigned. In this case, each
process is treated to be a task of the process force and the intcrprocess communication
costs are assumed to be known.
Several extensions to the basic task assignment model described above have been
proposed in the literature. In addition to the task assignment cost and the intertask
communicationcost parametersof the basic task assignmentmodel, the extendedmodels
take into account other parameters such as memory size requirements of the task and
memory sizeconstraintoftheprocessors, precedencerelationshipamong the tasks, and so
on.However, wewillnotdiscuss this topic any further becauseofthelimitedapplicability
ofthetaskassignmentapproachinpractical situations. Readers interestedinsomeofthese
extended models may refer to [Lo 1988, Chu and Lan 1987, Rao et a1. 1979].
7.4 LOAD-BALANCING APPROACH
The scheduling algorithmsusing thisapproach areknown asload-balancingalgorithmsor
load-leveling algorithms. These algorithms arc based on the intuition that, for better
resource utilization, it is desirable for the load in a distributed system to be balanced
evenly. Thus, a load-balancing algorithm tries to balance the total system load by
transparently transferringthe workload from heavily loaded nodes tolightly loaded nodes
in an attempt to ensure good overall performance relative to some specific metric of
system performance. When considering performance from the user point of view, the
metric involved is often the response time of the processes. However, when performance
is considered from the resource point of view, the metric involved is the total system
throughput. Incontrasttoresponse time,throughputisconcernedwith seeing thatallusers
are treated fairly and that all are making progress. Notice that the resource view of
maximizing resource utilization is compatible with the desire to maximize system
throughput. Thus thebasic goalofalmost alltheload-balancingalgorithmsistomaximize
the total system throughput.
7.4.1 A Taxonomy of l.oad-Balancing Algorithms
The taxonomy presented here is a hierarchy of the features of load-balancing algorithms.
The structure of the taxonomy is shown in Figure 7.3. To describe a specific load
balancingalgorithm, ataxonomy user traces paths through the hierarchy. Adescriptionof
this taxonomy is given below.
Static versus Dynamic
At the highest level, we may distinguish between static and dynamic load-balancing
algorithms. Static algorithms use only information about the average behavior of the
system, ignoring the current state of the system. On the other hand, dynamic algorithms
react to the system state that changes dynamically.

356 Chap. 7 • Resource Management
Load-balancingalgorithms
Dynamic
Deterministic Probabilistic
Cooperative Noncooperative
Fig.7.3 Ataxonomy of load-balancingalgorithms.
Staticload-balancingalgorithms aresimpler becausethereisnoneedtomaintain and
process system state information. However,thepotential ofstaticalgorithms islimited by
the fact that they do not react to the current system state. The attraction of dynamic
algorithms is that they do respond to system state and so are better able to avoid those
states with unnecessarily poor performance. Owing tothis reason, dynamic policies have
significantly greater performance benefits than static policies. However, since dynamic
algorithms must collect and react to system state information, they are necessarily more
complex than static algorithms.
Deterministic versus Probabilistic
Static load-balancing algorithms may be either deterministic or probabilistic. Determi
nistic algorithms use the information about the properties of the nodes and the
characteristics of the processes to be scheduled to deterministically allocate processes to
nodes. Notice that the task assignment algorithms basically belong to the category of
deterministic static load-balancing algorithms.
Aprobabilisticload-balancingalgorithmusesinformationregardingstaticattributesof
the system such asnumber of nodes, theprocessingcapability ofeach node, the network
topology,and soon,toformulate simple process placement rules. Forexample, suppose a
system has two processors PI andP2and four terminals tl, 1 2, 1 3, and t4' Then a simple
process placement rule can be, assign all processes originating at terminals 1 and 1 to
1 2
processorPI andprocessesoriginatingatterminals1
3
and14 toprocessorP2'Obviously,such
static load-balancing algorithms have limited potential to avoid those states that have
unnecessarily poor performance. For example, suppose at the time a particular process
originates at terminal II processor PI is very heavily loaded and processor P2is idle.
Certainly,processorP2isabetterchoice fortheprocessinsuchasituation.

Sec.7.4 • Load-BalancingApproach 357
In genera], the deterministic approach is difficult to optimize and costs more to
implement. The probabilistic approach is easier to implement but often suffers from
having poor performance.
Centralized versus Distributed
Dynamic scheduling algorithms may be centralized or distributed. In a centralized
dynamic scheduling algorithm, the responsibility of scheduling physically resides on a
single node. On the other hand, in adistributed dynamic scheduling algorithm, the work
involved in making process assignment decisions is physically distributed among the
various nodes of the system. In the centralized approach, the system state information is
collected at a single node atwhich all schedulingdecisions are made. This node iscalled
the centralized server node. All requests for process scheduling are handled by the
centralized server, which decides about the placement of a new process using the state
information storedinit.The centralizedapproachcanefficiently makeprocess assignment
decisions because thecentralizedserverknows boththe loadateach nodeandthenumber
ofprocesses needing service. Inthebasic method, t.heother nodes periodicallysendstatus
update messages to the central server node. These messages are used to keep the system
stateinformationup todate atthecentralizedserver node. One might considerhaving the
centralizedserverquery theother nodes forstate information. This wouldreduce message
traffic if state information was used to answer several process assignment requests, but
since nodes can change their load any time due to local activities, this would introduce
problems of stale state information.
A problem associated with the centralized mechanism is that of reliability. If the
centralized server fails, all scheduling in the system would cease. A typical approach to
overcome this problem would be to replicate the server on k+1nodes ifit is to survive
k faults (node failures). In this approach, theoverhead involved in keeping consistentall
the k+1replicas of the server may beconsiderably high.A typical solution toreduce this
overhead is to forgo strong consistency and use a cheaper mechanism to update the
multiple copies of the server. This solution is based on the idea that strict consistency is
not necessary in this case for the system to function properly.
Theimer and {Jantz[1989] proposed that if occasional delays in service of several
seconds areacceptable,onecanusethesimplerapproach ofreinstantiationtoimprovethe
reliability of the centralized scheduling mechanism. In this approach, rather than
maintaining k+1 server replicas, a single server is maintained and there are k entities
monitoring the server todetect its failure. When failure isdetected, a new instance of the
server is brought up, which reconstructs its state information by sending a multicast
message requesting immediatestateupdate.The time during which thescheduling service
is unavailable willbethe sum ofthe time todetect failure ofthe previous server,thetime
toloadtheserver program onthenewserver,thetimetoresolve thepossibility ofmultiple
concurrentinstantiationsamong the kentities, and the time toreconstructthe global state
information on the new server [Theimer and Lantz 1989].
In contrast to the centralized scheme, a distributed scheme does not limit the
scheduling intelligencetoone node.Itavoidsthebottleneck ofcollectingstateinformation
atasinglenodeandallowsthescheduler toreactquickly todynamic changes inthesystem

358 Chap. 7 • Resource Management
state.Adistributeddynamic scheduling algorithm iscomposedofkphysically distributed
entitiesel'e2' ...,ekeEachentityisconsideredalocalcontroller.Eachlocalcontrollerruns
asynchronously and concurrently with the others, and each is responsible for making
schedulingdecisions fortheprocessesofapredeterminedsetofnodes.Eachentitye,makes
decisions based on a systemwide objective function, rather than on a local one. Each e,
makes decisionsonanequal basis withtheother entities; thatis,there isnomaster entity,
evenforashortperiodoftime.Inafullydistributedalgorithm,eachnodehasitsownentity
and hence k=N for a system having N nodes. In this case, each entity is responsible for
makingschedulingdecisions fortheprocessesofitsownnode,whichincludes bothtransfer
oflocalprocessesandacceptance ofremote processes.
Cooperative versus Noncooperative
Distributed dynamic scheduling algorithms may be categorized as cooperative and
noncooperative. In noncooperative algorithms, individual entities act as autonomous
entities and make scheduling decisions independently of the actions of other entities. On
the other hand, in cooperative algorithms, the distributed entities cooperate with each
othertomake scheduling decisions. Hence, cooperativealgorithmsare more complexand
involve larger overhead than noncooperativeones. However,the stability ofacooperative
algorithm is better than that of a noncooperative algorithm.
7.4.1 ISlu.s Inhslgnlng load-Ialanclng Algorithms
Designing a good load-balancing algorithm is a difficult task because of the following
issues:
• Load estimation policy, which determines how to estimate the workload of a
particular node ofthe system
• Process transfer policy, which determines whether toexecute a process locally or
remotely
• State information exchange policy,which determines how toexchangethe system
load information among the nodes
• Location policy, which determines to which node a process selected for transfer
should be sent
• Priority assignmentpolicy,whichdeterminesthepriority ofexecutionoflocaland
remote processes at a particular node
• Migration limiting policy, which determines the total number of times a process
can migrate from one node to another
Theseissues arediscussed below.For thisdiscussion, wedivide theprocesses within
the system into two classes: local processes and remote processes. A localprocess isone
that is processed at its originating node and a remote process is one that is processed at
a node different from the one on which it originated. A new process, arriving from the
external world at a node, becomes a local process if it is admitted to that node for

Sec.7.4 • Load-BalancingApproach 359
processing. Otherwise, itistransferred across the network and becomes aremote process
at the destination node.
Load Estimation Policies
The main goal of load-balancing algorithms is to balance the workload on all the nodes
ofthe system. However, before an algorithm can attempt to balance the workload, it is
necessary to decide how to measure the workload of a particular node. Hence the first
issue inany load-balancingalgorithmistodecide onthe method tobeusedtoestimatethe
workloadofaparticularnode.Estimationoftheworkload ofaparticularnodeisadifficult
problem for which no completely satisfactory solution exists. A node's workload can be
estimated based on some measurable parameters. These parameters could include time
dependent and node-dependent factors such as the following:
• Total number ofprocesses on the node at the time ofload estimation
• Resource demands of these processes
• Instruction mixes of these processes
• Architecture and speed of the node's processor
Since themeasurementofload wouldoccur quite often and theloadwould reflect the
current state of the node, its calculation must he very efficient. This rules out an
exhaustive use of all parameters even iftheir relative importance is known. Thus several
load-balancing algorithms use the total number of processes present on the node as a
measure of the node's workload. However, several designers believe that this is an
unsuitable measure for such an estimate since the true load could vary widely depending
on the remaining service times for those processes. Therefore another measure used for
estimatinganode'sworkloadisthe sumoftheremainingservicetimes ofalltheprocesses
onthat node. However, inthis case another issue that must be resolved ishow toestimate
the remaining service time of the processes. Bryant and Finkel [1981] have proposedthe
use of one of the foJlowing methods for this purpose:
1. Memoryless method. This method assumes that all processes have the same
expectedremaining service time, independent of the time used so far.The use of
this method for remaining service time estimation of a process basically reduces
the load estimation method to that of total number of processes.
2. Pastrepeats. This method assumes that the remaining service time ofaprocess is
equal to the time used so far by it.
3. Distribution method. If the distribution of service times is known, the associated
process's remaining service time is the expected remaining time conditioned by
the time already used.
Neither the method of counting the total number of processes nor the method of
taking the sum of the remaining service times of all processes is suitable for use as load
estimation policies in modern distributed systems. This is because in modern distributed

360 Chap. 7 • Resource Management
systems,evenonanidlenode,severalprocessessuchasmailandnewsdaemons,window
managers,andsoon,existpermanently.Moreover,manyofthesedaemonprocesseswake
upperiodically,checktoseeifthereisanythingthattheyhavetodo,andifnot,goback
to sleep.Therefore,an acceptablemethodfor useas the loadestimationpolicy in these
systemswouldbetomeasuretheCPUutilizationofthenodes[Tanenbaum1995].Central
processing unit (CPU) utilization is defined as the number of CPU cycles actually
executedper unitof real time.Obviously,the CPU utilizationof a heavily loadednode
willbegreaterthantheCPU utilizationof a lightlyloadednode.TheCPU utilizationof
anodecan bemeasuredbysettingupatimertoperiodicallyobservetheCPUstate(idlel
busy).
Process Transfer Policies
The strategy of load-balancing algorithms is based on the idea of transferring some
processes from the heavily loaded nodes to the lightly loaded nodes for processing.
However,tofacilitatethis,it is necessarytodevisea policytodecidewhethera nodeis
lightlyor heavilyloaded.Mostoftheload-balancingalgorithmsusethethresholdpolicy
tomakethisdecision.Thethresholdvalueofanodeisthelimitingvalueofitsworkload
and is usedtodecidewhethera nodeislightlyorheavilyloaded.Thusa newprocessat
anodeisacceptedlocallyforprocessingiftheworkloadofthenodeisbelowitsthreshold
valueatthattime.Otherwise,anattemptismadetotransfertheprocesstoalightlyloaded
node. The threshold value of a node may be determined by any of the following
methods:
1. Static policy. In this method, each node has a predefined threshold value
depending on its processing capability. This threshold value does not vary with the
dynamic changes in workload at local or remote nodes. The main advantage of this
methodisthatnoexchangeofstaleinformationamongthenodesisrequiredindeciding
the thresholdvalue.
2. Dynamicpolicy. Inthismethod,thethresholdvalueofanode(nj) iscalculatedas
aproductoftheaverageworkloadofallthenodesandapredefinedconstant(c;). Foreach
node n., the value of c,depends on the processingcapabilityof node n,relative to the
processing capability of an other nodes. In this method, the nodes exchange state
information by using one of the state information exchange policies (described Jater).
Althoughthedynamicpolicygivesa morerealisticvalueofthresholdforeach node,the
overhead involved in exchange of state information makes its applicability
questionable.
Mostload-balancingalgorithmsuseasinglethresholdandthusonlyhaveoverloaded
andunderloadedregions[Fig.7.4(a)]. Inthissingle-thresholdpolicy,anodeacceptsnew
processes(eitherlocalorremote)if itsloadis belowthethresholdvalueandattemptsto
transfer local processes and rejects remote execution requests if its load is above the
thresholdvalue.Thereforethedecisionregardingboththetransferoflocalprocessesand
theacceptanceofremoteprocessesisdonebasedonasingle-thresholdvalue.Theuseof

Sec. 7.4 • Load-BalancingApproach 361
Overloaded
Overloaded
Highmark
1--------1
Threshold Normal
foo--------t Lowmark
Underloaded
Underloaded
Fig.7.4 The load regions of
(a) single-thresholdpolicy and
(a) (b)
(b) double-threshold policy.
a single-threshold value may lead to fruitless process transfers, making the scheduling
algorithm unstable because a node's load may be below the threshold when itdecides to
accept aremote process, butitsload may becomelarger thanthe threshold assoonasthe
remote process arrives. Therefore, immediately after receiving the remote process, the
node willagain trytotransfer oneor moreofitsprocesses tosomeother node.Moreover,
Alonso and Cova [1988] observed:
• A node should only transfer one or more of its processes to another node if such
a transfer greatly improves the performance of the rest of its local processes.
• A node should accept remote processes only if its load is such that the added
workload ofprocessing these incoming processes does not significantly affect the
service to the local ones.
Toreduce the instability of the single-threshold policy and to take care of these two
notions, Alonso and Cova [1988] proposed adouble-threshold policy called the high-low
policy. As shown in Figure 7.4(b), the high-low policy uses two threshold values called
high markand low mark, whichdivide thespaceofpossible load statesofanodeinto the
following three regions:
• Overloaded-above the high-mark and low-mark values
• Normal-above the low-mark value and below the high-mark value
• Underloaded-below both values
Anode's load state switches dynamically fromone region toanother, as shown inFigure
7.5.
Now depending on the current load status of a node, the decision to transfer a local
process ortoaccept aremote process isbasedonthefollowing policies [AlonsoandCova
1988]:

362 Chap. 7 • Resource Management
Job Job Job
arrival/departure arrival/departure arrival/departure
Fig.7.5 State transition diagram of the loadofanode incase ofdouble-thresholdpolicy.
• Whentheloadofthenodeisintheoverloaded region, newlocalprocesses aresent
to be run remotely and requests to accept remote processes are rejected.
• When theloadofthenode isinthenormalregion, newlocalprocesses runlocally
and requests to accept remote processes are rejected.
• When the load of the node is in the underloaded region, new local processes run
locally and requests to accept remote processes are accepted.
Notice that the high-low policy guarantees a predefined level of performance to the
node owners. It accounts for theoverhead that the load-balancing algorithm may incur in
transferring and receiving a remote process. A process will not be transferred to another
node unless it is worthwhile, and a remote process will not be accepted unless there is
enough excess capacity to handle it [Alonso and Cova 1988].
Location Policies
Once a decision has been made through the transfer policy to transfer a process from a
node, the next step is to select the destination node for that process's execution. This
selection is made by the location policy of a scheduling algorithm. The main location
policies proposed in the literature are described below.
Threshold. In this method, a destination node is selected at.random and a check
is made to determine whether the transfer of the process to that node would place it in a
state that prohibits the node to accept remote processes. If not, the process is transferred
to the selected node, which must execute the process regardless of its state when the
process actually arrives. On the other hand, if the check indicates that the selected node
isinastatethatprohibits ittoacceptremoteprocesses, another node isselected atrandom
and probed in the same manner.This continues until either a suitable destination node is
found or thenumberofprobes exceeds'astatic probe limitL p•Inthelatter case, the node
onwhich theprocess originated must execute theprocess. Eager etal. [1986b] performed
simulations by using the single static threshold transfer policy and this threshold location
policy.Theirsimulation results showed that theperformanceofthispolicy issurprisingly
insensitive to the choice of probe limit; the performance with a small (and economical)

Sec.7.4 • Load-BalancingApproach 363
probe limit (e.g., 3 or 5) is almost as good as the performance with a large probe limit
(e.g., 20).The simulationresults alsoshowed that thethresholdpolicy provides substantial
performance improvement relative to no load balancing. This indicates that very simple
schemes can yield significant benefits.
Shortest. In this method, L distinct nodes are chosen at random, and each is
p
polled in turn to determine its load. The process is transferred to the node having the
minimum load value, unless that node's load is such that it prohibits the node to accept
remote processes. If none of the polled nodes can accept the process, it is executed at its
originating node. If a destination node is found and the process is transferred there, the
destination node must execute the process regardless of its state at the time the process
actually arrives. A simple improvement to the basic shortest policy is to discontinue
probing whenever a node with zero load isencountered, since that node is guaranteed to
be an acceptable destination.
The shortest policy uses more state information, in a more complex manner, than
does thethresholdpolicy. But Eager et a1. [1986b] found, through their simulationresults,
that the performance of the shortest policy is not significantly better than that of the
simpler threshold policy. This suggests that state information beyond that used by the
threshold policy or a more complex usage of state information is of little benefit.
Bidding. In this method, the system is turned into a distributed computational
economywithbuyers and sellers ofservices [Waldspurgeretal. 1992,Malone etal. 1988].
Each node inthe network isresponsiblefor two roles with respect tothe bidding process:
manager and contractor. The manager represents a node having a process in need of a
location to execute, and the contractor represents a node that is able to accept remote
processes. Note that a single node takes on both these roles and no nodes are strictly
managers or contractors alone. Toselect a node for its process, the managerbroadcasts a
request-for-bids message to all other nodes in the system. Upon receiving this message,
the contractornodes return bids to the manager node. The bids containthe quoted prices,
which vary based on the processingcapability, memory size, resource availability, and so
on, of the contractor nodes. Of the bids received from the contractor nodes, the manager
node chooses the best bid. The best bid for a manager's request may mean the cheapest,
fastest, orbest price-performance,dependingonthe application for which therequestwas
made. Once the best bid is determined, the process is transferred from the manager node
to the winning contractor node. But it is possible that a contractor node may
simultaneously win many bids from many other manager nodes and thus become
overloaded. To prevent this situation, when the best bid is selected, a message is sent to
the ownerof that bid.Atthat point the bidder may choosetoaccept or reject that process.
A message is sent back to the concerned manager node informing it as to whether the
process hasbeen acceptedorrejected. Acontractornode mayreject awinning bidbecause
ofchangesinitsstate betweenthe time the bid was made and the time itwas notified that
it won the bid. If the bid is rejected, the bidding procedure is started all over again.
Bidding algorithms are interesting because they provide full autonomy to the nodes
todecide whethertoparticipate intheglobal schedulingprocess. For example, amanager
node has the power to decide whether to send a process to a contractor node, which
responds with bids, and a contractor node has the power to decide whether it wants to

364 Chap. 7 • Resource Management
accept remote processes. A contractornode is never forced to accept remote processes if
it does not choose to do so. On the other hand, the two main drawbacks of bidding
algorithms are that they create a great deal of communication overhead and it is very
difficult to decide a good pricing policy. Both factors call for a proper choice of the
amount and type of information exchanged during bidding. For a bidding algorithm, the
amount and type of information exchanged are generally decided in such a manner so as
to balance the effectiveness and performance of the algorithm. A variety of possibilities
existconcerningthetypeandamountofinformation exchanged inordertomakedecisions
[Stankovic and Sidhu 1984, Smith 1980, Hwang et aJ. 1982, Stankovic 1984].
Pairing. The method of accomplishing load balancing employed by the policies
described until nowistobalance orreduce the variancebetween theloads experiencedby
all the nodes ofthe system. Contrary tothis approach, the method of accomplishing load
balancing by the pairing policy is to reduce the variance of loads only between pairs of
nodes of the system. This location policy was proposed by Bryant and'Finkel [1981]. In
this method, two nodes that differ greatly in load are temporarily paired with each other,
and the load-balancing operation is carried out between the nodes belonging to the same
pair by migrating one or more processes from the more heavily loaded node to the other
node.Several nodepairsmayexist simultaneously inthesystem.Anodeonlytriestofind
a partner if it has at least two processes; otherwise migration from this node is never
reasonable. However, every node is willing to respond favorably to a pairing request.
Inthebasicpairingmethod,eachnodeaskssomerandomly chosen nodeifitwillpair
with it. While awaiting an answer, the querier rejects any queries from other nodes. If it
receives arejection, itrandomly selects another node and tries topair again. If itreceives
aquery from itsown intended partner, apair isformed.After the formation of apair,one
ormoreprocesses aremigrated fromthemoreheavily loaded nodeofthetwonodestothe
other node inorder tobalance theloadsonthesetwonodes.The processes tobemigrated
are selected bycomparing their expected time tocomplete on their current node with the
expected time tocomplete onitspartner.Migration delay isincluded inthisestimate. The
process with the best ratio of service time on the partner node to service time on the
current node isselected tobesent first. Decisions for migrating other processes arebased
ontheassumptionthatthefirstprocesshasbeenreceived bythepartner nodeandtheload
ofthepartner node has been updated toreflect thepresence ofthisprocess on it.The pair
is broken as soon as theprocess migration isover. During the time that a pair is inforce,
bothmembers ofthepairrejectother pairing queries. Some other variations ofthepairing
mechanism can be found in [Bryant and Finkel 1981].
State Information Exchange Policies
We have seen that the dynamic policies require frequent exchange of state information
among the nodes of the system. In fact, a dynamic load-balancing algorithm faces a
transmission dilemma because of the two opposing impacts the transmission of a
message has on the overall performance of the system. On the one hand, the
transmission improves the ability of the algorithm to balance the load. On the other
hand, it raises the expected queuing time of messages because of the increase in the

Sec.7.4 • Load-BalancingApproach 365
utilization of the communication channel. Thus proper selection of the state information
exchange policy is essential. The proposed load-balancing algorithms use one of the
following policies for this purpose.
Periodic Broadcast. In this method each node broadcasts its state information
after the elapse of every t units of time. Obviously this method is not good because it
generates heavy network traffic and also because there is a possibility of fruitless
messages (messages from those nodes whose statehas notchanged during thelasttunits
of time) being broadcast. Furthermore, the scalability of this method ispoor because the
number of messages generated for state information exchanges will be too large for
networks having many nodes.
Broadcast When State Changes. This method avoids the problem of fruitless
message exchanges of the periodic broadcast method by ensuring that a node broadcasts
itsstateinformation onlywhenthestateofthenodechanges.Anode'sstatechanges when
a process arrives at that node or when a process departs from that node. A process may
arrive at anode either from theexternal worldor from some other node in the system.A
process departs from anode wheneither itsexecution isover or it is transferred to some
other node.
A further improvement in this method can be obtained by observing that it is not
necessary to report every small change inthe state of a node'toallother nodes because a
node can participate in the load-balancing process only when it is either underloaded or
overloaded. Therefore inthe refined method, anode broadcasts itsstate information only
whenitsstateswitches fromthenormalloadregiontoeither theunderloaded regionorthe
overloaded region. Obviously this refined method works only with the two-threshold
transfer policy.
On-Demand Exchange. A node needs to know about the state of other nodes
only when it iseitherunderloaded or overloaded. The methodof on-demandexchangeof
state information is based on this observation. In this method a node broadcasts a
Statelnformationkequest message when its state switches from the normal load region to
either the underloaded region or the overloaded region. On receiving this message, other
nodes send their current state to the requesting node. Notice that this method also works
only with the two-threshold transfer policy.
Further reduction in the number of messages is possible in this method by
observing that the requesting node does not need the state information of all other
nodes. Rather, the state information of only those nodes is useful for the requesting
node, which can cooperate with it in the load-balancing process. That is, if the
requesting node is underloaded, only overloaded nodes can cooperate with it in the
load-balancing process and vice versa. Therefore, in the improved on-demand exchange
policy, the status of the requesting node is included in the StatelnformationRequest
message. On receiving the Statelnformationkequest message, only those nodes reply
that can cooperate with the requesting node in the load-balancing process. Other nodes
do not send any reply.
Exchange by Polling. All the methods described above use the method of
broadcasting duetowhichtheirscalability ispoor.The polling mechanism overcomes this

366 Chap. 7 • Resource Management
limitation byavoiding theuseofbroadcastprotocol. Thismethod isbasedon theideathat
there is no need for a node to exchange its state information with all other nodes in the
system. Rather,whenanodeneedsthecooperation ofsomeothernodeforloadbalancing,
it can search for a suitable partner by randomly polling the other nodes one by one.
Therefore state information is exchanged only between the polling node and the polled
nodes. The polling process stops either when a suitable partner is found or a predefined
poll limit is reached.
PriorityAssignment Policies
When process migration is supported by a distributed operating system, it becomes
necessary to devise a priority assignment rule for scheduling both local and remote
processesataparticularnode.Oneofthefollowing priority assignment rules maybeused
for this purpose:
1. Selfish. Local processes are given higher priority than remote processes.
2. Altruistic. Remote processes are given higher priority than local processes.
3. Intermediate.The priority of processes depends on the number of local processes
and thenumber ofremote processes attheconcerned node. Ifthe number oflocal
processes is greater than or equal to the number of remote processes, local
processes are given higher priority than remote processes. Otherwise, remote
processes are given higher priority than local processes.
LeeandTowsley [1986]studied theeffectofthesethreepriority assignmentpolicies
on the overall response time performance. The results of their study show the
following:
• The selfishpriority assignment ruleyields theworstresponse timeperformance of
the three policies. This is due to the extremely poor performance of remote
processes underthispolicy.However,ityieldsthebestresponse timeperformance
for local processes. Consequently, this policy imposes a severe penalty for
processes that arrive at a busy node and is beneficial for processes arriving at a
lightly loaded node.
• The altruistic priority assignment rule achieves the best response time perform
ance of the three policies. However, under this policy remote processes incur
lower delays than local processes, whichis somewhat unrealistic inthe sense that
localprocesses aretheprincipal workloadateach nodewhileremoteprocesses are
secondary workload.
• The performance of the intermediate priority assignment rule falls between the
other twopolicies. Interestingly enough, theoverall response timeperformanceof
this policy is much closer to that of the altruistic policy. Under this policy, local
processes are treated better than remote processes for a wide range of system
utilizations.

Sec.7.5 • Load-Sharing Approach 367
Migration-Limiting Policies
Another important policy to be used by a distributed operating system that supports
processmigrationistodecideabout the total numberoftimes aprocessshouldbeallowed
to migrate. One ofthe following two policies may be used for this purpose.
Uncontrolled. Inthis case, a remote process arriving at a node is treatedjust as
aprocess originatingat the node. Therefore, underthis policy, a processmay be migrated
any number of times. This policy has the unfortunate property ofcausing instability.
Controlled. Toovercome the instability problem ofthe uncontrolled policy, most
systems treat remote processes different from local processes and use a migration count
parameterto fixalimit on thenumberoftimes that aprocessmay migrate. Severalsystem
designersfeel that processmigrationisanexpensiveoperationand henceaprocessshould
not be allowed to migrate too frequently. Hence this group of designers normally favors
an irrevocable migration policy. Thatis, the upperlimit ofthe value of migrationcountis
fixed to I, and hence a process cannot be migrated more than once under this policy.
However, some systemdesignersfeel that multipleprocessmigrations,especiallyforlong
processes, may be very useful for adapting to the dynamically changing states of the
nodes. Thus this group of designers sets the upper limit ofthe value of migration count
to some value k > 1.The value of k may be decided either statically or dynamically. Its
value may also be different for processes having different characteristics. Forexample, a
long process (a process whose execution time is large) may be allowed to migrate more
times as compared to a short process.
7.5 lOAD-SHARING APPROACH
Several researchers believe that load balancing, with its implication of attempting to
equalize workload on all the nodes of the system, is not an appropriate objective. This is
because the overhead involved in gathering state information to achieve this objective is
normally very large, especially in distributed systems having a large number of nodes.
Moreover, load balancing in the strictest sense is not achievable because the number of
processes in a node is always fluctuating and the temporal unbalance among the nodes
exists at every moment, even if the static (average) load is perfectly balanced [Livny and
Melman 1982]. In fact, for the proper utilization of the resources of adistributed system,
itisnot required to balancethe load on all the nodes. Rather, it isnecessaryand sufficient
to prevent the nodes from being idle while some other nodes have more than two
processes. Therefore this rectification is often called dynamic load sharing instead of
dynamic load balancing.
7.5.1 Issues in Designing load-Sharing Algorithms
Similar to the load-balancing algorithms, the design of a load-sharing algorithm also
requires that properdecisions be made regarding load estimation policy, process transfer
policy, state informationexchange policy, location policy, priority assignmentpolicy, and

368 Chap. 7 • Resource Management
migration limitingpolicy.However,ascompared toloadbalancing,itissimplertodecide
about most of these policies in the case of load sharing. This is because, unlike load
balancing algorithms, load-sharing algorithms do not attempt to balance the average
workload on allthenodesofthesystem. Rather,theyonlyattempt toensure thatnonode
isidle when anode isheavily loaded.The priority assignment policies and the migration
limiting policies for load-sharing algorithms are the same as that for the load-balancing
algorithms. Hence their description will not be repeated again. Other policies for load
sharing are described below.
Load Estimation Policies
Since load-sharing algorithms simply attempt to ensure that no node is idle while
processes wait for service at some other node, it is sufficient to know whether a node is
busyor idle. Thus load-sharing algorithms normally employ the simplest load estimation
policy of counting the total number of processes on a node.
Once again the simple load estimation policy of counting the total number of
processes on a node is not suitable for use in modern distributed systems because of the
permanent existence of several processes on an idle node. Therefore, measuring CPU
utilization should be used as a method of load estimation in these systems.
Process Transfer Policies
Since load-sharing algorithms are normally interested only in the busy or idle states of a
node, most of them employ the all-or-nothing strategy. This strategy uses the single
threshold policy with the threshold value of all the nodes fixed at 1. That is, a node
becomes a candidate for accepting a remote process only when it has no process, and a
node becomes a candidate for transferring a process as soon as it has more than one
process. KruegerandLivny[1987]pointedoutthattheall-or-nothingstrategy isnotgood
inthesense thatanodethatbecomesidleisunabletoimmediately acquire newprocesses
to execute even though processes wait for service at other nodes, resulting in a loss of
available processing power in the system. They suggested that, to avoid this loss,
anticipatory transfers tonodes that are not idle butare expected to soon become idle are
necessary. Thus to take care of this loss, some load-sharing algorithms use a threshold
value of 2 instead of 1.
Notice here that if the measure of CPU utilization is used as the load estimation
policy,thehigh-low policy(asdiscussedforloadbalancing)shouldbeusedastheprocess
transfer policy.
Location Policies
In load-sharing algorithms, the location policy decides the sender node or the receiver
node of a process that is to be moved within the system for load sharing. Depending on
the type of node that takes the initiative to globally search for a suitable node for the
process, the location policies are of the following types:

Sec.7.5 • Load-SharingApproach 369
• Sender-initiatedpolicy, in which the sendernode ofthe process decides where to
send the process
• Receiver-initiatedpolicy, in which the receiver node of the process decides from
where to get the process
Each of these policies are described below along with their relative merits and
demerits.
Sender-InitiatedLocationPolicy. Inthesender-initiatedlocationpolicy,heavily
loaded nodes search forlightly loaded nodes towhich work may betransferred. That is,in
thismethod, whenanode'sloadbecomes morethanthethresholdvalue,iteitherbroadcasts
amessageorrandomlyprobes theother nodesonebyonetofindalightly loaded node that
canaccept oneormoreofitsprocesses.A nodeisaviablecandidateforreceivingaprocess
from the sendernode only ifthetransfer oftheprocess tothat node would notincrease the
receiver node's load above its threshold value. In the broadcast method, the presence or
absence of a suitable receiver node is known as soon as the sender node receives reply
messages from the other nodes. On the other hand, in the random probing method, the
probing continues until either a suitable receiver node is found or the number of probes
reaches astaticprobelimit,asL p.Ifasuitablereceivernodeisnotfound,thenodeonwhich
theprocess originatedmustexecutethat process. The method ofprobing withafixed limit
hasbetterscalabilitythanthebroadcastmethodbecauseitensuresthatthecostofexecuting
the load-sharingpolicy willnotbeprohibitiveeven inlarge networks. Eager eta1. [1986a]
foundthroughtheiranalysis thattheperformanceofthispolicyisinsensitivetothechoiceof
probe limit: theperformance withasmall probe limit (e.g., 3or5)isnearly asgood asthe
performancewithalargeprobe limit(e.g., 20).
Receiver-Initiated Location Policy. In the receiver-initiated location policy,
lightly loaded nodes search forheavily loaded nodes from which work maybetransferred.
That is, in this method, when a node's load falls below the threshold value, it either
broadcasts a message indicating its willingness to receive processes for executing or
randomly probes the other nodes one by one to find a heavily loaded node that can send
one or more of its processes. Anode isa viable candidatefor sending one of itsprocesses
only if the transfer of the process from that node would not reduce its load below the
threshold value. In the broadcast method, a suitable node isfound as soon as the receiver
node receives reply messages from the other nodes. On the other hand, in the random
probing method, the probing continues until eithera node is found from which a process
can beobtainedor the number ofprobes reaches astatic probe limit,L p•Inthe latter case,
the node waits for a fixed timeout period before attempting again to initiate a transfer.
Itmay be noted here that, in sender-initiatedalgorithms, scheduling decisions are usually
made at process arrival epochs (or a subset thereot), whereas in receiver..initiated
algorithms,schedulingdecisionsareusually made atprocessdepartureepochs (orasubset
thereof). Owing tothis,receiver-initiatedpoliciestypicallyrequire thepreemptivetransfer
of processes while sender-initiated policies can work even with those systems that do not
support preemptive process migration facility. A preemptive process migration facility
allows the transferof an executing process from one node to another. On the otherhand,

370 Chap. 7 • Resource Management
in systems supporting only non-preemptiveprocessmigration facility,aprocess can only
be transferred prior to beginning its execution. Preemptive process migration is costlier
thannon-preemptiveprocessmigration sincetheprocessstate,whichmustaccompany the
process to its new node, becomes.much more complex after execution begins. Receiver
initiated process transfers are mostly preemptive since it is unlikely that a receiver node
would open negotiation with apotential sender at the moment that a new process arrived
atthesender node.Process transfers thataresenderinitiated, however, maybeeither non
preemptive or preemptive, depending on which process the sender chooses to transfer.
Eager etal. [1986a]usedsimple analytical models forcomparing theperformanceof
the two policies relative to each other and to that of a system in which there is no load
sharing. Their results, which are valid over a wide range of system parameters, show the
following [Eager et al. 1986a]:
• Both sender-initiated and receiver-initiated policies offer substantial performance
advantages over the situation in which no load sharing is attempted.
• Sender-initiated policies are preferable to receiver-initiated policies at light to
moderate system loads. This is because in a lightly loaded system the receiver
initiated policy generates a large number of probe messages because all the free
nodes desperately hunt for work.
• Receiver-initiatedpolicies arepreferable athighsystemloads,butonly ifthecosts
of process transfer under the two strategies are comparable. This is because the
receiver-initiated policy does not put extra load on the system at critical times
(when the system is heavily loaded), but the sender-initiated policy generates a
large number of probe messages precisely when the system is heavily loaded and
can least tolerate extra load generated by probe messages.
• If the cost of process transfer under receiver-initiated policies is significantly
greater than under the sender-initiated policies due to the preemptive transfer of
processes, sender-initiated policies provide uniformly better performance.
State Information Exchange Policies
Since load-sharing algorithms do notaim atequalizing theaverage load on all the nodes,
it is not necessary for the nodes toperiodically exchange the state information with each
other. Rather, a node needs to know the state of other nodes only when it is either
underloaded or overloaded. Therefore, in load-sharing algorithms, a node normally
exchanges state information with other nodes only when its state changes. The two
commonly used policies for this purpose are described below.
Broadcast When State Changes. In this method, a node broadcasts a
StatelnformationRequest message when it becomes either underloaded or overloaded.
Obviously, in the sender-initiated policy, a node broadcasts this message only when it
becomes overloaded, and in thereceiver-initiated policy, this message is broadcast by a
nodewhenitbecomes underloaded. Inreceiver-initiated policies thatuseafixedthreshold
value of 1, this method of state information exchange is called broadcast-when-idle
policy.

Sec.7.6 • Summary 371
Poll When State Changes. Since a mechanism that uses broadcast protocol is
unsuitable for large networks, the polling mechanism is normally used in such systems.
In this method, when a node's state changes, it does not exchange state information
with all other nodes but randomly polls the other nodes one by one and exchanges state
information with the polled nodes. The state exchange process stops either when a
suitable node for sharing load with the probing node has been found or the number of
probes has reached the probe limit. Obviously, in sender-initiated policy, polling is done
by a node when it becomes overloaded, and in receiver-initiated policy, polling is done
by a node when it becomes underloaded. In receiver-initiated policies that use a fixed
threshold value of 1, this method of state information exchange is called poll-when-idle
policy.
7.6 SUMMARY
A resource manager of a distributed operating system schedules the processes in a
distributed system to any one or more or a pool of free resources that can optimize a
combination of resource usage, response time, network congestion, and scheduling
overhead. The process scheduling decisions are based on such factors as the resource
requirements of the processes, the availability ofthe various resources on different nodes
of the system, and the static and/or dynamic state information of the various nodes of the
system. A good global scheduling algorithm should possess features such as having no a
priori knowledge about the processes, being dynamic in nature, and having quick
decision-making capability, balanced system performance and scheduling efficiency,
stability, scalability, faulttolerance,andfairness ofservice. The three different approaches
used for the design ofglobal scheduling algorithms are the task assignmentapproach, the
load-balancing approach, and the load-sharing approach.
In the task assignment approach, the process assignment decisions are basically
basedonapriori knowledgeofthecharacteristicsof boththeprocesses tobeexecutedand
the system. The basic task assignrnent model deals with the assignment of tasks to the
various nodes of the system in such a manner so as to minimize interprocess
communicationcosts and improve the turnaround time for the completetask force. Some
extensions to the basic task assignment model also consider factors such as interference
costs, precedencerelationships,or memory sizeconstraints.The taskassignmentapproach
has limited applicability because itworks on the assumption that thecharacteristicsof all
the processes to be scheduled are known in advance and also because task assignment
algorithms are generally not dynamic in nature.
Inthe load-balancingapproach, theprocess assignmentdecisionsattempt toequalize
the average workload on all the nodes ofthe system. The basic problem associated with
the dynamic load-balancing algorithms is to decide about the amount of global state
information tobe used.Althoughonly limited success has been achieved inthis direction,
the results of the efforts made have shown that attempting to gather a large amount of
information to describe the current global state of the system more accurately is not
always beneficial. Hence, the degree of global knowledge in an algorithm must be, in

372 Chap.7 • ResourceManagement
some way,normalized to the complexity of the performance objective of the algorithm.
This relationship and normalization are the subject of current research.
Finally, in the load-sharing approach, the process assignment decisions attempt to
keep all the nodes busy if there are sufficient processes in the system for all the nodes.
This is achieved byensuring that no node is idle whileprocesses wait to beprocessed at
other nodes. Since load-sharing algorithms do not attempt to balance the load on all the
nodes, they are simpler and easier .to implement as compared to load-balancing
algorithms. "However, for the various global scheduling algorithms, there is no absolute
answer to the question,"Is algorithm A better than B?" Therefore, getting a better
understanding of the processes involved in global scheduling has to be the aim of study
of this type of algorithms.
EXERCISES
7.1. Forglobalscheduling algorithms, whyareheuristic methodsprovidingnear-optimal results
normallypreferabletooptimalsolutionmethods?
7.2. Whatis"processorthrashing"?Giveexamplesoftwoglobalschedulingalgorithmsthatmay
lead to processor thrashing. Suggest necessary measures to be taken to handle this
problem.
7.3. Supposeyouhavetodesignaschedulingalgorithmbasedonthetaskassignmentapproachfor
schedulingthetasksofprocessestobeprocessedinadistributed system.Whattypesofcost
informationwouldyouliketohaveforthetasksofaprocess?Suggestmethodsthatmaybe
usedtomakearoughestimateofthesecosts.
7.4. AsystemconsistsofthreeprocessorsPI'P2' and"P3' andaprocesshavingfourtasks11,12,
t3' and t4 is to beexecutedon thissystem.SupposeE
ij
isthecostof executingtaskt, on
processorPj andCijisthecommunicationcostbetweentaskst,andtj whenthetwotasksare
assigned to different processors. Let Ell=31, E I2=4, En=14, E 21 =I, E22 =5, £23=6,
E 31 =2, E 32=4, £33=24, E 41 =3, E 42=28, E 43=10, CI2=35, C 13=3, C I4=8, C23=6,
C =4,andC =23.Findanoptimalassignmentofthetaskstotheprocessorsandcalculate
24 34
thecostofoptimalassignment. Nowcomparethiscostwiththeassignmentcostofthecase
inwhichtI andt 2 areassignedtoPI' 13 isassignedtoP2' andt 4 isassignedtoP3.
7.5. AsystemconsistsoftwoprocessorsPI andP2' andaprocesshavingsixtasks1), t2,13,14,ts,
and1
6
istobeexecutedonthissystem.SupposeEijisthecostofexecutingtaskIionprocessor
Pj' Cij isthecommunicationcostbetweentasksIiandtj whenthetwotasksareassignedto
differentprocessors,andlijistheinterferencecostbetweentaskst,andt whenthetwotasks
j
areassignedto thesameprocessor. Interference costs reflectthedegreeof incompatibility
betweentwotasksandareincurredwhenthetwotasksareassignedtothesamenode[Lo
1988].LetEll=20,£12=50,E21=25,E
22=
10,£31=5,£32=20,£41 =10,E
42=20,
Est=10,
=
£52=20,£61 =50, £62=10,C 12= 15,C23=50,C 34 =15,C 4s=50, andC S6 15.Thevaluesof
allotherCij arezero.Furthermore, theinterferencecostbetweenanypairoftasksis10(i.e.,
Iij=10ifiisnotequaltoj). Findtheassignmentcostsforthefollowing cases:
(a) TaskII isassignedtoPI andallothertasksareassignedtoP2,andinterferencecostisnot
takenintoaccount.
(b) Task sets {lit 1 2, 13} and {/4' Is, t 6} are assigned to PI and P2, respectively, and
interferencecostisnottakenintoaccount.
(c) Task11 isassignedtop) andallothertasksareassignedtoP2' andinterference costis
takenintoaccount.

Chap. 7 • Exercises 373
(d) Task sets {tit t2, 13} and {t 4, '5' '6} are assigned to PI and P2' respectively, and
intetference cost istaken into account.
What conclusion can bedrawn from the assignment costs of these four cases?
7.6. Acomputersystem has three processorsPI'P2'andP3'It istobe used to process the same
type of processes, allof whichhave six tasks tit12,13,14, ts,and16'Assume thatthearrival
rate of processes isa deterministic process. Also assume that every task's execution time is
exponentially distributedwiththe meanexecution timeof tasks t., 12, 13, 14,15'and16 being
1,10,1,10,1,and 10,respectively.There isastrongprecedence relationshipamongthetasks
of aprocess, and task l ofaprocesscan beexecuted only when theexecution oftask t;(i <
j
j)ofthesameprocesshasbeencompleted.Whichoneofthefollowingassignments willyield
the best response times and which one will yield the worstresponse times forthe processes
executed on this system:
(a) Task pairs (t), ( 2), (t3' (4), and (ts, 16) are assigned to processors PI' P2' and P3'
respectively.
(b) Task pairs (I), (6), (12, 13), and (t4' t5) are assigned to processors PI' P2' and P3'
respectively.
(c) Task pairs (t 1, t 4), (12, 15), and (13, (6) are assigned to processors PI' P2' and P3'
respectively.
Give reasons for your answer.
7.7. AsystemhastwoprocessorsPI andP2withp) havinglimitedmemorycapacityandP2having
unlimited memory. A process having multiple tasks is to be executed on this system. The
execution costs for each task on each processor, the intertask communication costs, the
memory requirement of each task, and the total memory capacity of processorPI is given.
Suppose aJ is the assignment that minimizes the total execution and communication costs
without the memory size constraint, and a2 is the assignment that minimizes the total
execution and communication costs with the memory size constraint. Prove that the tasks
assigned toPI for assignment a2 is a subset of the tasks assigned toPI for assignment at.
7.8. Comment on the practical applicability of the task assignment approach as a scheduling
scheme fordistributed systems.
7.9. Comment on the practical applicability of the load-balancing approach as a scheduling
scheme for the following types of distributed systems:
(a) ALAN-based distributed system
(b) AWAN-based distributed system
(c) Adistributed system based on the processor-pool model
(d) Adistributed system based on the workstation-server model
7.10. Adistributedoperatingsystemdesigner isoftheopinionthatstateinformationinadistributed
system is typically gathered at a high cost. Therefore, for adistributed system based on the
processor-pool model,heorshedecides touseaload-balancing policythatusesthefollowing
simpleprocessplacementrule:Executeallprocesses originatingfrom terminalionprocessor
j (j« i). The valueofj isdefinedforall valuesofi,andfor several valuesofi, thevalueof
j may be the same.
Inyouropinion, isthedesigner'schoiceofthescheduling algorithmappropriateforthis
system?Givereasonsforyouranswer.Whatdrawbacks, ifany,doesthisschedulingalgorithm
have? If you feel that this algorithm is not appropriate for this system, suggest a suitable
global scheduling algorithm for this system.
7.11. Suppose you have todesign acentralized load-balancing algorithm for global scheduling of
processes inadistributed system.Whatissuesmustyouhandle?Suggestasuitablemethodfor
handling each of the issues mentioned by you.

374 Chap. 7 • Resource Management
7.12. Supposeyouhavetodesignaload-balancing algorithm foradistributed system.Whichofthe
selfish,altruistic,andintermediatepriorityassignment policies willyouuseinyouralgorithm
if the distributed system is based on the following:
(a) Workstation-server model
(b) Processor-pool model
Give reasons for your answer.
7.13. Suppose youhavedecided tousethehigh-low policyastheprocess transfer policyofaJoad
balancing algorithm for adistributed system. Suggest asuitable method that you will usein
your implementation for choosing the high-mark and low-mark values. Do these threshold
values have to be the same for all processors inthe system? Give reasons for your answer.
7.14. Load balancing in the strictest sense is not achievable indistributed systems. Discuss.
7.15. What are the main differences between the load-balancing and load-sharing approaches for
process scheduling in distributed systems? Which of the various policies to be used in the
implementation of the two approaches are different and which of them are same?
7.16. Suppose youhavetodesignaload-sharing algorithm foradistributed system. Willyouprefer
touseasender-initiated orareceiver-initiatedlocationpolicyinyouralgorithm? Givereasons
for your answer.
7.17. Suggest some policies that may be used for load estimation in load-balancing algorithms.
Discuss the relative advantages and disadvantages of the policies suggested by you. Which
one of the policies suggested by you can also be used for load-sharing aJgorithms? If none,
suggest a suitable load estimation policy for a load-sharing algorithm.
7.18. Asystem hastwo processorsPI andP2.Suppose ata particular instance of time,PI hasone
process with remaining service time of 200 seconds andP2has 100processes each with a
remaining service timeof 1second. Nowsupposeanewprocessenters thesystem.Calculate
the response time of the new process if:
(a) The new process is a )-second process and it isallocated toPI for execution.
(b) The new process isa )-second process and it isallocated toP2for execution.
(c) The new process isa 200-second process and it isallocated toPI for execution.
(d) The new process isa 200-second process and it isallocated toP2for execution
Assumethattherearenoothernewarrivalsinthesystem.Fromtheobtainedresults, whatcan
you conclude about load estimation policies to be used in load-balancing algorithms?
7.19. Adistributed systemdoesnotsupportpreemptiveprocessmigrationfacility.Youaretodesign
a load-sharing algorithm for scheduling of processes in this system. Will you use a sender
initiated or a receiver-initiated location policy for your algorithm? Give reasons for your
answer.
7.20. Adistributedsystemusestheall-or-nothing strategyastheprocesstransferpolicyforitsload
sharing algorithm. Explain why the processing capabilities of the processors of this system
may not beproperly utilized. Suggest asuitable method to overcome this problem.
7.21. What research issues do you think need further attention in the area ofglobal scheduling of
processes indistributed systems?
BIBliOGRAPHY
[Ahrens and Hansen 1995]Ahrens, J. P, and Hansen, C. D., "Cost-EffectiveData-Parallel Load
Balancing,"In:Proceedingsofthe24thAnnualInternationalConferenceonParallelProcessing,
eRe Press, Boca Raton, FL (August )995).

Chap. 7 • Bibliography 375
[Alonsoand Cova1988]Alonso,R., and Cova,L.L., "SharingJobsAmongIndependentlyOwned
Processors," In: Proceedings of the 8th International Conference on Distributed Computing
Systems, IEEE, New York, pp. 282-288 (June1988).
[Arora and Rana 1980] Arora, R. K., and Rana, S. P., "Heuristic Algorithms for Process
Assignment in Distributed Computing Systems," Information Processing Letters, Vol. 11,No.
4/5, pp. 199-203 (1980).
[Atallahetal, 1992]Atallah, M.1.,Black, D.C., Marinescu, D.C., Seigel,H.1.,and Casavant,T.
L., "Models and Algorithms for Co-scheduling Compute-Intensive Tasks on a Network of
Workstations," Journal ofParallel and DistributedComputing, Vol. 16, pp. 319-327 (1992).
[Barak and Shiloh 1985]Barak,A.,and Shiloh,A., "A Distributed Load Balancing Policy for a
Multicomputer," Software--Practice and Experience, Vol. 15, No.9, pp.901-913 (1985).
[Bokhari 1979]Bokhari, S. H.,"Dual Processor Scheduling with Dynamic Reassignment," IEEE
Transactions on Software Engineering, Vol. SE-5, No.4, pp. 341-349 (1979).
[Bokhari 1981a] Bokhari, S. H., "On the Mapping Problem," IEEE Transactions on Computers,
Vol.C-30, No.3, pp. 207--214 (]98!).
[Bokhari 1981b] Bokhari, S. H., "A Shortest Tree Algorithm for Optimal Assignments Across
SpaceandTime inaDistributedProcessorSystem,"IEEE Transactions onSoftware Engineering,
Vol. SE-7, No.6, pp. 583-589 (]981).
[Bonomi and Kumar 1990]Bonomi, F.,and Kumar, A., "Adaptive Optimal Load Balancingin a
Heterogeneous Multiserver System with a Central Job Scheduler," IEEE Transactions on
Computers, Vol.C-39, pp. 1232-1250 (1990).
[Bryant and Finkel 1981] Bryant, R. M., and Finkel, R.A., "A Stable Distributed Scheduling
Algorithm," In: Proceedings ofthe 2nd International Conference on Distributed Computing
Systems, IEEE,New York, pp. 314-323 (April 1981).
[Casavant and Kuhl 1986J Casavant, T. L., and Kuhl, 1. G., "A Formal Model of Distributed
DecisionMakingand ItsApplication to DistributedLoad Balancing,"In: Proceedingsofthe6th
International Conference on Distributed Computing Systems, IEEE, New York, pp. 232-239
(May 1986).
[Casavant and Kuhl 1988a) Casavant, T. L., and Kuhl, J. G., "A Taxonomy of Scheduling in
General Purpose Distributed Computing System," IEEE Transactions on Software Engineering,
Vol.SE-14, No.2, pp. 141-]54 (1988).
[Casavant and Kuhl 1988b} Casavant, T.L., and Kuhl, J.G., "Effects ofResponse and Stability
on SchedulinginDistributedComputingSystems,"IEEE Transactions onSoftware Engineering,
Vol.SE-14, No. 11,pp. 1578-1587 (1988).
[Chang and Oldham 1995)Chang, H.W.D., and Oldham, W.1. B., "DynamicTask Allocation
Models for Large Distributed Computing Systems," IEEE Transactions on Parallel and
DistributedSystems, Vol.6, No. 12, pp. 1301-1315 (J995).
[Chou and Abraham 1982]Chou, T.C. K., and Abraham, 1.A., "Load Balancing in Distributed
Systems," IEEE Transactions on Software Engineering, Vol.SE-8, No.4, pp. 401-412 (1982).
[Chou andAbraham1983]Chou,T.C. K., and Abraham,1.A., "LoadDistributionUnderFailure
in Distributed Systems," IEEE Transactions on Computers, Vol. C-32, No.9, pp. 799-808
(1983).
lChuetal, 1980]Chu,W.,Holloway,L.J., Lan, M.T.,and Efe, K.,"Task AllocationinDistributed
Data Processing," IEEE Computer, Vol. 13,pp. 57-69 (1980).

376 Chap. 7 • Resource Management
[ehuand Lan 1987] Chu, W.W.•and Lan, L.M-T.•"TaskAllocationand PrecedenceRelationsfor
Distributed Real-Time Systems:' IEEE Transactions on Computers. Vol. C-36. No.6. pp.
667-679 (1987).
[Clark and McMillin 1992JClark. H.•and McMillin. B., "DAWGS-A Distributed Compute
ServerUtilizingIdleWorkstations,"JournalofParallelandDistributedComputing.pp. 175-186
(February 1992).
[Dandamudi 1995]Dandamudi, S., "Performance Impact of Scheduling Discipline on Adaptive
LoadSharingin HomogeneousDistributedSystems," In: Proceedingsofthe J5th International
Conference on DistributedComputing Systems IEEE. New York(1995).
[Eageretal.19868]Eager, D. L., Lazowska,E. D., and Zahorjan,J., "A Comparisonof Receiver
Initiated and Sender-Initiated Adaptive Load Sharing:' Performance Evaluation, Vol. 6, pp.
53-68 (1986).
[Eageretal. 1986b]Eager. D. L., Lazowska, E.D., and Zahorjan,1.,"Adaptive Load Sharing in
Homogeneous Distributed Systems," IEEE Transactions on Software Engineering, Vol.SE-12.
No.5, pp. 662-675 (1986).
[Ere1982] Efe, K.,"Heuristic Models of Task Assignment Scheduling in Distributed Systems,"
IEEEComputer, Vol. 15.,pp. 50-56 (1982).
[Ereand Groselj 1989]Efe, K.,andGroselj, B.,"MinimizingControlOverheadsinAdaptiveLoad
Sharing,"In:Proceedingsofthe9thInternationalConference onDistributedComputingSystems,
IEEE. New York, pp. 307-3]5 (June 1989).
[Ere and Krishnamoorthy 1995] Efe, K., and Krishnamoorthy, V., "Optimal Scheduling of
Compute-Intensive Tasks on a Network of Workstations," IEEE Transactions on Parallel and
DistributedSystems, Vol.6, No.6, pp. 668-673 (1995).
[EreandSchaar1993J Efe,K., and Schaar, M., "Performanceof Co-scheduling on a Network of
Workstations," In: Proceedings ofthe 13th International Conference on DistributedComputing
Systems, IEEE, New York, pp. 525-53] (J993).
[EI·Rewini et al, 1994]EI-Rewini, H., Lewis, T., and Ali, H., Task Scheduling in Parallel and
DistributedSystems, Prentice-Hall, EnglewoodCliffs, NJ(1994).
[EI-Rewinietale1995]El-Rewini,H.,Lewis, T.,andAli, H.,"TaskSchedulinginMultiprocessing
Systems," IEEEComputer, Vol.28, No. ]2, pp. 27-37 (December J995).
[Ferguson etal, 1988] Ferguson,D.,Yemini,Y.,and Nikolaou,C.,"MicroeconomicAlgorithmsfor
Load Balancing in Distributed Computer Systems," In: Proceedings ofthe 8th International
Conference on DistributedComputing Systems, IEEE, New York,pp. 491-499 (1988).
[Gao et al. 1984] Gao, C., Liu, 1. W. S., and Railey, M., "Load Balancing Algorithms in
Homogeneous Distributed Systems," In: Proceedings ofthe 1984 International Conference on
ParallelProcessing, CRC Press, Boca Raton, FL,pp. 302-306(August 1984).
[Hagman 1986] Hagman, R., "Process Server: Sharing Processing Power in a Workstation
Environment," In: Proceedings ofthe 6th International Conference on Distributed Computing
Systems, IEEE, New York,pp. 260-267 (1986).
[Heirichand Taylor 1995]Heirich,A.,and Taylor, S., "A Parabolic Load BalancingMethod,"In:
Proceedings ofthe 24th Annual International Conference on Parallel Processing, CRC Press,
Boca Raton, FL (August 1995).
[Usu and Liu 1986] Hsu, C. H., and Liu, 1. W. S., "Dynamic Load Balancing Algorithm in
Homogeneous Distributed Systems," In: Proceedings ofthe 6th International Conference on
DistributedComputing Systems, IEEE, New York,pp. 216-222 (May 1986).

Chap. 7 • Bibliography 377
[Hwanget al, 1982]Hwang, K.,Croft, W.J., Goble, G. H.,Wah, B.W.,Briggs, F.A., Simmons,
W.R.,andCoates, C.L.,"A UNIX-BasedLocalComputerNetworkwithLoad Balancing,"IEEE
Computer, Vol. 15,pp. 55-66 (April 1982).
[Kleinrockand Korfhage 1989]Kleinrock, L.,and Korfhage, W.,"CollectingUnused Processing
Capacity:AnAnalysisofTransientDistributedSystem,"In:Proceedingsofthe9thInternational
Conference on DistributedComputing Systems, IEEE, New York,pp. 482-489 (June 1989).
[KruegerandChawla1991]Krueger, P.,andChawla,R.,"TheStealth DistributedScheduler,"In:
Proceedingsofthe11thIntemationalConferenceonDistributedComputingSystems,IEEE,New
York, pp. 336-343 (1991).
[Krueger and Livny 1987] Krueger, P.,and Livny, M., "The Diverse Objectives of Distributed
Scheduling Policies," In: Proceedings of the 7th International Conference on Distributed
Computing Systems, IEEE, New York,pp. 242-249 (September 1987).
[Kunz 1991] Kunz, T.,"The Influence of Different Workload Descriptions on a Heuristic Load
Balancing Scheme," IEEE Transactions on Software Engineering, Vol. SE-17, No.7, pp.
725-730 (1991).
[Lee and Towsley 1986] Lee, K. 1., and Towsley, D., "A Comparison of Priority-Based
Decentralized Load Balancing Policies," In: Proceedings ofthe 10thSymposium on Operating
SystemPrinciples,AssociationforComputingMachinery,NewYork,NY,pp.70-77 (November
1986).
[Lin and Raghavendra 1992] Lin, H. C., and Raghavendra, C. S., "A Dynamic Load-Balancing
Policy with aCentralJob Dispatcher(LBC)," IEEE Transactions onSoftwareEngineering, Vol.
18,No.2, pp. 148-158 (1992).
[Litzkow et at. 1988] Litzkow, M. 1.,Livny,M.,and Mukta,M.W.,"Condor-A Hunter of Idle
Workstations," In: Proceedings ofthe 8th International Conference on Distributed Computing
Systems, IEEE, New York,pp. 104-1]1(June 1988).
[Livny and Melman 1982] Livny, M., and Melman, M., "Load Balancing in Homogeneous
Broadcast Distributed Systems," In: Proceedings ofthe ACM Computer Network Performance
Symposium, pp. 47-55 (April 1982).
[Lo 1988] Lo, V.M., "Heuristic Algorithms for Task Assignment in Distributed Systems," IEEE
Transactions on Computers, Vol. 37, No. II, pp. 1384-1397 (1988).
[Maloneet al, 1988] Malone, T.W.,Fikes, R.E.,Grant, K.R.,and Howard, M.T.,"Enterprise:A
Market-like Task Scheduler for Distributed Computing Environments," The Ecology of
Computation, Huberman, B.A., Ed., North-Holland, Amsterdam, pp. 177-205 (1988).
[Mehra and Wah 1995a] Mehra, P., and Wah, B., "Synthetic Workload Generation for Load
BalancingExperiments," IEEEParallel and DistributedTechnology,Systems andApplications,
pp. 4-19 (Fall 1995).
[Mebra and Wah 1995b] Mehra, P.,and Wah, B., Load Balancing: An Automated Learning
Approach, World Scientific, River Edge, NJ(1995).
[Milojcic 1994] Milojcic, D. S., Load Distribution, Implementation for the Mach Microkemel,
VerlagVieweg, Wiesbaden (1994).
[Mirchandaneyetal, 1989a]Mirchandaney,R.,Towsley, D.,andStankovic,J.A.,"AdaptiveLoad
Sharing in Heterogeneous Systems," In: Proceedings ofthe 9th International Conference on
Distributed Computing Systems, IEEE, NewYork, NY, pp.298-306 (June 1989).
[Mircbandaneyetal.1989b]Mirchandaney,R.,Towsley,D.,andStankovic,1.A.,"Analysisofthe
Effects of Delays on Load Sharing," IEEE Transactions on Computers, Vol.38, pp. 1513-1525
(1989).

378 Chap. 7 • Resource Management
[Mukta and Livny1987]Mukta,M. W.,and Livny, M., "SchedulingRemoteProcessingCapacity
in aWorkstation-ProcessorBankComputing Systems," In: Proceedings ofthe 7thInternational
Conference on Distributed Computing Systems, IEEE, New York, NY, pp. 2-9 (September
1987).
[Niand Hwang1985]Ni,L.M., and Hwang,K.,"OptimalLoad BalancinginaMultipleProcessor
System with Many Job Classes," IEEE Transactions on Software Engineering, Vol.SE-Il, No.
5, pp. 491-496 (1985).
[Nietal. 1985J Ni, L. M., Xu, C. W.,and Gendreau,T.B., "A DistributedDraftingAlgorithm for
Load Balancing," IEEE Transactions on Software Engineering, Vol. SE-ll, No. 10, pp.
1153-1161 (1985).
[Nichols1987]Nichols,D.A.,"UsingIdle WorkstationsinaSharedComputingEnvironment,"In:
Proceedings ofthe IIthSymposium onOperating System Principles, Associationfor Computing
Machinery, New York, NY,pp. 5-12, (November 1987).
[Pulidasetal,1988]Pulidas,S.,Towsley, D.,andStankovic,1.A., "ImbeddingGradientEstimators
in Load Balancing Algorithms," In: Proceedings of the 8th International Conference on
DistributedComputing Systems, IEEE, New York, NY,pp. 482-490(1988).
[Raoet al, 1979] Rao, G.S., Stone, H. S., and Hu,T.C., "Assignment of Tasks in a Distributed
ProcessorSystem with Limited Memory," IEEE Transactions on Computers, Vol.C-28, No.4,
pp. 291-299 (1979).
[Shenand Tsai1985]Shen,C.C., and Tsai, W.H., "A GraphMatchingApproachtoOptimalTask
Assignmentin DistributedComputingSystems with MinimaxCriterion,"IEEE Transactionson
Computers, Vol.C-34, pp. 197-203 (1985).
[Shirazi et al, 1995] Shirazi, B.A., Hurson,A. R., and Kavi, K.M.(Eds.), Scheduling and Load
Balancing inParallelandDistributedSystems, IEEEComputerSocietyPress, LosAlamitos,CA
(1995).
[Shivaratriand Krueger1990] Shivaratri,N.G.,and Krueger, P.,"TwoAdaptiveLocationPolicies
for Global Scheduling Algorithms," In: Proceedings ofthe 10th International Conference on
DistributedComputing Systems, IEEE, New York, NY,pp. 502-509 (1990).
[Shivaratri et al, 1992]Shivaratri, N. G., Krueger, P.,and Singhal, M., "Load Distributing for
Locally Distributed Systems," IEEE Computer, Vol.25, No. 12,pp. 33-44 (1992).
[SinghalandShivaratri1994]Singhal,M.,andShivaratri,N.G.,AdvancedConceptsinOperating
Systems, McGraw-Hili, New York (1994).
[Smirni etal. 1995]Smirni, E., Rosti, E., and Serrazi, G., "PerformanceGains from Leaving Idle
Processors in Multiprocessor Systems;' In: Proceedings of the 24th Annual International
Conference on Parallel Processing, CRC Press, Boca Raton, FL (August 1995).
[Smith 1980] Smith, R. G., "TheContract Net Protocol: High-Level Communication and Control
.in a Distributed Problem-Solver," IEEE·Transactions on Computers, Vol. C-29, No. 12, pp.
1104-1113 (1980).
[Srinivasan and Jha 1995] Srinivasan, S., and Jha, N. K., "Task Allocation for Safety and
ReliabilityinDistributedSystems,"In:Proceedingsofthe24thAnnualInternationalConference
on Parallel Processing, CRe Press, Boca Raton, FL (August 1995).
[Stankovic 1984J Stankovic, J.A., "Simulation of ThreeAdaptive, Decentralized Controlled, Job
SchedulingAlgorithms," Computer Networks, Vol.8, No.3, pp. ]99-217 (1984).
[Stankovic 1985] Stankovic, 1. A., "Stability and Distributed Scheduling Algorithms," IEEE
Transactions on Software Engineering, Vol.SE-l1, No. 10,pp. 1141-1152 (1985).

Chap. 7 • Bibliography 379
[StankovicandSidhu1984)Stankovic,1.A.,and Sidhu,I.S.,"AnAdaptiveBiddingAlgorithmfor
Processes,Clustersand DistributedGroups,"In:Proceedingsofthe4thInternationalConference
on DistributedComputingSystems,IEEE,New York, NY,pp. 49-59 (May (984).
[Stone1977]Stone,ItS., "MultiprocessorSchedulingwith theAid of NetworkFlow Algorithms,"
IEEETransactionsonSoftwareEngineering,Vol.SE-3, No.1, pp.85-93 (1977).
[Stone 1978]Stone, H. S., "Critical Load Factors in Two-Processor Distributed Systems," IEEE
Transactions onSoftwareEngineering,Vol.SE-4, No.3, pp. 254-258 (1978).
[Stumm1988]Stumm,M., "TheDesignand ImplementationofaDecentralizedSchedulingFacility
for a Workstation Cluster," In: Proceedings of the 2nd IEEE Conference on Computer
Workstations, IEEE, New York, NY, pp. 12-22 (March 1988).
[Svensson 1990]Svensson, A., "History-An Intelligent Load Sharing Filter," In: Proceedingsof
the10thInternationalConferenceonDistributedComputingSystems,IEEE,New York, NY,pp.
546-553 (1990).
[Tanenbaum 1995]Tanenbaum, A. S.,Distributed OperatingSystems, Prentice-Hall, Englewood
Cliffs, Nl (1995).
[Tantawi and Towsley1985]Tantawi,A. N.,and Towsley, D.,"Optimal Static Load Balancingin
Distributed ComputerSystems," JournalofACM, Vol. 32, No.2, pp. 445-465 (1985).
[Theimer and Lantz 1989] Theimer, M. M., and Lantz, K. A., "Finding Idle Machines in a
Workstation-Based Distributed System," IEEE Transactionson SoftwareEngineering,Vol. 15,
pp. 1444-1458 (1989).
[Tilborg and Wittie 1984]Tilborg, A. M., and Wittie, L. D., "Wave Scheduling-Decentralized
SchedulingofTask ForcesinMulticomputers,"IEEETransactionsonComputers,Vol.C-33,pp.
835-844 (1984).
[Wah 1984] Wah, B. W., HA Comparative Study of Distributed Resource Sharing on
Multiprocessors," IEEETransactionson Computers, Vol.C-33, pp. 700-711 (1984).
[Wah and Juang 1985] Wah, B. W., and luang, J-Y,"Resource Scheduling for Local Computer
Systems with a Multiaccess Network," IEEETransactionsonComputers,Vol.C-34, No.2, pp.
1144-1157 (1985).
[Waldspurger et al, 1992] Waldspurger, C. A., Hogg, T., Huberman, B. A., Kephart, 1. 0., and
Stornetta, W. S., "Spawn: A Distributed Computational Economy," IEEE Transactions on
SoftwareEngineering,Vol. 18, No.2, pp. 103-117 (1992).
[Wangand Morris1985]Wang, Y.T.,and Morris,R.1.T.,"LoadSharinginDistributedSystems,"
IEEE TransactionsonComputers,Vol.C-34, No.3, pp. 204-217 (1985).
[WuandLiu1980]Wu,C.S., and Liu,M.T.,"AssignmentofTasks and Resourcesfor Distributed
Processing," In: Proceedings of the 1980 International Conference on Computers and
Communications(COMPc"ON-80), IEEE,NewYork, NY, pp. 655-662 (Fall 1980).
[Zhou 1988] Zhou, S., HA Trace-Driven Simulation Study of Dynamic Load Balancing," IEE'E
TransactionsonSoftwareEngineering,Vol. 14, pp. 1327-1341 (1988).

380 Chap. 7 • Resource Management
POINTERS TOBI8ll0GRAPHIES ONTHE INTERNET
Bibliographies containing references on the topics covered in this chapter can be found
at:
ftp:ftp.cs.umanitoba.calpublbibliographieslParallellLoad.Balance.l.html
ftp:ftp.cs.umanitoba.calpublbibliographieslParallellLoad.Balance.2.html
ftp:ftp.cs.umanitoba.ca/publbibliographieslParallellload.balance.5.html
ftp:ftp.cs.umanitoba.ca/publbibliographieslParalleVscheduling.html
ftp:ftp.cs.umanitoba.ca/publbibliographieslDistributedldshell.html

8
CHAPTER
Process
Management
8.1 INTRODUOION
In a conventional (centralized) operating system, process management deals with
mechanisms and policies for sharing the processor of the system among all processes.
Similarly, in a distributed operating system, the main goal-ofprocess management is to
make the best possible use of the processing resources of the entire system by sharing
them among all processes. Three important concepts are used in distributed operating
systems to achieve this goal:
1. Processorallocation deals with the process of deciding which process should be
assigned to which processor.
2. Processmigration deals with the movementofa process from itscurrentlocation
to the processor to which it has been assigned.
3. Threadsdeals with fine-grainedparallelismforbetterutilizationoftheprocessing
capability of the system.
The processorallocation concept has already been described in the previous chapter
on resource management. Therefore, this chapter presents a description of the process
migration andthreads concepts.
381

382 Chap. 8 • Process Management
8.2 PROCESS MIGRATION
Process migrationistherelocationofaprocessfromitscurrentlocation(thesource node)
to another node (the destination node). The flow of execution of a migrating process is
illustrated in Figure 8.1.
Source Destination
node node
r
I
r
Time ProcessP1 in I
,I
execution I
Execution I
I
suspended I
Freezing Transferof :
time control:
...z-Io------
--------...;;:~Execution
resumed
Process P in
1
execution
Fig. 8.1 Flowofexecution ofamigrating process.
A process may be migrated either before it starts executing on its source node or
during the course of its execution. The former is known as non-preemptive process
migration and the latter is known as preemptive process migration. Preemptive process
migration is costlier than non-preemptive process migration since the process environ
ment must also accompany the process to its new node for an already executing
process.
Process migration involves the following major steps:
1. Selection of a process that should be migrated
2. Selection of the destination node to which the selected process should be
migrated
3. Actual transfer of the selected process to the destination node
Thefirsttwostepsaretakencareofbytheprocessmigrationpolicyandthethirdstep
is taken care ofby the process migration mechanism. The policies for the selection of a
source node, a destination node, and the process to be migrated have already been
described in the previous chapter on resource management. This chapter presents a
description of the process migration mechanisms used by the existing distributed
operating systems.

Sec.8.2 • Process Migration 383
8.2.1 Desirable Features of a Good Proc.ss Migration
Mechanism
A good process migration mechanism must possess transparency, minimal interferences,
minimal residue dependencies, efficiency, robustness, and communication between
coprocesses.
Transparency
Transparency is an important requirement for a system that supports process migration.
The following levels of transparency can be identified:
1. Object access level. Transparency at the object access level is the minimum
requirementforasystem tosupport non-preemptiveprocess migration facility.Ifasystem
supportstransparencyattheobject access level, access toobjects suchasfilesanddevices
can bedone in alocation-independent manner.Thus, the object access level transparency
allows free initiation of programs at anarbitrary node. Ofcourse, to support transparency
atobject access level,thesystem mustprovide amechanism fortransparent object naming
and locating.
2. System call and interprocess communication level. So that a migrated process
does notcontinuetodepend upon itsoriginatingnodeafterbeing migrated, itisnecessary
that all system calls, including interprocess communication, are location independent.
Thus, transparency atthis levelmustbeprovided inasystem thatistosupport preemptive
process migration facility. However, system calls to request the physical properties of a
node need not be location independent.
Transparency of interprocess communication is also desired for the transparent
redirection of messages during thetransient state ofaprocess thatrecently migrated.That
is,once amessage has been sent, itshould reach itsreceiver process without the need for
resending it from the sender node in case the receiver process moves to another node
before the message is received.
Minimal Interference
Migration of a process should cause minimal interference to the progress of the process
involved and to the system as a whole. One method to achieve this is by minimizing the
freezing time of the process being migrated. Freezing time is defined as the time period
for which the execution of the process is stopped for transferring its information to the
destination node.
Minimal Residual Dependencies
No residual dependency should be left on the previous node. That is, a migrated process
should not in any way continue to depend on its previous node once it has started
executing on its new node since, otherwise, the following will occur:

384 Chap. 8 • Process Management
• The migrated process continues to impose a load on its previous node, thus
diminishing some of the benefits of migrating the process.
• A failure or reboot of the previous node willcause the process to fail.
Emciency
Efficiencyisanothermajorissueinimplementingprocessmigration.Themainsourcesof
inefficiency involved with process migration are as follows:
• The time required for migrating a process
• The cost of locatingan object (includes the migrated process)
• The cost of supporting remote execution once the process is migrated
All thesecosts should be kept to minimum as far as practicable.
Robustness
The process migration mechanism must also be robust in the sense that the failure of a
node other than the one on which a process is currently running should not in any way
affect the accessibility or execution of that process.
Communication between Coprocesses of a Job
One further exploitation of process migration is the parallel processing among the
processes of a single job distributed over several nodes. Moreover, if this facility is
supported, to reduce communication cost, it is also necessary that these coprocesses be
able to directly communicate with each other irrespective of their locations.
8.2.2 Proc.ss Migration M.chanlsms
Migration of a process is a complex activity that involves proper handling of several
sub-activities in order to meet the requirements of a good process migration
mechanism listed above. The four major subactivities involved in process migration
are as follows:
1. Freezing the process on its source node and restarting it on its destination node
2. Transferring the process's address space from its source node to its destination
node
3. Forwarding messages meant for the migrant process
4. Handlingcommunicationbetweencooperatingprocessesthathavebeenseparated
(placed on different nodes) as a result of process migration
The commonly used mechanisms for handling each of these subactivities are
described below.

Sec.8.2 • Process Migration 38S
Mechanisms for Freezing and Restarting a Process
Inpreemptiveprocess migration, theusual process istotakea"snapshot"oftheprocess's
state on its source node and reinstate the snapshot on the destination node. For this, at
some pointduring migration, theprocess isfrozen onitssource node, itsstateinformation
is transferred to its destination node, and the process is restarted on its destination node
using this state information. By freezing the process, we mean that the execution of the
process issuspended and allexternal interactionswith the process aredeferred. Although
the freezing and restart operations differ from system to system, some general issues
involved in these operations are described below.
Immediate and Delayed Blocking ofthe Process. Before a process can be
frozen, itsexecution must be blocked. Depending upon theprocess's current state, itmay
beblocked immediatelyor the blocking may have to bedelayed until the process reaches
a state when itcan be blocked. Some typical cases are as follows [Kingsbury and Kline
1989]:
1. If the process isnot executing a system call, itcan be immediately blocked from
further execution.
2. Iftheprocess isexecuting asystem call but issleeping ataninterruptiblepriority
(a priority at which any received signal would awaken the process) waiting for a
kernel event to occur, it can be immediately blocked from further execution.
3. If the process is executing a system call and is sleeping at a noninterruptible
priority waiting forakernel event tooccur, itcannot beblocked immediately.The
process's blocking has to be delayed until the system call iscomplete. Therefore,
in this situation, a flag is set, telling the process that when the system call is
complete, it should block itselffrom further execution.
Note that there may be some exceptions to this general procedure of blocking a
process. For example, sometimes processes executing in certain kernel threads are
immediately blocked, even when sleeping at noninterruptible priorities. The actual
mechanism varies from one implementation to another.
Fast and Slow I/O Operations. In general, after the process has been blocked,
thenextstepinfreezing theprocess isto waitforthecompletionofallfastI/Ooperations
(e.g., disk 110)associated with the process. The process isfrozen after the completionof
all fast I/O operations. Note that it is feasible to wait for fast I/O operations to complete
before freezing the process. However, it is not feasible to wait for slow 110 operations,
suchasthoseonapipeorterminal, because theprocess mustbefrozeninatimely manner
for the effectiveness of process migration. Thus proper mechanisms are necessary for
continuing these slow I/O operations correctly after the process starts executing on its
destination node.
Information about Open Files. A process's state information also consists of
the information pertaining to files currently open by the process. This includes such
information as the names or identifiers of the files, their access modes, and the current

386 Chap. 8 • Process Management
positionsoftheir filepointers. Inadistributedsystem thatprovides anetwork transparent
executionenvironment,thereisnoproblem incollectingthisstateinformationbecause the
same protocol isused toaccess local as well asremote files using the systemwideunique
file identifiers. However, several UNIX-based network systems uniquely identify files by
their full pathnames [Mandelberg and Sunderam 1988,Alonso and Kyrimis 1988].But in
these systems itisdifficult foraprocess inexecutiontoobtain afile'scompletepathname
owing to UNIX file system semantics. Apathnameloses significanceonce afilehas been
opened by aprocess because the operating system returns to the process a file descriptor
that theprocess uses toperform all I/Ooperationson thefile.Therefore, in such systems,
itisnecessarytosomehow preserve apointertothefilesothatthemigratedprocess could
continue to access it. The following two approaches are used for this:
1. Inthefirstapproach [MandelbergandSunderam 1988],alinkiscreatedtothefile
and the pathname of the link is used as an access point to the file after the process
migrates. Thus when the snapshot ofthe process's state is being created, a link (witha
special name) is created to each file that is in use by the process.
2. In the second approach [Alonso and Kyrimis 1988], an open file's complete
pathname is reconstructed when required. For this, necessary modifications have to be
incorporated in the UNIX kernel. For example, in the approach described in [Alonso and
Kyrimis 1988], each file structure, where information about open files is contained, is
augmented with a pointer to a dynamically allocated character string containing the
absolute pathname ofthe file to which it refers.
Anotherfile system issue is that one or more files being used by the process on its
source node may also bepresent on itsdestination node. For example, it islikely that the
code for system commandssuch asnroff,cc isreplicatedatevery node. Itwould be more
efficienttoaccess these files from the local node at which theprocess isexecuting, rather
than accessing them across the network from the process's previous node [Agrawal and
Ezzat 1987].Anotherexample istemporary files that would bemoreefficientlycreated at
the node on which the process is executing, as by default these files are automatically
deleted at the end of the operation. Therefore, for performance reasons, the file state
information collected at the source node should be modified properly on the process's
destination node in order to ensure that, whenever possible, local file operations are
performedinstead ofremote fileoperations. Thisalsohelpsinreducing theamount ofdata
to be transferredatthe time ofaddress space transfer because the files already present on
the destination node need not betransferred.
ReinstatingtheProcessonitsDestination Node. Onthedestinationnode, an
empty process state is created that is similar to that allocated during process creation.
Dependingupontheimplementation,thenewly allocatedprocess mayormaynothavethe
same process identifier as the migrating process. In some implementations, this newly
created copy ofthe process initially has a process identifierdifferent from the migrating
process in order to allow both the old copy and the new copy to exist and be accessible
at the same time. However, if the process identifier of the new copy of the process is
different from its old copy, the new copy's identifier is changed to the original identifier

Sec.8.2 • Process Migration 387
in a subsequent step before the process starts executing on the destination node. The rest
ofthe system cannot detect theexistenceof two copies of the process because operations
on both of them are suspended. Once all the state of the migrating process has been
transferredfromthesource nodetothedestinationnodeandcopied intotheempty process
state,thenewcopyoftheprocess isunfrozen andtheoldcopy isdeleted. Thus theprocess
is restarted on its destination node in whatever state it was in before being migrated.
Itmaybenotedherethatthemethod describedabovetoreinstate themigrant process
isfollowed inthemost simple and straightforwardcase. Several special casesmay require
special handling and hence more work. For example, when obtaining the snapshot, the
process may have been executing a system call since some calls are not atomic. In
particular, as described before, this can happen when the process is frozen while
performing an I/O operation on a slow device. Ifthe snapshot had been taken under such
conditions, correct process continuation would be possible only if the system call is
performed again.Thereforenormally acheck ismadefortheseconditions and,ifrequired,
the program counter is adjusted as needed to reissue the system call.
Address Space Transfer Mechanisms
A process consists of the program being executed, along with the program's data, stack,
and state. Thus, the migration of a process involves the transfer of the following types of
information from the source node to the destination node:
• Process's state, which consists of the execution status (contents of registers),
scheduling information, information about main memory being used by the
process (memory tables), I/O states (I/O queue, contents of I/O buffers, interrupt
signals, etc.), alistofobjects towhich theprocess hasaright toaccess (capability
list), process's identifier, process's user and group identifiers, information about
the files opened by the process (such as the mode and current position of the file
pointer), and so on
• Process's address space (code, data, and stack of the program)
For nontrivial processes, the size of the process's address space (several megabytes)
overshadowsthe sizeoftheprocess'sstateinformation(fewkilobytes). Therefore thecost
of migrating a process is dominated by the time taken to transfer its address space.
Although it is necessary to completely stop the execution of the migrant process while
transferring its state information, it is possible to transfer the process's address space
without stopping its execution. In addition, the migrant process's state information must
be transferred to the destination node before it can start its execution on that node.
Contrary to this, the process's address space can be transferred to the destination node
either before or after the process starts executing on the destination node.
Thus inallthesystems, themigrant process'sexecutionisstopped whiletransferring
its state information. However, due to the flexibility in transferring the process's address
space atanytime afterthemigration decision ismade, theexisting distributedsystems use
oneofthefollowing address space transfermechanisms: total freezing, pretransferring,or
transfer on reference.

388 Chap.8 • Process Management
TotalFreezing. Inthismethod, aprocess'sexecutionisstopped whileitsaddress
space is being transferred (Fig. 8.2). This method is used in DEMOSIMP [Powell and
Miller 1983], Sprite [Douglis and Ousterhout 1987], and LOCUS [Popek and Walker
1985]and is simple and easy to implement. Its main disadvantage is that if a process is
suspended for a long time during migration, timeouts may occur, and if the process is
interactive, the delay will be noticed by the user.
Source Destination
node node
Time
Execution
suspended
Migrationdecisionmade
Freezing Transferof
time addressspace
Execution
resumed
Fig.8.2 Totalfreezingmechanism.
Pretransferring, In this method, the address space is transferred while the
processisstillrunningonthesourcenode(Fig.8.3).Therefore,oncethedecisionhasbeen
made to migrate a process, itcontinues to run on its source node until its address space
has been transferred to the destination node. Pretransferring (also known asprecopying)
isdone asaninitial transfer ofthecomplete address spacefollowed byrepeated transfers
of the pages modified during theprevious transfer until thenumber of modified pages is
relativelysmalloruntilnosignificantreductioninthenumberofmodifiedpages(detected
using dirty bits) is achieved. The remaining modified pages are retransferred after the
process is frozen for transferring its state information [Theimer et al. 1985].
In the pretransfer operation, the first transfer operation moves the entire address
space and takes thelongest time, thusproviding thelongest time formodifications tothe
program's address space to occur. The second transfer moves only those pages of the
address space that were modified during the first transfer, thus taking less time and
presumably allowing fewer modifications to occur during its execution time. Thus
subsequent transferoperationshavetomovefewerandfewerpages,finallyconvergingto
zero or very few pages, which are then transferred after the process is frozen. It may be
noted here that the pretransfer operation is executed at a higher priority than all other

Sec.8.2 • Process Migration 389
Source Destination
node node
Time
Migrationdecisionmade
Transferof
address space
Freezing
lime
Execution
resumed
Fig. 8.3 Pretransfer mechanism.
programs on the source node to prevent these other programs from interfering with the
progress of the pretransfer operation.
This method is used in the V-System [Theimer et al. 1985]. In this method, the
freezing time is reduced so migration interferes minimally with the process's interaction
with other processes and the user.Although pretransferring reduces the freezing time of
theprocess, itmayincrease thetotal time for migration due tothepossibility ofredundant
page transfers. Redundant pages are pages that are transferred more than once during
pretransferring because they become dirty while the pretransfer operation is being
performed.
Transfer on Reference. This method is based on the assumption that processes
tend to use only a relatively small part of their address spaces while executing. In this
method, the process's address space isleft behind on itssource node,andas therelocated
processexecutes onitsdestination node,attempts toreferencememorypages resultsinthe
generation ofrequests tocopy inthedesired blocks from their remote locations.Therefore
in this demand-driven copy-on-reference approach, a page of the migrant process's
address space is transferred from its source node to its destination node only when
referenced (Fig. 8.4). However, Zayas [1987J also concluded through his simulation
results that prefetching of one additional contiguous page per remote fault improves
performance.
This method is used inAccent [Zayas 1987]. In this method, the switching time of
the process from its source node to its destination node is very short once the decision
about migrating the process has been made and isvirtually independentof the size of the
address space. However, this method is not efficient in terms of the cost of supporting
remote execution once the process is migrated, and part of the effort saved in the lazy
transfer of an address space must be expended as the process accesses its memory

390 Chap. 8 • Process Management
Source Destination
node node
Time
Execution
-r-+-suspende
-
d
....f_M:.:,.:igrationdecisionmade
Freezing
time
Execution
resumed
On-demandtransfer
ofaddressspace
Fig.8.4 Transfer-on-referencemechanism.
remotely.Furthermore, thismethodimposesacontinued loadontheprocess'ssourcenode
and results in failure of the process if the source node fails or is rebooted.
Message-Forwarding Mechanisms
In moving a process, it must be ensured that all pending, en-route, and future messages
arrive at the process's new location. The messages to be forwarded to the migrant
process's new location can beclassified into the following:
Type 1: Messages receivedatthesource nodeaftertheprocess'sexecution hasbeen
stopped onits source nodeandtheprocess'sexecution hasnotyet been startedonits
destination node
Type 2: Messages received at the source node after the process's execution has
started on its destination node
Type 3: Messages that are to be sent to the migrant process from any other node
after it has started executing on the destination node
The different mechanisms used for message forwarding in existing distributed
systems are described below.
Mechanism of Resending the Message, This mechanism is used in the
V-System [Cheriton 1988,Theimeret al. 1985] and Amoeba [Mullender et al. 1990].to
handle messages ofallthreetypes.Inthismethod, messages of types 1and2arereturned
to the senderas notdeliverable or aresimply dropped, with theassurance that thesender
of the message is storing a copy of the data and is prepared to retransmit it.

Sec.8.2 • ProcessMigration 391
For example, inV-System, amessageof type 1or2issimplydroppedand the sender
is prompted to resend it to the process's new node. The interprocess communication
mechanismofV-Systenlensures that senders will retry untilsuccessful receiptofa reply.
Similarly in Amoeba, for all messages of type 1, the source node's kernel sends a "try
again later, this process is frozen" message to the sender. After the process has been
deleted from the source node, those messages will come again at some point oftime, but
this time as type 2 messages. For all type 2 messages, the source node's kernel sends a
"this process is unknown at this node" message to the sender.
In this method, upon receipt of a negative reply, the senderdoes a locateoperation
to find the new whereabouts of the process, and communication is reestablished. Both
V-System and Amoeba use the broadcasting mechanism to locate a process (object
locating mechanisms are described in Chapter 10). Obviously, in this mechanism,
messages of type 3are sent direct!y to the process's destination node.
This method does not require any process state to be left behind on the process's
source node. However, the main drawback of this mechanism is that the message
forwarding mechanism ofprocess migration operation is nontransparent to the processes
interacting with the migrant process.
Origin Site Mechanism. This method is used in AIX's TCF (Transparent
Computing Facility) [Walker and Mathews 1989] and Sprite [Douglis and Ousterhout
1987].The process identifierofthese systemshas the process'soriginsite(or homenode)
embedded in it, and each site is responsible for keeping information about the current
locations of all the processes created on it.Therefore, a process's current location can be
simply obtained by consulting its origin site. Thus, in these systems, messages for a
particular process are always first sent to its origin site. The origin site then forwards the
messagetothe process'scurrentlocation. This methodisnot good from areliabilitypoint
of view because the failure of the origin site will disrupt the message-forwarding
mechanism. Anotherdrawback ofthis mechanism is that there isacontinuousload on the
migrant process'8 origin site even after the process has migrated from that node.
link Traversal Mechanism. In DEMOSIMP [Powell and Miller 1983], to
redirect the messages oftype 1,a message queue for the migrantprocess is createdon its
source node. All the messages ofthis type are placed in this message queue. Upon being
notified that the process isestablished on the destination node, all messages in the queue
are sent to the destination node as a part ofthe migration procedure.
To redirect the messages of types 2 and 3, a forwarding address known as link is
left at the source node pointing to the destination node of the migrant process. The
most important part of a link is the message process address that has two components.
The first component is a systemwide, unique, process identifier. It consists of the
identifier of the node on which the process was created and a unique local identifier
generated by that node. The second component is the last known location of the
process. During the lifetime of a link, the first component of its address never
changes; the second, however, may. Thus to forward messages of types 2 and 3, a
migrated process is located by traversing a series of links (starting from the node
where the process was created) that form a chain ultimately leading to the process's
current location. The second component of a link is updated when the corresponding

392 Chap. 8 • Process Management
process isaccessed from a node. This is done to improve the efficiency of subsequent
locating operations for the same process from that node.
The linktraversal mechanism used by DEMOSIMP for message forwarding suffers
from the drawbacks of poor efficiency and reliability. Several links may have to be
traversed to locate a process from a node, and if any node in the chain of links fails, the
process cannot be located.
Link Update Mechanism. In Charlotte [Artsy and Finkel 1989], processes
communicate via location-independent Jinks, whichare capabilities for duplex commu
nication channels. Duringthetransfer phaseofthemigrantprocess, thesource nodesends
link-update messages to the kernels controlling all of the migrant process's communica
tion partners. These link update messages tell the new address of each link held by the
migrant process and are acknowledged (by the notified kernels) for synchronization
purposes. This task is not expensive since it is performed in parallel. After this point,
messages senttothemigrantprocessonanyofitslinkswillbesentdirectly tothemigrant
process's new node. Communication requests postponed while the migrant process was
being transferred are buffered at the source node and directed to the destination node as
a part of the transfer process. Therefore, messages of types 1and 2are forwarded to the
destination node by the source node and messages of type 3 are sent directly to the
process's destination node.
Mechanisms for Handling Coprocesses
In systems that allow process migration, another important issue is the necessity to
provide efficient communication between a process (parent) and its subprocesses
(children), which might have been migrated and placed on different nodes. The two
different mechanisms used by existing distributed operating systems to take care of this
problem are described below.
Disallowing Separation of Coprocesses. The easiest method of handling
communication between coprocesses istodisallow their separation. This can beachieved
in the following ways:
1. By disallowing the migration of processes that wait for one or more of their
children to complete
2. By ensuring that when a parent process migrates, its children processes will be
migrated along with it
ThefirstmethodisusedbysomeUNIX-based network systems [AlonsoandKyrimis
1988, Mandelberg and Sunderam 1988] and the second method is used by V-System
[Theimeretal.1985].Toensurethataparentprocesswillalwaysbemigratedalongwithits
children processes, V-System introduced the concept of logical host. V-System address
spaces and their associated processes are grouped into logical hosts.A V-Systemprocess
identifierisstructuredasa(logical-host-id, local-index)pair.Intheextreme,eachprogram
can be run in its own logical host. There may bemultiple logical hosts associated witha
singlenode;however,alogicalhostislocaltoasinglenode.InV-System,allsubprocesses

Sec.8.2 • Process Migration 393
ofaprocesstypicallyexecutewithin asingle logicalhost.Migrationofaprocessisactually
migrationofthe logicalhost containingthat process.Thus, typically, all subprocessesofa
processaremigratedtogetherwhen theprocessismigrated[Theimeretal. 1985].
The main disadvantageofthis methodisthat itdoes not allow the use ofparallelism
withinjobs,which isachievedbyassigningthe various tasksofajobtothedifferentnodes
of the system and executing them simultaneously on these nodes. Furthermore, in the
method employed by V-System, the overhead involved in migrating a process is large
when its.logical host consists of several associated processes.
Home Node or Origin Site Concept. Sprite [Douglis and Ousterhout 1987]
uses its home node concept(previously described) for communication between aprocess
and its subprocess when the two are running on different nodes. Unlike V-System, this
allows thecompletefreedom ofmigratingaprocessor itssubprocessesindependentlyand
executing them on different nodes of the system. However, since all communications
between a parent process and its children processes take place via the home node, the
messagetraffic and the communicationcost increaseconsiderably. Similardrawbacks are
associated with the concept oforigin site of IJOCUS [Popek and Walker 1985].
8.2.3 Process Migration inHeterogeneous Systems
When a process is migrated in ahomogeneous environment, the interpretation ofdata is
consistent on both the source and the destination nodes. Therefore, the question of data
translation does not arise. However, when a process is migrated in a heterogeneous
environment, all the concerneddata must betranslatedfrom the sourceCPU format tothe
destination CPU format before it can be executed on the destination node. If the system
consistsoftwoCPU types, each processormustbeabletoconvertthedatafrom theforeign
processortype into itsown format. Ifathird CPU'isadded,each processormust beable to
translate between its own representation and that ofthe other two processors. Hence, in
general,aheterogeneoussystem having nCPUtypes musthaven(n-l)piecesoftranslation
software inorderto supportthe facility of.migratingaprocessfrom any node toany other
node.Anexampleforfourprocessortypesisshown inFigure8.5(a).This isundesirable,as
adding anewCPU type becomesamoredifficulttask over time.
Maguire and Smith [1988] proposed the use of the external data representation
mechanism for reducing the software complexity of this translation process. In this
mechanism, astandardrepresentation isused for the transportofdata, and each processor
needs only to be able to convert data to and from the standard form. This bounds the
complexity of the translation software. An example for four processor types is shown in
Figure 8.5(b). The process of converting from a particular machine representation to
external data representation format iscalled serializing,and the reverse process is called
deserializing.
The standarddata representationformat iscalledexternaldata representation,and its
designermust successfullyhandletheproblemofdifferentrepresentationsfordatasuch as
characters, integers, and floating-point numbers. Ofthese, the handling offloating-point
numbersneeds specialprecautions.The issues discussedbyMaguireand Smith [1988] for
handlingfloating-pointnumbersinexternaldata representationschemearegiven below.

394 Chap. 8 • Process Management
(a)
8 3
Externaldata
representation.._----\
M------f
7 4
(b)
Fig. 8.5 (a) Example illustrating theneedfor J2piecesoftranslation software
required inaheterogeneous system having4 typesofprocessors. (b)
Example illustrating theneedforonly 8piecesoftranslation software ina
heterogeneous systemhaving4typesofprocessors whentheexternal data
representation mechanism isused.
Afloating..point number representation consists ofanexponent part, amantissa part,
and a sign part. The issue of proper handling of the exponent and the mantissa has been
described separately because the side effects caused by an external data representation
affect each of the two components differently.
Handling the Exponent
Thenumberofbitsusedfortheexponentofafloating-point numbervariesfromprocessor
to processor. Let us assume that, for the exponent, processor A uses 8 bits, processor B
uses 16 bits, and the external data representation designed by the users of processor

Sec.8.2 • Process Migration 395
architecture A provides 12bits (an extra 4 bits for safety). Also assume that all three
representations use the same number of bits for the mantissa.
In this situation, a process can be migrated from processor A to B without any
problem in representing its floating-point numbers because the two-step translation
process of the exponent involves the conversion of 8 bits of data to 12bits and then 12
bits of data to 16 bits, having plenty of room for the converted data in both steps.
However, aprocess that has some floating-point data whoseexponentrequires more than
12bits cannot be migrated from processorBtoAbecause this floating-point data cannot
berepresentedintheexternal datarepresentation, whichhasonly 12bitsfortheexponent.
Note that the problem here is with the design of the external data representation, which
willnoteven allowdata transfer between twoprocessors, bothofwhichuse 16bitsforthe
exponent, because the external data representation has only 12bits for this purpose. This
problem can be eliminated by guaranteeing that the external data representation have at
least as many bits in the exponent as the longest exponent of any processor in the
distributed system.
A second type of problem occurs when a floating-point number whose exponent is
less than or equal to 12bits but greater than 8bits is transferred from processor B toA.
In this case, although the external data representation has a sufficient number of bits to
handle the data, processorA does not. Therefore, in this case, processor A must raise an
overflow or underflow (dependingon the signoftheexponent) upon conversion from the
external data representation or expect meaningless results. There are three possible
solutions to this problem:
1. Ensuring that numbers used by programs that migrat.ehave a smaller exponent
value than the smallest processor's exponent value in the system
2. Emulating the larger processor's value
3. Restricting the migration of the process to only those nodes whose processor's
exponent representation is at least as large as that of the source node's
processor
The first solution imposes a serious restriction on the use of floating-point numbers,
and this solution may be unacceptable by serious scientific computations. The second
solution maybeprohibitivelyexpensiveintermsofcomputationtime.Therefore, thethird
solution of restricting the direction of migration appears to be a viable solution. For this,
each node of the system keeps a list of nodes that can serve as destinations for migrating
a process from this node.
Handling the Mantissa
The first problem in handling the mantissa is the same as that of handling the exponent.
Letusassume that theexponentfield isofthe same sizeonallthe processors, and for the
mantissa representation, processor A uses 32 bits, processor Buses 64 bits, and the
external data representation uses48 bits. Duetosimilar reasons asdescribedforhandling
the exponent, inthis case alsothe migration of a process from processorA toB will have

396 Chap. 8 • Process Management
no problem, but the migration of a process from processor B to A will result in the
computation being carried out in "half-precision." This may not be acceptable when
accuracyoftheresult isimportant.Asinhandlingtheexponent,toovercomethisproblem,
the external data representation must have sufficient precision to handle the largest
mantissa, and the direction ofmigration should be restricted only to the nodes having a
mantissa at least as large as the source node.
The second problem in handling the mantissa is the loss of precision due to
multiple migrations between a set of processors, where our example processors A and
B might be a subset. This is a concern only in the mantissa case because loss of one
or more bits of the exponent is catastrophic, while loss of bits in the mantissa only
degrades the precision of computation. It may appear that the loss in precision due to
multiple migrations may be cumulative, and thus a series of migrations may totally
invalidate a computation. However, if the external data representation is properly
designed to be adequate enough to represent the longest mantissa of any processor of
the system, the resulting precision will never be worse than performing the calculation
on the processor that has the least precision (least number of bits for the mantissa)
among all the processors of the system. This is because the remote computations can
be viewed as "extra precision" calculations with respect to the floating point of the
processor with least precision.
Handling Signed-Infinity and Signed-Zero
Representations
Two otherissues that need to beconsidered in the design ofexternal data representation
arethesigned-infinityandsigned-zerorepresentations.Signedinfinity isavaluesupported
bysome architecturesthat indicatesthat thegeneratedresult istoo large (overflow)ortoo
small (underflow) to store. Otherarchitectures may use the sign bitofa value that would
otherwise be zero, thus giving rise to a signed zero.
Now the problem is that these representations may not be supported on all systems.
Therefore, while designing the translation algorithms, proper decisions must be made
about how to deal with these situations. However, in a good design, the external data
representation must take care of these values so that a given processor can either take
advantage ofthis extra information or simply discard it.
8.1.4 AcJvontQgas of ProcassMigration
Process migration facility may beimplemented in adistributed system for providing one
or more ofthe following advantages to the users:
1. Reducing average response time ofprocesses. The average response time ofthe
processesofanode increasesrapidly asthe loadon the node increases. Processmigration
facility may beused to reduce the average response time of the processes ofa heavily
loaded node bymigratingandprocessingsome ofitsprocessesonanode thatiseitheridle
or whose processing capacity is underutilized.

Sec.8.2 • Process Migration 397
2. Speeding up individualjobs. Process migration facility maybe used to speed up
individual jobs in two ways. The first method is to migrate the tasks of a job to the
different nodes of the system and to execute them concurrently. The second approach is
to migrate a job to a node having a faster CPU or to a node at which it has minimum
turnaround time due to various reasons (e.g., dueto specific resource requirementsof the
job). Of course, the gain in execution time must be more than the migration cost
involved.
3. Gaining higherthroughput. In a system that does not support process migration,
it is very likely that CPUs of all the nodes are not fully utilized. But in a system with
process migration facility, the capabilities of the CPUs of all the nodes can be better
utilizedbyusing a suitable load-balancingpolicy.This helps inimproving the throughput
of the system. Furthermore, process migration facility may also be used to properly mix
I/O and CPU-bound processes on a global basis for increasing the throughput of the
system.
4. Utilizing resources effectively. In a distributed system, the capabilities of the
variousresources suchasCPUs, printers, storagedevices, andsoon,ofdifferent nodesare
different.Therefore, dependinguponthenatureofaprocess, itcanbemigratedtothemost
suitable node toutilize the system resources in the mostefficient manner.This istrue not
only for hardware resources but also for software resources such as databases, files, and
so on. Furthermore, there are some resources, such as special-purpose hardware devices,
that are not remotely accessible by aprocess. For example, it may be difficult to provide
remote access to facilities to perform fast Fourier transforms or array processing or this
access may be sufficiently slow to prohibit successful accomplishment of real-time
objectives [Smith 1988].Process migration also facilitates the useof such resources by a
process of any node because the process can be migrated tothe resource's location for its
successful execution.
5. Reducing network traffic. Migrating a process closer to the resources it is using
most heavily (such as files, printers, etc.) may reduce network traffic in the system ifthe
decreased cost of accessing its favorite resources offsets the possible increased cost of
accessing itslessfavored ones. Ingeneral, whenever aprocess performs data reduction (it
analyzes and reduces the volume of data by generating some result) on some volume of
data larger than the process's size, it may be advantageous to move the process to the
location of the data [Smith 1988]. Another way to reduce network traffic by process
migration istomigrate andclustertwoor more processes, whichfrequently communicate
with each other, on the same node of the system.
6. Improving system reliability. Process migration facility may be used to improve
system reliability in several ways. One method is to simply migrate a critical process to
a node whose reliability is higher than other nodes in the system. Another method is to
migrate a copy of a critical process to some other node and to execute both the original
and copied processes concurrently on different nodes. Finally, in failure modes such as
manual shutdown, which manifest themselves as gradual degradation of a node, the
processes of the node, for their continued execution, may be migrated to another node
before the dying node completely fails.

398 Chap. 8 • Process Management
7. Improving system security. A sensitive process may be migrated and run on a
secure node that isnotdirectly accessible to general users, thus improving the security of
that process.
8.3 THMADS
Threads are a popular way to improve application performance through parallelism. In
traditional operatingsystems the basic unitof CPU utilization is aprocess. Each process
has its own program counter, its own register states, its own stack, and its own address
space. Ontheother hand, inoperatingsystems withthreads facility,thebasic unitofCPU
utilization is a thread. In these operating systems, a process consists of an address space
and one or more threads of control [Fig. 8.6(b)]. Each thread of a process has its own
programcounter, itsownregisterstates, anditsown stack. Butallthethreads ofaprocess
sharethesameaddress space.Hencetheyalsoshare thesameglobal variables. Inaddition,
allthreads ofaprocess alsosharethesamesetofoperatingsystemresources, suchasopen
files, child processes, semaphores, signals, accounting information, and soon. Due tothe
sharing ofaddress space,thereisnoprotectionbetween thethreadsofaprocess. However,
this is not aproblem. Protection between processes is needed because different processes
may belong to different users. But a process (and hence all its threads) is always owned
by a single user. Therefore, protection between multiple threads of a process is not
necessary.Ifprotectionisrequired between twothreads ofaprocess, itispreferabletoput
them in different processes, instead of putting them in a single process.
Addressspace Addressspace
ITJITJITJ
Fig. 8.6 (a) Single-threadedand(b)
multithreadedprocesses. Asingle
threaded process corresponds toa
process ofa traditional operating
(a) (b)
system.
Threads share a CPU in the same way as processes do. That is, on a uniprocessor,
threads run inquasi-parallel (time sharing), whereas on a shared-memory multiprocessor,
asmany threads can runsimultaneouslyasthereareprocessors. Moreover, like traditional
processes,threads cancreate childthreads, canblock waiting forsystemcalls tocomplete,
and can change states during their course of execution. At aparticularinstance of time, a
thread can be in anyoneofseveral states: running, blocked, ready,or terminated. Due to
these similarities, threads are often viewed as miniprocesses. Infact, inoperating systems

Sec.8.3 • Threads 399
with threads facility, a process having a single thread corresponds to a process of a
traditional operating system [Fig. 8.6(a)]. Threads are often referred to as lightweight
processes and traditional processes are referred to as heavyweightprocesses.
8.3.1 Motivations forUsing Threads
The main motivations for using a multithreaded process instead of multiple single
threaded processes for performing some computation activities are as follows:
1. The overheads involved in creating a new process are in general considerably
greater than those of creating a new thread within a process.
2. Switching between threads sharing the same address space is considerably
cheaper than switching between processes that have their own address spaces.
3. Threads allow parallelismtobecombined with sequential execution andblocking
system calls [Tanenbaum 1995].Parallelism improves performance and blocking
system calls make programming easier.
4. Resource sharing can beachieved more efficiently and naturally between threads
of a process than between processes because all threads of a process share the
same address space.
These advantages are elaborated below.
The overheads involved in the creation of a new process and building its execution
environment are liable to be much greater than creating a new thread within an existing
process. This ismainly because when anewprocess iscreated, itsaddress space hastobe
created from scratch, although a part of it might be inherited from the process's parent
process. I-Iowever, when a new thread is created, it uses the address space of its process
that need not becreated from scratch. For instance, incase of akernel-supported virtual
memory system, anewlycreated process willincurpagefaultsasdataandinstructions are
referenced for the first time. Moreover, hardware caches will initially contain no data
values for the new process, and cache entries for the process's data will becreated as the
process executes. These overheads may also occur in thread creation, but they are liable
tobeless.This isbecause when thenewlycreated threadaccesses codeanddata that have
recently been accessed by other threads within the process, it automatically takes
advantage of any hardware or main memory caching that has taken place.
Threads also minimize context switching time,allowing theCPUtoswitchfromone
unit of computation to another unit of computation with minimal overhead. Due to the
sharing of address space and other operating system resources among the threads of a
process, the overhead involved in CPU switching among peer threads is very small as
comparedtoCPU switching among processes having theirownaddress spaces.This isthe
reason why threads are called lightweight processes.
To clarify how threads allow parallelism to be combined with sequential execution
andblocking system calls, letusconsiderthedifferent waysinwhichaserver process can
be constructed. One of the following three models may be used to construct a server
process (e.g., let us consider the case of a file server) [Tanenbaum 1995]:

400 Chap. 8 • Process Management
1. Asasingle-threadprocess.Thismodel usesblockingsystemcallsbutwithoutany
parallelism. In this method, the file server gets a client's file access request from the
request queue, checks the request foraccess permissions, andifaccessisallowed, checks
whether a disk access is needed to service the request. If disk access is not needed, the
requestisserviced immediately andareplyissenttotheclientprocess.Otherwise, thefile
server sends adisk access request tothedisk serverand waitsforareply.Afterreceiving
the disk server's reply,it services theclient's request, sends a reply to theclient process,
and goes back to get the next request from the request queue.
In this method, the programming of the server process is simple because of the use
of blocking system call; after sending its request, the file server blocks until a reply is
received from thedisk server.However, ifadedicated machine is usedforthe fileserver,
the CPU remains idle while the file server is waiting for a reply from the disk server.
Hence, no parallelism is achieved in this method and fewer client requests are processed
per unit of time.
The performance of a server implemented as a single-thread process is often
unacceptable.Therefore, itisnecessarytooverlap theexecutionofmultipleclientrequests
by allowing the file server to work on several requests simultaneously. The next two
models support parallelism for this purpose.
2. As afinite-state machine. This model supports parallelism but with nonblocking
system calls. In this method, the server is implemented as a single-threaded process and
isoperated like a finite-state machine. Anevent queue ismaintained in which bothclient
request messages and reply messages from the disk server are queued. Whenever the
threadbecomes idle,ittakesthenextmessagefromtheeventqueue.Ifitisaclient request
message, a check is made for access permission and need for disk access. If disk access
isneeded to service the request, the fileserver sends adisk access request message to the
disk server. However, this time, instead of blocking, it records the current state of the
client'srequest inatableandthengoestogetthenextmessagefromtheeventqueue.This
message may either be a request from a new client or a reply from the disk server of a
previous disk access request. If it is a new client request, it is processed as described
above. On the other hand, if it is a reply from the disk server, the state of the client's
request thatcorresponds tothereply isretrieved from thetable,and theclient'srequest is
processed further.
Although the method achieves parallelism, it is difficult to program the server
processinthismethodduetotheuseofnonblocking systemcalls.Theserverprocessmust
maintain entries for every outstanding client request, and whenever a disk operation
completes, the appropriate piece of client state must be retrieved to find out how to
continue carrying out the request.
3. As a group ofthreads. This model supports parallelism with blocking system
calls. In this method, the server process is comprised of a single dispatcher thread and
multiple worker threads. Either the worker threads can be created dynamically,
whenever a request comes in, or a pool of threads can be created at start-up time to
deal with as many simultaneous requests as there are threads. The dispatcher thread
keeps waiting in a loop for requests from the clients. When a client request arrives, it
checks it for access permission. If permission is allowed, it either creates a new worker

Sec. 8.3 • Threads 401
thread or chooses an idle worker thread from the pool (depending on whether the
worker threads are created dynamically or statically) and hands over the request to the
worker thread. The control is then passed on to the worker thread and the dispatcher
thread's state changes from running to ready. Now the worker thread checks to see if
a disk access is needed for the request or if it can be satisfied from the block cache
that is shared by all the threads. If disk access is needed, it sends a disk access request
to the disk server and blocks while waiting for a reply from the disk server. At this
point, the scheduler will be invoked and a thread will be selected to be run from the
group of threads that are in the ready state. The selected thread may be the dispatcher
thread or another worker thread that is now ready to run.
This method achievesparallelismwhileretainingtheideaofsequential processesthat
make blocking system calls. Therefore, a server process designed in this way has good
performance and is also easy to program.
Wesawthemotivationforusingthreads inthedesign ofserverprocesses. Often there
are some situations where client processes can also benefit from the concurrency made
possible bythreads. Forexample,aclient processmayuseadivide-and-conqueralgorithm
to divide data into blocks that can be processed separately. Itcan then sendeach block to
aserver tobeprocessedand finally collectandcombinetheresults. Inthiscase,aseparate
client thread maybe used to handle each data block to interact with the different server
processes. Similarly, when a file is to be replicated on multiple servers, a separate client
thread can beused tointeract witheach server.Some client applicationuserinterfacescan
also benefit by using threads to give the interface back to a user while a long operation
takes place. Client processes that perform lots ofdistributed operations can also benefit
from threads by using a separate thread to monitor each operation.
Finally, the use of threads is also motivated by the fact that a set of threads using a
shared address space isthe most natural way toprogram many applications. Forexample,
in an application that uses the producer-consumer model, the producer and the consumer
processes must share a common buffer. Therefore, programming the application in such
a way that the producer and consumer are two threads of the same process makes the
software design simpler.
8.3.2 Models for Organizing Threads
Depending on an application's needs, the threads of a process of the application can be
organized in different ways. Three commonly used ways to organize the threads of a
process are as follows [Tanenbaum 1995]:
1. Dispatcher-workers model. We have already seen the use of this model in
designing aserver process. Inthis model, theprocess consists ofasingledispatcherthread
andmultiple worker threads. The dispatcher'thread accepts requests fromclients and,after
examiningthe request, dispatches the requesttoone ofthefree worker threads forfurther
processing of the request. Each worker thread works on a different client request.
Therefore multipleclient requests can beprocessed in parallel. Anexampleofthis model
is shown in Figure 8.7(a).

402 Chap.8 • Process Management
Requests
Port Aserverprocessfor
processingincomingrequests
(a)
Requests
Aserverprocessforprocessingincomingrequests
thatmaybeofthreedifferenttypes,eachtypeof
requestbeinghandledbyadifferentthread
(b)
Requests
Aserverprocessforprocessingincomingrequests,
eachrequestprocessedinthreesteps,eachstep
handledbyadifferentthreadandoutputofone
stepusedasinputtothenextstep
(c)
Fig.8.7 Models fororganizing threads: (a) dispatcher-workersmodel; (b) team
mode);(c) pipeline model.

Sec. 8.3 • Threads 403
2. Teammodel. In this model, all threads behave as equals in the sense that there is
no dispatcher-worker relationship for processing clients' requests. Each thread gets and
processes clients' requests on its own. This model is often used for implementing
specialized threads within aprocess. That is, each thread ofthe process is specialized in
servicing a specific type of request. Therefore, multiple types of requests can be
simultaneously handled by the process. An example of this model is shown in Figure
8.7(b).
3. Pipeline model. This model is useful for applications based on the producer
consumermodel, inwhichthe outputdatageneratedbyone part ofthe applicationis used
as input for another part of the application. In this model, the threads of a process are
organized as a pipeline so that the output data generated by the first thread is used for
processing by the secondthread, the outputofthe secondthread isused for processingby
the third thread, and so on. The outputofthe last thread in the pipeline isthe final output
ofthe process to which the threads belong. An example ofthis model is shown in Figure
8.7(c).
8.3.3 Issues In·Designing Q Threads Package
A system that supports threads facility must provide a set of primitives to its users for
threads-related operations. These primitives of the system are said to form a threads
package. Some of the important issues in designing a threads package are described
below.
Threads Creation
Threads can becreated eitherstaticallyor dynamically. In thestatic approach, thenumber
ofthreadsofaprocessremainsfixed for itsentirelifetime,while inthedynamicapproach,
the numberofthreadsofaprocesskeeps changingdynamically. Inthe dynamicapproach,
a process is started with a single thread, new threads are created as and when needed
duringtheexecutionofthe process,and athreadmay destroy itselfwhen itfinishes itsjob
by making an exit call. On the other hand, in the static approach, the number ofthreads
ofa process is decided either at the time ofwriting the corresponding program or when
the program is compiled. In the static approach, a fixed stack is allocated toeach thread,
but in the dynamic approach, the stack size ofa thread is specified as a parameterto the
system call for thread creation. Other parameters usually required by this system call
includeschedulingpriorityand the proceduretobeexecutedtorun this thread. The system
call returns a thread identifier for the newly created thread. This identifier is used in
subsequent calls involving this thread.
Threads Termination
Terminationofthreads isperformedinamannersimilartothe terminationofconventional
processes.That is, athread may eitherdestroy itselfwhen itfinishes itsjobby making an
exit call or be killed from outside by using the kill command and specifying the thread

404 Chap. 8 • Process Management
identifieras its parameter. In many cases, threads are never terminated. For example, we
saw.above that in a process that uses statically created threads, the number of threads
remains constant for the entire life of the process. In such a process, all its threads are
createdimmediatelyaftertheprocessstarts upand thenthesethreads areneverkilled until
the process terminates.
Threads Synchronization
Since all the threads ofa process share acommon address space, some mechanism must
be used to prevent multiple threads from trying to access the same data simultaneously.
Forexample, suppose two threads ofaprocessneed toincrementthesameglobal variable
within the process. For this to occursafely, each thread must ensure that it has exclusive
access for this variable for some period oftime.Asegment ofcode inwhich athread may
be accessing some shared variable is called a criticalregion.Topreventmultiple threads
from accessing the same data simultaneously, it is sufficient to ensure that when one
thread is executing in a critical region, no otherthread is allowed to execute in a critical
region in which the same data is accessed. That is, the execution of critical regions in
which the same data is accessed by the threads must be mutually exclusive in time. Two
commonlyused mutual exclusiontechniquesinathreads packagearemutex variables and
condition variables.
A mutex variable is like a binary semaphore that is always in one of two states,
locked or unlocked. A thread that wants to execute in a critical region performs a lock
operation on the corresponding mutex variable. If the mutex variable is in the unlocked
state, the lock operation succeeds and the state of the mutex variable changes from
unlocked to locked in a single atomic action. After this, the thread can execute in the
critical region. However, if the mutex variable is already locked, depending on the
implementation, the lock operation is handled in one ofthe following ways:
1. The thread is blocked and entered in a queue of threads waiting on the mutex
variable.
2. A status code indicating failure is returned to the thread. In this case, the thread
has the flexibility to continue with some otherjob. However, toenter thecritical
region, the thread has to keep retrying to lock the mutex variable until it
succeeds.
A threads package may support both by providing different operations for actually
locking and obtaining the status ofa.mutex variable.
In a multiprocessor system in which different threads run in parallel on different
CPUs, itmayhappen thattwothreads perform lockoperationsonthesame mutex variable
simultaneously. In such a situation, one ofthem wins, and the loser is either blocked or
returned a status code indicating failure.
Whenathreadfinishes executinginitscritical region, itperforms anunlockoperation
onthecorrespondingmutex variable.Atthistime,iftheblocking methodisusedandifone
ormorethreadsareblocked waitingonthemutexvariable,oneofthemisgiventhelockand
itsstateischangedfrom blockedtorunning whileothers continuetowait.

Sec.8.3 • Threads 405
Mutex variables are simple to implement because they have only two states.
However, their use is limited to guarding entries to critical regions. For more general
synchronization requirements condition variables are used. A condition variable is
associated with a mutex variable and reflects a Boolean state of that variable. Wait and
signal are two operations normally provided for a condition variable. When a thread
performs a wait operation on a condition variable, the associated mutex variable is
unlocked, and the thread is blocked until a signaloperation is performed by some other
thread on the condition variable, indicating that the event being waited for may have
occurred. When athread performs asignaloperation on thecondition variable, themutex
variableislocked, andthethread that wasblocked waitingonthecondition variable starts
executing in the critical region. Figure 8.8 illustrates the use of mutex variable and
condition variable for synchronizing threads.
Thread1 Thread2
Lock(mutex_A)
succeeds
Lock(mutex_A)fails
,: Wait(A_free)
,
Criticalregion ,
(usessharedresourceA) ':,Blockedstate
,
,
,
Unlock(mutex_A) TLock(mutex_A)
Signal(A_free) succeeds
Mutex_A isamutexvariableforexclusiveuseofsharedresourceA.
A_freeisaconditionvariableforresourceAtobecomefree.
Fig.8.8 Useof mutex variable and condition variable forsynchronizingthreads.
Condition variables are often used forcooperation between threads. For example, in
an application in which two threads of a process have a producer-consumerrelationship,
the producerthread creates data and puts itin abounded buffer,andthe consumer thread
takes the data from the buffer and uses itfor further processing. In thiscase, ifthe buffer
is empty when the consumer thread checks it, that thread can bemade to wait on a
nonemptycondition variable, and when the producerthread puts some data in the buffer,
it can signal the nonemptycondition variable. Similarly, if the buffer is full when the
producer threadchecks it,thatthread canbe madetowaitonanonfullcondition variable,
and when the consumer thread takes out some data from the buffer, it can signal the
nonfullcondition variable. Inthis way,thetwo threads can workincooperationwitheach
other by the use of condition variables.

406 Chap. 8 • Process Management
Threads Scheduling
Another important issueinthedesign ofathreads package ishowtoschedule thethreads.
Threads packages often provide calls to give the users the flexibility to specify the
scheduling policy to be used for their applications. With this facility, an application
programmercan usetheheuristics oftheproblem todecide themosteffective manner for
scheduling. Some of the special features for threads scheduling that may besupported by
a threads package are as follows:
1. Priority assignment facility. In a simple scheduling algorithm, threads are
scheduledonafirst-in,first-outbasisortheround-robin policyisusedtotimesharetheCPU
cyclesamongthethreadsonaquantum-by-quantumbasis,withallthreadstreatedasequals
by the scheduling algorithm. However, a threads-scheduling scheme may provide the
flexibility to the application programmers to assign priorities to the various threads of an
application inordertoensure thatimportant onescanberunonahigherpriority basis.
A priority-based threads scheduling scheme may be either non-preemptive or
preemptive. In the former case, once a CPU isassigned to a thread, the thread can use it
until it blocks, exits, or uses up itsquantum. That is, the CPU is not taken away from the
thread to which it has already been assigned even if another higher priority thread
becomes ready to run. The higher priority thread is selected to run only after the thread
that iscurrently using the CPU releases it. On the other hand, in the preemptive scheme,
ahigher priority thread always preempts alower priority one. That is, whenevera higher
priority thread becomes ready to run, the currently running lower priority thread is
suspended, and theCPU isassigned tothe higher priority thread. Inthis scheme, athread
can run only when no other higher priority thread is ready to run.
2. Flexibility to vary quantum size dynamically. A simple round-robin scheduling
scheme assigns a fixed-length quantum to timeshare the CPU cycles among the threads.
However, a fixed-length quantum is not appropriate on a multiprocessor system because
there may be fewer runnable threads than there are available processors. In this case, it
would be wasteful to interrupt a thread with a context switch to the kernel when its
quantum runs outonly tohave itplaced rightback inthe running state.Therefore, instead
of using a fixed-length quantum, a scheduling scheme may vary the size of the time
quantum inversely with the total number of threads in the system. This algorithm gives
good response time to short requests, even on heavily loaded systems, but provides high
efficiency on lightly loaded systems.
3. Handoffscheduling. A handoff scheduling scheme allows a thread to name its
successor if it wants to. For example, after sending a message to another thread, the
sending thread can give up the CPU and request that the receiving thread be allowed to
run next. Therefore, this scheme provides the flexibility to bypass the queue of runnable
threads and directly switch the CPU to the thread specified by the currently running
thread. Handoffscheduling can enhance performance if it iswisely used.
4. Affinity scheduling. Another scheduling policy that may be used for better
performanceonamultiprocessorsystem isaffinity scheduling. Inthis scheme, a threadis
scheduled on the CPU it last ran on in hopes that part ofits address space is still in that
CPU's cache.

Sec. 8.3 • Threads 407
Signal Handling
Signals provide software-generated interrupts and exceptions. Interrupts are externally
generated disruptions of a thread or process, whereas exceptions are caused by the
occurrence of unusual conditions during a thread's execution. The two main issues
associated with handling signals in a multithreaded environment are as follows:
1. A signalmustbe handledproperly no matterwhich threadofthe processreceives
it. Recall that in UNIX a signal's handler must be a routine in the process
receiving the signal.
2. Signalsmustbe preventedfrom gettinglost.Asignalgets lost whenanothersignal
ofthe sametype occursinsomeotherthreadbeforethe first one ishandledby the
thread in which itoccurred.This happensbecausean exceptionconditioncausing
the signal is'storedin aprocesswideglobal variablethatisoverwrittenbyanother
exception condition causing a signal ofthe same type.
An approach for handling the former issue is to create a separate exception handler
thread in each process. In this approach, the exception handler thread of a process is
responsible for handling all exception conditions occurring in any thread ofthe process.
When a thread receives a signal for an exception condition, it sends an exception
occurrence message to the exception handler thread and waits until the exception is
handled. The exception message usually includes information about the exception
condition, the thread, and the process that caused the exception. The exception handler
thread performs its function according to the type of exception. This may involve such
actions as clearing the exception, causing the victim thread to resume, or terminating the
victim thread.
On the other hand, an approach for handling the latter issue is to assign each thread
its own private global variables for signaling exception conditions, so that conflicts
betweenthreads overthe use ofsuchglobal variables neveroccur. Such variablesare said
to be threadwide global because the code of a thread normally consists of multiple
procedures. In this approach, new library procedures are needed to create, set, and read
these threadwide global variables.
8.3.4 Implementing a Threads Package
A threads package can be implemented either in user space or in the kernel. In the
description below, the two approaches are referred to as user-level and kernel-level,
respectively. In the user-level approach, the user space consists of a runtime system
that is a collection of threads management routines. Threads run in the user space on
top of the runtime system and are managed by it. The runtime system also maintains
a status information table to keep track of the current status of each thread. This
table has one entry per thread. An entry of this table has fields for registers' values,
state, priority, and other information of a thread. All calls of the threads package are
implemented as calls to the runtime system procedures that perform the functions
corresponding to the calls. These procedures also perform thread switching if the

408 Chap. 8 • Process Management
thread that made the call has to be suspended during the call. That is, two-level
scheduling is performed in this approach. The scheduler in the kernel allocates
quanta to heavyweight processes, and the scheduler of the runtime system divides a
quantum allocated to a process among the threads of that process. In this manner,
the existence of threads is made totally invisible to the kernel. The kernel functions
in a manner similar to an ordinary kernel that manages only single-threaded,
heavyweight processes. This approach is used by the SunOS 4.1 Lightweight
Processes package.
On the other hand, in the kernel-level approach, no runtime system is used and the
threads are managed by the kernel. Therefore, the threads status information table is
maintained within the kernel. All calls that might block a thread are implemented as
system calls thattraptothekernel. When athread blocks, thekernel selects anotherthread
to be run. The selected thread may belong to either the same process as that of the
previouslyrunning thread oradifferentprocess. Hence, theexistenceofthreads isknown
to the kernel, and single-level scheduling is used in this approach.
Figure 8.9 illustrates the two approaches for implementing a threads package. The
relative advantages and disadvantages ofthe approaches are as follows:
T
Processesandtheirthreads
User
ce
1
8P8 Runtimesystem
(maintainsthreadsstatusinformation)
Kernel Kernel
SPice (maintainsprocessesstatusinformation)
(a)
T
iUsler-----------I
Processesandtheirthreads
Kernel
Kernel
(maintainsthreadsstatusinformation)
.spLace ----'
."ig.8.9 Approachesforimplementinga
threadspackage:(a) userlevel;
(b)
(b) Kemellevel.

Sec.8.3 • Threads 409
1. The mostimportantadvantageofthe user-levelapproachisthatathreadspackage
can be implementedon top ofan existing operating systemthatdoes not supportthreads.
This is not possible in the kernel-level approach because in this approach the concept of
threads must be incorporated in the design ofthe kernel ofan operating system.
2. In the user-level approach, due to the use oftwo-level scheduling, users have the
flexibility to use their own customized algorithm to schedule the threads of a process.
Therefore, depending on the needs ofan application, a usercan design and use the most
appropriate scheduling algorithm for the application. This is not possible in the kernel
level approach because a single-level scheduler is used that is built into the kernel.
Therefore, users only have theflexibility tospecifythroughthe systemcall parametersthe
priorities to be assigned to the various threads of a process and to select an existing
algorithm from a set ofalready implemented scheduling algorithms.
3. Switching the context from one thread to another is faster in the user-level
approachthan inthekernel-levelapproach.Thisisbecauseinthe formerapproachcontext
switching is performed by the runtime system, while in the latter approach a trap to the
kernel is needed for it.
4. Inthe kernel-levelapproach, the statusinformationtablefor threadsismaintained
within the kernel. Due to this, the scalability of the kernel-level approach is poor as
compared to the user-level approach.
5. A serious drawback associated with the user-level approach is that with this
approachthe useofround-robinschedulingpolicytotimesharetheCPUcyclesamongthe
threads on aquantum-by-quantum basis is not possible [Tanenbaum 1995].This isdue to
the lack of clock interrupts within a single process. Therefore, once a thread is given the
CPU to run, there is no way to interrupt it, and it continues to run unless it voluntarily
gives up the CPU. This is not the case with the kernel-level approach, in which clock
interrupts occur periodically, and the kernel can keep track of the amount ofCPU time
consumed by a thread. When a thread finishes using its allocated quantum, it can be
interrupted by the kernel, and the CPU can be given to another thread.
A crude way to solve this problem is to have the runtime system request a clock
interrupt after every fixed unit of time (say every half a second) to give it control
[Tanenbaum 1995]. When the runtimesystemgets control, the schedulercan decide ifthe
thread should continue running or the CPU should now be allocated to another thread.
6. Anotherdrawback ofthe user-level approach is associated with the implementa
tion ofblocking system calls. In the kernel-level approach, implementation ofblocking
system calls is straightforward because when a thread makes such a call, it traps to the
kernel, where it is suspended, and the kernel starts a new thread. However, in the user
level approach, a thread should not be allowed to make blocking system calls directly.
This isbecause ifathreaddirectly makesablockingsystemcall, all threadsofits process
will be stopped, and the kernel will schedule anotherprocess to run. Therefore, the basic
purpose of using threads will be lost.
A commonly used approach to overcome this problem is to use jacket routines. A
jacket routine contains extra code before each blocking system call to first make a check

410 Chap. 8 • Process Management
toensure ifthecall willcause a trap tothekernel. Thecall ismade only ifit is safe(will
not cause a trap to the kernel); otherwise the thread is suspended and another thread is
scheduled torun. Checking the safety condition and making the actual call must bedone
atomically.
Withsomesuccess,fewattempts havebeenmadetocombine theadvantages ofuser
level and kernel-level approaches in the implementation of a threads package. For
example, the FastThreads package [Anderson et al. 1991]and the threads package of the
Psyche multiprocessor operating system [Marsh et aJ. 1991] provide kernel support for
user-level thread scheduling. On the other hand, Mach [Black 1990] enables user-level
code to provide scheduling hints to the kernel's thread scheduler. The details of threads
implementation in Mach is given in Chapter 12.
8.3.5 (ase Study: DCE Threads
The C Threads package developed for the Mach operating system and the Lightweight
Processes package developed for the SunOS are two examples of commercially
available threads packages. In addition, to avoid incompatibilities between threads
designs, IEEE has drafted a POSIX (Portable Operating System Interface for Computer
Environments) threads standard known as P-Threads. Two implementations of this
standard are the DCE Threads developed by the Open Software Foundation (OSF) for
the OSF/I operating system and the GNU Threads developed by the Free Software
Foundation for the SunOS. The DCEThreads package, which is based on the PlOO3.4a
POSIX standard, is described below as a case study. A description of another threads
package, the C Threads package, is given in Chapter 12 as a part of the description of
the Mach operating system.
Theuser-levelapproach isusedforimplementing theDCEThreads package.Thatis,
DCE provides a set of user-level library procedures for the creation, termination,
synchronization, andsoon,ofthreads.Toaccess thread servicesfromapplications written
inC, DeE specifies anapplication programming interface (API) that iscompatibletothe
POSIX standard. Ifasystem supporting DeEhasnointrinsic support forthreads, theAPI
provides an interface to the DCE threads library that is linked to application procedures.
On the other hand, if a system supporting DCEhas operating system kernel support for
threads, the DCE isset up to use this facility.In thiscase, theAPI serves as an interface
to the kernel-supported threads facility.
Threads Management
The DeE Threads package has a number of library procedures for managing threads.
Some of the important ones are as follows:
• pthread_create is used to create a new thread in the same address space as the
calling thread. The thread executes concurrently with its parent thread. However,
instead of executing the parent's code, it executes a procedure whose name is
specified as an input parameter to thepthread_create routine.

Sec.8.3 • Threads 411
• pthread_exit is used to terminate the calling thread. This routine is called by a
thread when ithas finished doing its work.
• pthreadjjoin is used to cause the caning thread to block itself until the thread
specified in this routine's argument terminates. This routine is similar to the wait
system call ofUNIX and may be usedby aparentthread to wait for achildthread
to complete execution.
• pthread_detach is used by a parent thread to disown a child thread. By calling
pthread_detach, the parent thread announces that the specified child thread will
never be pthreadjjoined (waited for). If the child thread ever calls pthread_exit,
its stack and other state information are immediately reclaimed. In normal cases,
this cleanup takes place after the parent has done a successful pthreadjjoin.
• pthread_cancel is used by a thread to kill another thread.
• pthread_setcancelis used by a thread to enable or disable ability ofotherthreads
to kill it. Itallows a thread to prevent it from getting killed by another thread at
such times when killing the thread mighthave devastatingeffects, for example, if
the thread has a mutex variable locked at the time.
Threads Synchronization
The DeE Threads package provides support for both mutex variables and condition
variables for threads synchronization. Mutex variables are used when access to a shared
resource by multiple threads must be mutually exclusive in time. On the other hand,
condition variables are used with mutex variables to allow threads to block and wait for
asharedresource alreadylocked by anotherthread until the thread using itunlocks itand
signals the waiting thread.
The DeEThreads package supports the following types of mutex variables, which
differ in how they deal with nested locks:
1. Fast. A fast mutex variable is one that causes a thread to block when the thread
attempts to lock an already locked mutex variable. Thatis, nestedlockingofafast mutex
variable is not permitted. Note that fast mutex variables may lead to deadlock. For
instance, ifa thread tries to lock the same mutex variable a second time, a deadlock will
occur.
2. Recursive. A recursive mutex variable is one that allows a thread to lock an
already locked mutex variable. That is, nested locking ofa recursive mutex variable is
permitted with arbitrarily deep nestings. Notice that recursive mutex variables will never
lead to deadlock, It is the responsibility of the application programmers to ultimately
unlock a recursive mutex variable as many times as it is locked.
3. Nonrecursive. A nonrecursive mutex variable is one that neither allows a thread
to lock an already locked rnutex variable nor causes the thread to block. Rather, an error
is returned to the thread that attempts to lock an already locked nonrecursive mutex
variable. Noticethatnonrecursive mutex variablesavoid the deadlockproblemassociated
with fast mutex variables.

412 Chap. 8 • Process Management
Some of the main DeE thread calls for threads synchronization are as follows:
• pthread_mutex_init is used to dynamically create a mutex variable.
• pthread_mutex_destroy is used to dynamically delete a mutex variable.
• pthread_mutex_lock is used to lock a mutex variable. If the specified mutex
variableisalreadylocked,thethreadthatmakesthiscallisblocked untilthemutex
variable is unlocked.
• pthread_mutex_trylockisusedtomakeanattempt tolockamutex variable.Ifthe
mutex variableisalready locked, thecalJreturns withanunsuccessful resultrather
than causing the thread to block.
• pthread_mutex_unlock is used to unlock a mutex variable.
• pthread_cond_init is used to dynamically create a condition variable.
• pthread_cond_destroy is used to dynamically delete a condition variable.
• pthread_cond_wait is used to wait on a condition variable. The calling thread
blocks until apthread_cond_signalor apthread_cond_broadcast is executed for
the condition variable.
• pthread_cond_signal is used to wake up a thread waiting on the condition
variable. Ifmultiple threads arewaitingonthecondition variable, onlyone thread
is awakened; others continue to wait.
• pthread_cond_broadcast is used to wake up all the threads waiting on the
condition variable.
Another area where mutual exclusion is needed is in the use of UNIX library
procedures. The standard library procedures of UNIX are not reentrant. Therefore, to
prevent inconsistencies that may be caused by. threads switching occurring at arbitrary
points in time, it is necessary to provide mutual exclusion for the individual calls.
DeE solves this problem by providingjacketroutines fora number of nonreentrant
UNIXsystemcalls(mostly110procedures).Threadscallthejacketroutines insteadofthe
UNIX system calls. The jacket routines take necessary action on behalf of the thread
before or after invoking the system call toavoid any potential problem. For example, the
jacketroutines ensure that only one thread calls any particular service at a time.
For several other UNIXprocedures, DeEprovides asingle global mutex variableto
ensure that only one thread at a time is active in the library.
Threads Scheduling
The DeEThreads package supports priority-based threads scheduling. Itallows theusers
to specify not only the priorities for individual threads but also the scheduling algorithm
to beused so that important threads can take priority over other threads, getting the
necessary CPU time whenever they needit.A usercan choose fromone of the following
threads-scheduling algorithms:
I. First in,first out (FIFO). In this method, the first thread of the first nonempty
highest priority queue isalways selected to run.The selected thread continues toexecute

Sec.8.3 • Threads 413
until iteither blocks orexits.After thethread finishes execution, thesame method isused
to select a new thread for CPU allocation. The algorithm may cause starvation of low
priority threads.
2. Roundrobin(RR). Inthis method, also, thefirst nonempty highest priority queue
islocated. However,instead ofrunning thefirstthreadonthisqueue tocompletion,allthe
threads on this queue are given equal importance by running each thread for a fixed
quantum in a round-robin fashion. This algorithm may also cause starvation of low
priority threads.
3. Default. In this method, the threads on all the priority queues are run one after
another using a time-sliced, round-robin algorithm. The quantum allocated to a thread
variesdependingonitspriority;thehigher thepriority,thelargerthequantum. Notice that
in this algorithm there is no starvation because all threads get to run.
The following system calls allow users to select a scheduling algorithm of their
choice and to manipulate individual threads priorities:
• pthread_setscheduleris used to select a scheduling algorithm.
• pthreadgetscheduleris used to know which scheduling algorithm iscurrently in
effect.
• pthreadsetprio is used to set the scheduling priority of a thread.
• pthreadgetprio is used to know the scheduling priority of a thread.
Signal Handling
Signals maybegenerateddue toeither anexception condition occurring during athread's
execution, such as a segmentation violation, or a floating-point exception or due to an
external interrupt, such as when t.he user intentionally interrupts the running process by
hitting theappropriate keyonthekeyboard. InDeE, anexceptioncondition ishandled by
the thread in which it occurs. However, an external interrupt is handled by all the
concerned threads. That is, when an external interrupt occurs, the threads package passes
it to all the threads that are waiting for the interrupt.
DeE also includes the POSIX sigwait and sigaction services that may be used for
signal handling instead of catching signal handlers in the traditional way.These services
operate at a different level than signal handlers but can achieve the same results. The
sigwait service allows a thread to block until one of a specified set of interrupt-based
signals (alsoknown asasynchronous signals)isdelivered. Ontheotherhand,thesigaction
service allows for per-thread handlers tobe installed forcatching exception-basedsignals
(also known as synchronous signals).
Error Handling
The UNIXsystem calls as well asthe standard PlOO3.4aP-Threads calls report errors by
setting a global variable, errno, and returning -1. In this method of handling errors, an
error may get lost when the global errno variable isoverwritten by an error occurring in

414 Chap. 8 • Process Management
some other thread before the previous errno value is seen and handled by the thread in
which it occurred. To overcome this problem, each thread in DeE has its own private
errno variable for storing its own error status.This variable is saved and restored along
with other thread-specific items upon thread switches. The error-handling interface of
DeE allows the programmers to inspect the value of this variable. Another method that
may beused in DeE for error handling is to have system calls raise exceptions when
errors occur.
8.4 SUMMARY
This chapter has presented a description of the two important process management
concepts in distributed operating systems: process migration and threads.
Process migration deals with the transparent relocation of a process from one node
to another in a distributed system. A process may be relocated before it starts executing
or during the course of its execution. The former is called non-preemptive process
migration and the latter is known as preemptive process migration. Preemptive process
migration is costlier than non-preemptive process migration because the handling of the
process's state, which must accompany theprocess toits new node, becomes much more
complex after execution begins.
Process migration policy deals with the selection of a source node from which to
migrate a process, a destination node to which the process will be migrated, and the
migrant process. These selection decisions are taken by a suitable global scheduling
algorithm used for the process migration policy.On the other hand, a process migration
mechanism deals with the actual transfer of the process from its source node to its
destination node and the forwarding and handling of related messages during and after
migration. The commonly used method forpreemptive process migration is to freeze the
process onitssourcenode,transferitsstateinformationtoitsdestination node,andrestart
the process on its destination node using this state information.
Thecostofmigratingaprocess isdominated bythetimetakentomigrate itsaddress
space. Totalfreezing, pretransferring, and transfer on reference are the mechanisms used
by the existing systems for address space transfer. The different mechanisms used for
message forwarding in the existing distributed systems are the mechanism of resending
themessage, theorigin sitemechanism, thelinktraversalmechanism, andthelink update
mechanism. The two different mechanisms forcommunication between a process and its
subprocesses that might have been migratedandplaced on different nodes are thelogical
host concept and the home node or origin site concept.
Process migration in heterogeneous systems becomes more complex than in
homogeneous systems due to the need for data translation from the source node data
format to the destination node data format. The external data representation mechanism
helpsinreducing thesoftwarecomplexity ofthistranslation process. Ofthe various types
ofdata, suchascharacters, integers, andfloating-point numbers, thehandling offloating
point numbers needs special precautions.
The existing implementations of process migration facility have shown that
preemptive process migration ispossible, although withhigher overhead and complexity

Chap. 8 • Exercises 415
than originally anticipated. The cost of migration led some system designers to conclude
thatitisnotaviablealternative, whileothers disagree. However,thetopicisstillanactive
research area with mixed reactions.
Threads areanincreasingly popular waytoimprove application performancethrough
parallelism. In operating systems with t.hreadsfacility, a process consists of an address
space and one or more threads of control. Each thread ofa process has its own program
counter, its own register states, and its own stack. But all the threads of a process share
the same address space. Threads are often referred to as lightweight processes, and
traditional processes are referred to as heavyweight processes.
A major motivation for threads is to minimize context switching time, allowing the
CPU to switch from one unitofcomputationtoanother unitofcomputationwithminimal
overhead.Another importantmotivation forthreads istoallowparallelism tobecombined
with sequential execution and blocking system calls. The useof threads isalso motivated
by the fact that a set of threads using a shared address space is the most natural way to
program many applications.
The three commonly used ways to organize the threads of a process are the
dispatcher-workers model, the team model, and the pipeline model,
The setofprimitivesprovided tousersforthreads-relatedoperations aresaidtoform
a threads package. Some of the important issues in designing a threads package are
creation, termination, synchronization, and scheduling of threads and handling of signals
and errors.
A threads package can be implemented either in user space or in the kernel. Both
approaches have their own advantages and limitations.
EXERCISES
8.1. Differentiate between preemptive and non-preemptive process migration. What are their
relative advantages and disadvantages? Suppose you have to design a process migration
facility for a distributed system. What factors will influence your decision to design a
preemptiveor a non-preemptive process migration facility?
8.2. What are some of the main issues involved in freezing a migrant process on itssource node
and restarting it on its destination node? Give a method for handling each of these issues.
8.3. What are the main similarities and differences between the implementation of the following
two activities:
(a) Interrupting a process to execute a higher priority process and then restarting the
interrupted process after some time on the same node
(b) Freezing a migrant process and then restarting iton a different node
8.4. From the point of view of supporting preemptive process migration facility, is astateless or
stateful file serverpreferable? Give reasons for your answer.
8.S. When a migrant process is restarted on its destination node after migration, it is given the
same processidentifierthat ithadon itssource node. Isthis necessary?Give reasons for your
answer.
8.6. The cost of migrating aprocess isdominated by the time taken totransfer its address space.
Suggest some methods that may be used to minimize this cost.

416 Chap. 8 • Process Management
8.7. Adistributed systemsupports DSM (Distributed SharedMemory)facility.Suggest asuitable
address spacetransfer mechanism thatyouwillusetodesign aprocess migration facility for
this system.
8.8. Which one or more of the address space transfer mechanisms described in this chapter are
suitable for a process migration facility with the following goals?
(a) High performance is the main goal.
(b) High reliability is the main goal.
of
(c) Effectiveness process migration policy is the main goal.
(d) Simple implementation is the main goal.
(e) Both reliability and effectiveness of process migration policy are important goals.
Ifmorethanonemechanism issuitableforaparticularcase,whichonewillyouprefertouse
and why?
8.9. Which one or more of the message-forwarding mechanisms described in this chapter are
suitable for a process migration facility with thefollowing goals?
(a) Transparency is the main goal.
(b) Reliability is the main goal.
(c) Performance is the main goal.
(d) Simple implementation is the main goal.
Ifmorethan one mechanisms are suitable foraparticular case, which one will youprefer to
use and why?
8.10. Which of the mechanisms described in this chapter to handle communication among
coprocesses are suitable for a process migration facility with the following goals?
(a) Performance is the main goal.
(b) Reliability is the main goal.
(c) Simple implementation is the maingoal.
Ifmore thanone mechanisms are suitable fora particularcase, which one will youprefer to
use and why?
8.11. What are some of the main issues involved in designing a process migration facility for a
heterogeneous distributed system?
8.12. The process migration facility of a distributed system does not allow free migration of
processes from one node to another but has certain restrictions regarding which node's
processes can be migrated to which other nodes of the system. What might be the reasons
behind imposing such a restriction?
8.13. When should theexternal data representation mechanism be used in thedesign of aprocess
migration facility? Suppose you have todesign theexternal data representation format fora
process migration facility.What important factors will influence your design decisions?
8.14. Adistributed system hasthree typesofprocessorsA,B,and C.The numbers ofbitsusedfor
theexponent ofafloating-point numberbyprocessors oftypesA,B,andCare8, 12,and 16,
respectively; the numbers of bits used for the mantissa of a floating-point number by
processors of typesAt B, and Care 16,32,and64, respectively. Inthis system, from which
processor typetowhichprocessor typeshouldprocess migration beallowed and fromwhich
processor type to which processor type should processor migration not be allowed? Give
reasons for your answer.
8.15. List some of the potential advantages and disadvantages of process migration.
8.16. Inoperating systems inwhichaprocess isthebasic.unitofCPU utilization, mechanisms are
provided toprotect aprocess from other processes. Do operating systems in which a thread
is the basic unit of CPU utilization need to provide similar mechanisms to protect a thread
from other threads? Give reasons for your answer.

Chap. 8 • Exercises 417
8.17. The conceptof threads isoften used indistributedoperatingsystemsfor betterperformance.
Can this concept also be useful for better performance in other multiprocessor operating
systems and in operating systems for conventional centralized time-sharing systems? Give
reasons for youranswer.
8.18. Listthemain differences and similarities between threads and processes.
8.19. What are the main advantages and disadvantages of using threads instead of multiple
processes?Give anexampleofanapplicationthat would benefit from the use of threads and
another application that would not benefit from the use of threads.
8.20. In a distributed system, parallelism improves performance and blocking system calls make
programming easier. Explain how the concept of threads can be used to combine both
advantages.
8.21. Give an example to show how a server process can be designed to benefit from the
concurrency made possible by threads. Now give an example to show how aclient process
can be designed to benefitfrom the concurrency made possible by threads.
8.22. Give a suitable example for each of the following:
(a) An application in which a process uses multiple threads that are organized in the
dispatcher-workers model
(b) An application in which a process uses multiple threads that are organized in the team
model
(c) Anapplication inwhich aprocess uses multiplethreads that areorganizedinthepipeline
model
8.23. A file server works in the following manner:
(a) It accepts a client request for file access.
(b) It then tries to service the request using data in a cache that it maintains.
(c) If the request cannot be serviced from the cached data, it makes a request to the disk
server for the data and sleeps until a reply is received. On receiving the reply, it caches
the data received and services the client's request.
Assumethatthehitratioforthecache is0.7.That is,70%ofalltherequests areservicedusing
cached data and access to disk server is needed only for serving 30% of all requests. Also
assume that,onacache hit,therequest servicetime is20msecandonacache misstherequest
servicetime is 100msec. How many requests per second can be serviced ifthe file serveris
implemented as follows?
(a) A single-threaded process
(b) A multithreaded process
Assume that threads switching time is negligible.
8.24. Differentiatebetweenhandoffschedulingand affinity schedulingofthreads. In your opinion,
which of the two is a more desirable feature for a threads package and why?
8.25. Write pseudocode for a threads-scheduling algorithm that provides the flexibility to vary
quantum size dynamically and also supports handoffscheduling.
8.26. What are the main issues in handling signals in a multithreaded environment? Describe a
method for handling each of these issues.
8.27. Discussthe relativeadvantagesanddisadvantagesofimplementingathreads packageinuser
space and in the kernel.
8.28. The operatingsystemofacomputeruses processesas the basic unitofCPU utilization.That
is, it does not support threads. Can threads facility be provided in this computer system
without modifying the operating system kernel? If no, explain why. If yes, explain how.

418 Chap.8 • ProcessManagement
BIBLIOGRAPHY
[Agrawal and Ezzat 1987] Agrawal, R., and Ezzat, A. K., "Location Independent Remote
ExecutioninNEST,"IEEE Transactionson SoftwareEngineering, Vol. SE-13,No.8 (1987).
[Alonsoand Kyrimis1988]Alonso,R.,andKyrimis,K.,"AProcessMigrationImplementationfor
a UNIXSystem,"In:Proceedings ofthe Winter1988 Usenix Conference, UsenixAssociation,
Berkeley, CA(February 1988).
[Anderson et al. 1991]Anderson, T. E., Bershad,B. N., Lazowska, E. D., and Levy,H. M.,
"SchedulerActivations: EffectiveKernelSupportfor theUser-Level Managementof Parallel
ism,"In:Proceedingsofthe13thACMSymposium onOperatingSystemPrinciples,Association
forComputingMachinery, NewYork, NY, pp.95-109 (1991).
[Artsyand Finkel 1989]Artsy,Y.,andFinkel,R.,"DesigningaProcessMigrationFacility,"IEEE
Computer, Vol. 22,pp.47-56 (1989).
[Black 1990] Black, D., "Scheduling Support for Concurrency and Parallelism in the Mach
OperatingSystem,"IEEE Computer,Vol. 23,pp.35-43 (1990).
[Butterfield and Popek 1984]Butterfield, D. A., and Popek,G. 1, "NetworkTaskingin the
LOCUSDistributedUNIXSystem,"In:Proceedings ofthe Summer 1984 Usenix Conference,
UsenixAssociation, Berkeley, CA,pp.62-71 (June1984).
[Cheriton 1988]Cheriton,D.R.,"TheVDistributed System,"Communications oftheACM, Vol.
31,No.3, pp.314-333 (1988).
[Coulouris et al, 1994JCoulouris,G. F.,Dollimore, 1.,and Kindberg, T.,Distributed Systems
Concepts and Design, 2nded.,Addison-Wesley, Reading,MA(1994).
[Douglisand Ousterhout 1987]Douglis,F.,andOusterhout, J.,"ProcessMigrationintheSprite
Operating System," In: Proceedings of the 7th International Conference on Distributed
Computing Systems, IEEE,NewYork, NY, pp. 18-25 (September 1987).
[DougUsand Ousterhout 1991]Douglis,F.,andOusterhout, J.,"Transparent ProcessMigration:
DesignAlternatives andtheSpriteImplementation," Software-Practice and Experience, Vol.
21,pp.757-785 (1991).
[Draves et al, 1991]Draves, R. P.,Bershad,B. N., Rashid,R. F.,and Dean, R. W.,"Using
ContinuationstoImplementThreadManagementandCommunicationinOperatingSystems,"In:
Proceedings of the 13th ACM Symposium on Operating System Principles, Association for
ComputingMachinery, NewYork, NY, pp. 122-136(1991).
[Ferrariand Sunderam 1995]Ferrari,A.,andSunderam, V. S.,"TPVM:DistributedConcurrent
ComputingwithLightweightProcesses,"In:Proceedingsofthe4thInternationalSymposiumon
High Performance DistributedComputing (August1995).
[Goscinski1991]Goscinski, A.,"DistributedOperating Systems,TheLogicalDesign,"Addison
Wesley, Reading,MA(1991).
[Huang et al, 1995]Huang,C.,Huang,Y., andMcKinley, P.K.,"AThread-Based Interfacefor
Collective Communication on ATM Networks," In: Proceedings of the 15th International
Conferenceon Distributed Computing Systems, IEEE,NewYork, NY(May-June 1995).
[Hunter 1988]Hunter,C., "ProcessCloning:A Systemfor Duplicating UNIXProcesses," In:
ProceedingsoftheWinter1988UsenixConference,UsenixAssociation,Berkeley,CA(February
1988).
[Ju11989]Jul,E.,"MigrationofLight-WeightProcessesinEmerald,"tcos Newsletter,Vol.3,No.
1,pp.20-23 (1989).

Chap.8 • Bibliography 419
[Jul et al. 1988]Jul,E.,Levy, H.,Norman, H.,andAndrew, B.,"Fine-Grained Mobility inthe
Emerald System," ACM Transactions on Computer Systems, Vol. 6, No.1, pp. 109-133
(1988).
[Kingsbury and Kline 1989) Kingsbury, B. A.,and Kline, J. T.,"Job and Process Recovery in a
UNIX-BasedOperatingSystem,"In: Proceedingsofthe Winter1989UsenixConference,Usenix
Association, Berkeley, CA, pp. 355-364 (1989).
[Litzkow 1987]Litzkow, M. 1.,"Remote UNIX-Turning Idle Workstations into Cycle Servers,"
In:Proceedingsofthe Summer1987UsenixConference,Usenix Association,Berkeley,CA (June
1987).
[1.101989]Lo,V.M.,"ProcessMigrationfor CornmunicationPerformance,"TCOSNewsletter, Vol.
3, No.1, pp. 28-30 (1989).
[Lockhart 1994J Lockhart, Jr., H. W., OSF DC'E: Guide to Developing DistributedApplications,
IEEEComputerSociety Press, Los Alamitos, CA (1994).
[Maguire and Smith 1988] Maguire, Jr., G.Q. and Smith, 1 M., "Process Migration: Effects on
Scientific Computation,"ACM-SIGPLANNotices, Vol. 23, No.3, pp. 102-106 (1988).
[MandelbergandSunderam1988]Mandelberg,K.I.,and Sunderam,V.S.,"ProcessMigrationin
UNIX Networks," In: Proceedings ofthe Winter 1988Usenix Conference, UsenixAssociation,
Berkeley, CA (February 1988).
[Marsh et ale1991] Marsh, B. D.,Scott, M.L.,LeBlanc, T.J., and Markatos, E.P.,"First-Class
User-Level Threads," In: Proceedings of the J3th ACM Symposium on Operating System
Principles,Association for Computing Machinery, NewYork, NY,pp. 110-121 (1991).
[MuIJenderetal, 1990]Mullender,S..J.,VanRossum,G.,Tanenbaum,A.S.,VanRenesse, R.,and
VanStaverene, H., "Amoeba: A Distributed Operating System for the 1990s," IEEE Computer,
Vol.23, No.5, pp.44--53 (1990).
[Nutt1991]NUH,G. L, CentralizedandDistributedOperatingSystems, Prentice-Hall,Englewood
Cliffs, NJ (1991).
[Popek and Walker 1985] Popek, G. r, and Walker, B. r, The LOCUS Distributed System
Architecture, MIT Press, Cambridge, MA (1985).
[PowellandMiller1983]Powell,M. L.,and Miller, B.P.,"ProcessMigrationinDEMOS/MP,"In:
Proceedings of the 9th ACM Symposium on Operating System Principles, Association for
Computing Machinery, New York, NY,pp. 110-119 (November 1983).
[Rosenberry et al. 1992) Rosenberry, W., Kenney, D., and Fisher, G., OSF DISTRIBUTED
C()MPU71NG ENVIR()NMENT, Understanding DeE, O'Reilly & Associates, Sebastopol, CA
(1992).
[Schwanet aJ. 1991]Schwan, K.,Zhou, H.,and Gheith,A.,"Real-TimeThreads,"ACM-SIGOPS
Operating Systems Review, Vol.25, No.4, pp. 35-46 (1991).
[Sinha et al. 1991] Sinha, P. K., Park. K., Jia, X., Shimizu, K., and Maekawa, M., "Process
Migration Mechanism inthe Galaxy Distributed Operating System," In: Proceedingsofthe 5th
International Parallel Processing Symposium, IEEE, New York, NY, pp. 611-618 (April
1991).
[Smith 1988] Smith, J. M., HA Survey of Process Migration Mechanisms," ACM-SIGOPS
Operating Systems Review, Vol.22, pp. 28-40 (July 1988).
[Stalling 1995] Stalling, W., Operating Systems, 2nd ed., Prentice-Hall, Englewood Cliffs, NJ
(1995).
[Tanenbaum 1995]Tanenbaum, A.S., Distributed Operating Systems, Prentice-Hall, Englewood
Cliffs, NJ (1995).

420 Chap. 8 • Process Management
[Tbeimer et al. 1985]Theimer, M. M., Lantz K.A., and Cheriton, D. R., "Preernptable Remote
Execution Facilities for the V System," In: Proceedings of the 10th ACM Symposium on
Operating System Principles, Association for Computing Machinery, New York,NY,pp. 2-12
(December 1985).
[Thekkathand Eggers 1994] Thekkath, R., and Eggers, S. J., "Impact of Sharing-Based Thread
PlacementonMultithreadedArchitectures,"In:Proceedingsofthe21stInternationalSymposium
on ComputerArchitecture, Association forComputing Machinery,New York,NY,pp. 176-186
(1994).
[Walker and Mathews 1989]'Walker,B. 1., and Mathews, R. M., "Process Migration in AIX's
Transparent Computing Facility (TCF)," TCOS Newsletter, Vol.3,No. I, pp. 5-7 (1989).
[Zayas 1987]Zayas, E. R., "Attacking the Process Migration Bottleneck,"In: Proceedings ofthe
11thACMSymposium on Operating Systems Principles,Association forComputingMachinery,
New York,NY,pp. 13-22 (November 1987).
POINTERS TO818UOGRflPHIES ONTHE INTERNET
Bibliographies containing references on Process Migration can be found at:
ftp:ftp.cs.umanitoba.calpublbibliographieslDistributed/migrate.html
ftp:ftp.cs.umanitoba.calpublbibliographieslDistributedldshell.html
Bibliography containing references on Threads and Multithreading can be found at:
ftp:ftp.cs.umanitoba.calpublbibliographies/Os/threads.htm1

9
CHAPTER
Distributed File
Systems
9.1 INTRODUCnON
In a computer system, a file is a named object that comes into existence by explicit
creation, is immune to temporary failures in the system, and persists until explicitly
destroyed. The two main purposes of using files are as follows:
1. Permanent storage of information. This is achieved by storing a file on a
secondary storage media such as a magnetic disk.
2. Sharing ofinformation. Files provide a natural and easy means of information
sharing. That is, a file can be created by one application and then shared with
different applications at a later time.
Afile system is a subsystem of an operating system that performs file management
activities such as organization, storing, retrieval, naming, sharing, and protection offiles.
It is designed to allow programs to use a set of operations that characterize the file
abstraction and free the programmers from concerns about the details of space allocation
and layout of the secondary storage device. Therefore, a file system provides an
abstraction of a storage device; that is, it is a convenient mechanism for storing and
retrieving information from the storage device.
421

422 Chap.9 • Distributed FileSystems
A distributedfile system provides similar abstraction to the users of a distributed
system and makes it convenient for them to use files in a distributed environment. The
design and implementation of a distributed file system, however, is more complex than a
conventional file system due to the fact that the users and storage devices are physically
dispersed.
In addition to the advantages of permanent storage and sharing of information
provided by the file system of a single-processor system, a distributed file system
normally supports the following:
1. Remote information sharing. A distributed file system allows a file to be
transparently accessed by processes ofany node of the system irrespective of the file's
location. Therefore, a process on one node can create a file that can then be accessed at
a later time by some other process running on another node.
2. Usermobility.Inadistributedsystem, usermobility implies thatausershould not
be forced to work on a specific node but should have the flexibility to work on different
nodesatdifferent times.Thisproperty isdesirableduetoreasons suchascoping withnode
failures, suiting the nature ofjobs of some users who need to work at different places at
different times, andenabling useofanyoftheseveral nodes in thoseenvironments where
workstations are managed as a common pool. A distributed file system normally allows
a user to work on different nodes at different times without the necessity of physically
relocating the secondary storage devices.
3. Availability.For better fault tolerance, filesshould beavailable for useeven inthe
event of temporary failure of one and more nodes of the system. To take care of this, a
distributed file system normally keeps multiple copies of a file on different nodes of the
system. Each copy iscalled a replicaof the file. In an ideal design, both theexistence of
multiple copies and their locations are hidden from the clients.
4. Diskless workstations. Disk drives are relatively expensive compared to the cost
of most other parts in a workstation. Furthermore, since a workstation is likely to be
physically placed in the immediate vicinity ofa user, the noise and heat emitted from a
disk drive are annoying factors associated with a workstation. Therefore, a diskless
workstation is more economical, is less noisy, and generates less heat. A distributed file
system, with its transparent remote file-accessing capability, allows the use of diskless
workstations in a system.
A distributed file system typically provides the following three types of services.
Each can be thought of as a component of a distributed file system.
1. Storage service. It deals with the allocation and management of space on a
secondary storage device that is used for storage of files in the file system. Itprovides a
logical view ofthe storage system by providing operationsfor storing and retrieving data
in them. Most systems use magnetic disks as the secondary storage device for files.
Therefore, the storage service isalso known asdiskservice.Furthermore, several systems
allocate disk space in units of fixed-size blocks, and hence, the storage service is also
known as block service in these systems.

Sec.9.2 • Desirable Features of aGood Distributed File System 423
2. Truefile service. It is concerned with the operations on individual files, such as
operationsfor accessingand modifyingthedatainfiles and forcreatingand deletingfiles.
To perform these primitive file operations correctly and efficiently, typical design issues
ofatrue file servicecomponentincludefile-accessing mechanism,file-sharing semantics,
file-caching mechanism, file replication mechanism, concurrency control mechanism,
data consistency and multiple copy update protocol, and access control mechanism. Note
that the separation of the storage service from the true file service makes it easy to
combine different methods of storage and different storage media in a single file
system.
3. Name service. It provides a mapping betweentext names for files and references
to files, that is, file lOs. Text names are requiredbecause, as describedinChapter 10,file
IDsareawkwardanddifficultforhumanusers torememberanduse.Most filesystemsuse
directories to perform this mapping. Therefore, the name service is also known as a
directory service. The directory service is responsible for performing directory-related
activities such as creation and deletion of directories, adding a new file to a directory,
deleting a file from a directory, changing the name of a file, moving a file from one
directory to another, and so on.
The design and implementation ofthe storage service ofa distributed file system is
similarto that of the storage serviceof acentralizedfile system. Readers interestedin the
details of the storage service may refer to any good book on operating systems
[Tanenbaum 1987, Silberschatzand Galvin 1994].The design and implementationdetails
of the name service will be presented in the next chapter. Therefore, this chapter will
mainly deal with the design and implementation issues of the true file servicecomponent
of distributed file systems.
9.2 DESIRA8lE FEATURES OF AGOODDISTRI8UTED FilE
SYSTEM
A good distributed file system should have the features described below.
1. Transparency. The following four types of transparencies are desirable:
• Structure transparency. Although not necessary, for performance, scalability, and
reliability reasons, a distributed file system normally uses multiple file servers.
Each file server is normally a user process or sometimes a kernel process that is
responsible for controlling a set of secondary storage devices (used for file
storage) ofthe node on which itruns. In multiple file servers, the multiplicity of
file servers should be transparent to the clients of a distributed file system. In
particular, clients should not know the numberor locationsofthe file servers and
the storage devices. Ideally, adistributedfile system shouldlook to itsclients like
a conventional file system offered by a centralized, time-sharing operating
system.

424 Chap. 9 • Distributed File Systems
• Accesstransparency. Both local and remote files should beaccessible inthesame
way.That is, the file system interface should not distinguish between local and
remote files, and the file system should automatically locate an accessed file and
arrange for the transport ofdata to the client's site.
• Naming transparency. The name ofa file should give no hint as to where the file
islocated.Furthermore,afileshouldbeallowed tomovefromonenodetoanother
in a distributed system without having to change the name of the file.
• Replicationtransparency.Ifafileisreplicatedonmultiplenodes,boththeexistence
ofmultiplecopies andtheirlocations shouldbehiddenfromtheclients.
2. User mobility. In a distributed system, a user should not be forced to work on a
specific node butshould have the flexibility to workondifferent nodes atdifferent times.
Furthermore, the performance characteristics of the file system should not discourage
usersfromaccessing theirfilesfrom workstationsother thantheoneatwhichtheyusually
work. One way to support user mobility is to automatically bring a user's environment
(e.g., user's home directory) at the time of login to the node where the user logs in.
3. Performance.Theperformanceofafilesystem isusually measured astheaverage
amount of time needed to satisfy client requests. In centralized file systems, this time
includes thetimeforaccessing thesecondary storagedeviceonwhichthefileisstoredand
the CPU processing time. In a distributed file system, however, this time also includes
network communication overhead when the accessed file is remote. Although acceptable
performance is hard to quantify, it is desirable that the performance of a distributed file
system should becomparable tothat of acentralizedfile system. Users should never feel
the need to make explicit file placement decisions to improve performance.
4. Simplicityandeaseofuse. Several issues influencethe simplicity and ease ofuse
of a distributed file system. The most important issue is that the semantics of the
distributed file system should be easy to understand. This implies that the user interface
to the file system must be simple and the number of commands should be as small as
possible. Inan ideal design, the semantics ofadistributed file system should be the same
as that of a file system for a conventional centralized time-sharing system. Another
importantissue forease of use is that the file system should beable tosupport the whole
range of applications.
5. Scalability. It is inevitable that a distributed system will grow with time since
expandingthenetworkbyadding newmachines orinterconnectingtwo networks together
iscommonplace. Therefore, a good distributed file system should be designed to easily
cope with the growth ofnodes and users in the system. That is, such growth should not
cause serious disruption of service or significantloss ofperformance tousers. In short,a
scalable design should withstand high service load, accommodate growth of the user
community, and enable simple integration of added resources.
6. Highavailability.Adistributedfilesystem shouldcontinue tofunction even when
partial failures occur due to the failure of one or more components, such as a
communication link failure, a machine failure, or a storage device crash. When partial
failures occur, the file system may show degradation in performance, functionality, or

Sec.9.2 • Desirable Features of aGood Distributed FileSystem 425
both. However, the degradation should be proportional, in some sense, to the failed
components. For instance, it isquite acceptable that the failure causes temporary loss of
service to small groups of users.
High availability and scalability are mutually related properties. Both propertiescall
for a design in which both control and data are distributed. This is because centralized
entities such as a central controller or a central data repository introduce both a severe
point of failure and a performance bottleneck. Therefore, a highly available and scalable
distributed file system should have multiple and independent file servers controlling
multiple and independent storage devices. Replication of files at multiple servers is the
primary mechanism for providing high availability.
7. High reliability.Inagood distributedfile system, the probability of loss ofstored
data should be minimized as far as practicable. That is, users should not feel compelled
tomake backupcopiesoftheir files becauseofthe unreliabilityofthe system. Rather, the
file system should automatically generate backupcopies of critical files that can be used
in the event of loss of the original ones. Stable storage is a popular technique used by
several file systems for high reliability.
8. Data integrity. A file isoften shared by multiple users. For a shared file, the file
systemmust guaranteethe integrityofdata storedinit.Thatis,concurrentaccess requests
from multiple users who are competing to access the file must be properly synchronized
by the use of some form of concurrency control mechanism. Atomic transactions are a
high-levelconcurrencycontrol mechanismoftenprovidedtothe users by afile systemfor
data integrity.
9. Security. A distributed file system should be secure so that its users can be
confident of the privacy of their data. Necessary security mechanisms must be
implemented to protect information stored in a file system against unauthorized access.
Furthermore, passing rights to access a file should be performed safely; that is, the
receiver ofrights should not beable topass them further ifhe or she isnot allowedto do
that.
A consequence of large-scale distributed systems is that the casual attitude toward
security is not acceptable. A fundamental question is who enforces security. For this, the
general design principle is that a system whose security depends on the integrity ofthe
fewest possible entities is more likely to remain secure as it grows.
10. Heterogeneity. As a consequence of large scale, heterogeneity becomes
inevitable in distributed systems. Heterogeneous distributed systems provide the
flexibility totheirusers to usedifferentcomputerplatformsfor differentapplications. For
example, a user may use a supercomputer for simulations, a Macintosh for document
processing,and aUNIX workstationfor programdevelopment.Easy access toshareddata
across these diverse platforms would substantially improve usability. Therefore, a
distributedfile systemshouldbe designedtoallow a variety of workstationsto participate
in the sharing of files via the distributed file system. Another heterogeneity issue in file
systems is the ability to accommodate several different storage media. Therefore, a
distributed file system should be designed to allow the integration of a new type of
workstation or storage media in a relatively simple manner.

426 Chap. 9 • Distributed File Systems
9.3 FilEMODELS
Differentfilesystemsusedifferentconceptualmodelsofafile.Thetwomostcommonly
usedcriteriaforfilemodelingarestructureandmodifiability. Filemodelsbasedonthese
criteriaaredescribedbelow.
9.3.1 Unstructured andStructured Files
Accordingtothesimplestmodel,afileisanunstructuredsequenceofdata.Inthismodel,
thereis nosubstructureknownto the file serverandthecontentsof each fileof the file
systemappearsto the file server as an uninterpretedsequenceof bytes.The operating
systemisnotinterestedintheinformationstoredinthefiles.Hence,theinterpretationof
themeaningandstructureofthedatastoredinthefilesareentirelyuptotheapplication
programs.UNIXandMS-DOSuse this file model.
Anotherfilemodelthatisrarelyusednowadaysisthestructuredfilemodel.Inthis
model, a file appears to the file server as an ordered sequenceof records. Recordsof
differentfilesof thesamefilesystemcan beofdifferentsize.Therefore,manytypesof
filesexistinafilesystem,eachhavingdifferentproperties.Inthismodel,arecordisthe
smallestunitoffiledatathatcanbeaccessed,andthefilesystemreadorwriteoperations
arecarriedouton a setof records.
Structuredfilesareagainoftwotypes-files withnonindexedrecords andfileswith
indexedrecords. Intheformermodel,afilerecordisaccessedby specifyingitsposition
withinthefile,forexample,thefifthrecordfromthe beginningofthe fileorthesecond
recordfromtheendof the file.In thelattermodel,recordshaveone or morekeyfields
andcanbeaddressedbyspecifyingthevaluesofthekeyfields.Infilesystemsthatallow
indexedrecords,afileismaintainedasa B-treeorothersuitabledatastructureorahash
tableis usedto locaterecordsquickly.
Most modern operating systems use the unstructured file model. This is mainly
becausesharingofafilebydifferentapplicationsiseasierwiththeunstructuredfilemodel
ascomparedtothestructuredfilemodel.Sinceafilehasnostructureintheunstructured
model,differentapplicationscan interpretthecontentsof a fileindifferentways.
In additionto data items, filesalso normallyhaveattributes. A file's attributesare
informationdescribingthatfile.Eachattributehasanameandavalue.Forexample,typical
attributesofafilemaycontaininformationsuchasowner,size,accesspermissions,dateof
creation,dateoflastmodification,anddateoflastaccess.Userscanreadandupdatesomeof
theattributevaluesusingtheprimitivesprovidedbythefilesystem.Notice,however,that
althoughausermayreadthevalueofanyattribute,notallattributesareusermodifiable.For
example,a usermayupdatethe valueof the accesspermissionsattribute,but heor she
cannotchangethevalueofthesizeordateofcreationattributes.Thetypesofattributesthat
canbeassociatedwithafilearenormallyfixedbythefilesystem.However,afilesystem
maybedesignedtoprovidetheflexibilitytocreateandmanipulateuser-definedattributesin
additiontothosesupportedbythefilesystem.
File attributesare normally maintainedand used by the directory servicebecause
they are subject to different access controls than the file they describe. Notice that

Sec. 9.4 • File-Accessing Models 427
although fileattributes aremaintained andusedbythedirectory service, theyarestored
withthecorresponding fileratherthanwiththefilenameinthedirectory. Thisismainly
because many directory systems allowfilestobereferenced bymorethanonename.
9.3.2 Mutable Qnd Immutabl. Files
According to the modifiability criteria, files are of two types-mutable and immutable.
Most existing operating systems use the mutable file model. In this model, an update
performed on a file overwrites on itsold contents to produce the new contents. Thatis, a
file is represented as a single stored sequence that is altered by each update operation.
On the other hand, some more recent file systems, such as the Cedar File System
(CFS) [Gifford et at 1988],use the immutable file model. In this model, afilecannot be
modified once it has been created except to be deleted. The file versioning approach is
normally used to implement file updates, and each file is represented by a history of
immutable versions. That is, rather than updating the same file, a new version of the file
is created each time achange is made to the file contents and the old version is retained
unchanged. Inpractice, the useofstorage space may bereduced bykeeping only arecord
of the differences between the old and new versions rather than creating the entire file
once again.
Gifford et al. [1988] emphasized that sharing only immutable files makes iteasy to
support consistentsharing. Duetothisfeature, itismucheasier tosupport filecaching and
replication inadistributed system with the immutablefile model because iteliminatesall
the problems associated with keeping multiple copies of a file consistent. However, due
to the need to keep multiple versions of afile, the immutable file model suffers from two
potential problems-increased use of disk space and increased disk allocation activity.
Some mechanism is normally used toprevent the disk space from filling instantaneously.
For example, the CFS [Gifford et al. 1988] uses a keep parameteras the number ofmost
current versions of afile to be retained. To keep track of the most current versions, each
filename is suffixed with a version number, for example, "demo.abc!6" is a name for
version 6 of the file with the base name of demo.abc. Therefore, if the value of the keep
parameter of this file is 1,the creation of a new version of the file will cause the file
demo.abc!6 to be deleted and its disk space to be reused for the new file named
demo.abel? When the valueofthekeep parameterisgreaterthan 1forafile,thatis,when
multiple versions ofafileexist, users may useaparticularversion ofthefilebyspecifying
itsfull name. However, whenthe version numberpartofafile isnotspecified, bydefault,
CFS uses the lowest version number for some operations, such as delete, and highest
version number for other operations, such as open.
9.4 FilE-ACCESSING MODELS
The manner in which a client's request to access a file is serviced depends on the file
accessing model used by the file system. The file-accessing model of a distributed file
system mainly depends on two factors-the method used for accessing remote files and
the unit of data access.

428 Chap.9 • Distributed FileSystems
9.4.1 Accessing kmot. FII.s
A distributed file system may use one of the following models to service a client's file
access request when the accessed file is a remote file:
1. Remote service model. In this model, the processing of the client's request is
performed at the server's node. That is, the client's request for file access is delivered
to the server, the server machine performs the access request, and finally the result is
forwarded back to the client. The access requests from the client and the server replies
for the client are transferred across the network as messages. Notice that the data
packing and communication overheads per message can be significant. Therefore, if
the remote service model is used, the file server interface and the communication
protocols must be designed carefully to minimize the overhead of generating messages
as well as the number of messages that must be exchanged in order to satisfy a
particular request.
2. Data-caching model. In the remote service model, every remote file access
request results in network traffic. The data-caching model attempts to reduce the amount
ofnetwork trafficbytakingadvantage ofthelocality feature found infileaccesses. Inthis
model, if the data needed to satisfy the client's access request is not present locally, it is
copied fromtheserver'snode totheclient'snodeand iscached there.The client'srequest
is processed on the client's node itselfby using the cached data. Recently accessed data
are retained in the cache for some time so that repeated accesses to the same data can be
handled locally.A replacement policy, such as the least recently used (LRU), is used to
keep the cache size bounded.
Ascomparedtotheremote access model, this model greatly reduces network traffic.
However, in this model, there is a possibility that data of a particular file may
simultaneously beavailable indifferent caches. Therefore, a write operation often incurs
substantial overheadbecause, inaddition tomodifying thelocallycached copyofthedata,
the changes must also be made in the original file at the server node and in any other
caches having the data, depending on the relevant sharing semantics. The problem of
keeping thecached dataconsistentwiththeoriginal filecontent isreferred toasthecache
consistency problem. Methods to handle this problem will be described later in this
chapter.
As compared to the remote service model, the data-caching model offers the
possibility of increased performance and greater system scalability because it reduces
network traffic, contention for the network, and contention for the file servers.
Therefore, almost all existing distributed file systems implement some form of
caching. In fact, many implementations can be thought of as a hybrid of the remote
service and the data-caching models. For example, LOCUS [Popek and Walker 1985]
and the Network File System (NFS) [Sandberg et al. 1985] use the remote service
model but add caching for better performance. On the other hand, Sprite [Nelson et al.
1988] uses the data-caching model but employs the remote service method under
certain circumstances.

Sec.9.4 • File-Accessing Models 429
9.4.2 Unit or Data Trans'.r
Infile systemsthat use the data-caching model, an importantdesignissue istodecidethe
unit ofdata transfer. Unit ofdata transferrefers to the fraction (or its multiples) ofa file
data that is transferred to and from clients as a result of a single read or write operation.
The four commonly used data transfer models based on this factor are as follows:
1. File-leveltransfermodel. Inthis model, when anoperationrequires filedata tobe
transferredacross the networkineitherdirection betweenaclientand aserver, the whole
file is moved.
In addition to its conceptual simplicity, this model has several advantages
[Satyanarayanan et al. 1985]. First, transmitting an entire file in response to a single
request is more efficientthan transmitting it page by page in response to several requests
because the network protocol overhead is required only once. Second, it has better
scalability because it requires fewer accesses to file servers, resulting in reduced server
load andnetworktraffic. Third, disk access routinesonthe servers canbebetter optimized
if it isknown that requests are always for entire files rather than for random disk blocks.
Fourth, once an entire file is cached at a client's site, it becomes immune to server and
network failures. Hence the model also offers a degree of intrinsic resiliency. Finally, it
also simplifies the task of supporting heterogeneous workstations. This is because it is
easiertotransformanentire file atone time from the form compatiblewiththefilesystem
ofserverworkstation tothe formcompatible with the filesystem oftheclient workstation
or vice versa.
On the other hand, the main drawback of this model is that it requires sufficient
storage space on the client's node for storing all the required files in their entirety.
Therefore, thisapproachfails towork with very large files, especiallywhen theclient runs
onadiskless workstation. Even when theclient's workstation isnotdiskless, files that are
larger than the local disk capacity cannot be accessed at all. Furthermore, if only a small
fraction ofa file is needed, moving the whole file is wasteful.
Amoeba [Mullender and Tanenbaum 1984], CFS [Gifford et al. 1988], and the
Andrew File System (AFS-2) [Satyanarayanan 1990b] arc a few examples of distributed
systems that use the file-level transfer model. AFS-3, which is an upgraded version of
AFS-2, also alJows files to be cached in large chunks (64 kilobytes) rather than in their
entirety. This feature was later incorporated inAFS to allow a workstation to access files
that are too large to fiton its local disk.
2. Block-level transfer model. In this model, file data transfers across the network
betweenaclientand aservertake place inunits offile blocks. Afileblock isacontiguous
portionofafileand isusually fixed inlength. Forfile systemsinwhich blocksizeisequal
to virtual memory page size, this model is also called a page-level transfer model.
The advantageofthis model isthat itdoes notrequireclientnodes tohavelarge storage
space. It also eliminates the need to copy an entire file when only a small portion of the
file data is needed. Therefore, this model can be used in systems having diskless
workstations. Itprovides large virtual memory for clientnodes that do nothave their own
secondary storage devices. However, when anentirefile istobe accessed, multipleserver

430 Chap. 9 • Distributed FileSystems
requests are needed in this model, resulting in more network traffic and more network
protocol overhead. Therefore, this model has poor performance as compared to the file
level transfer model when the access requests are such that most files have to be
transferred in their entirety. The Apollo Domain File System [Leach et al. 1983], Sun
Microsystem's NFS [Sandberg 1987], LOCUS [Popek and Walker 1985], and Sprite
[Nelson et al, 1988] are a few examples ofdistributed systems that use the block-level
transfer model.
3. Byte-level transfer model. In this model, file data transfers across the network
betweenaclientand a servertake place in units ofbytes. This model provides maximum
flexibility because it allows storage and retrieval ofan arbitrary sequential subrange ofa
file, specified by an offset within a file, and a length. The main drawback ofthis model
is the difficulty in cache management due to the variable-length data for different access
requests. The CambridgeFile Server[Dion 1980,Mitchell and Dion 1982, Needham and
Herbert 1982] uses this model.
4. Record-level transfer model.The three file data transfer models described above
are commonly used with unstructured file models. The record-level transfer model is
suitablefor use with those file models in which file contents are structured in the form of
records. In this model, file data transfersacross the network betweenaclientand aserver
take place in units ofrecords. The Research Storage System (RSS) [Gray 1978, Gray et
al. 1981],which supportscomplexaccess methodstostructuredandindexedfiles, usesthe
record-level transfer model.
9.5 FILE-SHRRING SEMANTICS
A shared file may be simultaneously accessed by multiple users. In such a situation, an
important design issue for any file system is to clearly define when modifications offile
data made by a user are observable by other users. This is defined by the type of file
sharing semantics adopted by a file system. Levy and Silberschatz [1990] defined the
following types of file-sharing semantics:
1. UNIX semantics. This semantics enforces an absolute time ordering on all
operations and ensures that every read operation on a file sees the effects ofall previous
write operations performed on that file [Fig. 9.1(a)]. In particular, writes to an open file
by a user immediately become visible to other users who have this file open at the same
time.
The UNIX semantics is commonly implemented in file systems for single
processor systems because it is the most desirable semantics and also because it is
easy to serialize all read/write requests. However, implementing UNIX semantics in a
distributed file system is not an easy task. One may think that this semantics can be
achieved in a distributed system by disallowing files to be cached at client nodes and
allowing a shared file to be managed by only one file server that processes all read
and write requests for the file strictly in the order in which it receives them. However,
even with this approach, there is a possibility that, due to network delays, client

Sec. 9.5 • File-Sharing Semantics 431
Originalfile Retrievedfile Retrievedfile
contents contents contents
[U]] 'a IbIe' lalblcldlel
i t t
Append(c) Read Append(d) Append(e) Read
I I I I I I
t 1 t t 1 1
1 2 3 4 5 6
~ ! !
1a Ib Ic I Ia IbIc IdI lalblcldlel
New file New file New file
contents contents contents
(a)
Node boundary Node boundary
I I
I I
I I
I
I
J
I
1 3:sendsread request 1 1:originalfilecontentsis: t 2:sendsappend(c)request
I
[ill] :
I
(4: read requestofclient :
node 1reachesand :
[illJisreturned :
toclientnode 1 :
15:appendrequestof :
client node 2reaches :
and thefilecontentsis:
updatedtoIa Ib Ic I:
(b)
Fig. 9.1 (u) Exampleof UNIX file-sharing semantics;(b) anexampleexplaining why
itisdifficult toachieve UNIX semanticsinadistributed file systemeven
when the shared file is handled byasingle server.
requests from different nodes may arrive and get processed at the server node in an
order different from the actual order in which the requests were made [Fig. 9.1(b)].
Furthermore, having all file access requests processed by a single server and
disallowing caching on client nodes is not desirable in practice due to poor
performance, poor scalability, and poor reliability of the distributed file system.
Therefore, distributed file systems normally implement a more relaxed semantics of
file sharing. Applications that need to guarantee UNIX semantics for correct function
ing should use special means (e.g., locks) for this purpose and should not rely on the
underlying semantics of sharing provided by the file system.

432 Chap. 9 • Distributed File Systems
2. Session semantics. For this semantics, the foJIowing file access pattern is
assumed: Aclient opens a file,performs a series ofread/write operations on the file, and
finally closes the file when he or she is done with the file. A session is a series of file
accesses made between the open and close operations. In session semantics, all changes
made to a file during a session are initially made visible only to the client process (or
possibly to all processes on the client node) that opened the session and are invisible to
other remote processes who have the same file open simultaneously. Once the session is
closed, the changes made to the file are made visible to remote processes only in later
starting sessions. Already open instances of the file do not reflect these changes.
Notice that with session semantics multiple clients areallowed toperform bothread
andwriteaccessesconcurrentlyonthesamefile.Inthiscase,eachclientmaintainsitsown
image of the file. When a client closes its session, all other remote clients who continue
to use the file are actually using a stale copy of the file. Furthermore, using session
semantics raises the question of what should bethe final file image when multiple file
sessions, each one having adifferent file image, are closed one after another.Notice that
when a session is closed its file image is sent back to the server, so the final file image
depends on whocloseslast.However,thereisapossibility thatduetonetwork delays file
images from different nodes may arrive and get processed at the server node in an order
different from the actual order in which the sessions wereclosed. Therefore, in practice,
it is easier to implement the alternative that says that the final file image is the image
maintained by one of the active sessions; the actual one is left unspecified. That is, the
final file image is nondeterministic.
Observe that session semantics should be used only with those file systems that use
the file..leveltransfer model.This isbecausecoupling the session semantics withcaching
partsoffilesmaycomplicate matters, sinceasession issupposedtoread theimageofthe
entire file that corresponds to the time it was opened.
3. Immutable shared-files semantics. This semantics is based on the use of the
immutable file model. Recall thatan immutable filecannot bemodified once ithas been
created. According to this semantics, once the creator of a file declares it to be sharable,
thefileistreatedasimmutable,sothatitcannot bemodifiedanymore.Changes tothefile
arehandledbycreatinganewupdatedversionofthefile.Eachversionofthefileistreated
asanentirely newfile.Therefore, thesemantics allows filestobesharedonlyintheread
only mode. With this approach, since shared files cannot be changed at all, the problem
of when to make the changes made to a file by a user visible to other users simply
disappears.
4. Transaction..like semantics. This semantics is based on the transaction mecha..
nism, which is a high-level mechanism for controlling concurrent access to shared,
mutable data. A transaction is a set of operations enclosed in..between a pair of begin_
transaction- and end_transaction..like operations. The transaction mechanism ensures
that the partial modifications made to the shared data by a transaction will not be
visible to other concurrently executing transactions until the transaction ends (its end_
transaction is executed). Therefore, in multiple concurrent transactions operating on a
file, the final file content will be the same as if all the transactions were run in some
sequential order. In the Cambridge File Server [Needham and Herbert 1982], the

Sec.9.6 • File-Caching Schemes 433
beginning and end of a transaction are implicit in the open and close file operations,
and transactions can involve only one file. Thus, a file session in that system is
actually a transaction.
9.6 FILE-CACHING SCHEMES
File caching has been implemented in several file systems for centralized time-sharing
systems to improve file I/O performance (e.g., UNIX [McKusick et a1. 1985]). The idea
infilecaching inthese systems isto retain recently accessedfile data inmain memory, so
that repeated accesses to the same information can be handled without additional disk
transfers. Because of locality in file access patterns, file caching reduces disk transfers
substantially, resulting in better overall performance of the file system. The property of
localityinfile access patternscan as well beexploitedindistributedsystemsbydesigning
a suitable file-caching scheme. In addition to betterperformance, a file-caching scheme
for a distributed file system may also contribute to its scalability and reliability because
it ispossibletocache remotely locateddata on aclient node. Therefore,every distributed
file systeminserious use today usessome form offilecaching. EvenAT&T's RemoteFile
System (RFS) [Rifkin et a1. ]986], which initially avoided caching to emulate UNIX
semantics, now uses it.
In implementinga file-caching schemefor acentralizedfile system, one has to make
several key decisions, such as the granularity ofcached data (large versus small), cache
size (large versus small, fixed versus dynamically changing), and the replacementpolicy.
Agood summary ofthese design issues is presented in [Smith 1982]. In additionto these
issues, a file-caching scheme for a distributed file system should also address the
following key decisions:
I. Cache location
2. Modification propagation
3. Cache validation
These three design issues are described below.
9.6.1 Cachelocation
Cache location refers to the place where the cached data is stored. Assuming that the
original locationofafile ison its server'sdisk, there are three possiblecachelocations in
a distributed file system (Fig. 9.2).
1. Server's main memory. When no caching scheme is used, before a remote client
can access a file, the file must first be transferred from the server's disk to the server's
main memory and then across the network from the server's main memory to the client's
main memory. Therefore, the total cost involved is one disk access and one network

434 Chap.9 • Distributed FileSystems
Nodeboundary
Client's Server's ®
mainmemory mainmemory
® Client's Server's
disk disk
Notavailablein Original
disklessworkstations filelocation
CD
Nocaching
®
Cachelocatedinserver'smainmemory
®
Cachelocatedinclient'sdisk
o Fig.9.2 Possiblecache locationsina
file-cachingscheme foradistributed
Cachelocatedinclient'smainmemory
filesystem.
access. Acache located in theserver's main memory eliminates thedisk access cost on a
cache hit, resulting in a considerable performance gain as compared to no caching.
The decision tolocate thecache intheserver'smain memory may betaken due
toone or more of thefollowing reasons. Itiseasy toimplement and istotally transparent
to the clients. It iseasy to always keep the original file and cached data consistent since
bothreside onthesamenode.Furthermore, sinceasingleserver manages boththecached
data and the file, multiple accesses from different clients can be easily synchronized to
support UNIX-like file-sharing semantics.
However, having the cache in the server's main memory involves a network access
for each file access operation by a remote client and processing of the access request by
theserver.Therefore, itdoesnoteliminate thenetworkaccesscostanddoes notcontribute
to the scalability and reliability of the distributed file system.
2. Client'sdisk. The second option is to have the cache in a client's disk. A cache
located in a client's disk eliminates network access cost but requires disk access cost on
acache hit.Acacheonadiskhasseveraladvantages.Thefirstisreliability.Modifications
tocached data are lostinacrash ifthecache iskept in volatile memory.Moreover, ifthe
cached data is kept on theclient'sdisk, the data isstill there during recovery and there is
no need to fetch it again from the server's node. The second advantage is large storage
capacity.Ascompared toamain-memory cache, adiskcache hasplenty ofstorage space.
Therefore, more data can be cached, resulting in a higher hit ratio. Furthermore, several

Sec.9.6 • File-Caching Schemes 435
distributed file systems use the file-level data transfer model in which a file is always
cached initsentirety. Inthese systems, ifafile istoo large tofitinamain-memorycache,
theadvantagesoffilecachingcannotbeavailed for it.Therefore,disk cache isparticularly
useful forthose systemsthat usethe file-level transfermodel becauseitallows thecaching
of most large files unless the file tobe cached islarger than the availabledisk space. The
third advantage is disconnected operation. A system mayuse a client's disk caching and
the file-level transfer model to support disconnectedoperation. Aclient's disk cache also
contributes to scalability and reliability because on a cache hit the access request can be
serviced locally without the need to contact the server.
The main drawback ofhaving cached d.ataon a client's disk is that this policy does
not work ifthe system isto support diskless workstations. Furthermore, with thiscaching
policy, a disk access is required for each access request even when there is cache hit.
Therefore, the access time isstill considerably large. Notice that a server's main-memory
cache eliminatesdiskaccess butrequires networkaccess onacache hit.Ontheother hand,
a client's disk cache eliminates network access but requires disk access on a cache hit.
Therefore, when a decision has to be made whether to do caching in the server's main
memory or theclient'sdisk, the former issomewhatfaster, and itisalways much simpler
[Tanenbaum 1995].
3. Client's main memory. The third alternative isto have thecache inaclient's main
memory. A cache located in aclient's main memory eliminates both network access cost
and disk access cost. Therefore, itprovides maximumperformance gain on acache hit. It
also permits workstations to be diskless. Like a client's disk cache, a client's main
memory cache also contributes to scalability and reliability because on a cache hit the
access request can be serviced locally without the need to contact the server. However, a
client'smain-memorycache isnotpreferabletoaclient'sdiskcache whenlargecache size
and increased reliability of cached data are desired.
The relative advantagesofthethree cache locationpoliciesaresummarizedinFigure
9.3. In conclusion, a main-memory cache and a disk cache emphasize different
functionality. While a main-memory cache emphasizes reduced access time, adisk cache
emphasizes increased reliability and autonomy of client machines. Furthermore, when
faced with achoice betweenhaving acache on the servernode versus theclient node, the
latter is always preferable because it also contributes to scalability and reliability.
9.6.2 Modification PropagQtion
In file systems in which the cache is located on clients' nodes, a file's data may
simultaneously be cached on multiple nodes. In such a situation, when the caches of all
these nodes contain exactly the same copies of the file data, we say that the caches are
consistent. It is possible for the caches to become inconsistent when the file data is
changed by one of the clients and the corresponding data cached at other nodes are not
changed or discarded.
Keeping file data cached at multiple client nodes consistent is an important design
issue in those distributed file systems that use client caching. A variety of approaches to

436 Chap. 9 • Distributed File Systems
Costofremoteaccessincaseofnocaching
=onediskaccess+onenetworkaccess
Cachelocation Accesscoston Advantages
cachehit
Server'smain Onenetwork 1.Easytoimplement
memory access
2.Totallytransparentto
theclients
3.Easytokeeptheoriginal
fileandcacheddata
consistent
4.EasytosupportUNIX-
likefile-sharingsemantics
Clientsdisk Onedisk 1.Reliabilityagainstcrashes
access
2.largestoragecapacity
3.Suitableforsupporting
disconnectedoperation
4.Contributestoscalability
andreliability
--
Clientsmain 1.Maximumperformancegain
memory
2.Permitsworkstationstobe
diskless
3.Contributestoscalability
andreliability
Fig.9.3 Summary of therelative advantages of thethreecache location policies.
handle this issue have been proposed and implemented. These approaches depend on the
schemes used for the following cache design issues for distributed file systems:
1. When topropagate modifications madetoacached datatothecorresponding file
server
2. How to verifythe validity of cached data
The modification propagation schemes arepresented belowandthecache validation
schemes are presented in the next section.
A distributed file system may use one of the modification propagation schemes
described below. The file-sharing semantics supported by the distributed file system
depends greatly on the modification propagation scheme used. Furthermore, the
modification propagation scheme used has a critical effect on the system's performance
and reliability.

Sec. 9.6 • File-Caching Schemes 437
Write-through Scheme
In this scheme, when a cache entry is modified, the new value is immediately sent to
the server for updating the master copy of the file. This scheme has two main
advantages-high degree of reliability and suitability for UNIX-like semantics. Since
every modification is immediately propagated to the server having the master copy of
the file, the risk of updated data getting lost (when a client crashes) is very low. A
major drawback of this scheme is its poor write performance. This is because each write
access has to wait until the information is written to the master copy of.the server.
Notice that with the write-through scheme the advantages of data caching are only for
read accesses because the remote service method is basically used for all write accesses.
Therefore, this scheme is suitable for use only in those cases in which the ratio of read
to-write accesses is fairly large.
Delayed-Write Scheme
Although the write-through scheme helps on reads, it does not help in reducing the
network traffic for writes. Therefore, to reduce network traffic for writes as well, some
systems usethedelayed-writescheme. Inthisscheme, when acacheentry ismodified, the
new value iswritten only tothecache and theclientjustmakes anote thatthecache entry
has been updated. Some time later, all updated cache entries corresponding to a file are
gathered together and sent to the server at a time.
Depending on when the modifications are sent to the file server, delayed-write
policies are of different types. Three commonly used approaches are as follows:
1. Writeonejectionfrom cache. Inthismethod, modified datainacacheentryissent
to the server when the cache replacement policy has decided to eject it from the client's
cache. This method can result in good performance, but some data can reside in the
client's cache for a long time before they are sent to the server [Ousterhout et al. 1985].
Such data are subject to reliability problem.
2. Periodic write. In this method, the cache is scanned periodically, at regular
intervals, and any cached data that have been modified since the last scan are sent to
the server. Sprite [Nelson et al. 1988] uses this method with an interval of 30
seconds.
3. Writeonclose. Inthismethod, themodificationsmade toacached databyaclient
are sent to the server when the corresponding file is closed by the client. Notice that the
write-on-close policy is a perfect match for the session semantics. However, it does not
help much in reducing network traffic for those files that are open for very short periods
or are rarely modified. Furthermore, the close operation takes a long time because all
modified data mustbe written totheserver before theoperationcompletes. Therefore, this
policy should be used only in cases in which files are open for long periods and are
frequently modified. The ITCFileSystem [Satyanarayananetal, 1985]usesthe write-on
close policy.

438 Chap.9 • Distributed FileSystems
The delayed-write policy helps in performance improvement for write accesses due
to the following reasons:
1. Writeaccessescomplete morequickly becausethenewvalueiswrittenonlyinthe
cache of the client performing the write.
2. Modified data may be deleted before it is time to send them to the server. For
example, many programs create temporary files, use them, and then delete them
soon after they are created. In such cases, modifications need not be propagated
at all to the server, resulting in a major performance gain.
3. Gathering of all file updates and sending them together to the server is more
efficient than sending each update separateIy.
Delayed-write schemes, however, suffer from reliability problems, since modifica
tions not yet sent to the server from a client's cache will be lost if the client crashes.
Anotherdrawback ofthisapproach isthatdelaying thepropagationofmodificationstothe
server results in fuzzier file-sharing semantics, because when another process reads the
file, what it gets depends on the timing.
9.6.3 (ache ValidationSchemes
A file data may simultaneously reside in the cache of multiple nodes. The modification
propagation policy only specifies when the master copy of a file at the server node is
updated upon modification of a cache entry. Itdoes not tell anything about when the file
data residing in the cache of other nodes is updated. Obviously, a client's cache entry
becomes stale as soon as some other client modifies the data corresponding to the cache
entry in the master copy of the file. Therefore, it becomes necessary to verify if the data
cached at aclient node isconsistent with the master copy.If not,thecached data must be
invalidated and the updated version of the data must be fetched again from the server.
There are basically two approaches to verify the validity of cached data-the client
initiated approach and the server-initiated approach [Levy and Silberschatz 1990]. These
are described below.
Client-Initiated Approach
Inthisapproach, aclient contacts theserver andchecks whether itslocally cached data is
consistent with the master copy.The file-sharing semantics depends on the frequency of
the validity check. One of the following approaches may be used:
1. Checkingbeforeeveryaccess.This approach defeats the mainpurpose ofcaching
because the server has to be contacted on every access. But it is suitable for supporting
UNIX-like semantics.
2. Periodicchecking.Inthismethod, acheckisinitiated everyfixedinterval oftime.
Themainproblem ofthismethodisthatitresults infuzzierfile-sharing semantics because
the data on which an access operation is performed is timing dependent.

Sec.9.6 • File-Caching Schemes 439
3. Checkonfileopen.Inthismethod,aclient'scacheentryisvalidatedonly when the
clientopensthe correspondingfile for use. This methodis suitablefor supportingsession
semantics.Observethatonemethodforimplementingsessionsemanticsinadistributedfile
systemisto use the file-level transfermodel coupledwith the write-on-closemodification
propagationpolicyand thecheck-on-file-opencachevalidationpolicy.
The validity check is performed by comparing the time of last modification ofthe
cachedversionofthe data with the server'smastercopy version. Ifthe two are the same,
thecacheddataisuptodate. Otherwise,itisstale and hencethecurrentversionofthedata
isfetchedfrom the server. Insteadofusingtimestamps,versionnumbersorchecksumscan
be used.
Server-Initiated Approach
Ifthe frequencyofthe validitycheckishigh, theclient-initiatedcachevalidationapproach
generates a large amount of network traffic and consumes precious server CPU time.
Owing to this reason, the AFS that initially used the client-initiated approach (in AFS-I)
switched to the server-initiated approach (in AFS-2 and AFS-3) [Satyanarayanan 1990b,
1992].
In this method, a client informs the file server when opening a file, indicating
whether the file is being opened for reading, writing, or both. The file server keeps a
record ofwhich client has which file open and in what mode. Inthis manner, the server
keeps monitoringthe file usage modes being used bydifferentclientsand reacts whenever
it detects apotential for inconsistency. A potential for inconsistency occurs when two or
more clients try to open a file in conflicting modes. For example, if a file is open for
reading, other clients may be allowed to open it for reading without any problem, but
opening it for writingcannotbe allowed. Similarly, a new clientshouldnot be allowed to
open a file in any mode if the file is already open for writing. Whenaclientcloses a file,
it sends an intimation to the server along with any modifications made to the file. On
receiving such an intimation, the server updates its record of which client has which file
open in what mode.
When anew clientmakes arequestto openanalreadyopen fileand iftheserverfinds
that the new open modeconflicts with the already open mode, the servercan bedesigned
to deny the request or queue the request or disable caching and switch to the remote
service mode of operation for that particular file by asking all the clients having the file
open to remove that file from their caches. The method of disabling caching is used in
Sprite [Nelson et a1. 1988].
Although the server-initiated approach described above is quite effective, it has the
following problems:
1. It violates the traditional client-server model in which servers simply respond to
service request activities initiated byclients. This makes the code for client and
server programs irregular and complex.
2. Itrequiresthat file serversbe stateful.Asexplainedlater, statefulfile servershave
a distinct disadvantage over stateless file servers in the event of a failure.

440 Chap. 9 • Distributed File Systems
3. A check-on-open, client-initiated cache validation approach must still be used
along with the server-initiated approach. For example, a client may open a file,
cache it, and then close it after use. Upon opening it again for use, the cache
content must be validated because there is a possibility that some other client
might have subsequently opened, modified, and closed the file.
In another server-initiated approach, known as the callback policy in AFS
[Satyanarayanan 1990b],acacheentryisassumedtobevalidunlessotherwise notifiedby
the server.Inthismethod,insteadofrecording theaccessmodesofallclients foranopen
file, theserver only keeps a record of the clients who havecached afile (or a part of it).
Thisrecordismaintainedonaper-filebasis,andcorresponding toeachfilemaintainedby
theserver,theserver maintainsalistoftheclients whohavethefile's datacached intheir
cache irrespective of whether the client currently has the file open for use or not. The
server promises to notify all clients of acached file before allowing any modification to
the file by any other client.
In AFS, which implements session semantics, whenever a server receives a
request to close a file that has been modified, it notifies all the clients having that file
data in their caches to discard their cached data and consider it invalid. Clients having
this file open at that time discard their copy when the current session is over. Other
clients discard their copies at once. Notice that in this method the server need not be
informed about opens of already cached files. It need only be informed about the
close of a writing session. As a result, the server need not receive open validation
requests oflocally cached files. If the client machine crashes, it is assumed that all its
local files may be inconsistent. Therefore, upon recovery from a cache, it generates a
cache validation request for each file that is cached on its local disk.
9.7 FilE REPUCATION
High availability is a desirable feature of a good distributed file system and file
replication is the primary mechanism for improving file availability. A replicated file
is a file that has multiple copies, with each copy located on a separate file server. Each
copy of the set of copies that comprises a replicated file is referred to as a replica of
the replicated file.
9.7.1 Dlrr.,..-c. ktw.8n Replication and Caching
Replication isoftenconfusedwithcaching, probably becausetheybothdeal withmultiple
copies ofa data. However, the two concepts have the following basic differences:
1. A replica is associated with a server, whereas a cached copy is normally
associated with a client.
2. The existence of a cached copy is primarily dependent on the locality in file
access patterns, whereas the existence of a replica normally depends on
availability and performance requirements.

Sec.9.7 • FileReplication 441
3. Ascomparedtoacached copy,areplica ismorepersistent,widely known, secure,
available, complete, and accurate.
4. A cached copy is contingent upon a replica. Only by periodic revalidation with
respect to a replica can a cached copy be useful.
Satyanarayanan[1992] distinguishesareplicated copy from acachedcopy bycalling
themfirst-class replicas and second-class replicas, respectively.
9.7.2 Advantages ofR.plicatlon
The replication of data in a distributed system offers the following potential benefits:
1. Increasedavailability. One of themost importantadvantages ofreplication isthat
itmasks and tolerates failures inthe network gracefully. In particular, the system remains
operational and available to the users despite failures. By replicating critical data on
servers with independent failure modes, the probability that one copy of the data will be
accessible increases. Therefore, alternate copies ofareplicated data can beused when the
primary copy is unavailable.
2. Increased reliability. Many applications require extremely high reliability of their
data stored in files. Replication is very advantageous for such applications because it
allows the existence of multiple copies of their files. Due to the presence of redundant
information inthe system, recovery from catastrophic failures (such as permanentloss of
data of a storage device) becomes possible.
3. Improved response time. Replication also helps in improving response time
because itenables data to be accessed either locally or from a node to which access time
is lower than the primary copy access time. The access time differential may arise either
because of network topology or because of uneven loading of nodes.
4. Reduced network traffic. If a file's replica is available with a file server that
resides on aclient's node, theclient's access requests can beserviced locally,resulting in
reduced network traffic.
5. Improvedsystemthroughput. Replicationalsoenables severalclients' requests for
access tothesamefile tobeserviced inparallel bydifferent servers, resulting inimproved
system throughput.
6. Betterscalability.Asthe number of usersofashared filegrows, having allaccess
requests for the file serviced by a single file server can result inpoor performancedue to
overloadingofthefileserver.Byreplicatingthefileonmultiple servers, thesamerequests
can now be serviced more efficiently by multiple servers due to workload distribution.
This results in better scalability.
7. Autonomous operation. In a distributed system that provides file replication as a
service to their clients, all files required by a client for operation during a limited time
period may bereplicatedonthefile server residing attheclient'snode.This willfacilitate

442 Chap. 9 • Distributed File Systems
temporary autonomous operation of client machines. A distributed system having this
feature can support detachable, portable machines,
9.7.3 RapllcQtlon Transparency
Transparency is an important issue in file replication. A replicated file service must
function exactly like a nonreplicated file service but exhibit improved performance and
reliability. That is, replication offiles should be designed to be transparent to the users
so that multiple copies of a replicated file appear as a single logical file to its users.
For this, the read, write, and other file operations should have the same client interface
whether they apply to a nonreplicated file or to a replicated file. Two important issues
related to replication transparency are naming of replicas and replication control.
Naming of Replicas
In systems that support object replication. a basic question is whether different replicas
of an object should be assigned the same identifier or different identifiers. Obviously,
the replication transparency requirement calls for the assignment of a single identifier
to all replicas ofan object. Assignment of a single identifier to all replicas ofan object
seems reasonable for immutable objects because a kernel can easily support this type
ofobject. Any copy found by the kernel can be used, because all copies are immutable
and identical; there is only one logical object with a given identifier. However, in
mutable objects, different copies ofa replicated object may not be the same (consistent)
at a particular instance oftime. In this case, ifthe same identifier is used for all replicas
of the object, the kernel cannot decide which replica is the most up-to-date one.
Therefore, the consistency control and management of the various replicas ofa mutable
object should be performed outside the kernel. Hence it is the responsibility of the
naming system to map a user-supplied identifier into the appropriate replica of a
mutable object. Furthermore, ifall replicas are consistent, the mapping must provide the
locations of all replicas and a mechanism to identify the relative distances of the
replicas from the user's node.
Replication Control
Another transparency issue is providing replication control. Replication control includes
determining the number and locations of replicas of a replicated file. That is, do the
users play any role in determining how many copies of a replicated file should be
created and on which server should each copy be placed? In a replication transparent
system, the replication control is handled entirely automatically, in a user-transparent
manner. However, under certain circumstances, it is desirable to expose these details to
users and to provide them with the flexibility to control the replication process. For
instance, if replication facility is provided to support autonomous operation of
workstations, users should be provided with the flexibility to create a replica of the
desired files on their local nodes. Similarly, because of the nature of job, if a user

Sec.9.7 • FileReplication 443
normally works on two or three different nodes at different times, it may be desirable
to replicate some of the frequently used files on servers located on all his or her
workstations.
Depending on whether replication control is user transparent or not, the replication
process is oftwo types:
1. Explicitreplication. Inthistype, users aregiven theflexibilitytocontrol theentire
replicationprocess. Thatis, when aprocess creates afile, it specifies the server on which
the file should be placed. Then, ifdesired, additional copies of the file can be created on
other servers on explicit request by the users. Users also have the flexibility to delete one
or more replicas ofa replicated file.
2. Implicit/lazy replication. In this type, the entire replication process is automati
cally controlled by the system without users' knowledge. That is, when a process creates
a file, it does not provide any information about its location. The system automatically
selects one server for the placement of the file. Later, the system automatically creates
replicas of the file on otherservers, based on some replication policy used by the system.
The system must be intelligentenough to create and allow the existenceofonly as many
replicas as are necessary and should automatically delete any extracopies when they are
no longer needed. Lazy replication is normally performed in the background when the
server has some free time.
9.7.4 MulticopyUpdate Problem
As soon as a file system allows multiple copies of the same (logical) file to exist on
different servers, it is faced with the problem of keeping them mutually consistent. That
is, a situation must not beallowed toarise whereby independentconflicting updates have
been made to different copies of the same file. In fact, maintaining consistency among
copies when a replicated file is updated is the major design issue of a file system that
supports replication of files. Some of the commonly used approaches to handle this issue
are described below.
Read-Only Replication
This approach allows the replication of only immutable files. Since immutable files are
used only in the read-only mode and because mutable files cannot be replicated, the
multicopy update problemdoes not arise. Files known tobefrequently read and modified
only once in a while (once in several months), such as files containingthe object code of
system programs, can be treated as immutable tiles for replication using this approach.
Read-Any-Write-All Protocol
The read-only replication approach is too restrictive in the sense that it allows the
replication of only immutable files. Obviously, a replication scheme that can support the
replication of mutable files as well is desirable. A simple multicopy update protocol that

444 Chap. 9 • Distributed File Systems
allows the replication of mutable files is the read-any-write-all protocol. In this method,
a read operation on a replicated file is performed by reading any copy of the file and a
write operation by writing to all copies of the file. Some form of locking has to be used
tocarry outawriteoperation. That is,before updating anycopy,allcopies arelocked, then
they are updated, and finally the locks are released tocomplete the write. The protocol is
suitable for implementing UNIX-like semantics. Notice that, in this protocol, the
availabilityofawriteoperation isseverely restricted sinceallcopies mustbeavailable for
the operation tocomplete, but the read operation isresponsive since the nearest copy can
be used.
Available-Copies Protocol
The main problem withtheread-any-write-allprotocol isthata writeoperationcannotbe
performed ifany ofthe servers having acopy of the replicated file isdown at the time of
the writeoperation. The available-copiesprotocol relaxes thisrestrictionand allows write
operationstobecarried outeven when someoftheservers having acopy ofthereplicated
filearedown. Inthismethod, areadoperation isperformedbyreading any availablecopy,
butawrite operation isperformedbywriting toallavailable copies. The basic idea behind
the correct functioning of this protocol is that when a server recovers after a failure, it
brings itselfup to date by copying from other servers before accepting any user request.
Failed servers (sites) are dynamically detected by high-priority status management
routines andconfiguredoutofthesystem while newlyrecovered sitesareconfiguredback
in.This protocol providesbetteravailability than theread-any-write-allprotocol butdoes
not prevent inconsistencies in the presence of communication failures such as network
partition. Only clean, detectable site crashes are handled correctly by this method.
Primary-Copy Protocol
Another simple method to solve the multicopy update problem is the primary-copy
protocol. In this protocol, for each replicated file, one copy is designated as the primary
copy and all theothers aresecondary copies. Read operationscanbeperformedusing any
copy, primary or secondary. But, all write operations are directly performed only on the
primary copy. Each server having a secondary copy updates its copy either by receiving
notification of changes from the server having the primary copy or by requesting the
updated copy from it.
The consistency semantics implementeddepends on when the secondary copies are
updated. Forinstance. forUNIX-likesemantics. whentheprimary-copyserverreceives an
updaterequest, itimmediatelyorders allthesecondary-copyservers toupdate theircopies.
Some form oflocking is used and the write operationcompletesonly when all the copies
have been updated. Therefore, in this case, the primary-copy update protocol is simply
another method ofimplementing the read-any-write-alJ protocol.
A fuzzierconsistency semantics results ifa writeoperationcompletesas soonas the
primary copy hasbeen updated. The secondarycopies arethenlazily updated eitherinthe
backgroundor whenrequestedforan updated version bytheir servers. Inthis way,allthe
secondary copies will ultimately get updated and reach a consistent state.

Sec. 9.7 • File Replication 445
Quorum-Based Protocols
Theread-any-write-allandavailable-copiesprotocolscarinothandlethenetworkpartition
problem in which the copies of a replicated file are partitioned into two more active
groups. Moreover, the primary-copy protocol is too restrictive in the sense that a write
operation cannot be performed if the server having the primary copy is down. Gifford
[1979a] presented a simple quorum protocol that is capable of handling the network
partition problem and can increase the availability of write operations at the expense of
read operations.
Aquorum-basedprotocol works asfollows. Suppose thatthere areatotal ofncopies
ofareplicated file F. Toread thefile, aminimum r copies of Fhave tobeconsulted. This
setof rcopies iscalled aread quorum. Similarly, toperform awrite operationon the file,
aminimum wcopies ofFhavetobewritten.Thissetofwcopies iscalled awritequorum.
The restriction onthechoice ofthe values of rand wisthat the sum of theread and write
quorums must be greater than the total number of copies n (r +w>n). This restriction
guaranteesthatthere isanonnull intersectionbetween every readquorum andevery write
quorum. That is, there isat least one commoncopy of the file between every pairof read
and write operations resulting in at least one up-to-date copy in any read/write quorum.
Since the quorum protocol does not require that write operations be executed on all
copies of a replicated file, some copies will be obsolete, and therefore, it becomes
necessary to beable to identify acurrent (up-to-date) copy inaquorum. This is normally
achieved by associating a version number attribute with each copy. The version number
of a copy is updated every time the copy is modified. A copy with the largest version
number in a quorum is current. The new version number assigned to each copy is one
more than the version number associated with the current copy.
A read is executed as follows:
1. Retrieve a read quorum (any rcopies) of F.
2. Of the rcopies retrieved, select the copy with the largest version number.
3. Perform the read operation on the selected copy.
A write is executed as follows:
1. Retrieve a write quorum (any w copies) of F.
2. Of the w copies retrieved, get the version number of the copy with the largest
version number.
3. Increment the version number.
4. Write the new value and the new version number to all the wcopies of the write
quorum.
To further clarify how this protocol works, let us consider the example of Figure
9.4(a). There are total of eight copies of the replicated file (n=8) and the values of read
and write quorums are 4 and 5, respectively (r=4, w=5). Therefore, the condition
r+w>n is satisfied. Now suppose a write operation is performed on the write quorum

446 Chap. 9 • Distributed File Systems
Readquorums
,,_---h~ ~_"
""1 2 -, : 1 •
," "\ 8 :I : 2
I , ,I • I •
\
\, 7 _----- 3 ,/ 3 4
........... ......'"
5 6
Writequorums
(a) (b)
Fig.9.4 Examples ofquorum consensus algorithm: (a) n =8 r =4 w=5~ (b) n=8
= t t t
r 2 w=7.
t
comprised of copies 3, 4, 5, 6, and 8.All these copies get the new version and the new
version number. Now any subsequent read operation will require a read quorum of four
copies because r=4. Obviously anyreadquorum willhavetocontain atleast onecopy of
theprevious writequorum. Forthereadquorum shown inthefigure,copy number 3isthe
common copy.When the version numbers of thecopies belonging totheread quorum are
seen, the version number ofcopy number 3is found tobelarger than the other copies of
the read quorum. Therefore, the read operation is performed using copy number 3.
Another example in which r=2 and w=7 is shown in Figure 9.4(b).
The quorum protocol described above is a very general one, and several special
algorithms can be derived from it. A few are described below.
1. Read-any-write-allprotocol.Theread-any-write-allprotocol isactually aspecial
=
case ofthegeneralizedquorum protocol with r 1and w=n.This protocol issuitable for
use when the ratio of read to write operations is large.
2. Read-all-write-anyprotocol. Forthis protocol r=nand w=1.This protocol may
be used in those cases where the ratio of write to read operations is large.
3. Majority-consensus protocol. In this protocol, the sizes of both the read quorum
=
and the write quorum are made either equal or nearly equal. For example, if n 11,a
possible quorum assignment for this protocol will be r=6 and w=6. Similarly, when
n= 12,a possible quorum assignment will be r=6 and w=7. This protocol iscommonly
used in those cases for which the ratio of read to write operations is nearly 1.
4. Consensus with weighted voting. In all the quorum-based protocols described
above, allcopies ofareplicated filearegiven equal importance. Inother words,allcopies
areassigned asingle vote.Thegeneralized quorum protocolcanalsobeusedtomodelthe
varying "importance"ofdifferent copies of areplicated file by assigning each copy some
number of votes. The votes per copy can be adjusted for performance or reliability

Sec.9.8 • FaultTolerance 447
reasons. For example, supposethat ofthe nreplicasof a replicatedfile, which are located
on different nodes, the replica at node A is accessed more frequently than otherreplicas.
This fact can be modeled by assigning more votes to the copy at node A than other
replicas.
In this approach, a read quorum of r votes is collected to read a file and a write
quorumofwvotes to write a file. Since the votes assignedto eachcopyare not the same,
the size of a read/write quorum depends on the copies selected for the quorum. The
numberofcopiesinthequorumwill be less ifthe numberof votes assignedtothe selected
copies is relatively more. On the otherhand, the numberof copies in the quorum will be
more if the number of votes assigned to the selected copies is relatively less. Therefore,
to guarantee that there is a nonnull intersection between every read quorum and every
write quorum, the values of rand ware chosen such that r+w is greater than the total
number of votes (v) assigned to the file (r+w>v). Here, v is the sum of the votes ofall
the copies of the file.
9.8 FAUlT TOlERANCE
Fault tolerance is an important issue in the design of a distributed file system. Various
types of faults could harm the integrity of the data stored by such a system. For instance,
aprocessor loses the contents of its main memory in the event of acrash. Such a failure
could result in logically complete but physically incomplete file operations, making the
data that are stored by the file syst.eminconsistent. Similarly,during arequestprocessing,
the serverorclientmachine may crash,resultinginthe loss ofstate informationofthe file
beingaccessed.This may have an uncertaineffecton the integrity offile data. Also, other
adverse environmental phenomena such as transient faults (caused by electromagnetic
fluctuations) or decay ofdisk storage devices may result in the loss or corruption ofdata
stored by a file system. A portion ofa disk storage device is said to be "decayed" if the
data on that portion of the device are irretrievable.
The primary file properties that directly influence the ability of a distributed file
system to tolerate faults are as follows [Levy and Silberschatz 1990]:
1. Availability. Availability of a file refers to the fraction of time for which the file
is available for use. Note that the availability property depends on the locationofthe file
and the locations of its clients (users). For example, if a network is partitioned due to a
communicationlink failure, afile may beavailabletothe clientsof some nodes, but atthe
same time, it may not be available to the clients ofothernodes. Replication is a primary
mechanism for improving the availability of a file.
2. Robustness. Robustness of a file refers to its power to survive crashes of the
storage device and decays of the storage medium on which it is stored. Storage devices
that are implementedby using redundancy techniques, such as astable storagedevice, are
often used to store robustfiles. Note that arobustfile may not beavailableuntil the faulty
component has been recovered. Furthermore, unlike availability, robustness is independ
ent of either the location ofthe file or the location ofits clients.

448 Chap. 9 • Distributed File Systems
3. Recoverability. Recoverability of a file refers toitsability toberolled back toan
earlier, consistent state when an operation on the file fails or is aborted by the client.
Notice that a robust file is not necessarily recoverable and vice versa. Atomic update
techniques such as a transaction mechanism are used to implement recoverable files.
The filereplication technique hasalready beendescribed intheprevious section.The
atomic transactions mechanism used for atomic update is described in the next section.
The stable-storagetechnique andtheeffect ofaservice paradigm onthefault tolerance of
distributed file systems are described below.
9.8.1 Stabl. Storag.
In context of crash resistance capability, storage may be broadly classified into three
types:
I. Volatilestorage,suchasRAM, whichcannot withstandpowerfailures ormachine
crashes. Thatis,thedata storedinavolatilestorage islostintheevent ofapower
failure or a machine crash.
2. Nonvolatile storage, such asadisk, whichcan withstand CPU failures butcannot
withstand transient 1/0 faults and decay of the storage media. Although fairly
reliable, nonvolatile storage media suchasadisk havecomplicatedfailure modes
and may prove to be insufficiently reliable for storing critical data.
3. Stable storage, which can even withstand transient 110faults and decay of the
storage media. It is a storage approach introduced by Lampson [1981].
The basic idea of stable storage is to use duplicate storage devices to implement a
stable device and to try to ensure that any period when only one of the two component
devices isoperational issignificantly lessthanthemean timebetween failures (MTBF) of
astabledevice.Therefore, adisk-based stable-storagesystemconsists ofapairofordinary
disks(saydisk 1anddisk2)thatareassumed tobedecayindependent. Eachblockondisk
2isanexact copy ofthecorrespondingblockondisk 1.Unexpected faultsthataffectdisk
storage may occur, but effective fault tolerance facilities are provided toensure that both
the disks are not damaged at the same time.This isachieved by imposing restrictions on
how the two disks are accessed.
As withconventional disks, thetwo basicoperations related toastable disk areread
and write.A read operation first attempts to read from disk 1.If itfails, the read is done
from disk 2.A write operation writes to both disks, but the write to disk 2 does not start
until that for disk 1has been successfully completed. This is to avoid the possibility of
both disks getting damaged at the same time by ahardware fault. Read and write actions
to each of the disks use retries of actions to tolerate the effects of transient hardware
faults.
In addition, there is a crash recovery action that restores the internal consistency of
data stored onthe twodisks afteracrash hasoccurred. This recovery action compares the
contents of the two disks block by block. Whenever two corresponding blocks differ,the

Sec.9.8 • FaultTolerance 449
blockhaving incorrectdata isregeneratedfrom thecorrespondingblock ontheotherdisk.
The correctness of a data block depends on the timing when the crash occurred. For
instance, ifthe system crashes afterdisk 1isupdated butbefore disk 2isupdated or while
disk 2 is being updated, the data block on disk 1is the correctone. On the other hand, if
the system crashes while disk 1is being updated, the data block on disk 2 is the correct
one.Ofcourse, inthe lattercase, theupdate operation must beperformedonce again from
the beginning.
Noticethatastable-storagesystemusesordinary fallibledisksandconverts them into
reliable virtual devices whose probability offailure isnegligible. Stable storage issuitable
for those applications that require a high degree of fault tolerance, such as atomic
transactions.
9.8.2 Effect of Service Paradigm onFault Tol.rance
A server may beimplementedbyusing anyoneofthefollowing two serviceparadigms
stateful or stateless. The two paradigms are distinguished by one aspect of the client
server relationship-whether or not the history of the serviced requests between a client
and a server affects the execution of the next service request. The stateful approach
depends onthehistory oftheserviced requests, butthestateless approach does notdepend
on it.
Stateful File Servers
A stateful file server maintains clients' state information from one access request to the
next. That is, for two subsequent requests made by a client to a stateful server for
accessing a file, some state information pertaining to the service performed for the
client as a result of the first request execution is stored by the server process. This state
information is subsequently used when executing the second request. To allow the file
server to decide how long to retain the state information of a client, all access requests
for a file by a client are performed within an open and a close operations called a
session. The server creates state information for a client when the client starts a new
session by performing an open operation, maintains the state information for the entire
duration of the session, and discards the state information when the client closes the
session by performing a close operation. To illustrate how a stateful file server works,
let us consider a file server for byte-stream files that allows the following operations
on files:
Open (filename, mode): This operation is used to open a file identified byfilename
inthe specifiedmode. When theserverexecutes thisoperation, itcreates anentry for
this file inafile-table that it uses for maintainingthe file state informationof all the
open files.The file stateinformation normally consistsoftheidentifierofthefile,the
open mode, andthecurrent position ofanonnegativeinteger pointer,called theread
write pointer. When a file is opened, its read-write pointer is set to zero and the
server returns to the client a file identifier (fid) that is used by the client for
subsequent accesses to that file.

450 Chap. 9 • Distributed FileSystems
Read tfid, n, buffer): This operation is used to get n bytes of data from the file
identified byftd into the specifiedbuffer.When the server executes this operation, it
returns to theclient n bytes offile data startingfrom the bytecurrentlyaddressedby
theread..write pointerand then increments the read-write pointer by n.
Write tfid, n,buffer): Onexecutionofthis operation,the server takes nbytes ofdata
from the specified buffer, writes it into the file identified byfid at the byte position
currently addressed by the read-write pointer, and then increments the read-write
pointerby n.
Seekifid, position):This operationcausestheservertochangethe value ofthe read
write pointerofthe file identified bylid to the new value specified as position.
Close(jid): This statementcausesthe servertodelete from usfile-table the file state
information ofthe file identified bylid.
The file server mentioned above is stateful because it maintains the current state
information for a file that has been opened for use by a client. Therefore, as shown in
Figure 9.5, after opening a file, if a client makes two subsequent Read tfid, 100, buj)
requests, for the first request the first 100 bytes (bytes 0 to 99) will be read and for the
second request the next 100 bytes (bytes 100 to 199) will be read.
Clientprocess Serverprocess
Open(filename,mode) ...
Filetable
RfW
Return(fid) fid ModeIpointer
,..,.
..
Read(fid,100,buf)
Return(bytes0to99)
Read(fid,100,buf)
Return
(bytes100to199)
Fig.9.5 Anexample ofastateful fileserver.
Stateless File Servers
A stateless file server does not maintain any client state information. Therefore every
request from a client must be accompanied with all the necessary parameters to
successfullycarryoutthedesiredoperation.That is,each request identifiesthefileand the
position in the file for the read/write access. For example, a server for byte-stream files
that allows the following operations on files is stateless:

Sec.9.8 • FaultTolerance 451
Read (filename, position, n, buffer): On execution of this operation, the server
returns to the client n bytes of data ofthe file identified byfilename. The returned
data isplacedinthe specifiedbuffer. Thevalueofthe actualnumberofbytes written
isalsoreturnedto the client.The positionwithinthe file from wheretobeginreading
is specified as the position parameter.
Write ifilename, position, n, buffer): When the server executes this operation, it
takes nbytesofdata from the specifiedbufferand writes it into the file identifiedby
filename. The position parameter specifies the byte position within the file from
where to start writing. The server returns to the client the actual number of bytes
written.
As shown in Figure 9.6, this file server does not keep track of any file state
information resulting from a previous operation. Therefore, if a client wishes to have
similar effect as that in Figure 9.5, the following two read operations must be carried
out:
Read (filename, 0, 100,buj)
Read (filename, 100, 100,buj)
Clientprocess Serverprocess
Filestateinformation
File AIW Read
nameMode pointer (filename, 0,100,buf) ....
Return (0to99bytes)
Read
(filename, 100,100,buf)
...
Return
(100to199bytes)
Fig.9.6 Anexample ofastateless fileserver.
Notice that in this case, there is no need to use the session concept with open and
close operations because each file operation stands on its own. However, as shown in
Figure9.6, a client process normally keeps trackofthe state information ofthe files that
are in use by it. Sun Microsystems' NFS uses stateless file servers [Sandberg et a1.
1985].

452 Chap. 9 • Distributed FileSystems
Advantages of Stateless Service Paradigm in Crash
Recovery
The useofstateless fileserversbymanydistributedfilesystems isjustifiedbythefactthat
stateless servers have a distinct advantage over stateful servers in the event of a failure.
For example, with stateful servers, if a server crashes and then restarts, the state
information that it was holding may be lost and theclient process might continue its task
unaware of the crash, producing inconsistent results. Similarly, when a client process
crashesand then restarts, theserver isleftholding state informationthat isnolonger valid
butcannoteasily bewithdrawn. Therefore, thestatefuJservice paradigmrequires complex
crash recovery procedures. Both client and server need to reliably detect crashes. The
server needs to detect client crashes so that it can discard any state it is holding for the
client, and the client must detect server crashes so that it can perform necessary error
handling activities.
The stateless service paradigm makes crash recovery very easy because no client
stateinformationismaintained bytheserver andeach request containsalltheinformation
that is necessary to complete the request. When a server crashes while serving a request,
the client need only resend the request until the server responds, and the server does no
crash recovery at all. When a client crashes during request processing, no recovery is
necessary foreithertheclient or theserver.Therefore stateless servers can beconstructed
around repeatableoperations. That is, if aclient just resends a request until a response is
received for it, data will never be lost due to a server crash.
The stateless service paradigm, however, imposes the following constraints on the
design ofthe distributed file system [SiIberschatz and Galvin 1994]:
1. Each request ofthe stateless service paradigm identifies the file by itsfilename
instead ofa low-level file identifier. If the translation of remote names to local names is
done for each request, the request processing overhead will increase. To avoid the
translationprocess, each file should have asystemwideunique low-level name associated
with it.
2. Theretransmissionofrequests byclients requires thattheoperationssupported by
stateless servers be idempotent. Recall that an idempotent operation has the same effect
and returns the same output no matter how many times it is executed repeatedly. Self
containedread and writeoperationsare idempotent, since they useanabsolute bytecount
to indicate the position within a file and do not rely on an incremental offset. Similarly,
operationstodelete afileshould alsobemade idempotentifthestateless serviceparadigm
is used.
The stateless service paradigm also suffers from the drawbacks of longer request
messages and slower processing of requests. Request messages are longer because every
request must be accompanied with all the necessary parameters to successfully carry out
the desired operation. On theotherhand, request processing isslower because a stateless
server does not maintain any state information to speed up the processing. Furthermore,
in some cases, stateful service becomes necessary. For instance, in internetworks,
messages may not be received in the same order in which they were sent. A stateful

Sec.9.9 • Atomic Transactions 453
service ispreferableinsuchacase, since bythemaintainedstate itispossibletoorder the
messages correctly (see Chapter 3). Similarly, ifthe file system uses the server-initiated
cache validation approach, itcannot usethestateless service paradigmsince theserverhas
to maintain a record of which files are cached by which clients (see Section 9.6.3).
9.9 ATOMIC TRANSACTIONS
An atomic transaction (or just transaction for short) is a computation consisting of a
collection of operations that take place indivisibly in the presence of failures and
concurrent computations. That is, either all of the operations are performed successfully
ornone oftheireffects prevail, andother processes executingconcurrentlycannot modify
or observe intermediate states of the computation. Transactions help to preserve the
consistency of a set of shared data objects in the face of failures and concurrent access.
They makecrash recovery mucheasier,because atransactioncanonlyendintwostates
transaction carried out completely or transaction failed completely.
Transactions have the following essential properties:
1. Atomicity. This property ensures that to the outside world all the operations of a
transaction appear to have been performed indivisibly. Two essential requirements for
atomicity are atomicity with respect to failures and atomicity with respect to concurrent
access. Failure atomicity ensures that if a transaction's work is interrupted by a failure,
any partially completedresults will beundone. Failure atomicity isalso known asthe all
or-nothingpropertybecause atransaction isalways performedeither completelyor notat
all. On the other hand, concurrency atomicity ensures that while a transaction is in
progress, other processes executing concurrently with the transaction cannot.modify or
observe intermediatestatesofthetransaction. Onlythefinalstate becomes visible toother
processes after the transaction completes. Concurrency atomicity is also known as
consistencyproperty because a transaction moves the system from one consistent state to
another.
2. Serializability. This property (also known as isolation property) ensures that
concurrently executing transactions do not interfere with each other. That is, the
concurrentexecutionofasetoftwoormoretransactionsisseriallyequivalentinthesense
that the ultimate result ofperformingthem concurrently isalways the same as ifthey had
been executed one at a time in some (system-dependent) order.
3. Permanence. This property (also known asdurabilityproperty)ensures thatonce
a transaction completes successfully, the results of its operations become permanent and
cannot be lost even if the corresponding process or the processor on which itis running
crashes.
To easily remember these properties, Harder and Reuter [1983] suggested the
mnemonic ACID,whereA,C,I,andDrespectively standforatomicity (failureatomicity),
consistency (concurrency atomicity), isolation (serializability), and durability (perma
nence). Therefore, transaction properties are also referred to as ACIDproperties.

454 Chap. 9 • Distributed FileSystems
9.9.1 N••d for TransactionsIn FII. Service
Q
The provision oftransactions in a file service is needed for two main reasons:
1. For improving the recoverability of files in the event of failures. Due to the
atomicity property of transactions, if a server or client process halts unexpectedly due to
a hardware fault or a software error before a transaction is completed, the server
subsequently restores any files that were undergoing modification to their original states.
Notice that for a file service that does not supporttransaction facility, unexpected failure
ofthe client or server process during the processing ofan operation may leave the files
that were undergoing modification in an inconsistent state. Without transaction facility, it
may bedifficultoreven impossibleinsomecases torollback(recover)thefiles fromtheir
current inconsistent state to their original state.
2. For allowing the concurrent sharing of mutable files by multiple clients in a
consistentmanner. Iffile access requests from multiple clients foraccessing the same file
areexecutedwithoutsynchronization,thesequencesofreadandwriteoperationsrequested
bydifferentclientsmaybeinterleavedinmanyways,someofwhichwouldnotleavethefile
intheintendedstate.Therefore,unsynchronizedexecutionofaccess requests frommultiple
clients,ingeneral,resultsinunpredictableeffectsonthefile.Transactionfacility isbasically
a high-level synchronization mechanism that properly serializes the access requests from
multipleclientstomaintain theshared fileinthe intended consistentstate.
The following examples illustrate how the transaction facility ofa file service helps
to prevent file inconsistencies arising from events beyond a client's control, such as
machine or communication failures or concurrent access to files by other clients.
Inconsistency Due to System Failure
Considerthe banking transactionofFigure 9.7, which iscomprisedoffour operations(oJ'
a2, a3' a4) for transferring $5 from account X to account Z. Suppose that the customer
account records are stored in a file maintained by the file server. Read/write access to
customeraccountrecords aredone bysending access requests tothefileserver. Inthebase
fileservicewithout transactionfacility,thejoboftransferring$5fromaccountXtoaccount
Zwill beperformedbytheexecutionofoperations0I,02'03,anda4inthatorder.Suppose
the initial balance in both the accounts is $100. Therefore, if all the four operations are
performed successfully, the final balances in accounts X and Z will be $95 and $105,
respectively.Now suppose asystem failure occurs afteroperationa3 hasbeen successfully
performedbutbeforeoperationa4hasbeenperformed.Inthissituation,accountXwillhave
81: readbalance (x)ofaccount X
82: readbalance (z)ofaccountZ
83: write(x- 5)toaccount X Fig.9.7 Asetofoperations totransfer$5
84: write(z+5)toaccount Z fromaccountXtoaccountZ.

Sec.9.9 • AtomicTransactions 455
been debitedbut accountZwill nothave been credited.Therefore$5 vanishesbecausethe
final balances in accounts Xand Z are $95 and $100, respectively (Fig. 9.8). Successful
reexecution of the four operations will cause the final balances in accounts Xand Z to
become$90 and$105, respectively,which isnotwhat wasintended.
LettheinitialbalanceinboththeaccountsofFigure9.7be$100.
Successful
Unsuccessfulexecution
execution
a1:x= 100 a1:x=100
a2:z=100 a2:z=100
a3: x=95 a3:x=95
a4: z= 105 Systemcrashes
Finalresult Finalresult
x=95 VVhenthefour VVhenthefour
z= 105 operationsarenot operationsaretreated
treatedasatransaction asatransaction
x=95 x=100
z=100 z=100
Fig. 9.8 Possible final results insuccessful and unsuccessful executions with and
withouttransaction facility of the operationsof Figure9.7.
On the other hand, in a file service with transaction facility, the four operations a),
a2' a3' and a4 can be treated as a single transaction so that they can be performed
indivisibly. In this case, if the transaction gets executed successfully, obviously the final
balances of accounts X and Z will be $95 and $105, respectively. However, if the
transaction fails in-between, the final balances in accountsXand Zwill be rolled back to
$100 each, irrespective of what was the intermediate state of the balances in the two
accounts when the failure occurred(Fig. 9.8). Therefore, incase ofa failure, the balances
of the two accounts remain unchanged and the reexecution of the transaction will not
cause any inconsistency.
Inconsistency Due to ConcurrentAccess
Consider the two banking transactions T} and TzofFigure 9.9. Transaction T 1, which is
meant for transferring $5 from accountXto accountZ,consists offour operationsal' a2,
a3' and a4. Similarly, transaction T 2, which is meant for transferring $7 from account Y

456 Chap. 9 • Distributed File Systems
81: readbalance(x)ofaccountX
82:readbalance(z)ofaccountZ
83:write(x- 5)toaccountX
84:write(z+5)toaccountZ
T 1:Transfer$5fromaccountXtoaccountZ.
b1:readbalance(y)ofaccountY
b2:readbalance(z)ofaccountZ
~:write(y- 7)toaccountY
b4:write(z+7)toaccountZ
T2:Transfer$7fromaccountYtoaccountZ. Fig.9.9 Twobanking transactions.
to account Z~ consists of four operations bI, b 2, b 3, and b 4•The net effects of executing
the two transactions should be the following:
• To decrease the balance in account Xby $5
• To decrease the balance in account Yby $7
• To increase the balance in account Z by $12
Assuming that theinitial balance inall thethreeaccounts is$100, the final balances
ofaccounts X,1':andZaftertheexecution ofthetwotransactions shouldbe$95,$93,and
$112, respectively.
In a base file service without transaction facility,ifthe operations corresponding to
the two transactions are allowed toprogress concurrently andif thefile system makes no
attempt to serialize the execution of these operations, unexpected final results may be
obtained. This is because the execution of the operations corresponding to the two
transactions may get interleaved in time in an arbitrary order. Two such possible
interleavingsthatproduce unexpected finalresults areshowninFigure 9.10.Thecauseof
theerror is that both clients are accessing the balance in account Zand then altering itin
a manner that depends on its previous value.
In a file service with transaction facility, the operations of each of the two
transactionscan beperformed indivisibly, producing correctresults irrespectiveof which
transaction is executed first. Therefore, a transaction facility serializes the operations of
multiple transactions to prevent file inconsistencies due to concurrent access. However,
notice that the complete serialization of all transactions (completing one before the next
one isallowed tocommence) thataccessthesamedataisunnecessarily restrictive andcan
produce long delays in the completion of tasks. In many applications, it is possible to
allow some parts of multiple concurrent transactions to be interleaved in time and still
produce thecorrectresult.Forinstance, twopossible interleavings oftheoperations ofthe
two transactions of Figure 9.9 that produce correct results are shown in Figure 9.11.

Sec. 9.9 • AtomicTransactions 457
Lettheinitialbalanceinallthethreeaccounts
ofthetwotransactionsofFigure9.9be$100.
Anillegalschedule Anotherillegalschedule
81:x=100 b 1:y=100
b.:y= 100 b 2:z=100
82:z=100 81:x=100
b 2:z > 100 82:z=100
83:x=95 b 3:y=93
b 3:y=93 b 4:z=107
84:z=105 83:x=95
b 4:z= 107 84:z=105
Finalresult Finalresult
x=95 x=95
Fig.9.10 Twointerleavings of the y=93 y=93
operations of thetwo
z=107 z= 105
transactions of Figure
9.9that produce
unexpected final results. Time
Lettheinitialbalanceinallthethreeaccounts
ofthetwotransactionsofFigure9.9be$100.
Alegalschedule Anotherlegalschedule
81:x=100 b 1:y=100
b 1:Y= 100 b 2:z=100
82:z=100 8 1:x=100
83:x=95 b 3:y=93
84:z=105 b
4:z=107
b 2: v-105 82:z=107
b 3:z=93 83: x=95
b
4:z=112
84:z=112
Finalresult Finalresult
x=95 x=95
Fig. 9.11 Twointerleavings of the y=93 y=93
operations ofthe two z=112 z=112
transactions of Figure
9.9 thatproduce correct
final results. Time

458 Chap. 9 • Distributed File Systems
Any interleaving of the operations of two or more concurrent transactions is known
as a schedule. All schedules that produce the same final result as if the transactions had
been performed oneatatime insome serialorder aresaidtobeserially equivalent. Serial
equivalenceisusedasacriteria forthecorrectness ofconcurrently executing transactions.
It is up to the system to ensure that a serially equivalent schedule is always selected to
execute aset ofconcurrently executing transactions. By allowing the system the freedom
tochoose anyordering oftheoperations itwants to,provided itproduces thecorrect final
result, we eliminate the need for programmers to do their own mutual exclusion, thus
simplifying the programming task.
9.9.2 Or-rations for Transaction-lased FileService
Inafilesystem thatprovides atransaction facility,atransaction consists ofasequence of
elementary file access operations such as read and write. The actual operations and their
sequence that constitute a transaction is application dependent, and so it is the client's
responsibilitytoconstruct transactions. Therefore, theclientinterface tosuchafilesystem
must include special operations for transaction service. The three essential operations for
transaction service are as follows:
begin_transaction ~ returns (TID): Begins anew transaction and returns a unique
transaction identifier (TID). This identifier is used in other operations of this
transaction. All operations within a beginfransaction and an end_transaction form
the body of the transaction.
end_transaction (TID) ~ returns (status): This operation indicates that, from the
viewpoint of the client, the transaction completed successfully. Therefore the
transaction is terminated and an attempt is made to commit it. The returned status
indicates whether thetransaction hascommittedorisinactive because it wasaborted
by either the client or the server. If the transaction commits, all of its changes are
madepermanent andsubsequent transactions willseetheresults ofallthechanges to
filesmadebythistransaction. Ontheotherhand, ifthetransaction wasaborted, none
of the changes requested so far within the transaction will become visible to other
transactions. Atransaction isaborted either on explicit request bytheclient orinthe
event of system failures that disrupt the execution of the transaction.
abort_transaction (TID): Aborts the transaction, restores any changes made so far
within the transaction to the original values, and changes its status to inactive. A
transaction isnormally abortedintheeventofsomesystemfailure. However,aclient
may use thisprimitive tointentionally abort atransaction. Forinstance, consider the
transaction of Figure 9.12, which consists of three write operations on a file. In
Figure 9.12(a), all three operations are performed successfully, and after the end_
transaction operation, the transaction makes the new file data visible to other
transactions. However,inFigure 9.12(b), the third writeoperation could notsucceed
because of lack of sufficient disk space. Therefore, in this situation, the client may
use theabort_transaction operation to abort thetransaction so that the results of the
two write operations are undone and the filecontents isrestored back tothe value it

Sec.9.9 • AtomicTransactions 459
=
TID begin_ transaction
write(TID file,position,n,buffer)--+returns(ok)
wri1e(TID
Jfile,position,n,buffer)--+returns(ok)
write(TID, J file,position,n,buffer)-+returns(ok)
end_ transaction(TID)
Fig. 9.12 Illustratingthe useof
(a)
abort_transaction
operation: (a) allthree
operationsare performed TID=begin_transaction
successfullyso the write(TID,file,position, n,buffer)-+returns IOkl
write(TID,file,position, n,buffer)--+returns ok
transactioncommits; (b) all write(TID,file,position, n,buffer)--+returns(diskfullerror)
three operationscould not abort_ transaction(TID)
beperformedsuccessfully so
the transaction wasaborted. (b)
hadbefore thetransactionstarted. Notice that,once atransactionhasbeencommitted
or aborted in the server, its state cannot be reversed by the client or the server.
Consequently, an abortfransaction request would fail if a client issued it after that
transaction had been committed.
In a file system with transaction facility, in addition to the three transaction service
operations described above, the transaction service also has file access operations. Each
file access operation ofthe transaction service corresponds to an elementary file service
operation. The parameters of the file access operations of the transaction service are the
same as those of the elementary file service operations except for an additional argument
to specify the transaction identifier (TID) of the transaction to which the operation
belongs. Forinstance,thefollowing arefileaccess operationsfortransactionservice ofthe
stateless server for the byte-stream files of Section 9.8.2:
Tread (TID, filename, position, n, buffer): Returns to the client n bytes of the
tentative data resulting from the TID if any has been recorded; otherwise it has the
same effect as Read (filename, position, n, buffer).
Twrite (TID,filename, position, n,buffer): Has the same effect as Write (filename,
position, n, buffer) but records the new data in a tentative form that is made
permanent only when the TID commits.
9.9.3 Recovery Techniques
From thepoint ofviewofaserver,atransactionhastwophases (Fig.9.13).The firstphase
starts when theserverreceivesabegin_transactionrequestfromaclient. Inthisphase, the
file access operations in the transaction arc performed and the client adds changes to file
items progressively. On execution of the endtransaction or abort_transactionoperation,
the first phase ends and the second phase starts. In the second phase, the transaction is
either committed or aborted. In a commit, the changes made by the transaction to file
items are made permanentsoastomake them visible toothertransactionsaswell. Onthe
other hand, in an abort, the changes made by the transaction to file items are undone to
restore the files to the state they were in before the transaction started.

460 Chap.9 • Distributed FileSystems
T
beglR-transaction
r
Ctientadds
changesto
Executionoffile
accessoperations F irstphase p fil r e og it r e e m ss s ively
inthetransaction
thatare
1 recordedina
reversiblemanner
bytheserver.
end_transaction or abort_transaction
r r
Changesmade
bythetrans
Commit Abort actiontofile
(makechangesmade (undochangesmade Secondphase itemsareeither
bythetransactionto bythetransactionto p re e c rm or a d . e n d en o tl r Y
fileitemspermanent) fileitems) undoneby
1 theserver.
1 1
Fig.9.13 The twophasesofatransaction.
The fact that in the secondphase the transaction may eitherbecommittedor aborted
requiresthat the file updateoperations inthe first phasemust be performed in such a way
that they may beeitherpermanentlyrecordedor undone. Therefore, while atransactionis
initsfirst phase, and hence subjecttoabortion,itsupdatesmust berecordedinareversible
manner. The two commonly used approaches for recording file updates in a reversible
manner are the file versions approach and the write-ahead log approach.
File Versions Approach
A basic technique to ensure file recoverability is to avoid overwriting the actual data in
physical storage. The file versions approach is based on this technique. As shown in
Figure 9.14, in this approach, when a transaction begins, the current file version is used
forallfile access operations(withinthetransaction)thatdo notmodifythefile. Recall that
as soon as a transaction commits, the changes made by it to a file become public.
Therefore, the current version of a file is the version produced by the most recently
committed transaction.
Whenthefirstoperationthatmodifiesthefileisencounteredwithin thetransaction,the
servercreatesatentativeversion ofthefileforthe transactionfrom thecurrentfile version
and performsthe updateoperationon this versionofthefile. Fromnow on, all subsequent
file accessoperations(read or write) within the transactionare.performedon this tentative
file version. Whenthe transactioniscommitted, the tentativefile version is made the new
currentversionand thepreviouscurrentversion ofthe file isadded to the sequenceofold
versions.Ontheotherhand, ifthetransactionisaborted,thetentativefile versionissimply
discardedand thecurrentfile version continuestoremainthecurrentversion.

Sec.9.9 • Atomic Transactions 461
Transactionprogress Fileversionmanagement
begin_trrsactlon Currentfile
version (\{)
,
Access / , , ,
Vcisused forallfile / , ,
access operationsin / ,
,
thetransactionthatdo / ,
/ , ,
moor / ,
,
not thefile Create I ,
/ , ,
,
,
Atentative ,
Firstfile update ,
fileversion ,
o in p t e h r e at t i r o a n ns e a n c c t o io u n ntered t ( r V a , n ) s fo a r ct t i h o e n , , ,
,
1 Access ,
,
,
,
,
,
Vtisusedforall ,
,
subsequentfileaccess ,
,
,
,
operatiiS(readorwrite) Vtbecomes ,
-------..
~
v,
isadded ~
~__~~~~ thenew tothe ,
~cr V r tto e v, n v~rS t ion
o
se
ld
q
v
u
e
e
r
n
s
c
io
e
n
o
s
f ~
•"
V cremains
thecurrent
version
."ig.9.14 The file versions approach forrecording file updates inareversible manner.
Notice that a transaction can modify more than one file. In this situation, there is a
tentative version ofeach fileforthetransaction. Furthermore,sinceafilemaybeinvolved
in several concurrent transactions, it may have several tentative versions existing at the
same time. In this situation, when one of the concurrent transactions commits, the
tentativeversion correspondingtothat transactionbecomes thecurrent version ofthefile.
Since the remaining tentative versions of the file are no longer based on the current
version, they are handled as follows. When the next transaction out of the concurrent
transactions commits, and if there are noserializabilityconflicts between this transaction
and the previously committed transactions, the tentative version corresponding to this
transactionismergedwiththecurrent version, creating anewcurrent versionthatincludes
the changes made by all of the transactions that have. already committed. On the other
hand, if there are serializability conflicts, all the transactions that are involved except the
first one to commit are aborted. A serializability conflict occurs when two or more
concurrenttransactionsareallowed toaccess thesamedata items inafileandoneormore
of these accesses is a write operation.

462 Chap. 9 • Distributed FileSystems
Shadow Blocks Techniquefor Implementing File Versions. The tentative
versions of files behave like copies of the file version from which they are derived.
However, copying theentirefileforeach transaction that modifiesitmaybe wasteful and
prohibitive, especially forlargefiles.The shadow blocks technique isanoptimizationthat
allows the creation of a tentative version of a file without the need to copy the full file.
In fact, it removes most of the copying.
Afile system uses some formof indexing mechanism toallocate disk space tofiles.
Asshown inFigure 9.15(a),inthismethod, theentire disk spaceispartitionedintofixed-
'--- ~ 3
5
0 1 2 3 7
4 5 6 7 2 10
8 11
8 9 10 11
6 13
12 13 14 15 9 14
12 15
Thestoragespaceofa Indexfor Indexfor Listof
diskpartitionedintoblocks fileF file ~ freeblocks
1
(a)
: : 3
: : 5
I • • I 7
• I
I• 0 I• 10
~O,
:Atransaction : : 11
~
:
:
4
m
a
o
n
d
d
ifie
a
s
pp
b
e
lo
n
c
d
k
s
:
:
3
1 :
ITransaction:I 13
18newblocktoI : aborts I 4 14
. :thefiledata: 5: : 1 15
CurrentIndex: : Tentative
offile~ I ••index of • I ' : . Current Listof
Li~~~~_o!!i~e_~!._!r':':_~l~~~
3 : • : file • F 1 DI: I
5 •I •I I 4
7 ITransaction' 7
:0 10 : commits : 10
: : 0 11
11 11 I I
13: : 3 13
13
14 14: : 1 14
• •
15: : 5 15
15
Listof : : Current Listof
Listof
freeblocks freeblocks; ;indexoffileF 1 freeblocks
(b)
Fig.9.1S Theshadowblocks technique forimplementing fileversions:(a) example
ofdisk blocks, fileindices, andlistof freeblocks;(b) allocationand
deallocation ofblocksasthetransaction progresses.

Sec.9.9 • Atomic Transactions 463
length byte sequencescalled blocks. The file system maintains an index for each file and
a list of free blocks. The index for a particular file specifies the block numbers and their
exact sequence used for storing the file data. For example, in Figure 9.15(a), file F uses
1
the blocks numbered 0, 4, 1in that sequence and file F2 uses the blocks numbered 2, 8,
6, 9, 12 in that sequence. On the other hand, the list of free blocks contains the block
numbers that are currently free and may be allocated to any file for storing new data.
In the shadow blocks technique, a tentative version of a file is created simply by
copying the index of the current version of that file. That is, a tentative index of the file
iscreated from itscurrent index. Now when afile update operationaffects ablock, anew
disk block is taken from the free list, the new tentative value is written in it, and the old
block number inthe tentative indexisreplaced bythe block number ofthe newblock [see
Fig. 9.15(b)]. File update operations that append new data to the file are also handled in
the same way by allocating new blocks from the free list for the appended data and
extending the tentative index of the file. The new blocks allocated to a tentative version
of a file are called shadow blocks. Subsequent writes to the same file block by the
transaction are performed on the same shadow block.
All file access operations in the transaction are performed by using the tentative
index while file access operations of other processes are performed by using the current
index.Therefore, theprocess running thetransaction seesthemodified fileversion, butall
other processes continuetoseetheoriginal fileversion. Notice thatunmodifiedfileblocks
are shared between these two file versions.
As shown in Figure 9.15(b), if the transaction aborts, the shadow blocks of the
tentative version of the file are returned to the list of free blocks and the tentative index
is simply discarded. On the other hand, if the transaction commits, the tentative index is
made thecurrent index ofthe file, and ifthere isnoneed toretain theold file version, the
blocks of the original file version whose data were modified by the transaction are added
to the list of free blocks and the old current index is discarded.
The Write-Ahead Log Approach
Another commonly used technique for recording file updates in a recoverable manner is
the write-ahead log approach. In this method, for each operation of a transaction that
modifies a file, a record is first created and written to a log file known as a write-ahead
log. After this, the operation is performed on the file to modify its contents.
A write-ahead log is maintained on stable storage and contains a record for each
operation t.hatmakeschanges tofiles.Eachrecordcontains theidentifierofthetransaction
that is making the modification, the identifierof the file that is being modified, the items
of the file that are being modified, and the old and new values of each item modified.
Toillustrate how thelog works, let us again considerthe transaction for transferring
$5 from account X to account Z. As shown in Figure 9.16, for each operation of the
transaction that makes changes to file items, a record is first created and written to the
write-ahead log. In each record, the old and new values of each item modified by the
operation are separated by a slash.
When the transaction commits, a commit record is written to the write-ahead log.
Notice that since the changes due to update operations were made on the file itself, there

464 Chap. 9 • Distributed FileSystems
x= 100;
z=100;
begin_transaction
readbalance (x)ofaccountX;
readbalance (z)ofaccountZ; Log
-----.I
write(x- 5)toaccount X; x=100/951
1------1
write(z+5)toaccount Z;
t'ig.9.16 Anexample ofcreation of
end_transaction
write-ahead logrecords.
isnoneed tochange thefileitems whenthetransaction commits. On theotherhand,ifthe
transactionaborts, theinformationinthewrite-aheadlogisusedtorollbacktheindividual
file items to their initial values, thus bringing the files affected by the transaction to their
original state. Forrollback, thewrite-aheadlogrecords are usedone byone, startingfrom
the last record and going backward, to undo the changes described in them.
The write-aheadlogalso facilitates recovery from crashes. For instance, suppose the
server processhandling the transaction ofFigure 9.16 crashes after writing the log record
for the second write operation in the transaction. After the server's machine is restarted,
the write-ahead log is used to identify the exact point of failure. For this, the value ofthe
changed data item in the last record of the log (in this case z) is compared with its value
in the file. One ofthe following actions are taken depending on the status ofthe file:
1. If the value in the file is 100,it means that thecrash occurred before the file was
updated. Therefore, the value of the data item in the file is changed to 105.
2. On the otherhand, if the value in the file is 105,it means that the crash occurred
after the file was updated, so nothing needs to be done in this case.
9.9.4 Concurrency Control
Serializability is an important property of atomic transactions that ensures that
concurrently executing transactions do not interfere with each other. However, we have
seen in the example of Figure 9.10 that if the operations of two or more concurrent
transactions accessing the same data item are allowed to progress uncontrolled, the
operations of these transactions may get interleaved in time in an arbitrary order,
producing unexpected final results. Therefore, to prevent data inconsistency due to
concurrent access by multiple transactions, every transaction mechanism needs to
implement a concurrency control algorithm.
A good concurrency control mechanism allows maximum concurrency with
minimum overhead while ensuring that transactions are run in a manner so that their
effects on shared data are serially equivalent. The simplest approach for concurrency
control would be toallow the transactions toberun one at atime sothattwo transactions
never run concurrentlyand hence there isnoconflict. However, thisapproachisnotgood
because it does not allow any concurrency. Total elimination of concurrency is neither

Sec.9.9 • Atomic Transactions 465
acceptable nor necessary because a transaction normally accesses only a few of the large
number of files available in a system. This observation gives rise to another simple
approach toconcurrency control. Twotransactions should be allowedtorun concurrently
only if they do not use a common file (or data item ingeneral). Although better than the
previous approach, this approach is also unnecessarily restrictive because a pair of
transactions may access the same data item in a manner that does not cause any conflict
(inconsistency of the data item). Furthermore, it is usually not possible to predict which
data items will be used by a transaction. Therefore, more flexible concurrency control
algorithmsare normally used byatransaction mechanism. The mostcommonly usedones
are locking, optimistic concurrency control, and timestamps. Descriptions of these three
approaches follow.
Locklng
This is the oldest and the most widely used approach. In the basic locking mechanism, a
transaction locks adata item before accessing it.Each lock islabeled with thetransaction
identifier and only the transaction that locked the data item can access it any number of
times. Other transactions that want to access the same data item must wait until the data
item is unlocked. All data items locked by a transaction are unlocked as soon as the
transaction completes (commits or aborts). Locking is performed by the transaction
service asapartofthedata access operations, andclients have noaccess tooperations for
locking or unlocking data items.
OptimizedLockingfor Better Concurrency. The basic locking scheme is too
restrictive and some optimizations have been proposed for better concurrency. Two of
them are described below.
1. Type-specific locking. A simple lock that is used for all types of accesses to data
items reduces concurrency morethan isnecessary. Better concurrencycanbeachieved by
using type-specific locking scheme in which more than one type of locks are used based
on thesemantics of access operations. For instance, letusconsiderthesimple case oftwo
types of access operations readand write. Notice that multiple transactions that read the
same data item but never write on it do not conflict. Therefore, when a transaction is
accessingadata item in theread-only mode, there isnoreason tokeep those transactions
waiting that also want to access the data item in the read-only mode. Therefore, instead
ofusingasingle lockforbothreadandwriteaccesses, separate locks(readlocksandwrite
locks) should be used for the two operations. With these two types of locks, the locking
rules are given in Figure 9.17. If a read lock is set on a data item, other read locks are
permitted but write locks are not permitted. Therefore, an item locked with a read lock
cannot have itsvalue changed byother transactions. Ontheother hand, whenawrite lock
isset, noother locks ofany kind are permitted. Therefore, adata item locked withawrite
lock cannot be accessed (read or written) by another transaction.
2. Intention-to-write locks. Recall that a transaction has two phases and the updates
made by a transaction to a data item are tentative in the first phase and are made
permanent only in the second phase. Each transaction is unable to observe the other

466 Chap. 9 • Distributed File Systems
Typeoflocktobeset
Typeoflock
alreadyset
Read Write
None Permitted Permitted
Read Permitted Notpermitted
Write Notpermitted Notpermitted
Fig.9.17 Locking rules incase of readlocks
and write locks.
transactions' tentative values.Therefore, whenareadlockisset,insteadofpreventing any
other transaction from writing the locked data item, a transaction should be allowed to
proceed with itstentative writes untilitisready tocommit. The valueof theitem will not
actually change until the writing transaction commits, so, if it is suspended at that point,
the item remains unchanged until the reading transaction releases its lock.
Based on the observation above, for better concurrency, Gifford [1979b] proposed
theuseofan"intention-to-writelock"(I-write) andacommitlockinstead ofawritelock.
Therefore, three types-oflocks areused inthisscheme, andthelocking rules forthem are
given in Figure 9.18. Notice that if a read lock is set, an I-write lock is permitted on the
data item and vice versa. This is because the effects of write are not observable by any
other transaction until the writing transaction commits. If an l-write lock is set, no other
transaction isallowed tohaveanI-write lockon thesamedata item.Acommit lock isnot
permitted if any other type of lock is already set on the data item. Therefore, when a
Typeoflocktobeset
Typeoflock
alreadyset
Read I-write Commit
None Permitted Permitted Permitted
Read Permitted Permitted Notpermitted
I-write Permitted Notpermitted Notpermitted
Commit Notpermitted Notpermitted Notpermitted
Fig.9.18 Lockingrulesincaseofreadlocks,I-write locks,andcommit locks.

Sec.9.9 • AtomicTransactions 467
transactionhavinganI-writelockcommits,itsI-writelockisconvertedtoacommitlock,
sothatifthereareanyoutstandingreadlocks,thetransactionmustwaituntilitispossible
to setthecommitlock.
Two-Phase Locking Protocol. Because of the potential for increased concur
rency, itistemptingto lock adata item for use byatransactiononly forthe periodduring
which the transaction actually works on it and to release the lock as soon as the access
operation is over. However, locking and unlocking data items precisely at the moment
they are needed (or no longer needed) can lead to inconsistency problems. For instance,
two commonlyencounteredproblemsdue toearly release (releasingimmediatelyafter the
access operation finishes) of read locks and write locks are as follows:
1. Possibilityofreadinginconsistentdata incase oftwoormore readaccesses bythe
same transaction. This is because when read locks and write locks are released early,
between two subsequent read accesses to the same data item by a transaction, another
transaction may update the same data item and commit. Therefore, the second read may
not see the same value of the data item as the first read. Notice that ifa transaction reads
a data item only once per transaction, there is no harm in releasing its read lock early.
2. Need for cascaded aborts. Suppose a transaction releases a write lock early and
then some other transaction locks the same data item, performs all its work, and commits
before the first transaction. Afterward, suppose the first transaction aborts. In this
situation, the already committed transaction must now be undone because its results are
based on a value of the data item that it should never have seen. Aborting of already
committedtransactions whenatransactionaborts isknown ascascadedaborting. Because
of the large overhead involved in cascaded aborting, it is better to avoid it.
To avoid the data inconsistency problems, transaction systems use the two-phase
locking protocol. Inthe first phase ofatransaction, known asthegrowingphase,alllocks
needed by the transaction are gradually acquired. Then in the second phase of the
transaction, known as the shrinking phase, the acquired locks are released. Therefore,
once a transaction has released any of its locks, itcannot request any more locks on the
same or other data items. It has been proved [Eswaran et al. 1976]that if all transactions
use the two-phase·Iocking protocol, all schedules formed by interleaving their operations
are scrializable.
GranularityofLocking. 'Thegranularityoflocking refers totheunit oflockable
data items. In a file system supporting transactions, this unit is normally an entire file, a
page, or a record. If many transactions share files, the granularity oflocking can have a
significant impact on how many transactions can be executed concurrently. For instance,
iflocks can beappliedonly to whole files,concurrencygets severelyrestricteddue to the
increased possibility of false sharing. False sharing occurs when two different
transactionsaccess twounrelated data itemsthatreside inthesame file(Fig.9.19). Insuch
a situation, even though the two data items can be accessed concurrently by the two
transactions, this isnotallowed becausethegranularity of locking isafile. Notice that the
locking granularity increases concurrency by reducing the possibility of false sharing.

468 Chap.9 • Distributed FileSystems
Transaction T accesses T
datainthi 1 sarea 1
Transaction T2 accesses..-----e 7:
datainthisarea 2
Afile Fig.9.19 False sharing.
However, finer locking granularity leads to larger lock management overhead,
complicates implementation of recoverable files, and is more likely to lead to
deadlocks.
HandlingofLockingDeadlocks. The locking scheme can lead todeadlocks.A
deadlock is a state in which a transaction waits for a data item locked by another
transactionthatintum waits, perhaps viaachain ofother waiting transactions,forthefirst
transaction to release some ofits locks. Since a transaction cannotrelease any lock until
itfinishes, none ofthetransactionsinvolvedinsuchacircularwaitcan proceedunless one
of them is aborted. For example, suppose two transactions T) and Tzhave locked data
items D) and D 2,respectively. Now supposethat T) requests alock onD
2
and T
2
requests
alock on D).Adeadlockresults because each transaction has an item of data locked that
the other needs to access.
A detaileddescriptionofthe commonly used techniques for handlingdeadlocks was
presented in Chapter 6. In transactions, one ofthe following techniques may be used:
1. Avoidance. One method to handle the deadlock problem is to prevent deadlocks.
Deadlockscan be preventedbyenforcingthat requeststolock data items be always made
in a predefined order so that there can be no cycle in the who-waits-for-whom graph.
Although the method is quite effective, it may cause data items to be locked too soon,
resulting in reduced concurrency.
2. Detection. Deadlocks can be detected by constructing and checking who-waits
for-whom graph. A cycle in the graph indicates the existence ofa deadlock. When such
a cycle is detected, the server must select and abort a transaction out of the transactions
involved in the cycle.
3. Timeouts. Associating a timeout period with each lock is another method for
handlingdeadlocks.Thatis,a lock remains invulnerablefor afixed period, after which it
becomes vulnerable. A data item with a vulnerable lock remains locked if no other
transaction is waiting for it to get unlocked. Otherwise, the lock is broken (the data item
isunlocked)and the waiting processispermittedtolockthedata itemforaccessingit.The
transaction whose lock has been broken is normally aborted.

Sec. 9.9 • Atomic Transactions 469
Three major drawbacks ofthe timeout approach are(a)itishardtodecide the length
of the timeoutperiod for a lock; (b) in an overloaded system, the number of transactions
getting aborted duetotimeouts willincrease, resulting inincreased overheadforrerunning
the aborted transactions; and (c) the method favors short transactions over long
transactions because transactions taking a long time are more likely to be penalized.
Optimistic Concurrency Control
This approach for concurrency control by Kung and Robinson [1981] is based on the
observation that access conflicts in concurrently executing transactions are very rare.
Therefore, inthisapproach, transactions areallowed toproceed uncontrolled uptotheend
of the first phase. However, in the second phase, before a transaction is committed, the
transaction is validated to see if any of its data items have been changed by any other
transaction since it started. The transaction is committed if found valid; otherwise it is
aborted.
For the validation process, two records are kept of the data items accessed within a
transaction-areadsetthatcontains thedata itemsread bythetransaction andawriteset
that contains the data items changed, created, or deleted by the transaction. To validate a
transaction, its read set and write set are compared with the write sets of all of the
concurrent transactions that reached the end of their first phase before it.The validation
fails ifany data item present in the read setor write setof the transaction being validated
is also present in the write set of anyof the concurrent transactions mentioned above.
Two main advantages of the optimistic concurrency control approach are as
follows:
1. It allows maximum parallelism because all transactions are allowed to proceed
independently in parallel without any need to wait for a lock.
2. It is free from deadlock.
However, it suffers from the following drawbacks:
1. It requires that old versions of files corresponding to recently committed
transactions be retained for the validation process. This is not necessary either with
locking or timestamping.
2. Although the approach is free from deadlock, it may cause the starvation of a
transaction. This isbecause atransaction thatfails validation isaborted and then restarted
allover again. But ifthetransactioncomes intoconflict withother transactions fortheuse
of data items each time it is restarted, it can never pass the validation checks.
Tosolve thestarvationproblem, KungandRobinson suggested thattheserver should
detect a transaction that has been aborted several times. When such a transaction is
detected, it should be given exclusive access to the data items it uses by the use of a
critical section protected by a semaphore,
3. Inanoverloadedsystem, thenumber oftransactionsgetting aborted due toaccess
conflicts may go up substantially, resulting in increased overhead for rerunning the

470 Chap. 9 • Distributed FileSystems
aborted transactions. Optimistic concurrency control is not a suitable approach for such
situations.
Mullender and Tanenbaum [1985] suggested that locking should be used in
transactions in which several files are changed and where thechance of two transactions
usingthesamedataitemishigh.Ontheotherhand,optimisticconcurrency control should
be used for transactions using one file and in which the likelihood of two transactions
accessing the same data item is·low.
Timestamps
In the optimistic concurrency control approach, a transaction is validated only after it
has completed its first phase. If the validation fails because some conflict is detected,
the transaction is aborted and restarted all over again. Execution of operations of the
transaction that follow the operation that caused the conflict is actually a waste in this
case. Notice that the overhead involved in executing the operations of the transaction
that follow the operation that caused the conflict can be totally avoided if it is possible
to detect the conflict right at the time when the operation causing it is executed. This
is because the transaction can be aborted immediately at the point where the conflict
occurs and it can then be restarted. This has been made possible in the timestamps
approach, in which each operation in a transaction is validated when it is carried out.
If the validation fails, the transaction is aborted immediately and it can then be
restarted.
To perform validation at the operation level, each transaction is assigned a unique
timestamp at the moment it does begin_transaction. In addition, every data item has a
read timestamp and a write timestamp associated with it. When a transaction accesses a
data item, depending on thetypeofaccess (read or write), thedata item'sread timestamp
or write timestamp isupdated to the transaction's timestamp.
As usual, the write operations of a transaction are recorded tentatively and are
invisible toother transactions untilthetransaction commits.Therefore, whenatransaction
is in progress, there will be a number of data items with tentative values and write
timestamps. The tentative valuesandtimestamps become permanent whenthe transaction
commits.
Before performing a read operation or a write operation on a data item, the server
performs a validation check by inspecting the timestamps on the data item, including the
timestamps on its tentative values that belong to incomplete transactions. The rules for
validation are as follows:
Validation ofa Write Operation. If the timestamp of the current transaction
(transaction that requested the write operation) iseither equal to or more recent than the
read and (committed) write timestamps of the accessed data item, the write operation
passes thevalidation check.Therefore atentative writeoperationisperformed inthiscase.
On theother hand, if thetimestamp ofthecurrent transaction isolder than the timestamp
of the last read or committed write of the data item, the validation fails. This is because
another transaction has accessed the data item since the current transaction started.
Therefore the current transaction is aborted in this case.

Sec. 9.9 • Atomic Transactions 471
Validation ofa Read Operation. If the timestamp of the current transaction
(transactionthat requestedtheread operation)ismore recentthan the write timestampsof
allcommittedand tentativevalues ofthe accesseddataitem, the read operationpasses the
validationcheck. However, the readoperationcan be performedimmediatelyonly ifthere
are no tentative values ofthe data item; otherwiseitmust wait until thecompletionof the
transactions having tentative values of the data item. On the other hand, the validation
check fails and the current transaction is aborted in the following cases:
1. The timestamp ofthe currenttransaction is olderthan the timestamp ofthe most
recent (committed) write to the data item.
2. The timestamp ofthe currenttransaction is olderthan that ofa tentative value of
the data item made by another transaction, although it is more recent than the
timestamp of the permanent data item.
Noticethat inthe approachdescribedabove, atransactioncancompleteitsfirstphase
only ifall itsoperationshave been consistentwith those ofearliertransactions.Therefore,
ifatransactioncompletesitsfirst phase, itcan always becommitted,althoughitmay have
to wait for earlier transactions that have tentative copies of shared data items to
commit.
The timestamp-based concurrency control scheme described above is used in the
SDD-I database system [Bernstein et al. 1980]. A similar scheme using timeouts is
described in [Reed 1983]. Timestamp-based concurrency control schemes are deadlock
free.
9.9.5 Distributed TransQctlon Service
Distributed file systemshavingtransaction facility need to supportdistributed transaction
service. A distributed transaction service is an extension ofthe conventional transaction
service, which can supporttransactions involving files managed by more than one server.
When a transaction involves multiple servers, all the servers need to communicate with
one another to coordinate their actions during the processing ofthe transaction so as to
achieverecoverabiJity and concurrency control over theentire set offile operations inthe
transaction.
A simple approach to coordinate the actions of multiple servers involved in a
distributed transaction would be to enforce that all client requests pass through a single
server. However, to avoid unnecessarycommunicationoverhead, adistributedtransaction
service normally allows client requests to be sent directly to the server that holds the
relevant file instead of directing them via a single server. This approach is described
below.The description isbased on theconceptsintroducedin [Israel eta1. 1978]and used
in the XDFS file service [Mitchell and Dion 1982, Sturgis et al. 1980].
In adistributedtransaction service, aclientbegins atransaction by sending abegin_
transaction request to any server. The contacted server executes the beginfransaction
requestandreturnsthe resultingTID totheclient. This serverbecomesthecoordinatorfor
the transaction and is responsible for aborting or committing it and for adding other
servers called workers.

472 Chap. 9 • Distributed FileSystems
Workers are dynamically added to the transaction. For this, adistributed transaction
service has a new operation in addition to the operations of a traditional transaction
service: The request
add_transaction (TID, server_id of coordinator)
informs a server that it is involved in the TID.
Beforean access request is sent toa server that hasnotyetjoinedthetransaction, an
addjransaction request is sent to the server. When the server receives the add_
transaction request, it records the server identifier of the coordinator, makes a new
transaction record containing the TID, and initializes a new log to record the updates to
local files from the transaction. It also makes a call to the coordinator to inform it of its
intention to join the transaction. In this manner, each worker comes to know about the
coordinatorand the coordinatorcomes to know about and keeps a list of all the workers
involved in the transaction. This information enables the coordinatorand the workers of
the transaction to coordinate with each other at commit time.
Two-Phase Multiserver Commit Protocol
The most crucial part in the design of a distributed transaction service is the committing
of distributed transactions. In a distributed transaction, since the files changed within the
transaction are stored on multiple servers, the commit protocol becomes more
complicated. A crash of one server does not normally affect other servers, and hence the
commitprotocol must ensure that the transaction is not committedand its changes to the
files are completed on some servers if it cannot be completed on all servers involved.
The general protocol for committing distributed transactions has two phases. The
two-phase multiserver commit protocol given in [Gray 1978] is described below.
When the client of a distributed transaction makes an end_transaction request, the
coordinator and the workers in the transaction have tentative values in their logs
describing the operations that affect their own files. The coordinator is responsible for
deciding whether the transaction should beaborted or committed; if any server is unable
to commit, the whole transaction must be aborted. Therefore, the end_transaction
operation is performed in two phases-preparation phase and commitment phase. The
actions involved in each phase are described below.
Preparation Phase.
1. The coordinator makes an entry in its log that it is starting the commit
protocol.
2. It then sends a prepare message to all the workers telling them to prepare to
commit. The message has a timeout value associated with it.
3. When a worker gets the message, itchecks to see ifit is ready to commit(i.e., it
has not previously aborted its part of the transaction). If so, it makes an entry in
its log and replies with a ready message. Otherwise, it replies with an abort
message.

Sec. 9.9 • Atomic Transactions 473
CommitmentPhase. Atthis point, thecoordinatorhas received a readyorabort
reply from each worker or the prepare message has timed out:
1. Ifall the workers are ready to commit, the transaction iscommitted. For this, the
coordinatormakes an entry in its log indicating that the transaction has been committed.
It then sends a commitmessage to the workers asking them to commit. At this point, the
transaction is effectively completed, so the coordinator can report success to the client.
On the other hand, if any of the replies was abort or the prepare message of any
worker got timed out, the transaction is aborted. For this, the coordinatormakes an entry
in its log indicating that the transaction has been aborted. It then sends anabortmessage
to the workers asking them to abort and reports failure to the client.
2. Whenaworker receives thecommitmessage, itmakes acommittedentryinitslog
and sends a committed reply to the coordinator. At this point, the part of the transaction
with the worker is treated as completed and its records maintained by the worker are
erased.
3. When the coordinator has received a committed reply from all the workers, the
transaction is considered complete, and an its records maintained by the coordinator are
erased. The coordinator keeps resending the commit message until it receives the
committedreply from all the workers.
9.9.6 N8st8dTransactions
Nested transactions are ageneralization of traditional transactions in which atransaction
may be composed of other transactions called subtransactions. A subtransaction may in
turn have itsown subtransactions. Inthis way,transactionscan benested arbitrarily deep,
forming a family of transactions.
Treeterminology isnormally usedindescribing relationships among thetransactions
belonging tothesame family.When atransactionstarts, itconsists ofonlyonetransaction
(process) called the top-level transaction. This transaction may fork off children, giving
rise tosubtransactions. Each ofthese children may again forkoff itsown children, giving
rise to a further level of subtransactions. When a transaction forks a subtransaction, it is
called theparentofthesubtransactionandthesubtransactionisreferred toasitschild.The
terms ancestors and descendants are also used. A transaction is an ancestor and a
descendant of itself.
Committing of Nested Transactions
In a nested-transactions system, a transaction may commit only after all its descendants
have committed. However, atransaction may abort atany time.Therefore, inorder for an
entire transaction family to commit, its top-level transaction must wait for other
transactions in the family to commit.
Asubtransactionappears atomic toitsparent. That is,theoperations itperforms take
place indivisibly with respect to both failures and concurrent computations just as for
traditional transactions. Therefore, the changes made todata items bythe subtransaction

474 Chap.9 • Distributed FileSystems
become visible to its parent only after the subtransaction commits and notifies this to its
parent.Asaresult, theactual committingofany updates performed bythe subtransaction
iscontingentuponthecommit ofeach ancestor transaction allthe wayuptothetop-level
transaction.
On the other hand, ifa failure occurs that causes a subtransaction toabort before its
completion, all of its tentative updates are undone, and its parent is notified. The parent
may then choose to continue processing and try to complete its task using an alternative
method oritmayabort itself.Therefore, theabortofasubtransaction may notnecessarily
cause its ancestors to abort. However, if afailure causes anancestor transactionto abort,
the updates of all its descendant transactions (that have already committed) have to be
undone. Thus no updates performed within an entire transaction family are made
permanent until the top-level transaction commits. Only after the top-level transaction
commits is success reported to the client.
Advantages of Nested Transactions
Nested-transactions facility is considered to be an important extension to the traditional
transaction facility (especially in distributed systems) due to its following main
advantages:
1. It allows concurrency within a transaction. That is, a transaction may generate
several subtransactionsthatruninparaIJelondifferent processors. Notice thatallchildren
of a parent transaction are synchronized so that the parent transaction still exhibits
serializability.
2. It provides greater protection against failures, in that it allows checkpoints to be
established withinatransaction.This isbecausethesubtransactions ofaparenttransaction
fail independently of the parent transaction and of one another. Therefore, when a
subtransaction aborts, its parent can still continue and may fork an alternative
subtransaction in place of the failed subtransaction in order to complete its task.
9.10 DESIGN PRINCIPLES
Based on hisexperience with theAFS andother distributed file systems, Satyanarayanan
[1992] has stated the following general principles for designing distributed file
systems:
1. Clients have cycles to burn. This principle says that, if possible, it is always
preferable toperform anoperation onaclient'sown machine rather thanperforming iton
aserver machine. Thisisbecause aserver isacommon resource forallclients, andhence
cycles of a server machine are more precious than the cycles of client machines. This
principle aims at enhancing the scalability of the design, since it lessens the need to
increase centralized (commonly used) resources and allows graceful degradation of
system performance as the system grows in size.

Sec.9.11 • Case Study:DeE DistributedFile Service 475
2. Cache wheneverpossible. Betterperformance, scalability, usermobility, and site
autonomy motivate thisprinciple. Caching of dataat clients' sitesfrequently improves
overallsystemperformancebecause itmakesdataavailablewhereveritisbeingcurrently
used, thus savingalarge amountofcomputingtime and networkbandwidth. Cachingalso
enhances scalability because it reduces contention on centralized resources.
3. Exploit usage properties. This principle says that, depending on usage properties
(access and modification patterns), files should be grouped into a small numberofeasily
identifiableclasses, and then class-specificpropertiesshouldbeexploitedfor independent
optimization for improved performance. For example,files known to be frequently read
and modified only once in a while can be treated as immutable files for read-only
replication. Files containing the object code ofsystem programs are good candidates for
this class.
Notice that the use ofdifferent mechanisms for handling files belongingto different
classes for improved performance makes the design ofa file systemcomplex. Hence, for
simplicity of design, some designers prefer to use a single mechanism for handling all
files.
4. Minimize systemwideknowledge andchange.'Thisprincipleisaimed atenhancing
thescalabilityofdesign.Thelargerisadistributedsystem, t.hemoredifficultitistobeaware
at all times of the entire state of the system and to update distributed or replicated data
structuresinaconsistentmanner.Therefore,monitoringorautomaticallyupdating ofglobal
information should be avoided 'as far as practicable. The callback approach for cache
validationandtheuseofnegativerights inanaccesscontrollist(ACL) basedaccesscontrol
mechanism(describedinChapter II)are two instancesoftheapplicationofthis principle.
The useofhierarchicalsystemstructureisalsoanapplicationofthisprinciple.
5. Trustthefewestpossibleentities.This principleisaimed atenhancingthesecurity
ofthe system. For example, itis much simplertoensuresecurity based on theintegrityof
the much smallernumberofservers ratherthan trusting thousands ofclients. In this case,
it is sufficient to only ensure the physical security ofthese servers and the software they
run.
6. Batch if possible. Batching often helps in improving performance greatly. For
example, groupingoperations togethercan improvethroughput, although itisoften atthe
cost oflatency. Similarly, transfer of data across the network in large chunks rather than
as individual pages is much more efficient. The full file transferprotocol isan instanceof
the application ofthis principle.
9.11 CASE STUDY: DCE DISTRIBUTED FilE SERVICE
Two of the popular commercial file systems for distributed computing systems are Sun
Microsystems' Network File System(NFS) and Open Software Foundation's Distributed
File Service(DFS). DFS isone ofthe many servicessupportedby DistributedComputing
Environment (DeE).
As a case study ofhow the concepts and the mechanisms described in this chapter
can be used to build a distributed file system, DFS is briefly described below. A good

476 Chap. 9 • Distributed FileSystems
description ofNFScanbefound in[Khanna 1994,Sandberg 1987,Sandbergetal. 1985].
However, in the following description of DFS, the majordifferences with NFS have also
been pointed out.
DeE'sDFShasseveralattractive features andpromisestoplayamajorroleinfuture
distributed computing environments. It is derived from the CMU Andrew File System
(AFS)butpossesses manynewfeatures.For instance,AFSsupportssession semantics for
shared files, but DFS.supports more accurate single-site UNIX semantics.
DFS isbasically aDeE application that makes useofother servicesof DCE.It uses
DeE threads to handle multiple file access requests simultaneously, Remote Procedure
Call (RPC) for client-server communication during file access operations, Distributed
TimeService (DTS)forsynchronizationoftheclocksofmultipleservers, security service
for client-server authentication and authorization at the time of file access requests, and
directory service toprovide asingle global name space forall files so that any file inthe
entire system can be accessed by any client from any cell by simply specifying the file
name.
DFShas been designed to allow multiple file systems to simultaneously exist on a
nodeofaDeEsystem.Forexample, UNIX, NFS,andDFS's ownfilesystem cancoexist
on a node to provide three different types of file systems to the users of that node. The
local file system of DFS that provides DFS on a single node is called Episode[Chutani
et al. 1992).As may beexpected, when multiple file systems coexist on a node, features
specific to DFS are available only to those users who use the Episode file system. DFS
features are not available to the users of other file systems.
9.11.1 DFS FII.Model
Like UNIX, DFS uses the unstructured file model in which a file is an unstructured
sequence of data. A DFS file server handles the contents of a file as an uninterpreted
sequence of bytes. A single file can contain up to 242 bytes.
Like UNIX, DFS also uses the mutable file model. That is,foreachfile there isjust
onestored sequence that isaltered byupdate operations. Notethat although DFS usesthe
mutable filemodel, ithasafacility called cloning(described later)that allows two stored
sequences of a file to exist simultaneously; one of these is the version of the file before
cloning and the other contains the changes made to the file after cloning.
9.11.2 DFS File Syst.m Model
AsshowninFigure9.20,theDFSfilesystem modelhasfourlevelsofaggregation.Atthe
lowest level are individualfiles.At the next level are directories. Each directory usually
contains several files.Above directories arefilesets.Each fileset usually contains several
directories. Finally, at the highest level is an aggregate that usually contains multiple
filesets. Each disk partition holds exactly one aggregate.
Like a file system of UNIX, a fileset is a group of files that are administered
(moved, replicated, backed up, etc.) as a set. However, unlike UNIX, a fileset is
normally a subtree of a file system and not the entire file system tree. For example,
a fileset may contain all the files of a single user, or all the files of a group of related

Sec. 9.11 • Case Study: DCE Distributed File Service 477
Aggregate
(oneaggregateperdiskpartition)
~I~
FS1 FS2·•• FSj (filesets)
~I~
01 02··· OJ (directories)
~I~
Fig. 9.20 Fourlevels of aggregationinthe
Fk (files)
DFSfile system model.
users. With this difference, DI~S allows multiple filesets per disk partition, a
management advantage over UNIX or NFS, which allow only a single file system per
disk partition. The main advantage is that disk space can be more efficiently utilized
by dynamically rebalancing the space occupancy of different partitions by moving
filesets from nearly full partitions to relatively empty partitions as and when
needed.
9.11.3 DFS File-Accessing Model
Distributed File Service relies on a client-server architecture and uses the data-caching
model for file accessing. A machine in a DCE system is a DFS client, a DFS server, or
both. A DFS client is a machine that uses files in filesets managed by DFS servers. The
main software component of a DFS client machine is the DFS cache manager, which
caches parts of recently used files to improve performance.
On the other hand, a DFS server is a machine having its own disk storage that
manages files in the filescts stored on its local disk and services requests received from
DFS clients. A DFS server machine has the following software components:
1. Episode. This is the DFS local file system.
2. Tokenmanager.Thetoken manger isusedtoimplementthetoken-based approach
for handling multicache consistency problems (this approach is described later).
3. File exporter. The file exporter accepts file access requests from clients and
returns replies tothem.The interactionbetween clients and thefileexporter isdone using
DeE RPC. In addition to handling requests for the Episode files, the file exporter also
handles requests foralltheother file systems that exist onthat node. Italsohandles client
authentication for establishing secure communication channels between clients and the
DFS server. The file exporter is multithreaded so that several client requests can be
handled simultaneously.

478 Chap. 9 • Distributed FileSystems
4. Fileset server.The fileset servermanages the local filesets. It keeps track ofhow
many filesets there are and which fileset belongs to which disk partition. It also provides
commands that can be used by the system administrator to obtain fileset information, to
manipulate disk quotas offilesets, and to create, delete, duplicate, move, backup, clone,
or restore an entire fileset.
5. Fileset location server.The fileset location serverkeeps information about which
DFS servers are managing which filesets in the cell. If a fileset is moved from one DFS
servertoanotheror isreplicatedon anotherDFS server, the fileset locationserverrecords
these changes. Given the name ofa file, the fileset location server returns the address of
the DFS serverthat manages the fileset that contains the file. If a fileset is replicated, the
addressesofall the DFS servers that manage itare returned. When a DFS clientaccesses
a file by specifying its name, its cache manager gets the address ofthe DFS server that
manages the fileset ofthe file from the fileset location server.The cache managercaches
this information for future use.
6. Replication server.The replicationservermaintainstheconsistencyofthereplicas
offilesets.
Ofthe above mentioned six components ofa DFS server, the former three reside in
the kernel space and the latter three reside in the user space.
When a DFS client makes a file access request, the cache manager of the client's
machinefirst checksto see ifthe requested data isalready presentin itscache. If the data
is found in the cache, the file access operation is performed locally without contacting a
DFS server. Otherwise, the cache managerdoes an RPC with the appropriate DFS server
askingforthedata.The data receivedfrom the DFS serveriscachedbythecache manager
for future use.
DFS uses the block-level transfermodel for the unit ofdata transfer. The block size
is64 kilobytes, so many (in some environmentsmost) files will betransferredand cached
in their entirety.
To mention about file accessing in NFS, NFS also uses the data-caching model for
file accessing and the block-level transfer model for the unit ofdata transfer. The block
size in NFS is 8 kilobytes. For performance improvement, in addition to data caching,
NFS also uses a read-ahead mechanism. In this mechanism, after the file system
component on the client machine receives the block that contains the needed data, it
automaticallyissues arequestfor the next block, sothat itwill beavailablelocally incase
it is needed shortly.
9.11.4 DFS FII.-Sharlng S.mantlcs
The strongestfeature ofDFS is that, in spite ofusing the data-caching model, it supports
the single-site UNIX semantics. Thatis, every read operationon a file sees the effects of
all previous write operations performed on that file. This is achieved in the manner
described below.
Recall that each DFS server has a component called token manger. The job ofthe
token manager is to issue tokens to clients for file access requests and to keep track of

Sec.9.11 • Case Study: DeE DistributedFileService 479
which clients have been issued what types of tokens for which files. A client cannot
perform the desired file operation on a piece of file data until it possesses the proper
token.
The use of a token-based approach to implement the single-site UNIX file-sharing
semantics can best be illustrated with the help of an example (see Fig. 9.21). For
simplicity, in this example weassume that there isonly one type of token forall types of
file access operations.
Clientmachine Servermachine Clientmachine
~ 8 6'
(a)
Clientmachine Servermachine Clientmachine
1.Access F 63
1
Client Server
A
Hold.stokenand 2.Tokenand Tokenforfile F
1
dataforfileF
1
dataofF1 giventoclientA
(b)
Clientmachine 4.Revoke Servermachine Clientmachine
tokenfor F 1 3.Access F 1
Client Server Client
A B
5.Updated Tokenforfile F 6.Token and Holdstokenand
dataofF 1 giventoclient B 1 updateddata dataforfile F
(ifany) ofF 1
1
(c)
I4'ig.9.21 Token-basedapproachof DFS for implementingthe UNIX file-sharing
semantics: (a) initial state of aservermachine and two client machines;
(b) state after client Areceives the token and data for file F I; (c) state after
client Breceives the token and data for file F I_
Figure 9.21(a) shows the initial state of a server and two client machines. At this
time, client A makes a request to the server for accessing file Fl' The server checks its
state informationto see ifthe token for file F1has been given to any other client. Itfinds
that it has not been given to any other client, so it sends the token and data of file F to
I
clientAandmakes arecord ofthisinformationforfuture use.ClientAcaches thereceived
file data and then continues to perform file access operations on this data as many times
as needed. Figure 9.21(b) shows the state of the server and client machines afterclient A
receives the token and data for file Fl'

480 Chap.9 • Distributed FileSystems
Now supposeafter sometimethatclient Bmakes arequesttotheserverforaccessing
the same file FJ. The serverchecks its state information and finds that the token for file
F. has been given toclientA.Therefore, itdoes not immediately send the token and data
for file F toclientB.Rather itfirst sends amessagetoclientA asking back the token for
1
fileFl.Onreceivingtherevocationmessage, clientAreturns thetoken along withupdated
data offile F] (if anyupdates were made) and invalidatesits cacheddata for file F). The
serverthen updates its local copy offile F) (if needed) and now returnsthe token and up
to-datedataoffileF1toclient B.ClientBcachesthereceivedfiledata and then continues
to perform file access operations on this data as many times as needed. Figure 9.21(c)
shows thestate ofthe server andclient machinesafter client Breceivesthe token anddata
forfile.F1•Inthisway,thesingle-systemUNIX file-sharingsemanticsisachievedbecause
the serverissues the token for afile to only one client at atime. The clientthat possesses
the token is assured exclusive access to the file.
To maximize performance, better concurrency is achieved by using the following
techniques:
1. Type-specific tokens. Notice that multiple clients that read the same file but
never write on it do not conflict. Therefore, when a client is accessing a file in the
read-only mode, there is no reason why other clients that also want to access the file
in the read-only mode should not be given the token for the file. Hence, instead of
using a single token for all types of operations on a file, type-specific tokens are used.
Separate tokens exist for open, read, write, lock, check file status, and update file
status operations on files. The server knows the rules for token compatibility. That is,
it will issue multiple read tokens for read accesses to a piece of file data but will not
issue any read token or write token for a piece of file data for which a write token
has already been issued to a client.
2. Fine-grained tokens. To minimize the problem of false sharing, tokens for
conflicting operations refer only to a portion of a file instead of the entire file. For
instance, tokens for open, check file status, and update file status operations apply to the
entire file, but tokens for read, write, and lock operations apply only to a portion of a
file.
Every token has an expiration time of 2 minutes. Therefore, if a client does not
respond (eitherdue to a crash or some other reason) to a token revocation message from
aserver, the serverjustwaits for2minutes and then acts as ifthe token has been returned
by the client.
To mention the file-sharing semantics in NFS, NFS does not support the UNIX
semantics for file sharing. It has been widely criticized for having a fuzzy file-sharing
semantics in which a write to a file performed by a client on its cached copy of the
file data mayor may not be seen when another client reads the file, depending on
timing. This is because in NFS each cached data block has a timer of 3 seconds
associated with it. A cached data block is discarded when its timer expires. In
addition, whenever a cached file is opened, the client contacts the server to find out
when the file was last modified. If the last modification was done after the client's

Sec.9.11 • Case Study: DeE DistributedFileService 481
copy was cached, the client discards the old copy from its cache, gets the updated
copy from the server, and caches this copy in its cache. Finally, NFS uses the periodic
write modification propagation scheme with a 30-second interval. That is, once every
30 seconds all the modified blocks of a cache are sent to the server. Due to the use
of timer-based mechanisms for discarding cached entries and for modification propa
gation, NFS does not make good use of data caching.
Another important difference between NFS and DFS that is worth mentioning here
is that NFS allows every machine to be both a client and a server at the same time. That
is, any client may also be a server and any server may also be a client. All machines are
independentandexist indifferentadministrativeenvironments. However, inDFS, aserver
assumes the dependency of clients in the same cell. Therefore, the server-client
relationship in DFS is much more a convention for master-slave than NFS.
9.11.5 File-Caching Scheme InDFS
We have already seen that in DFS recently accessed file data are cached by the cache
manager of client machines. The local disk of a client machine is used for this
purpose. However, in a diskless client machine, the local memory is used for caching
file data.
As shown in Figure 9.21, in DFS, modifications made to a cached file data are
propagatedto the file serveronly when the clientreceivesatoken revocationmessagefor
the file data. The same is true for cache validation scheme. That is, a cached file data of
aclientmachine is invalidated(itscache entry isdiscarded) only when theclientreceives
a token revocation message for the file data from the file server. As long as the client
possessesthe token forthe specifiedoperationonthe filedata, thecacheddata isvalidand
the client can continue to perform the specified operation on it. In effect, the approach
used for cache validation is a server-initiated approach.
The NFS schemes for modification propagation and cache validation have already
been described in the previous section.
9.11.6 ReplicQtion Qnd Cloning inDFS
DistributedFile Serviceprovidesthefacility toreplicatefileson multiplefile servers. The
unit ofreplication is a fileset, That is, all files ofa fileset are replicated together.
The existence of multiple replicas of a file is transparent to normal users (client
applications). That is, a filename is mapped to all file servershaving areplicaofthe file.
Therefore, given afilename, the fileset location serverreturns the addresses ofall the file
servers that have a replica of the fileset containing the file. DFS uses the explicit
replication mechanismfor replication control. That is, the numberofreplicas for a fileset
and their locationsare decidedbythe systemadministrator.The fileset serverhas asingle
command for replicating an entire fileset.
The replication server of a server machine is responsible for maintaining the
consistencyofthe replicas of filesets. The primary-copy protocol isused for this purpose.
Thatis,for each replicatedfileset, one copy isdesignated as the primary copy and all the
others are secondary copies. Read operations on a file in a replicated fileset can be

482 Chap. 9 • Distributed File Systems
performed using any copy of the fileset, primary or secondary. But all update operations
on a file in the fileset are directly performed only on theprimary copy of the fileset.The
replicationserveroftheprimary copyperiodicallysends theupdated versions ofmodified
files to the replication servers of the secondary copies, which then update their own
replicas.of the fileset.
NFS does not provide the facility to replicate files on multiple servers.
In addition toallowing replication of filesets, DFS alsoprovidesthe facility toclone
filesets. This facility allows thecreation ofanew virtualcopyofthefileset inanother disk
partition and the old copy is marked read only. This facility may be used by the system
administrator to maintain an old version of a fileset, allowing the recovery of the old
version of an inadvertently deleted file. For example, the system administrator might
instructthesystem toclone afileseteverydayatmidnight sothatthepreviousday'swork
always remains intact in an old version of all files. If a user inadvertently deletes a file,
he or she can always get the old version of the file and once again perform the current
day's updates on it.
Cloning ofafilesetdoes nottakemuchtime becauseonlyavirtualcopy ofthefileset
is made. That is, only the data structures for the files in the fileset are copied to the new
partition and thefiledataisnotcopied.Theolddata structures intheoriginalpartition are
marked read only. Therefore, both sets of data structures point to the same data blocks.
When a file in the new fileset is updated, new data blocks are allocated for writing the
updated version ofthe data and the corresponding file data structure in the new partition
isupdated topoint tothenewdata blocks.Arequest to update afile intheoriginal fileset
is refused with an error message.
9.11.7 Fault Tolaranc.
In addition to allowing replication of filesets, another important feature of DFS that
helps in improving its fault tolerance ability is the use of the write-ahead log approach
for recording file updates in a recoverable manner. In DFS, for every update made to
a file, a log is written to the disk. A log entry contains the old value and the new value
of the modified part of the file. When the system comes up after a crash, the log is
used to check which changes have already been made to the file and which changes
have not yet been made. Those that have not been made are the ones that were lost
due to system crash. These changes are now made to the file to bring it to a consistent
state. Ifan update is lost because the crash occurred before the log for the update was
recorded on the disk, it does not create any inconsistency because the lost update is
treated as if the update was never performed on the file. Therefore the file is always
in a consistent state after recovery.
Notice that inthe log-based crashrecovery approach used inDFS, the recovery time
isproportional tothe length of thelogand isindependentofthe size of the diskpartition.
Thisallows faster recoverythantraditional systems likeUNIX,inwhichtherecovery time
of a file system is proportional to the size of its disk partition.
For fault tolerance, the main approach used by NFS is to use stateless file servers.
Notice from Figure 9.21 that DFS servers are stateful because they have to keep track of
the tokens issued to the clients.

Sec. 9.]I • Case Study: DeE Distributed File Service 483
9.11.8 Atomic Transactions
The DeE does not provide transaction processing facility either as a part of DFS or
as an independent component. This is mainly because DCE currently does not possess
services needed for developing and running mission critical, distributed on-line
transaction processing (OLTP) applications. For instance, OLTP applications require
guaranteed data integrity, application programming interface with simplified trans
action semantics, and the ability to extend programs to support RPCs that allow
multiple processes to work together over the network to perform a common task. Such
services are not currently supported by DCE. However, users of DeE who need
transaction processing facility can use Transarc Corporation's Encina OLTPtechnol
ogy. Encina expands on the DCE framework and provides a set of standards-based
distributed services for simplifying the construction of reliable, distributed OLTP
systems with guaranteed data integrity. In particular, the services offered by Encina for
distributed OLTP include full data integrity with a transactional two-phase commit
protocol, a high-level application programming interface with simplified transaction
semantics, and additional transactional semantics required for achieving deterministic
results with RPCs.
In addition to Encina, two other transaction processing environments gaining
popularity are Customer Information Control System (CIC'S) and Information Manage
ment System (IMS), both from IBM. CICS is already being used by more than 20,000
customersinmore than 90countriesworldwide. Encinaoffers interoperabilitywith IBM's
CICS. Therefore, CICS can be implemented on top ofthe DCE and Encina technology.
9.11.9 User Interfaces to DFS
Distributed File Service supports the following types ofuser interfaces for differenttypes
of users:
1. File service interface. DFS uses native operating systemcommandsfor directory
and file operations so that users do not need to learnnew commands. For example, users
on UNIX systems will use cd to change directory, Isto list directory contents, mkdir to
create a new directory, and so on. DFS also has several commands that work only for its
own file system (Episode). These includecommands to checkquotas ofdifferentfilesets,
to locate the serverofa file, and so on. To access a file, a client may specify the file by
its global name or by its cell relative name. Details ofthe object-naming mechanism of
DeE are given in Chapter 10.
2. Application programming interface. The application programming interface to
DFS is very similar to UNIX. Therefore, application programmers can use standard file
system calls likefopen () for opening a file,tread() to read from a file,fwrite () to write
to a file,[close () to close a file, and so on. In fact, most existing software will work
immediately by simply recompiling with the DFS libraries.
3. Administrativeinterface.The administrativeinterfaceofDFS providescommands
thatallow the systemadministratortohandle filesets, toinstall orremoveDFSfile servers,

484 Chap. 9 • Distributed FileSystems
andtomanipulateACLsassociated withfilesanddirectories.Thecommandsforhandling
filesets areusedbythesystemadministratortocreate,delete, move,replicate, clone, back
up, or restore filesets. Forexample, the system administratormay move filesetsfrom one
servermachine tootherservermachines tobalancetheloadacrossallfileservermachines
in a cell.
On the other hand, the commands to install or remove file servers allow the system
administrator to dynamically reconfigure the system as needs change. For example, the
system administratormaynoticethattheDFSperformanceofacellisnotsogoodbecause
there are too many clients and only a few file servers. In this case, he or she may install
anewfileserverinthecellandmovesomeofthefilesetsfromalreadyexisting fileservers
of the cell to this file server.
Finally, the commands to manipulate ACLs are used by the system administrator to
revoke some of the access permissions already given to some users or to give additional
permissions to some users.
9.11 SUMMARY
A file is a named object that comes into existence by explicit creation, is immune to
temporary failures in the system, and persists until explicitly destroyed. A file system is
a subsystem of an operating system that performs file management activities such as
organization,storing, retrieval, naming, sharing, and protection of files.Adistributed file
systemisadistributedimplementationoftheclassical time-sharing modelofafilesystem.
Inaddition totheadvantages ofpermanent storageandsharingofinformation provided by
the file system of a single-processor system, a distributed file system normally supports
the following: remote information sharing, user mobility, availability, and diskless
workstations.
The desirable features ofagood distributed filesystem are-transparency (structure
transparency, access transparency, naming transparency, and replication transparency),
user mobility, performance, simplicity and ease of use, scalability, high availability, high
reliability, data integrity, security, and heterogeneity.
From the viewpoint ofstructure, filesareoftwotypes-unstructuredand structured.
On the other hand, according to modifiability criteria, files may be mutable or
immutable.
The twocomplementarymodels for accessing remote filesareremote service model
and data-caching model. In file systems that use the data-caching model, an important
design issue is to decide the unit of data transfer. The four commonly used units for this
purpose are file, block, byte, and record.
In shared files, the file-sharing semantics defines when modifications of file data
made by a user are observable by other users. The four commonly used file-sharing
semantics are UNIX semantics, session semantics, immutable shared-files semantics, and
transaction-like semantics.
Every distributed file system in serious use today uses some form of file
caching because, in addition to better performance, it also contributes to its seal-

Sec.9.12 • Summary 485
ability and reliability. In a distributed file system, a cache may be located in a
server's main memory, a client's disk, Of a client's main memory. Keeping file data
cached at multiple client nodes consistent is an important design issue in distributed
file systems that use client caching. The approaches for handling this issue depend
on the schemes used to propagate modifications made to cached data to the
corresponding file server and to verify the validity of cached data. The write-through
scheme and the delayed-write scheme are the two commonly used schemes for
modification propagation. The cache validation approaches may either be client
initiated or server initiated.
A replicated file is a file that has multiple copies, with each copy located on a
separate file server. Each copy of the set of copies that comprises a replicated file is
referred to as a replica of the replicated file. Replication of files in a distributed system
offers the following potential benefits: increased availability, increased reliability,
improved response time, reduced network traffic, improved system throughput, better
scalability, and autonomous operation. Maintaining consistency among copies when a
replicated fileisupdated isthemajordesign issueofafilesystem thatsupportsreplication
of files. Some of the commonly used approaches to handle this issue are read-only
replication, read-any-write-all protocol, available-copies protocol, primary-copy proto
col, and quorum-based protocols.
Fault tolerance is an important issue in the design of a distributed file system. The
three primary file properties that directly influence theability ofadistributed file system
to tolerate faults are availability, robustness, and recoverability. Replication is a primary
mechanism for improving the availability of a file. Robust files are implemented by
redundancy techniques such as stable storage. Recoverable files are realized by atomic
update techniques.
A server may be implemented by using either a stateful or a stateless service
paradigm. A stateful file server maintains clients' state information from one access
request to the next. On the other hand, a stateless file server does not maintain any
client state information. Therefore, every request from a client must be accompanied
with all the necessary parameters to successfully carry out the desired operation. The
stateless service paradigm makes crash recovery very easy because no client state
information is maintained by the server and each request contains all the needed
information.
An atomic transaction (or just transaction for short) is a computation consisting
of a collection of operations that take place indivisibly in the presence of failures and
concurrent computations. The three essential properties of transactions are atomicity,
serializability, and permanence. A distributed transaction service is an extension of the
traditional transaction service, which can support transactions involving files managed
by more than one server. Nested transactions are a generalization of traditional
transactions in which a transaction may be composed of other transactions called
subtransactions.
The general principles for designing distributed file systems as proposed by
Satyanarayanan are-know that clients have cycles to burn, cache whenever possible,
exploit usage properties, minimize systemwide knowledge and change, trust the fewest
possible entities, and batch if possible.

486 Chap. 9 • Distributed File Systems
EXERCISES
9.1. Inwhataspectsisthedesign ofadistributedfile systemdifferentfrom thatofafilesystemfor
a centralizedtime-sharing system?
9.2. Name the main components of a distributed file system. What might be the reasons for
separatingthe various functions ofadistributed file system into these components?
9.3. In the design ofa distributed file system, high availability and high scalability are mutually
related properties. Discuss.
9.4. Inthe designofadistributedfile system,high performanceand high reliabilityare conflicting
properties. Discuss.
9.5. Whatis an immutablefile? Can a file system be designedto function correctly by using only
immutablefiles? Ifno,explainwhy.Ifyes, explainhow thebasic fileoperations(create,read,
write, delete) can be performed in this file system for shared files.
9.6. Discussthe relativeadvantagesanddisadvantagesofusing full-file cachingand block caching
models for the data-caching mechanism of adistributed file system.
9.7. Ofthefour data transfermodelsthat may beused inadistributedfilesystemthat uses thedata
cachingmodel for file accessing, which models are suitableforeach of the following types of
distributed systems:
(a) A distributed system that supports diskless workstations
(b) A distributed system in which each node has large disk storage space
(c) A distributed system that uses the structured file model in which each file is a group of
records
Ifmore than one model issuitableforaparticularcase, which model will you prefertouseand
why?
9.8. In youropinion, where (in servermemory, inclientdisk, or inclientmemory) shouldacache
for cachingdata be locatedinthe followingtypes ofdistributedfile systems (give reasonsfor
your answer):
(a) One that supports diskless workstations
(b) One that uses the file-level transfer model as unit of data access
(c) One that uses session semantics
(d) One that is designed to occasionally support disconnected operation
(e) One in which the ratio of numberof clients to numberof file servers is very large
(t) One that has to handle fairly large files
(g) One that supports UNIX-like file-sharing semantics
Ifmore than one locationissuitablefor aparticularcase, which one will you preferto useand
why?
9.9. Supposeyou have todesignthecachingschemeofadistributedfile systemthat has tosupport
session semantics. Suggest a suitable solution for each of the following issues in your
design:
(a) Where to locate a cache?
(b) What should be the unit of data caching?
(c) When should modification to a cached data be propagated to its mastercopy?
(d) Whenshouldthevalidationtocheckifacachedcopyofadata isconsistentwith itsmaster
copy be performed?
Give reasons for youranswer.

Chap. 9 • Exercises 487
9.10. A distributed operating system designer is of the opinion that since both replication and
caching of objects provide more or less similar advantages to a distributed system, both
concepts need not beimplementedinthe same distributedsystem. Isheor she correct?Give
reasons foryouranswer. Nowdifferentiateamong thefollowingtypesofdistributedoperating
systems by listing their relative advantages and disadvantages:
(a) One that implements object caching but no object replication
(b) One that implements object replication but no object caching
(c) One that implements both object caching and objectreplication
9.11. Inthedesign of adistributedoperatingsystem, thedata-cachingmechanism may beused for
caching many different types of data. A separate cache can bemaintained for each type of
data. Inyour opinion, isitnecessary toalways keep acached data uptodate? Ifyes,explain
why. If no, give an example in which a system can function correctly even when processes
access cached data that are not always up to date.
9.12. Suppose a file system uses the client-initiated approach for validating the contents of client
caches. Also suppose that the validity check is performed by comparing the time of last
modificationofthecached version ofthedata with theserver'smastercopy version.This file
system will not function correctly in a system in which the clocks of various nodes are not
synchronized. Suggestascheme that can beused with theclient-initiatedapproachforcache
validation insystems in which clocks of various nodes are not synchronized.
9.13. Differentiate among the following properties of adistributed file system:
(a) High degree of availability
(b) High degree of recoverability
(c) High degree of robustness
Name a suitable mechanism that may be used for implementing each of these properties.
9.14. Explain how a stable-storage system converts fallible disks into reliable devices. In the
discussion in this chapter, we saw a stable-storage system designed with two conventional
disks. Can this idea be extended to three disks for better crash resistance capability? If no,
explain why. If yes, explain how.
9.15. A stateful file serverrecords state information for its clients. What problems are associated
with this type of file server? Give two examples where it might be necessary to use stateful
file servers.
9.16. Adistributed system based on the workstation-servermodel provides on-line help facility to
its users by allowing the users to read the on-line manuals by using the mancommand.
Suggest different possible locations for storing the manuals in the system and discuss their
relative advantages and disadvantages.
9.]7. What is a transaction? What are the two main factors that threaten the atomicity of
transactions? Describe how atomicity is ensured for a transaction in both commit and
abort.
9.18. Why are transactions needed in a file service? Give suitable examples to illustrate how
transactions help in doing the following:
(a) Improving the recoverability of files in the event of failures
(b) Allowing the concurrent sharing of mutable files by multiple clients in a consistent
manner
9.19. Discuss theneed forserializabilityproperty intransactions.What isthemaingoalindevising
amechanismforsatisfyingthisproperty?Describeatleastthreemechanismsthatmaybeused
in the implementation of a transaction facility for satisfying this property. Now compare the
mechanisms described by you to show how close each of them is to the main goal.

488 Chap. 9 • Distributed File Systems
9.20. Give two methods that may be usedinthedesign ofafilesystem torecord updates toa file
in areversible manner.That is, thefile system provides theflexibility tothe users tocancel
theupdatesmadetoafilewithinanopen-close sessionandrevertthefilebacktothestatethat
it was in before thestart of the session.
9.21. Let theinitial balance inall the three accounts ofthetwobanking transactions ofFigure 9.9
be$100. For these two transactions, enumerate all the schedules thatproduce different final
values ofz. Which of these are legal schedules?
9.22. An application consists of three transactions T}, T 2, and T 3 that are defined below:
T}: begin_transaction
read(x); readtz); write(x-5); write(z+5);
end_transaction
T 2: begin_transaction
readiz); write(z-8); read(y); write(y+8);
end_transaction
T 3: begin_transaction
read(x); write(x+4); read(y); write(y-4);
end_transaction
Describe how the concurrency of these three transactions can be controlled by using the
following:
(a) Same type of locksfor both read and write operations
(b) Type-specific locks
(c) Intention-to-write locks
(d) Optimistic concurrency control scheme
(e) Timestamp-based concurrency control scheme
9.23. What are serially equivalent schedules? For the three transactions of the previous exercise
give at least six schedules that are serially equivalent.
9.24. Figure 9.10 shows two schedules of theoperations of thetwotransactions of Figure 9.9 that
produced unexpected final results. Show that ifboth transactions use the two-phase locking
protocol, these schedules produce correct results.
9.25. Prove that if all transactions ·of an application use the two-phase locking protocol, all
schedules formed by interleaving their operations are serializable.
9.26. What isfalse sharing? Discuss theimportance ofgranularity oflocksincombating thefalse
sharing problem.
9.27. What is a transaction deadlock? Give an example to illustrate how a transaction deadlock
occurs when:
(a) The same type of locks are used for both read and write operations.
(b) Type-specific locks are used.
Nowgiveamethodthatmaybeusedtoavoidtransaction deadlocks andapply themethodto
your examples to show how it prevents the deadlocks thatoccurred incases (a) and (b).
9.28. Suppose the optimistic concurrency control mechanism is usedinthe implementation ofthe
transaction mechanism of a file system. Give two examples of concurrently executing
transactions in this system:
(a) One in which the validation check is successful for all transactions and they are
committed successfully
(b) One in which the validation check fails and a transaction has to beaborted

Chap.9 • Bibliography 489
9.29. Inwhatmanneristhetimestamp-basedconcurrencycontrolschemebetterthantheoptimistic
concurrency control scheme? For the example giyen by you for case (b) of the previous
exercise, usethetimestamp-basedconcurrencycontrolschemeandshowthatthetransaction
thatwasabortedcannot complete itsfirstphase inthiscase.
9.30. Whatadvantagesdoesnestedtransactionsfacilityhaveovertraditionaltransactionfacilityin
a distributed system?Answer thefollowing questions fora nestedtransactionsfamily:
(a) Can atransactioncommit independentofother transactions inthefamily?
(b) Can atransaction abort independentof other transactions inthefamily?
(c) When do the changes made to data items by a transaction become visible to other
transactions inthe family?
(d) What happens ifa transaction inthefamilyfails?
(e) Whenare the updates performed byatransaction madepermanent?
(f) Whendoes theentire transactionfamilycommit and successisreportedtotheclient?
Givereasonsfor your answer.
9.31. Inadistributedsystembasedontheworkstation-servermodel,aservermachineisdedicated
toworkasafileserver.Thefilesystemusestheremoteservicemodelforprocessingclients'
requests. With the passage of time, the number of workstations gradually increases in the
system, and it isfound thatthesystem performancehasdegraded becausethefile server is
oftenoverloadedbyrequestsfromclients.Asanexpertofdistributedoperatingsystems,you
are contacted to solve this problem. Describethree different solutionsthat may be used to
solvethis problem.
9.32. Describethetwo-phase,multiservercommitprotocol.Inthebankingtransactiongivenbelow,
supposeaccountsXandYbelongtotwodifferentbranchesofabank.Thelocalrecordsofthe
two branches are managed byfileservers51 and52, respectively.
a1:begin_transaction
a2: read balance(x) ofaccountX
a3:read balance (y) of account Y
a4: write (x-5) to accountX
as: write(y+5) to account Y
a6: end_transaction
For the above transaction, if S1 is the coordinator and 52is the worker, give the list of
messagesexchangedamongtheclientandfileservers5) andS2forthesuccessfulexecution
ofthis transaction. Whatwill happenif52crashes afterperformingoperationas?
BIBLIOGRAPHY
[Agrawal and Jalote 1995]Agrawal, G., and Jalote, P.,"Coding-Based ReplicationSchemes for
DistributedSystems,"IEEE Transactions onParalleland DistributedSystems, Vol.6,No.3,pp.
240-251 (1995).
[Ahamadetal.1991]Ahamad,M.,Ammar,M.H.,andCheung,S.Y.,"Multi-dimensionalVoting,"
ACMTransactionsonComputerSystems, pp.399-431(November1991).
[Barghouti and Kaiser 1991] Barghouti, N. S., and Kaiser, G. E., "Concurrency Control in
Advanced Database Applications," ACM Computing Surveys, Vol. 23, No.3, pp. 269-318
(1991).
[Bernstein and Goodman 1981]Bernstein, P.A., and Goodman, N., "Concurrency Control in
DistributedDatabaseSystems,"ACMComputingSurveys,Vol.13,No.2, pp.185-221 (1981).

490 Chap. 9 • Distributed File Systems
[Bernstein and Goodman 1984] Bernstein, P. A., and Goodman, N., "An Algorithm for
ConcurrencyControl andRecovery inReplicated DistributedDatabases,"ACMTransactions on
DatabaseSystems, Vol.9,No.4, pp. 596-615 (1984).
[Bernsteinetal. 1980]Bernstein,P.A.,Shipman, D.W.,andRothnie,1.B.,"ConcurrencyControl
in aSystem for DistributedDatabases(SDD-l),"ACMTransactionsonDatabase Systems, Vol.
5, No.1, pp. 18-51 (1980).
(Bernstein et al, 1987]Bernstein, P.A., Hadzilacos, V., and Goodman, N., Concurrency and
Recovery in Database Systems, Addison-Wesley, Reading, MA, pp. 289-307 (1987).
[Brown et al. 1985]Brown, M., KoJling, K., and Taft, E., "The Alpine File System," ACM
Transactions on Computer Systems, Vol.3, No.4, pp. 261-293 (1985).
[Chen et al. 1995)Chen, K.,Bunt, R. B., and Eager, D.L.,"Write Caching in Distributed File
Systems,"In:Proceedingsofthe 15thIEEEInternationalConferenceonDistributedComputing
Systems, IEEE, New York,NY(May-June 1995).
[Chutani et al. 1992]Chutani,S.,Anderson, o.T.,Kazar, M.L.,Leverett, B. W.,Mason, W.A.,
andSidebotham,R.N.,"The EpisodeFileSystem,"In:Proceedingsofthe1992 USENIX Winter
Conference. USENIX Association, Berkeley, CA, pp. 43-60(1992).
[Claybrook1992]Claybrook, B.,OLTP:OnlineTransactionProcessingSystems, JohnWiley,New
York(1992).
[Coulouris et ale1994] Coulouris, G. F., Dollimore, J., and Kindberg, T., Distributed Systems
Concepts and Design, 2nd ed.,Addison-Wesley, Reading, MA(1994).
[Davcevand Burkhard 1985]Davcev,D., and Burkhard, W.A., "Consistency and Recovery
Control forReplicated Files," In:Proceedingsofthe10thACMSymposiumonOperatingSystems
Principles, Association for Computing Machinery, New York, NY, pp. 87-96 (December
1985).
[Dion1980JDion, 1, "The CambridgeFile Server," ACMOperating Systems Review, Vol.14,No.
4, pp. 26-35 (1980).
[Eswaran etal, 1976JEswaran, K.P.,Gray,1 N.,Lorie, R.A.,andTraiger,I. L., "The Notions of
Consistencyand PredicateLocks inaDatabase System,"Communications oftheACM, Vol.19,
No. 11,pp. 624-633 (1976).
[Fridrich and Older 1985J Fridrich, M., and Older, W., "Helix: The Architecture of the XMS
Distributed File System," IEEE Computer, pp. 21-29 (1985).
[GilTord19798]Gifford, D. K.,"Weighted VotingforReplicated Data," In:Proceedings ofthe 7th
ACMSymposium onOperating Systems Principles, Association forComputingMachinery, New
York,NY,pp. 150-159 (December 1979).
[Gifford 1979b]Gifford, D.K.,"Violet:AnExperimentalDecentralizedSystem,"ACMOperating
Systems Review, Vol. 13,No. 5 (1979).
[GilTord et al, 1988]Gifford, D. K., Needham, R. M., and Schroeder, M. D., "The Cedar File
System," Communications oftheACM, Vol.31, No.3, pp. 288-298 (1988).
[Goscinski 1991]Goscinski, A., Distributed Operating Systems, The Logical Design, Addison
Wesley, Reading, MA (1991).
[Gray 1978]Gray, J. N., "Notes on Database Operating Systems," Lecture Notes in Computer
Science, Vol.60, Springer-Verlag, Berlin, pp. 393-481 (1978).
[Gray and Cheriton ·1989] Gray, C., and Cheriton, D., "Leases: An Efficient Fault-Tolerant
Mechanism for Distributed File System Consistency," In: Proceedings of the 11th ACM
Symposium on Operating Systems Principles, AssociationforComputingMachinery, NewYork,
NY,pp. 202-210 (1989).

Chap. 9 • Bibliography 491
(Gray and Reuter 1993] Gray, 1., and Reuter, A., Transaction Processing: Concepts and
Techniques, Morgan Kaufmann, San Francisco, CA(1993).
[Gray etal.1981]Gray,1.N.,Melones, P.,Blasgen,M.W.,Lorie,R.A.,Price,T.G., Putzulu,G.
P., and Traiger, I. L., "The Recovery Manager of the System R Database Manager," ACM
Computing Surveys, Vol. 13, No.2, pp. 223-242 (1981).
[HarderandReuter1983]Harder,T.,and Reuter, A.,"PrinciplesofTransaction-OrientedDatabase
Recovery," Computing Surveys, Vol. 15, No.4 (1983).
[Herlihy ]986]Herlihy, M., "A Quorum-ConsensusReplication MethodforAbstractDataTypes,"
ACMTransactions on ComputerSystems, Vol.4, No.1, pp. 32-53 (1986).
[Howardetale1988]Howard,1.H.,Kazar, M.L.,Menees,S.G.,Nichols,D.A.,Satyanarayanan,
M., Sidebotham, R.N.,and West, M.1.,"Scale and Performance in a DistributedFileSystem,"
AC'MTransactions on ComputerSystems, "01.6, No.1, pp. 51-81 (1988).
[Israel etal, 1978]Israel,J. E., Mitchell,J.G., and Sturgis,H.E., "SeparatingDatafrom Function
in a Distributed File System," In: D. Lanciaux (Ed.), Operating Systems: Theory and Practice,
North Holland,Amsterdam, pp. 17-22 (1978).
[Jalote 1994]Jalote, P.,Fault Tolerance in DistributedSystems, Prentice-Hall, Englewood Cliffs,
NJ(1994).
[Kazaretal, 1990]Kazar, M.L., Leverett, B.W.,Anderson, O.T.,Apostolides, V.,Bottos, B.A.,
Chutani, S., Everhart, C. F.,Mason, W.A., Tu, S. T., and Zayas, E. R., "Decorum File System
Architectural Overview," In: Proceedings ofthe 1990 USENIX Summer Conference, USENIX
Association, Berkeley, CA, pp. 151-163 (1990).
[Khanna 1994) Khanna, R. (Ed.), Distributed Computing: Implementation and Management
Strategies, Prentice-Hall, Englewood Cliffs, NJ (1994).
[Kistler 1995] Kistler, J.J.,DisconnectedOperationinaDistributedFile System, Springer-Verlag,
New York, NY (1995).
[Kistler and Satyanarayanan 1992] Kistler, J. J., and Satyanarayanan, M., "Disconnected
Operation in the Coda File System," ACMTransactions on Computer Systems, Vol. 10, No.1
(1992).
[Kotzand Ellis1993] Kotz, D., and Ellis, C. S., "Cachingand Writeback Policies in Parallel File
Systems," Journal ofParalleland DistributedComputing, Vol. 17,Nos. 1and 2,pp. 140-145
(1993).
[Kumar 1991J Kumar, A., "Hierarchical Quorum Consensus: A New Algorithm for Managing
Replicated Data," IEEETransactions on Computers,Vol.40, No.9, pp. 996-1004 (1991).
(Kung and Robinson 1981J Kung, H. T., and Robinson, J. T., "00 Optimistic Methods for
Concurrency Control," ACM Transactions on Database Systems, Vol. 6, No.2, pp. 213-226
(1981).
[Ladin et al, 1992]Ladin, R.,Liskov, B.,Shrira, L., and Ghemawat,S., "Providing Availability
UsingLazyReplication,"ACMTransactions on ComputerSystems,Vol. 10,No.4,pp. 360-391
(1992).
(Lampson1981]Lampson,B.W.,"AtomicTransactionsinDistributedSystems-Architectureand
Implementation," In: B. W. Lampson, M.Paul, and H. 1. Siegart (Eds.),AnAdvancedCourse,
Lecture Notes in Computer Science, Vol. 105,Springer-Verlag, New York, NY, pp. 246-264
(1981).
(Lazowska et al. 1986] Lazowska, E. D., Zahorjan, J., Cheriton, D., and Zwaenepoel, W., "File
AccessPerformanceofDisklessWorkstations,"ACMTransactionson ComputerSystems,Vol.4,
No.3, pp. 238-268 (1986).

492 Chap. 9 • Distributed File Systems
[Leachet al. 1983]Leach, ~ 1.,Levine, P.H., Douros, B. P.,Hamilton,J.A., Nelson, D. L., and
Stumpf, B. L., "TheArchitecture ofan Integrated Local Network," IEEE Journal on Selected
Areas in Communication, Vol.SAC-I, No.5, pp. 842-857 (1983).
[Levine1986]Levine, P.H., "TheApolloDOMAINDistributedFile System," In Y.Parkeret al.,
(Eds.),DistributedOperatingSystems: TheoryandPractice,NATOASI Series,Springer-Verlag,
New York, NY, Vol.F28, pp. 241-260 (1986).
[Levyand Silberschatz 1990]Levy, E.,and Silberschatz,A.,"DistributedFile Systems: Concepts
and Examples,"ACMComputing Surveys, Vol.22, No.4, pp. 321-374 (1990).
[Liskovetal,1991]Liskov,B.,Ghemawat,S.,Gruber,R.,Johnson,P,Shrira,L.,andWilliams,M.,
"Replication in the Harp File System," In: Proceedings of the 13th ACM Symposium on
Operating Systems Principles, Association for Computing Machinery, New York, NY, pp.
226-238 (1991).
[LockhartJr.1994]Lockhart,Jr.,H.W.,OSFDCE:GuidetoDevelopingDistributedApplications,
IEEEComputerSociety Press, Los Alamitos, CA (1994).
[Lyonetal.1985]Lyon,B.,Sager,G.,Chang,J.M.,Goldberg,D.,Kleiman,S.,Lyon,T.,Sandberg,
R.,Walsh, D., and Weiss, ~, "Overviewof the SUN Network File System," In: Proceedings of
the USENIXConference, USENIXAssociation, Berkeley, CA, pp. 1-8 (January 1985).
[McKusick et al. 1985] McKusick, M. K., Karels, M. J., and Leffler, S. J., "Performance
Improvementsand Functional Enhancements in4.3BSD," In: Proceedingsofthe 1985USENIX
Summer Conference, USENIX Association, Berkeley, CA, pp. 519-531 (1985).
[MitchellandDion1982]Mitchell,1.G.,andDion,1.,"AComparisonofTwoNetwork-Based File
Servers," Communications of(heACM, Vol.25, No.4, pp. 233-245 (1982).
[Morrisetal,1986]Morris,1.H.,Satyanarayanan,M.,Conner, M.H.,Howard,1.H.,Rosenthal,
D. S. H., and Smith, F. D., "Andrew: A Distributed Personal Computing Environment,"
Communications oftheACM, Vol.29, No.3, pp. 184-201 (1986).
[Moss1985]Moss, E.,Nested Transactions,AnApproach toReliableDistributed Computing,MIT
Press, Cambridge, MA (1985).
[Mueller et al. 1983]Mueller, E. T., Moore, 1. D., and Popek, G. J., "A Nested Transaction
Mechanism for LOCUS," In: Proceedings ofthe 9th ACM Symposium on Operating Systems
Principles, Association for Computing Machinery, New York, NY,pp. 71-85 (1983).
[Mullender and Tanenbaum 1984] Mullender, S. J., and Tanenbaum, A. S., "Immediate Files,"
Software Practice and Experience, Vol. 14,No.4, pp. 365-368 (1984).
[Mullenderand Tanenbaum 1985] Mullender, S.1., and Tanenbaum,A. S., "A Distributed File
ServiceBasedonOptimisticConcurrencyControl,"In:Proceedingsofthe10thACMSymposium
on Operating Systems Principles, Association for Computing Machinery, New York, NY, pp.
51-62 (December 1985).
[Needham and Herbert 1982] Needham, R. M., and Herbert, A. J., the Cambridge Distributed
Computing System,Addison-Wesley, Reading, MA (1982).
[Nelsonet al, 1988] Nelson, M. N., Welch, B. B., and Ousterhout, 1. K., "Caching in the Sprite
Network File System," ACM Transactions on Computer Systems, Vol.6, No. I, pp. 134-154
(1988).
[Ousterhoutetal. 1985] Ousterhout,J.K.,Costa,D., Harrison, D., Kunze, J.A., Kupfler, M.,and
Thompson,J.G., "ATrace-DrivenAnalysisofthe UNIX 4.2 BSDFile System,"In: Proceedings
ofthe 10th.ACM Symposium on Operating Systems Principles, Association for Computing
Machinery, New York, NY,pp. 15-24 (December 1985).

Chap. 9 • Bibliography 493
[Popek and Walker 1985] Popek, G. 1., and Walker, B. 1., The LOCUS Distributed System
Architecture, MITPress, Cambridge, MA(1985).
[Ramamritham and Chrysanthis 1996] Ramamritham, K., and Chrysanthis, P. K. (Eds.),
Advances in Concurrency Control and Transaction Processing, IEEE Computer Society Press,
LosAlamitos, CA (1996).
[Rangarajanetale1995]Rangarajan, S.,Setia,S.,andTripathi,S.K.,"AFault-TolerantAlgorithm
forReplicated Data Management,"IEEE Transactions on ParallelandDistributedSystems, Vol.
6, No. 12,pp. 1271-1282 (1995).
[Reed 1983] Reed, D. P., "Implementing Atomic Actions on Decentralized Data," ACM
Transactions on ComputerSystems, Vol.1,No.1, pp. 3-23 (1983).
[Rifkinet al. 1986]Rifkin, A. P.,Forbes, M. P.,Hamilton, R. L., Sabrio, M., Shar, S., and Yueh,
K., "RFS Architectural Overview," In: Proceedings ofthe USENIX Conference, Atlanta, GA,
USENIXAssociation, Berkeley,CA, pp. 248-259 (1986).
[Rosenberry et al. 1992] Rosenberry, W., Kenney, D., and Fisher, G., OSF DISTRIBUTED
COMPUTING ENVIRONMENT, Understanding DeE, O'Reilly, Sebastopol, CA (1992).
[Sandberg 1987]Sandberg, R., "The Sun Network File System: Design, Implementation and
Experience," In: Proceedings of the USENIX Summer Conference, USENIX Association,
Berkeley,CA, pp. 300-314 (June 1987).
[Sandbergetale1985]Sandberg, R.,Goldberg,D.,Kleinman,S.,Walsh,D.,andLyon,B.,"Design
andImplementationoftheSlJN Network FileSystem," In:Proceedingsofthe USENIXSummer
Conference, Portland, OR, USENIX Association, Berkeley,CA, pp. 119-130(June 1985).
[Santifaller 1994]Santifaller, M.,TCPI1PandONCINFS,lntemetworkinginaUNIXEnvironment,
2nd ed., Addison-Wesley, Reading, MA (1994).
[Satyanarayanan 1990a]Satyanarayanan, M., "A Survey of Distributed File Systems," Annual
Review ofComputer Science, Vol.4, pp. 73-]04 (1990).
[Satyanarayanan1990b]Satyanarayanan, M.,"Scalable, Secure,andHighlyAvailableDistributed
FileAccess," IEEE Computer, Vol.23, No.5, pp. 9-21 (May 1990).
[Satyanarayanan 1992]Satyanarayanan, M., "The Influence of Scale on Distributed File System
Design," IEEE Transactions on Software Engineering, Vol.18,No. 1(January 1992).
lSatyanarayanan 1993]Satyanarayanan, M., "Distributed File Systems," In: S. Mullender (Ed.),
Distributed Systems, 2nd ed., Association for Computing Machinery, New York, NY, pp.
353-383 (1993).
[Satyanarayananetal. 1985]Satyanarayanan, M.,Howard,1.H.,Nichols, D.A.,Sidebotham, R.
N.,Spector,A. Z., and West,M.J., "The ITC Distributed File System: Principles and Design,"
In: Proceedings ofthe 10thACMSymposium on Operating Systems Principles, Association for
Computing Machinery, New York,NY,pp. 35-50(December 1985).
[Satyanarayananetale1990]Satyanarayanan, M.,Kistler,1.1.,Kumar,P.,Okasaki,M.E.,Siegel,
E. H., and Steere, D. C., "Coda: A HighlyAvailable File System for aDistributed Workstation
Environment," IEEE Transactions on Computers, Vol.39, No.4, pp.447-459 (April 1990).
[Schroeder et al. 1985]Schroeder, M. D., Gifford, D. K., and Needham, R.M., "A Caching File
System for a Programmer's Workstation," In: Proceedings ofthe 10th ACM Symposium on
Operating Systems Principles, Association forComputing Machinery,NewYork,NY,pp.25-34
(December 1985).
[SilberschatzandGalvin1994]Silberschatz,A.,andGalvin,P.B.,OperatingSystemConcepts,4th
ed., Addison-Wesley, Reading, MA (1994).

494 Chap. 9 • Distributed File Systems
[Singhal andShivaratri1994]Singhal,M.,andShivaratri,N.G.,AdvancedConcepts inOperating
Systems, McGraw-Hili, New York(1994).
[Smith 1982] Smith, A. 1., "Cache Memories," ACM Computing Surveys, Vol. 14, No.3, pp.
473-530 (1982).
[Stalling 1995] Stalling, W., Operating Systems, 2nd ed., Prentice-Hall, Englewood Cliffs, NJ
(1995).
[Sturgiset al, 1980] Sturgis, H., Mitchell,1.G., and Israel, 1.,"Issues in the Design and Use of a
Distributed File System,"ACMOperating Systems Review, Vol.14,No.3, pp. 55-69 (1980).
[Svobodova 1984] Svobodova, L., "File Servers for Network-Based Distributed Systems,"ACM
Computing Surveys, Vol. 16,No.4, pp. 353-398 (1984).
[Tanenbaum 1987JTanenbaum,A.S., Operating Systems: Design and Implementation, Prentice
Hall, Englewood Cliffs, NJ (1987).
[Tanenbaum 1995] Tanenbaum, A. S., DistributedOperating Systems, Prentice-Hall, Englewood
Cliffs, NJ(1995).
[Terry 1987]Terry,D.B.,"CachingHints inDistributedSystems,"IEEE Transactions onSoftware
Engineering, Vol.SE-13, No.1, pp. 48-54 (1987).
[Tichy and Ruan 1984] Tichy, W. F., and Ruan, Z., "Towards a Distributed File System," In:
ProceedingsoftheSummer USENIXConference, USENIX Association,Berkeley, CA,pp.87-97
(June 1984).
[Tomlinson et al, 1985J Tomlinson, G. M., Keeffe, D., Wand, I. C., and Wellings, A. 1., "The
PULSE Distributed File System," Software Practice and Experience, Vol. 15, No. 11, pp.
1087-1101 (1985).
[ThrekandShasha1992]Turek,1.,andShasha, D.,"TheMany FacesofConsensusinDistributed
Systems," IEEE Computer, Vol.25, No.6, pp. 8-17 (J992).
[Van Renesse and Tanenbaum 1988] Van Renesse, R., and Tanenbaum, A. S., "Voting with
Ghosts," In: Proceedings ofthe 8th IE~EE International Conference on Distributed Computing
Systems, IEEE Press, New York,NY,pp. 456-461 (June 1988).
[Weihl 1993] Weihl, W. E., "Transaction-Processing Techniques," In: S. Mullender (Ed.),
Distributed Systems, 2nd ed., Association for Computing Machinery, New York, NY, pp.
329-352 (1993).
[Weikum 1991] Weikum, G., "Principlesand Realizationof Multilevel Transaction Management,"
ACMTransactions on Database Systems, Vol. ]6, No. r, pp. ]32-]40(1991).
POINTERS TO IIIUOGRAPHIES ON THE INTERNET
Bibliographies containing references on Distributed File Systems can be found at:
ftp:ftp.cs.umanitoba.calpub/bibliographieslDistributed/distfs.html
ftp:ftp.cs.umanitoba.calpub/bibliographieslDistributed/dist.sys.html
Bibliography containing references on Object Replication in DistributedSystems can be
found at:
ftp:ftp.cs.umanitoba.calpub/bibliographieslDistributed/Dist.Sys.html

Chap.9 • Pointers toBibliographies ontheInternet 495
Bibliography containing references on AFS (Andrew File System) can be found at:
http:www.transarc.comlProductIAFSIFAQ/faq.html#-sub6

10
CHAPTER
Naming
10.1 INTRODUmON
Adistributed systemsupportsseveraltypesofobjectssuchasprocesses,files,I/Odevices,
mailboxes,andnodes.Thenamingfacility ofadistributed operatingsystemenables users
and programs to assign character-string names to objects and subsequently use these
names to refer to those objects. The locating facility, which is an integral part of the
namingfacility,mapsanobject's nametotheobject'slocationinadistributed system.The
naming and locating facilitiesjointly form a naming system that provides the users with
an abstraction of an object that hides the details of how and where an object is actually
located inthe network.It provides afurther level ofabstraction whendealing withobject
replicas. Given an object name, it returns a set of the locations of the object's replicas.
The naming system plays a very important role in achieving the goal of location
transparency in a distributed system.In addition to facilitating transparent migration and
replication of objects, the naming system also facilitates object sharing. If various
computations want to act upon the same object, they are enabled to do so by each
containing anamefortheobject.Although thenamescontained ineachcomputation may
not necessarily be the same, they are mapped to the same object in this case.
This chapter presents a description of the various approaches in the design and
implementation of naming systems for distributed systems.
496

Sec. 10.2 • Desirable Features of aGood Naming System 497
10.1 DESIRABLE FEATURES OF AGOOD NAMING SYSTEM
A good naming system for a distributed system should have the features described
below.
1. Location transparency. Location transparency means that the name ofan object
should not reveal any hint as to the physical location ofthe object. That is, an object's
name shouldbe independentofthe physicalconnectivityor topology ofthe system,or the
current location ofthe object.
2. Location independency. For performance, reliability, availability, and security
reasons, distributed systems provide the facility of object migration that allows the
movement and relocation ofobjects dynamically among the various nodes of a system.
Location independency means that the name ofan object need not be changed when the
object's location changes. Furthermore, a user should be able to access an object by its
same name irrespective of the node from where he or she accesses it. Therefore, the
requirementoflocationindependencycalls foraglobal namingfacility with the following
two features:
• An object at any node can be accessed without the knowledge of its physical
location (location independency of request-receiving objects).
• An object at any node can issue an access request without the knowledge of its
own physical location (location independency of request-issuing objects). This
property is also known as user mobility.
A location-independent naming systern must supportadynamic mapping scheme so
that it can map the same object name to different locations at two different instances of
time. Therefore, location independency is a stronger property than location transparency
[Levy and Silberschatz 1990].
3. Scalability. Distributed systems vary in size ranging from one with a few
nodes to one with many nodes. Moreover, distributed systems are normally open
systems, and their size changes dynamically. Therefore, it is impossible to have an a
priori idea about how large the set of names to be dealt with is liable to get. Hence
a naming system must be capable of adapting to the dynamically changing scale of a
distributed system that normally leads to a change in the size of the name space. That
is, a change in the system scale should not require any change in the naming or
locating mechanisms.
4. Uniformnaming convention. In many existingsystems,different ways ofnaming
objects, called naming conventions, are used for naming different types of objects. For
example, file names typically differ from user names and processnames. Instead of using
such nonuniformnamingconventions,agood namingsystemshouldusethe same naming
convention for all types ofobjects in the system.

498 Chap. 10 • Naming
5. Multiple user-defined names for the same object. For a shared object, it is
desirable that different users of the object can use their own convenient names for
accessing it. Therefore, a naming system must provide the flexibility to assign
multiple user-defined names to the same object. In this case, it should be possible for
a user to change or delete his or her name for the object without affecting those of
other users.
6. Group naming. A naming system should allow many different objects to be
identified by the same name. Such a facility is useful to support broadcast facility or to
group objects for conferencing or other applications.
7. Meaningful names. A name can be simply any character string identifying
some object. However, for users, meaningful names are preferred to lower level
identifiers such as memory pointers, disk block numbers, or network addresses. This
is because meaningful names typically indicate something about the contents or
function of their referents, are easily transmitted between users, and are easy to
remember and use. Therefore, a good naming system should support at least two
levels of object identifiers, one convenient for human users and one convenient for
machines.
8. Performance. The most important performance measurementof a naming system
istheamount oftimeneededtomapanobject'snametoitsattributes, suchasitslocation.
In a distributed environment, this performance is dominated by the number of messages
exchanged during the name-mapping operation. Therefore, a naming system should be
efficient in the sense that the number of messages exchanged in a name-mapping
operation should be as small as possible.
9. Fault tolerance. A naming system should be capable of tolerating, to some
extent, faults that occur due to the failure of a node or a communication link in a
distributed system network. That is, the naming system should continue functioning,
perhaps in a degraded form, in the event of these failures. The degradation can be in
performance, functionality, or both but should be proportional, in some sense, to the
failures causing it.
10. Replication transparency. In a distributed system, replicas of an object are
generally createdtoimproveperformance andreliability.Anaming system should support
the use of multiple copies of the same object ina user-transparent manner.That is, if not
necessary, a user should not be aware that multiple copies of an object are in use.
11. Locating the nearest replica. When a naming system supports the use of
multiple copies of the same object, it is important that the object-locating mechanism
of the naming system should always supply the location of the nearest replica of the
desired object. This is because the efficiency of the object accessing operation will be
affected if the object-locating mechanism does not take this point into consideration.
This is illustrated by the example given in Figure 10.1, where the desired object is
replicated at nodes N 2, N 3, and N 4 and the object-locating mechanism is such that it
maps to the replica at node N 4 instead of the nearest replica at node N 2. Obviously
this is undesirable.

Sec.10.3• Fundamental Terminologies andConcepts 499
c O:~tt--
__..Object
---
----------------------~--
Iftheobjectlocatingmechanism mapstonode N4instead ofnode N2?
Fig. 10.1 Illustrating theimportance of locating the nearest replica ofanobject.
12. Locating all replicas. In addition to locating the nearest replica of an object, it
is also important from areliability point of view that all replicas ofthe desired object be
located bythe object-locating mechanism. Aneed forthis property isillustratedinFigure
10.2, where the nearest replica at node N5 is currently inaccessible due to a
communication link failure in the network. In this case, another replica of the object at a
farther node N can be used.
4
Clientnode
Object
Ns
Fig. 10.2 Illustrating the importance of locating allthe replicasofanobject.
10.3 FUNDAMENTAL TERMINOLOGIES AND CONCEPTS
The naming system is one of the most important components of a distributed operating
system because it enables other services and objects to be identified and accessed in a
uniform manner. Inspiteoftheimportanceofnames, nogeneral unified treatmentofthem
exists in the literature. This section defines and explains the fundamental terminologies
and concepts associated with object naming in distributed systems.
10.3.1 Name
Anameisastringcomposedofasetofsymbolschosen fromafinitealphabet. Forexample,
SINHA, #173#4879#5965, node-1!node-2!node-3!sinha, laIble, 25A2368DM197, etc.
care all valid names composed of symbols from the ASCII character set. A name is also
called an identifierbecause itisused todenote or identify anobject. Aname may also be

500 Chap. 10 • Naming
thought of as a logical object that identifies a physical object to which it is bound from
among acollection ofphysicalobjects.Therefore, thecorrespondencebetween namesand
objects is the relation of binding logical and physical objects for the purpose of object
identification.
10.3.2 Human-Orl4Mted and System-Oriented Names
Names are used to designate or refer to objects at all levels of system architecture. They
have various purposes, forms, and properties depending on the levels at which they are
defined. However, an informal distinction can be made between two basic classes of
names widely used in operating systems-human-oriented names and system-oriented
names.
Ahuman-orientednameisgenerally acharacterstringthatismeaningful toitsusers.
For example, lusertsinhalproject-Ilfile-I is a human-oriented name. Human-oriented
names are defined by their users. For a shared object, different users of the object must
have the flexibility todefine theirown human-orientednamesfortheobject foraccessing
it. Flexibility must also be provided so that a user can change or delete his or her own
name for the object without affecting those of other users. For transparency, human
oriented names should be independent of thephysical location or the structure of objects
they designate. Human-oriented names are also known as high-levelnames because they
can be easily remembered by their users.
Human-oriented names are not unique for an object and are normally variable in
length not only for different objects but also for different names for the same object.
Hence, they cannot be easily manipulated, stored, and used by the machines for
identification purpose. Moreover, it must be possible at some level to uniquely identify
every object in theentire system.Therefore, inaddition to human-oriented names, which
are useful for users, system-oriented names are needed to be used efficiently by the
system.These namesgenerally arebitpatternsoffixedsizethatcanbeeasily manipulated
and stored bymachines. They areautomatically generated bythe system. They should be
generated in a distributed manner to avoid the problems of efficiency and reliability of a
centralizedunique identifier generator.Theyarebasically meantforusebythesystem but
may also be used by the users. They are also known as unique identifiers and low-level
names.
Figure 10.3showsasimplenaming model basedonthesetwotypesofnames. Inthis
naming model, a human-oriented name is first mapped (translated) to a system-oriented
namethatisthenmappedtothephysical locationsofthecorrespondingobject'sreplicas.
10.3.3 NameSpace
A naming system employs one or more naming conventions for name assignment to
objects. For example, a naming system may use one naming convention for assigning
human-oriented names to objects and another naming convention for assigning system
oriented names toobjects. The syntactic representation of a name as well as its semantic
interpretation depends on the naming convention used for that name. The set of names
complying with a given naming convention is said to form a name space [Terry 1984].

Sec. 10.3 • Fundamental Terminologies and Concepts 501
Physicaladdresses
ofthenamedobject
,.--------...,,
~
I \
I
I
Human-oriented t--r-----... System-oriented t---.~----___
name name
, ,
First-level Second-level \, ,----------"~ I
mapping mapping
Fig.10.3 Asimple naming model based on the useofhuman-orientedand
system-orientednames inadistributedsystem.
10.3.4 Flat Name Space
The simplestnamespaceisaflat name space wherenamesare characterstringsexhibiting
no structure. Namesdefined in aflat name space are calledprimitiveorflat names. Since
flat names do not have any structure, it is difficult to assign unambiguous meaningful
names to a large set of objects. Therefore, flat names are suitable for use eitherfor small
name spaces having names for only afew objects or for system-oriented names that need
not be meaningful to the users.
10.3.5 Partitioned Name Space
When there is a need to assign unambiguous meaningful names to a large set of objects,
a namingconventionthat partitionsthe name space into disjointclasses isnormally used.
When partitioning is done syntactically, which is generally the case, the name structure
reflectsphysicalor organizationalassociations. Each partitionofapartitionednamespace
is called a domain ofthe name space.
Each domain of a partitioned name space may be viewed as a flat name space by
itself, and the names defined in a domain must be unique within that domain. However,
two differentdomainsmay have acommonnamedefinedwithinthem. Anamedefinedin
a domain is called a simple name. In a partitioned name space, all objects cannot be
uniquely identifiedbysimplenames, and hence compoundnamesare used for thepurpose
ofuniqueidentification.Acompoundname iscomposedofone or more simplenamesthat
are separatedbya special delimitercharactersuch asI,$, @, %, and soon (followingthe
UNIX file system convention, the delimitercharacterI will be used for the examples and
discussionpresentedin this chapter). For example, lalble is acompoundname consisting
of three simple names a, b, and c.
A commonly used type ofpartitioned name space is the hierarchical name space, in
which the name space is partitioned into multiple levels and is structured as an inverted

502 Chap.10 • Naming
tree.Each nodeofthename space treecorrespondstoadomain ofthename space. Inthis
type ofname space, the number of levels may be either fixed or arbitrary. For instance,
the Grapevine system manages a name space tree having two levels [Birrell et aI. 1982],
while in the Xerox Clearinghouse, a name space tree has three levels [Oppen and Dalal
1983]. On the other hand, the DARPA Internet Domain Naming System [Su and Postel
1982] and the Universal Directory Service proposed in [Lantz et a1. 1985] allow a name
space tree tohave arbitrarily many levels. Names defined inahierarchical name space are
called hierarchical names.
Hierarchical names have been used infile systems for many years and have recently
beenadoptedfornamingother objects aswell indistributed systems. Several examplesof
hierarchical name spaces are also found in our day-to-day life. For instance, telephone
numbers fully expanded to include country and area codes form a four-level hierarchical
name space and network addresses in computer networks form a three-level hierarchical
name space where the three levels are for network number, node number, and socket
number.
10.3.6 Name Server
Name spaces are managed by name servers. A name server is a process that maintains
information about named objects and provides facilities that enable users to access that
information. It acts to bind an object's name to some of its properties, including the
object's location [Terry 1984].
In practice, several name servers are normally used for managing the name space of
object names in a distributed system. Each name server normally has information about
only a small subset of the set of objects in the distributed system. The name servers that
store the information about an object are called the authoritative name servers of that
object[Terry 1984].Todetermine the authoritative name servers for every named object,
thename service maintains authorityattributesthatcontain aJistoftheauthoritativename
servers for each object.
Partitioned name spaces are easier to manage efficiently as compared to flat name
spaces because theyenable the amount ofconfigurationdatarequired ineach name server
tobereduced sinceitneedonly bemaintained foreachdomain andnotforeach individual
object. For example, in a hierarchical name space, it is sufficient that each name server
storeonlyenough information tolocate theauthoritativename servers forthe rootdomain
ofthe name tree.The authoritativename servers ofthe root domain, inturn, should know
thelocationsoftheauthoritativenameserversofthedomainsthatbranch outfrom theroot
domain. Ingeneral, theauthoritativename servers of adomain should know thelocations
oftheauthoritativename servers ofonly those domains that branch out from thatdomain.
For example, in the name space tree of Figure 10.4, all name servers must know the
locations ofthe authoritative name servers of domain D the authoritative name servers
J;
of domain D) need only know the locations of the authoritative name servers of domains
Dz,D 3, and D 4; and the authoritative name servers of domain D 2 need only know the
locations of the authoritative name servers of domains Dsand D 6•Therefore, the amount
ofconfigurationdata that must bemaintained byname servers at the various levels ofthe
hierarchy is proportional to the degree of branching of the name space tree. For this

Sec.10.3 • Fundamental Terminologies andConcepts S03
Fig. 10.4 Domains ofahierarchical name space.
reason, hierarchicaJ naming conventions with several levels are often better suited for
naming large number ofobjects.
It is important to note that the internal representation of a name server need not
necessarily reflect the structure ofthe abstract naming system. For example, rather than
adopt to ahierarchical internalstructure, aname servermay compresshierarchical names
into a flat data structure and do string comparisons ofentire compound names instead of
matching a sequence of simple-name components. In fact, some name servers use a
procedure-based strategy rather than table-based strategy to implement the bindings in a
given domain of the name space. However, one-to-one correspondence between name
servers and domains facilitates easy management of partitioned name spaces.
10.3.7 Name Agent
The distribution of the name service and the locations of the name servers should be
transparent to the clients ofa name service. This transparency is achieved through name
agents that act between name servers and their clients. A name agent maintains a
knowledgeofexistingname servers. When aclientrequests for a name service, the name
agent uses the proper communication protocol to transfer the user's request to a proper
name server. On receiving a reply from the server, it forwards it to the client.
Name agents maybeoftwo types: (a)private, which work forasingle client, and (b)
shared, which work for several clients. A private name agent is structured as a set of
subroutines that are linked to the clientprogram. On the otherhand, a shared name agent
is structured as a part of an operating system kernel, with system calls to invoke name
service operations, or as a separate process that is accessed via interprocess communica
tion primitives.
Name agentsare known by various names. For instance, in theCSNETName Server
they are called "name server agent programs," in the D~~RPA Internet Domain Name

S04 Chap.10 • Naming
Service (DNS) they are called "resolvers," in the DeE Directory Servicethey are caJled
"clerks," and in the COSIE Name Server they are called "user interfaces."
Names are always associated with some context. A context can be thought of as the
environmentin which aname is valid. Becauseall names are interpretedrelativeto some
context,acontext/namepair issaid toform aqualifiedname that can beused for uniquely
identifying an object.
The notion ofcontexthas provedtobevery useful forpartitioninganame space into
smallercomponents. Often, contextsrepresentadivision of the name space along natural
geographical,organizational,.or functional boundaries[Terry1986].Inapartitionedname
space, each domain corresponds to a context ofthe name space. Names in a context can
be generated independently ofwhat names exist in any other context. Therefore, a name
may occur in more than one context. Contexts may also be nested, as in the case of
hierarchical name spaces. For example, in the name space tree ofFigure 10.5, context C
3
isnested within contextC 2, which inturn is nested within context C 1•In nested contexts,
a qualified name consists of a series of names identifying, respectively, a context, a
subcontext, a sub-subcontext followed by a name inside the last sub-sub-...context. For
example, in Figure 10.5,the qualified name for object 0 that is associated with context
1
C 3 will be C.IC 2/C3101•
For the purpose of name management, contexts provide a means ofpartitioning the
naming information database so that it may be distributed among multiple name servers.
Contexts represent indivisible units for storage and replication ofinformation regarding
named objects [Terry 1986].
Object(On)
Fig. 10.5 Nested contexts.

sos
Sec.10.3 • FundamentalTerminologies andConcepts
10.3.9 Name Resolution
Name resolution is the process of mapping an object's name to the object's properties,
such as its location. Since an object's properties are stored and maintained by the
authoritative name servers of that object, name resolution is basically the process of
mapping an object's name to the authoritative name servers of that object. Once an
authoritativename serveroftheobject hasbeenlocated,operationscanbeinvoked toread
or update the object's properties.
Each name agent in a distributed system knows about at least one name server a
priori.Togetaname resolved, aclient firstcontacts itsnameagent, whichintum contacts
a known name server, which may in turn contact other name servers. Because name
servers must beabletoidentifyeachother,theyarealsotreatedasobjects andareassigned
names. However, to avoid potential cycles, name servers at a given level of a naming
system generally identify each other with system-oriented, low-level names.
In a partitioned name space, the name resolution mechanism traverses a resolution
chain from onecontexttoanother untiltheauthoritativenameservers ofthenamed object
are encountered. As shown in Figure 10.6,agiven name isfirst interpreted inthecontext
towhich itisassociated. The interpretationeither provides theauthoritative name servers
of the named object or returns a new name and a new context to interpret that name. In
theformer casethe nameresolution process ends, whileinthelattercasetheinterpretation
process continues as before until the authoritative name servers of the named object are
encountered. Notice that if all the contexts involved in the resolution of a name are
managed by a single name server, then the complete name resolution operation iscarried
outbyasingle name server; otherwisemore thanone nameserver isinvolved inthename
resolution operation.
10.3.10 Abbreviation/Alias
In case of partitioned name spaces, a qualified name, which is normally a compound
name, mayconsist ofseveral simple names and may be verylong. Itmaybeinconvenient
for a user to specify the qualified name of an object every time he or she wants to use it.
Therefore, in a naming convention for partitioned name spaces, users are generally
allowed to specify and use their own short-form substitutes for qualified names called
abbreviations. All abbreviations defined by a user form a private context of that user.A
mapping of abbreviations toqualified names is maintained on aper private context basis.
When a user specifies an abbreviation, it is first converted to a qualified name by using
the mapping associated with the private context of that user. Therefore, different users
mayusethesameabbreviationtoidentify different objects.Forexample, twousers,user-l
anduser-2, mayusethesameabbreviationmy-object-l toidentify theirobjects having the
qualified names luser-l/project-llgroup-llobject-l and /user-2/project-5/group-3/object
1, respectively. Abbreviations are also know as aliases. Notice that aliases are generally
simple names that need to be unique only in a single private context. The symbolic links
facility of the UNIX file system is an example of aliasing facility in a naming system.
Incase of aliasing, abinding associates a single simple name with aqualified name.
However, a user may wish to identify an object by one of two or more simple names. A

S06 Chap.10 • Naming
Context(n)
Context(n-1)
Context(1)
Authoritativenameservers
ofthenamedobject
Context(i) = contextatleveli
=
Name(i) validnameincontextatlevel(i)
=
f mappingfunctionfromthenameincontext(i) Fig.10.6 Context-by-contextname
ili
toanewnameincontext(j) resolution mechanism.
naming system mayprovide thisflexibility toitsusers byaJlowingmany-to-one bindings
within acontext. That is,more thanone simple name maybebound tothe samequalified
name within a given context. If this facility is provided, the simple names bound to the
same qualified name are called synonyms or nicknames.
10.3.11 Absolute andRelatlv.Names
Another method to avoid the necessity to specify the full qualified name of an object in
tree-structurednamespacesistheuseoftheconceptofcurrent workingcontext. Acurrent
working context is also known by the shorter names current contextor working context.
According to this concept, a user is always associated with a context that is his or her
current context. A user can change his or her current context whenever he or she
desires.

Sec. 10.3 • Fundamental Terminologies andConcepts 507
In this method, object names can beof two types-absoluteor relative. Anabsolute
namebegins at the root context of the name space tree and follows a path down to the
specifiedobject, giving the contextnames onthe path. Ontheotherhand, arelativename
defines a path from the currentcontextto the specifiedobject. It iscalledarelative name
becauseitis"relativeto" (starts from) the user'scurrentcontext. Forexample, inthe tree
structuredname space ofFigure 10.7,ifthe user's currentcontextis root-context/context
1/context-3, the relative name context-S/object-J refers to the same object as does the
absolute name root-context/context-J/context-3/context-5/object-l. Means are also pro
vided inthis method to refer tothe parent contextof acontext. Forexample, inthe UNIX
file-naming convention, which uses tree-structured name space, two periods (..,
pronounced "dot dot") refer to a directory's parent directory (a directory is basically a
context). The root directory is its own parent.
User's
currentcontext
\.-------.
object-1 -------_.object-m
object-l --------. object-n
Fig.10.7 Atree-structured name space toillustrate absolute andrelative names.
In this method, a user mayspecify an object in any ofthe following ways:
1. Using the full (absolute) name
2. Using a relative name
3. Changing the current context first and then using a relative name

508 Chap. 10• Naming
Forexample, inFigure 10.7,iftheuserwhosecurrentcontextisroot-context/context
llcontext-3wantstospecifytheobject root-context/context-2/context-4/object-l,heorshe
can do so in the following ways:
1. Specify the absolute name of the object, which is root-contextlcontext-Zlcontext
4/object-l.
2. Specify thenameoftheobject relative tohisorhercurrent context as..I..Icontext
2/context-4Iobject-l. In thisrelative name, the first dotdot refers to the parent context of
the user's current context, which is context-I, and the second dot dot refers to the parent
context of context-I,which is root-context.
3. Notice from the above two name specification methods that arelative name may
beequally longorevenlongerthantheabsolute nameinsomecases.Thisdefeats themain
purpose of theuseofrelative names. Hence thethird methodistofirstchange thecurrent
context of the userand thenrefer totheobject byanamerelative to thenew context. For
instance, in theexample above, the user may first use the context-changing command to
change his or her current context to the context named root-contextlcontext-2Icontext-4
andthen simply specify theobject byasimple name object-I that isrelative to the user's
new current context.
10.3.11 Generic andMulticast Names
We saw that the use of synonyms requires the naming system to support many-to-one
bindings. On the other hand, the use of generic and multicast names requires the naming
system to support one-to-manybindings. That is,the naming system must allow asimple
name to be bound to a set of qualified names.
In a generic naming facility, a name is mapped to anyone of the set of objects to
which it is bound. This type of facility is useful in situations such as when a user wants
arequest tobeserviced byanyoftheserverscapable ofservicingthatrequest andtheuser
is not concerned with which server services his or her request.
In agroup or multicastnaming facility,a name is mapped toall the members of the
set of objects to which it is bound. This type of facility is particularly useful for the
broadcasting or multicasting of a message.
A naming system that supports descriptive/attribute-based names allows an object to be
identified byasetofattributes orproperties thatdescribe theobject anduniquely identify
itamong thegroup ofobjects inthe name space.Apartitioned name space having arigid
structure (a rigid set of attributes) is an example of a descriptive naming convention. In
thiscase, each domain of the name space is used todefine anattribute ofthe objects. An
attribute has both a type and a value. The type indicates the format and meaning of the
value field. For example, a typical set of attributes that may be used for uniquely
= =
identifying a file object may be User Sinha, Creation date = 1995104106,File type
Source, Language =Pascal, Name = Quicksort.

Sec.10.4• System-Oriented Names sO'
Note that an attribute value may be the same for several objects, but all attributes
consideredtogetherrefertoasingleobject. Moreover,itisnot alwaysnecessarytospecify
all the attributes of a naming convention to identify an object. Attribute-based naming
systemsusually workbasedon the idea that aquerymustsupplyenoughattributesso that
the target object can be uniquely identified. Also notice that in a partitioned name space
using descriptive naming convention, domains can be arranged in any arbitrary manner.
Multicast or group naming facility can be easily provided with attribute-based
naming by constructing an attribute for a list of names. Group names are particularly
useful in forming mail distribution lists and access control lists.
10.3.14 Source-Routing Name
Many name spaces mirror the structure of the underlying physical network. When the
structure ofa name space has the same form as the underlying network ofa distributed
system, the namespacedefines source-routingnames. Asource-routingname identifiesa
path throughthenetworkofadistributedsystem.The UNIX-to-UNIXCopy (UUCP)name
space that definesnamesoftheform host-I!host-2!host-3!sinhaisanexampleofasource
routing name space. The UUCPstyle names are calledsource-routing names becausethe
route throughthe networkisspecifiedatthe source computer,For instance,intheexample
above, thespecifiedroute isfrom host-ltohost-2 tohost-3tosinha.The UUCPstylenames
arerelativenamesbecausethey must beinterpretedrelativetothestartingpoint.
10.4 SYSTEM-ORIENTED NAMES
System-oriented names normally have the following characteristic features:
1. They are large integers or bit strings. For example, in a typical naming system,
system-orientednamesmay be membersofsomelarge set ofintegers,such asthe integers
up to 2128_1.
2. They are also referred to as unique identifiers because in most naming systems
they are guaranteed to be unique in both space and time. That is, these names do not
change during their lifetime, and once used, they are never reused. Therefore, in the
naming systemdiscussed above, a 128-bitpatternreferseitherto nothingor,if itrefers to
anything, tothe samething atalltimes. This isthe main reasonwhy unique identifiersare
so large.
3. Unlike human-oriented names that are variable in length, all system-oriented
names of a naming system are ofthe same size irrespective ofthe type or locationofthe
object identified by these names. This allows the naming ofall objects uniformly.
4. Since all the system-oriented names ofa naming system are of uniform size and
also are normallyshorterthan human-orientednames, manipulationslike hashing,sorting,
and soon, can beeasilyperformedon them. Hence,they are suitableforefficienthandling
by machines.

510 Chap.10 • Naming
5. They are hard to guess and hence are sometimes also used for security..related
purposes. For example, they may be used as tickets of permission to access objects. A
computationthatpresents avalidname maybeassumed tohavebeengivenitdeliberately
rather than to have guessed or invented it.This is an example of use of protected names
or capabilities, one of the classical approaches to access control management.
6. System-oriented names are automatically generated by the system.
An important issueassociated with system..oriented names ishow tocreate aunique
name foreach object inthe system. Before describing the methods tohandle thisissue, it
is useful to mention that system..oriented names can be either unstructured or structured
(see Fig. 10.8 for an example of each type). Unstructured names have a single field of
large integers or bit strings that uniquely identifies an object but does not provide any
other information abouttheobject. Ontheother hand,structurednamescontain morethan
one component, some of which provide information about the properties of the object
identified by its name. For example, a structured name may have a component that
specifies the node on which the object was created or the node on which the object is
currently located. Thetwobasic approaches forthecreation ofthesetwotypes ofsystem..
oriented names are discussed below.
Asinglefieldoflargeintegersorbitstrings
(a)
Nodeidentifier Localuniqueidentifier
Fig. 10.8 (a) Unstructuredand (b) structured
system-orientednames.
(b)
10.4.1 CentrallHd Approach forGaneratlng
System-Oriented Names
Although thecentralized approach may beusedforthegeneration of bothunstructured as
well as structured system-orientednames, itis mainly used by anaming system that uses
unstructured names. Inthis method, astandard, uniform global identifier is generated for
each object in the system by a centralized global unique identifier generator, and some
method isusedtopartition thisglobal namespace among thelocaldomains ofeach node.
Each node either directly binds these global identifiers to the locally created objects or it
maps its local identifiers for the locally created objects to these global identifiers. In the
latter case, binding of local identifiers to global identifiers can be permanent or
temporary.

Sec.10.4 • System-Oriented Names 511
The centralizedapproach has the advantages that it is simple and easy to implement
and isthe only methodfor generating unstructured global unique identifiers. However, it
suffers from the drawbacks of poor efficiency and poor reliability. The single global
unique identifier generator may become a bottleneck for large name spaces. Also, the
continuousgenerationofglobal unique identifiersistotally dependentonthereliabilityof
the centralized global unique identifier generator.
10.4.2 Distributed Approach forG8neratlng
System-Oriented Names
To overcome the drawbacks of poor efficiency and poor reliability of the centralized
approach, several naming systems use the distributed approach for the generation of
system-oriented names. However, distributed generation forces the naming system to use
structured object identifiers. In the distributed approach, the hierarchical concatenation
strategy is used to create global unique identifiers. In this strategy, each identification
domain is assigned a unique identifier, and a global unique identifier for an object is
created by concatenating the unique identifierofa domain with an identifierused within
this domain. Therefore, global unique identifiers generated in this manner basically
consist of two components-the domain identifier and an identifier within this domain.
Notice that each component may in turn consist of two or more subcomponents. For
example, the domain identifier for a wide area network may consist of two sub
components-network identifier and node identifier within this network.
The next question that arises is how to generate unique identifiers within a domain
in an efficient manner. One method to do this is to treat each node of the system as a
domain ofthe name space and treat areading from the real-timeclock (called timestamp)
of a node as a unique identifier within the node. With this, global unique identifiers take
the form of the pair (node-JD, timestamp). Another method is to treat each server as a
domain and then to allow aserver togenerateobject identifiers fortheobjects itserves in
a server-specific manner. In this case, global unique identifiers take the form of the pair
(server-JD, server-specific-unique-identifier).
Althoughthedistributedapproachhasbetter efficiencyand reliability ascomparedto
the centralized approach, it suffers from the following drawbacks:
1. In a heterogeneous environment, the form and length of identifiers may be
different for different computers (nodes), resulting in nonuniform global unique
identifiers. Itmaybeawkwardorinefficientforapplicationstobeprepared todeal
with the nonuniformity ofthese low-level identifiers.
2. Node boundaries or servers are explicitly visible in this scheme.
10.4.3 Generating Unique Identifiers intheEvent
of Crashes
Anotherimportantproblemrelated with thecreationof unique identifiers (whetherglobal
or local) is to be able tocreate unique identifiers in the face of crashes. Acrash may lead
toloss of the state information of a unique identifiergenerator. Therefore, upon recovery,

512 Chap. 10 • Naming
the unique identifier generator may not function correctly, which may result in the
generationofnonuniqueidentifiers. Two basic approachesare used to solve this problem
[Watson 1981]:
1. Usingaclock thatoperates acrossfailures. In this method, aclock is used at the
locationofthe unique identifiergenerator. This clockisguaranteed tocontinueoperating
across failures, and it will not recycle during the period within which the needed
identifiers must be unique. However, to implement this method, one may require rather
long identifiers, depending on the granularity ofthe clock interval needed.
2. Usingtwoormorelevelsofstorage. In thismethod, two ormore levels ofstorage
are used and the unique identifiers are structured in a hierarchical fashion with one field
for each level. A counter associated with each level contains the highest value of the
corresponding field assigned. The current values of these fields are cached in main
memory. When a lower level field is about to cycle or the associated storage device
crashes,thenexthigherlevelcounterisincrementedandthelowerlevelcountersarereset.
If a stable storage (the information on which can survive crashes) is used, two levels of
storage (the upper level being the main-memory storage and the lower level being the
stable storage) are sufficient for creating a safe and efficient unique identifier generator.
As compared to the first mechanism, this mechanism can yield shorter identifiers, but it
is more complex to implement.
10.5 OlJEa-lOCADNG MECHANISMS
Object locating is the process ofmapping an object's system-oriented unique identifier
(UID for short) to the replica locationsofthe object. It may be noted herethat the object
locating operation is different and independent of the object-accessing operation. In a
distributed system, object locating is only the process of knowing the object's location,
that is, the node on which it is located. On the otherhand, object accessing involves the
process ofcarrying out the desired operation (e.g., read, write) on the object. Therefore,
the object-accessing operation starts only after the object-locating operation has been
carried out successfully.
Several object-locating mechanisms have been proposed and are being used by
various distributedoperatingsystems. Thesemechanismsarebriefly describedbelow.The
suitability ofa particularmechanism for a distributed system depends on various factors
such as the scaleofthesystem, the type ofUID being usedbyits naming system, whether
the system supports objectmigration, whetherthe system supports location transparency
ofobjects, and so on.
10.5.1 Broadcasting
In this method, an object is located by broadcastinga request for the objectfrom aclient
node.The requestisprocessedbyall nodes and then thenodes currentlyhaving theobject
reply backtotheclientnode.Amoeba[MullenderandTanenbaum 1986]uses this method
for locating a remote port.

Sec.10.5 • Object-Locating Mechanisms 513
The method is simple and enjoys a high degree of reliability because it supplies
all replica locations of the target object. However, it suffers from the drawbacks of
poor efficiency and scalability because the amount of network traffic generated for
each request is directly proportional to the number of nodes in the system and is
prohibitive for large networks. Therefore, this method is suitable only when the
number of nodes is small, communication speed is high, and object-locating requests
are not so frequent.
10.5.2 Expanding Ring Broadcast
Pure broadcasting is expensive for large networks. Moreover, direct broadcasting to all
nodes may not be supported by wide-area networks. Therefore, a modified form of
broadcasting, called expandingringbroadcast(ERB) [Wiebe 1986],isnormallyemployed
inaninternetworkthat consists of local area networks (LANs) connectedbygateways. In
this method, increasingly distant LANs are systematically searched until the object is
found or until every LAN has been searched unsuccessfully. The distance metric used is
a hop. A hop corresponds to a gateway between processors. For example, if a message
fromprocessorAtoprocessorBmust passthrough atleasttwogateways, AandBaretwo
hops distant. Processorson the same LAN are zero hopdistant. A ring isthesetofLANs
acertain distance away from aprocessor.Thus, Ringo[A] isA'slocal network,Ring][A] is
the set of LANs one hop away, and so on.
AnERB search works asfollows. Suppose that processorA needsto locateobject X.
=
Beginning withi 0,arequest message isbroadcast toallLANs inRing;[A]. Ifaresponse
is received, the search ends successfully. Otherwise, after a timeout period haselapsed, i
isincrementedby 1and therequest broadcastisrepeated. Thering size iisbounded from
above by the diameter of the internetwork.
Notice that this method does not necessarily supply all the replica locations of an
object simultaneously, although it does supply the nearest replica location. Furthermore,
theefficiency ofanobject-locatingoperation isdirectly proportionaltothedistance ofthe
object from the client node at the time of locating it.
10.5.3 Encoding Location ofObj8ct within ItsUID
This scheme.uses structured object identifiers. One field of the structured UID is the
location oftheobject. Given aUID,thesystem simply extracts thecorrespondingobject's
location from its DID by examining the appropriate field of the structured VID. The
extracted location is the node on which the object resides.
This is a straightforward and efficient scheme. One restriction of the scheme,
however, isthat anobject isnotpermitted tomoveonce itisassigned toanode,since this
would require its identifier to change. Consequently, an object is fixed to one node
throughout its lifetime. Another limitation of the scheme is that it is not clear how to
support multiple replicas ofanobject.Therefore, theuseof thisobject-locatingscheme is
limited to those distributed systems that do not support object migration and object
replication.

514 Chap.10 • Naming
10.5.4 S.archlng Cr.atorNoeM First andThen
8roadcastlng
This scheme is a simple extension of the previous scheme. The included extension is
basically meant for supporting object migration facility. The method is based on the
assumption that it is very likely for an object to remain at the node where it was created
(although it may not be always true). This is because object migration is an expensive
operation and objects do not migrate frequently.
In this scheme, the location fieldof the structured UIDcontains the identifier of the
node on which the object was created. Given a UID, the creator node information is
extracted from the UID and arequest is sent to that node. If the object no longer resides
on its creatornode, a failure reply is returned back to the client node. In case of failure,
the object is located by broadcasting the request from the client node. This method of
object locating is used in Cronus [Schantz et al. 1986].
Ascompared tothe broadcasting scheme, thismethod helps inreducing the network
traffic to a great extent. Furthermore, the scheme is more flexible than the method of
encoding the location ofanobject within its UID because itallows the system to support
object migration. However, the use of broadcast protocol to locate those objects that are
not found on their creator nodes limits the scalability of this mechanism.
10.5.5 Using Forward Location Pointers
This scheme isanextension oftheprevious scheme.The goalofthisextension istoavoid
theuse of broadcast protocol. Aforward location pointerisareference used at a node to
indicate the new location of anobject. Whenever an object is migrated from one node to
another, a forward location pointer is left at its previous node. Therefore, to locate an
object, thesystemfirstcontacts thecreator nodeoftheobject andthensimply follows the
forward pointer or chain of pointers to the node on which the object currently resides.
The method hastheadvantages of totally avoiding the useofbroadcast protocol and
allowing the supportofobject migration facility.However,the method practically hasthe
following major drawbacks:
1. The object-locating cost is directly proportional to the length of the chain of
pointers to be traversed and grows considerably as the chain becomes longer.
2. Itisdifficult,oreven impossible, tolocateanobjectifanintermediate pointerhas
been lost or is unavailable due to node failures. Therefore, the reliability of the
mechanism is poor.
3. The method introduces additional system overhead for upkeep.
10.5.6 Using Hint (acheand8roadcastlng
Another commonly used approach is the cache-broadcast scheme. In this method, a
cache is maintained on each node that contains the (UJD, last known location) pairs
of a number of recently referenced remote objects. Given a UID, the local cache is

Sec. 10.6 • Human-Oriented Names 515
examined to determine if it has an entry for the UID. If an entry is found, the
corresponding location information is extracted from the cache. The object access
request is then sent to the node specified in the extracted location information. If the
object no longer resides at that node, however, the request is returned with a negative
reply, indicating that the location information extracted from the cache was outdated.
If the specified DID is not found in the local cache or if the location information of
the object in the local cache is found to beoutdated, a message is broadcast throughout
the network requesting the current location of the object. Each node that receives the
broadcast request performs an internal search for the specified object. A reply message
is returned to the client node from the node on which the object is found. This location
of the object is then recorded in the client node's cache. Notice that a cache entry only
serves as a hint because it is not always correct.
This scheme can be veryefficient ifa high degree of locality isexhibited inlocating
objects from a node. It is also flexible since it can support object migration facility.
Furthermore, the method of on-use update of cached information avoids the expense and
delay of having to notify other nodes when an object migrates. One problem with this
scheme, however, isthat broadcast requests will clutterup the network, disturbing all the
nodes even though only a single node is directly involved with each object-locating
operation.
This is the most commonly used object-locating mechanism in modern distributed
operating systems. It is used inAmoeba [Mullenderet al, 1990],V-System [Cheriton and
Mann 1989],DOMAIN [Leach et a1. 1983],NEXUS [Tripathi et a1. 1987],Mach [Rashid
1987], and Chorus [Rozier et al. 1988].
Figure 10.9 summarizes the various object-locating mechanisms discussed above.
Note that schemes (c), (d), and (e) require structured DID whereas schemes (a), (b), and
(j)can work with both unstructuredandstructuredUIDs.Also notice thatonly scheme (c)
facilitates theobject-locatingoperationtobecarried out withoutthe need toexchangeany
message with any other node. However, it is inflexible because itdoes not support object
migration and object replication facilities.
10.6 HUMAN-ORIENTED NAMES
System-oriented names such as 3JA5 2B5F AD19 BlCs, though useful for machine
handling, are not suitable for use by users. Users will have a tough time if they are
required to remember these names or type them in. Furthermore, each object has only a
single system-orientedname, and therefore all the users sharing anobject must remember
and use its only name. Toovercome these limitations, almost all naming systems provide
the facility to the users todefine and use their own suitable names for the various objects
in the system. These user-defined object names, which form a name space on top of the
name space for system-oriented names, are called human-oriented names.
Human-oriented names normally have the following characteristics:
1. They are character strings that are meaningful to their users.
2. They are defined and used by the users.

516 Chap.10 • Naming
CD
Broadcastrequestmessage
®
Replyfromthenodeon
whichtheobjectislocated
(a)
// .. _- \\
/
CD
Searchingnodesat0
hopdistance
Il "\ ®
Searchingnodesat1
hopdistanceifthe
. searchof0hopfails
,,,,
,,,
(J)
h
S
o
e
p
ar
s
c
d
h
i
i
s
n
t
g
an
n
c
o
e
d
i
e
ft
s
h
a
e
ti
,,, searchesupto;-1
hopsfail
,
<.
.
'
(b)
o
Obje o ctlocation Clientnode
CD
@ Extractingobject'slocationfrom
itsUID.Nomessageexchange
withanyothernodeisrequired
o forlocatingtheobject
(c)
Fig. 10.9 Object-locating mechanisms: (a) broadcasting; (b) expanding-ring
broadcast;(c)encoding thelocationofanobject withinitsUID;
(d)searchingthecreator nodefirstandthenbroadcasting; (e)usingforward
locationpointers; (f) usinghintcache andbroadcasting. (Continued onnext
page.)

Sec.10.6• Human-Oriented Names S17
Creatornode
Objectfocation
0) Searchingthecreatornode
®
Negativereply
®
Broadcastrequestmessage
®
@ Replyfromthenodeon
whichtheobjectislocated
(dJ
Creatornode
®~G)
Objectlocati_on Clientnode
0),®.®
Pathof
@ messageforwarding
@ Replyfromthenodeon
®
whichtheobjectislocated
(e)
0) Searchingoflocalcache
®
Broadcastrequest message
®
Replyfrom thenodeon
®
whichtheobjectislocated
(f)
:Fig.10.9 (Continued.)
3. Different users can define and use their own suitable names for a shared object
That is, the facility of aliasing is provided to the users.
4. Human-oriented names are variable in length not only in names for different
objects but even in different names for the same object.
5. Due to thefacility ofaliasing, the same name maybeused bytwodifferent users
atthe sametime torefertotwodifferent objects. Furthermore, auser mayusethe
same name at different instances of time to refer to different objects. Therefore,
human-oriented names are not unique in either space or time.

518 Chap.10 • Naming
Because of the advantages of easy and efficient management of name space,
hierarchically partitioned name spaces are commonly used for human-oriented object
names. When ahierarchical naming convention is used, itis important to decide whether
to use a constant number of levels or an arbitrary number of levels. That is, ifthe names
of a hierarchical naming system are of the form c]/c2/...Ic., should i be constant or
arbitrary?Both schemes have their own advantages and drawbacks. The main advantage
ofthe constant-level scheme isthat it is simpler and easier to implementas compared to
the arbitrary-levelscheme.This isbecause allsoftware inthearbitrary-levelscheme must
be able to handle an arbitrary number of levels, sosoftware formanipulating names tend
to be more complicated than those for constant-level scheme. The disadvantage of the
constant-levelschemeisthatitisdifficulttodecide thenumberoflevels,andifnewlevels
are to be added later,considerably more work has to be done because all the algorithms
for name manipulation must beproperly changed to take care of the new levels.
On the other hand, arbitrary-level schemes have the advantage of easy expansion by
combiningindependently existing namespaces intoasingle namespace. For example, as
shown in Figure 10.10, two independent name spaces may be combined into one name
new-root
--
.......
.............." ",-- root-2
"" .r>:
...,
,
\
\
,/I -us<er-1. user-2
\
\,
\
: project-1
,
,I
I \~
,I \,file-1
I
, ,I I \ \ ,
,,
,
""'" -
-,' ........
Thenewnamespace
Fig.10.10 Combiningtwo name spaces toform asingle name space byadding a
new root.

Sec.10.6• Human-Oriented Names 519
space by adding a new root, making the existing roots its children. The major advantage
isthatifaname wasunambiguouswithinitsoldname space, itisstillunambiguouswithin
its new name space, even if the name also appeared in some other name space that was
combined. There is no need to change any of the algorithms for manipulating names.
Arbitrary-level schemes, being more flexible than constant-level schemes, are used
by most recent distributed systems. Hence, subsequent sections of this chapterdeal with
arbitrary-level hierarchical name spaces.
The major issues associated with human-oriented names are as follows:
1. Selecting a proper scheme for global object naming
2. Selecting a proper scheme for partitioning a name space into contexts
3. Selecting a proper scheme for implementing context bindings
4. Selecting a proper scheme for name resolution
These issues are described in the next section.
10.6.1 Human-Oriented Hierarchical Naming Schemes
Basically there arc four approaches for assigning systemwide unique human-oriented
names to the various objects in a distributed system. These are described below.
Combiningan Object'sLocalNamewith Its HostName
This naming scheme usesanamespace thatiscomprisedofseveral isolated name spaces.
Each isolated name space corresponds to a node of the distributed system, and a name in
thisname space uniquely identifies anobject ofthe node. Intheglobal system, objects are
named by some combination of their host name and local name, which guarantees a
systemwide unique name. In Ibis [Tichy and Ruan 1984], for instance, file objects are
identified uniquely bynames having the form host-name:local-name, where host-name is
the identifierofthenodeonwhichthefileresides and local-nameisaUNIX-like absolute
pathname for the file that identifies it uniquely on its host node.
This naming scheme is simple and easy to implement. Its main drawback, however,
is that it is neither location transparent nor location independent. Furthermore, it is
inflexible inthe sense thatanobject'sabsolute namechanges every time anobject moves
from one node to another. Therefore, it is not suitable for modem distributed operating
systems.
Interlinking Isolated Name Spaces into a Single Name
Space
In this scheme also the global name space is comprisedofseveral isolated name spaces.
However, unlike the previous scheme in which the isolated name spaces remain isolated
throughout, theisolated namespacesarejoinedtogethertoformasinglenaming structure.
The position of these componentname spaces inthe naming hierarchy isarbitrary. In the

S20 Chap. 10 • Naming
naming structure, a component name space may be placed below any other component
name space either directly or through other componentname spaces. In this scheme there
isnonotionofabsolutepathname. Eachpathnameisrelativetosomecontext,either tothe
current working context or to the current component name space.
This naming scheme is used in UNIX United [Brownbridge et al. 1982] tojoin any
number of UNIX systems to compose a UNIX United system. In the single naming
structure of UNIX United, each component system is a complete UNIX directory tree
belonging toacertain node.The rootofeachcomponent namespace isassigned aunique
name so that they become accessible and distinguishable externally. A component's own
root is still referred to as I and still serves as the starting point of all pathnames starting
with aI. The UNIX notation ../is used to refer to the parent of a component's own root.
Therefore, there is only one root that is its own parentand that is not assigned a string
name, namely, the root of the composite structure, which isjust a virtual root needed to
make the whole structure a single tree.
Asimple example ofsuchanaming structure isshown inFigure 10.10,inwhich the
composite name structure is comprised of two component name spaces A and B whose
roots are named root-I and root-Z,respectively.As anillustration of relative path naming
in this scheme, note that from within the component name space A, file-I of the two
componentname spacesA and Bwillbereferred toasluser-Ilproject-Ilfile-I and ..Iroot
2luser-lIproject-l/file-l, respectively. Similarly, if the current working directory of a
client is project-I of the component name space B,file-I of the two component name
spaces A and B will be referred to as ../../.Jroot-Iluser-Ilproject-Ilfile-I and file-l,
respectively.
The main advantage of this naming scheme is that it is simple tojoinexisting name
spaces into a single global name space. However, in naming systems that employ this
scheme for global object naming, an important issue that needs to be handled is to allow
clients to continue to use names that were valid in the independent name spaces without
the need to update them to match the new name space structure. For example, in Figure
10.10,luser-llproject-l/file-l is a name used by clients before the two component name
spaces wereintegrated. Beforeintegration, thisnamereferred tofile-l ofcomponentname
spaceA forclients of componentname spaceA and tofile-l ofcomponentname space B
for clients of component name space B.Therefore, it is desirable that after integration of
the two component name spaces, this name should still refer to the same object for the
clients of the two component name spaces. The Global Name Service (GNS), which was
designed and implemented by Lampson and colleagues at the DEC Systems Research
Center [Lampson 1986], provides an interesting solution to this problem. Its approach,
which is based on the use of directory identifiers (DIs) and a table of well-known
directories, is described below.
A GNS has two parts <directoryname, value name>. The first part identifies a
directory and the second part refers to a value tree or some portion of a value tree. Each
directory has a unique DI associated with it. Figure 10.10 has been redrawn in Figure
10.11(a) to illustrate the GNS naming scheme.
In the GNS naming convention, file-l of both the component name spaces A and
B, before their integration, will be referred to as <./user-l, project-l/file...I». After
integration ofthe two component name spaces, when a client of component name space

Sec.10.6• Human-Oriented Names 521
Well-knowndirectories
#108=#311/root-1
#251=#311/root-2
project-1 project-1
1 1
file-1 file-1
(a)
Well-knowndirectories
#311/root-1/user-3
project-1 project-1
! !
file-1 file-1
(b)
Fig. 10.11 The GNS approach of accommodatingchanges inexistingname spaces:
(a) combiningtwo name spaces toform asingle name space;
(b) restructuringaname space.
A uses this name, its local name agent, which is aware of the local root directory's DI,
prefixes the Dl (#108), thus producing the name <#lOB/user-},project-llfile-I», On the
other hand, when a client of component name space B uses the same name, its local
name agent prefixes the Dl (#251) of the root directory of component name space B,
thus producing the name <#251/user-l, project-Ilfile-l», 'The name agent then passes
this derived name in a lookup request to a GNS server. Relative names are similarly
converted into local absolute names by a local name agent before being passed to a
GNS server for lookup.
Toallow a GNS server to locate a directory given its DI (such as #108), GNS uses
a table of well-known directories. This table contains a list of the root directories of all

522 Chap. 10• Naming
componentname spaces. Since a name space that was created byintegrating two or more
name spaces mayagain beintegratedafter some time with some other name space toform
anew name space, this table basicallyhas entries for all those directories that are used as
workingroots within theentire name space. InourexampleofFigure 10.11(a),since there
are only two working roots (root-l and root-2), this table has two entries. Each GNS
server has a copy of this table.
Wheneverthe real root ofthe name space changes, all GNS servers are informed of
the new location ofthe real root. Names that start with the real root (such as <new-root/
root-l/user..l,project-lIfile-I» are interpreted by a GNS server in the usual manner. On
the other hand, names that start with the DI of a working root (such as <#1OB/user-l,
project-llftle-l» areinterpretedbyusing theinformationforthecorrespondingDI'sentry
from the table ofwell-known directories. Therefore, the GNS scheme allows the use of
both absolute names and names that are reJative to a working root.
Restructuring of a name space for accommodating organizational changes is also
supported by GNS. As an example, let usassume that the two component name spaces A
andBofFigure 10.10correspondtotwodifferentcompaniesthatwere integratedandnow
correspond to two branches ofa single company. Now suppose sometime later user...l of
the branch corresponding to the component name space B is transferred to the branch
corresponding to the component name space A. To accommodate this change, if the
subtree of user-l is simply moved to the root...] directory, the following problems
occur:
1. User-I is already present in root...1. This problem can be easily solved by
assigning a new user identifier, say user...3, to the transferred user.
2. Absolute names of the form <new-root/root...Zluser-L, ...> will no longer work
and need to bechanged. GNS solves this problem by inserting a "symbolic link"
inplace oftheoriginal entry for user-I inthe root-2directory, as shown inFigure
10.11(b). The GNS directory lookup procedure interprets the link as aredirection
to the new directory location of this user.
Sharing Remote Name Spaces on Explicit Request
This scheme, popularized by Sun Microsystems' Network File System (NFS) [Sandberg
et al. 1985], is also based on the idea of attaching the isolated name spaces of various
nodes tocreate anew name space. However, unlike thepreviousschemeinwhich asingle
namingstructureiscreatedbythe system, in this scheme users aregiventhe flexibility to
attach acontextofaremote name space toone ofthecontextsoftheir local name spaces.
Once a remote context is attached locally, its objects can be named in a location
transparent manner. The goal is to allow some degree ofsharing among the name spaces
ofvarious nodes in a transparent manner on explicit request by the users. Therefore, the
global view ofthe resulting name structure is a forest oftrees, one for each node, with
some overlapping due to the shared subtrees. Notice that, unlike the previous scheme in
which theentirename spaceofeach nodeisattachedtothesinglenaming structure,inthis
scheme the users have the flexibility to attach to their local name space tree either a
complete remote name space tree or a part ofit (subtree). Also notice that a request to

Sec.10.6• Human-Oriented Names 523
share aremotename space affects only the node from which the request was made and no
other node. This ensures node independence. The naming of file objects in NFS is
described below as an illustration of this scheme.
In NFS, the mount protocol is used to attach a remote name space directory to a
local name space. Therefore, to make a remote directory accessible in a transparent
manner from a node, a mount operation has to be performed from the node. The node
that performs the mount operation is called a client node and the node whose name
space directory is mounted is called a server node. NFS allows every machine to be
both a client and a server at the same time, A server must export all those directories
of its name space tree that are to be made accessible to remote clients. The complete
subtree rooted at an exported directory is exported as a unit. Therefore, all directories
below an exported directory automatically become accessible to a client when the
exported directory is mounted on the client's local name space. This is because when
a client mounts a server's directory, the mounting process sets up a link between the
mount point of the client's local name space and the exported directory of the server's
name space. To allow directories to be automatically exported by a server when it is
booted, the list of directories t.o be exported by the server is maintained in the fete/
exports file.
The semantics of the mount operation are that a server's file system directory
exportedbythe server ismounted over adirectory oftheclient'sfilesystem.The mounted
directory's subtree ofthe server file system appears to be an integral part of the client's
file system, replacingthesubtree descendingfrom theclient'slocaldirectory onwhich the
server's directory was mounted. The local directory becomes the name of the root ofthe
newly mounteddirectory. Hence, after the mount operation, local userscan access files in
the mounted remote directory inatotally transparentmanner.Toprogramsrunning on the
client machine, there is (almost) no difference between a file located on a remote file
server and a file located on the local disk.
A client can mount a directory in one of the following ways:
1. Manual mounting. In this case, the client uses the mount command any time a
server'sdirectory istobemounted ontheclient'slocal namespace.Theumountcommand
allows the client to unmount the directory when the client no longer needs to access the
files inthe subtree of the server's directory. Aclient must beasuperuserto usethe mount
and umountcommands.This method provides the tlexibility to the clients todynamically
mount or unmount servers' directories depending on changing needs.
2. Staticmounting, Inthe manual mountingscheme, amountcommandistobeused
every time a server's directory is to be mounted by aclient. The static mounting scheme
allows clients to automatically mount the directories of their choice out of the directories
exported by servers without manual intervention. In this method, the shell script for the
commands to mount the desired directories are written inafile called /etefrc on the client
machine. This shell script is automatically executed when the client machine is booted.
Therefore, all mountings automatically take place at boot time.
3. Automounting. In the static mounting scheme, since mounting is done at boot
time, all servers' directories listed intheletclrcfileofaclient are mountedeven when not

524 Chap.10 • Naming
being used by the client. The automounting scheme allows a server's directory to be
mounted only when the client needs it. That is, in this scheme servers' directories are
mounted and unmounted automatically on a need basis.
In this scheme, a server's directory is associated with a client's local directory, but
actual mounting is performed only at the time the client invokes a command to access a
file in the name space below the server's directory. Once mounted, the server's directory
remains mounted foraslongasitisneeded. Whenever5minuteshaveelapsedwithout the
name space below the server's directory being accessed by the client, the server's
directory is automatically unmounted.
Another important feature of this scheme is that instead of a single server's
directories, a set of servers' directories may also be simultaneously associated with a
directory ofthe client's local name space. In this case, when the client accesses a file in
thename space below themounted directory forthefirsttime, theoperatingsystem sends
a message to each of the servers whose directories are associated. The first one to reply
wins, and its directory is mounted.
The automounting scheme helps achieve fault tolerance because a serverneed beup
and working only when the client actually needs it. Moreover, when a set ofservers are
associated instead of a single server, only one of them need be up.
For an illustration of the file-mounting mechanism of NFS, consider the example
given in Figure 10.12 based on the example given in [Silberschatz and Galvin 1994].
Figure IO.12(a) and (b) shows parts ofthe name trees of nodes N) and N 2, respectively,
that are ofinterest here.The triangles represent subtrees ofdirectories thatare ofinterest.
Since nomounting hasbeendoneyet,onlylocalfilescanbeaccessed ateach node..Figure
10.12(c) shows the structure of the portion of interest of the name tree of node N after
J
mounting the directory luser21dir2of node N over the directory luserJldirJ of node N
2 J•
After this mounting, dir2 becomes a shared directory, being shared by the users of nodes
N J and N 2• Users on node N I can access any file within the dir2 directory by using the
prefix luserlldirlldir2. Notice thatthe original subtree below thedirectory luserlldirl on
node N is no longer visible.
J
An advantage of this scheme is that it provides the flexibility to a user to attach
only the necessary portions of remote name spaces on his or her local node's name
space. Another advantage of this scheme is that it is simple to implement as a
modification to UNIX. However, it suffers from several drawbacks. The main drawback
is that the scheme does not provide a consistent global name space for all nodes. Thus
the same object may have different "absolute" names when viewed from different
nodes. For example, Figure 10.12(c) and (d) shows the name tree structures of nodes
N, and N respectively, after some mountings done on user requests. Now the absolute
2t
names of an object in the directory dir2 will have the prefixes luserJldirlldir2 and
luser21dir2 on nodes N, and N 2, respectively. This lack of absolute naming facility can
cause difficulties for distributed application programs, since processes running on
different nodes are in different naming domains. For example, an application using
several nodes to process a data file would run into trouble if the file name were
specified as luserJlprojectlldatal; rather than opening the same file, participating
processes on nodes having identifiers rome and paris would respectively open Iromel

Sec.10.6• Human-Oriented Names S25
user1 dir3 , , user2
," ",
,," ",
dir1 '------------,
,
d
,
ir2
, ,
, ,
,," ,, ,
, ,
"------------'.
(a) (b)
user1 dir3 user2
dir1
dir1 dir2
'- , - ,, - ' -
,,
-
,
d
"
- ir - 2
-
-
.
- - - . - , -, "- "-
,,
-
,
-
,
-
, ,
--
,
-
, .
-
...,
-
,
- ...., -'.
(c) (d)
Fig. 10.12 Illustratingthe file-mounting mechanismof NFS naming scheme: (a) a
part of the name tree of node Nt;(b)apart of the name treeof node N 2;
(c) structureof name treeof node N 1 after mountingdir2 of node N 2 over
dirl;(d) structureof name tree of node N2after mountingdirt of node N,
over dir3.
user1lprojectl/datal and /paris/user1/projectl/datal. Even if the user were to explicitly
specify /paris/userl/projectl/datal, the program would fail if rome did not have paris's
file system mounted, or worse, had some other node's file system mounted under the
name /paris. The only way to ensure that all nodes use the same name for the same
object is by careful manual management of each node's mount table. Systems using this
approach do not scale well, because if every node mounts every other, the number of
mount points in the system is proportional to n2 (n = total number ofnodes), producing
a significant management and computing overhead.
Another serious drawback of this scheme is its administrative complexity. The
effects of a failed node, or taking a node off-line, are that some arbitrary set of
directories on different nodes become unavailable. Likewise, migrating objects from one
node to another requires changes in the name spaces of all the affected nodes.
Furthermore, the scheme does not provide complete transparency because the specifica
tion of a remote directory as an argument for the mount operation is done in a
nontransparent manner; the location (that is, node identifier) ofthe remote directory has
to be provided. Location independency is also not supported by the scheme because,
as discussed above, the absolute name of an object depends on the node from which
it is being accessed.

526 Chap.10 • Naming
A Single Global Name Space
Several distributed operating systems such asLOCUS [Walkeretal. 1983],Sprite [Welch
and Ousterhout 1986], V-System [Cheriton and Mann 1989], and Galaxy [Sinha et al.
1991a] use the single global name space approach for object naming. In this scheme, a
single global name structure spans all the objects of all the nodes in the system.
Consequently, the same name space is visible to all users and an object's absolute name
is always the same irrespective of the location of the object or the user accessing it.
Therefore,thisscheme supports bothtypes oflocation independency-thatofthelocation
oftheaccessingobject (e.g.,process) andthelocation oftheaccessed object. Theproperty
ofuser mobility forauser,that is,location independencyoftheaccessingobject, can also
be provided in the mount mechanism of NFSby mounting a shared file system over the
user's home directories on all machines in a network. However, supporting this properly
for all users in NFS will require significant management and computing overhead.
The single global name space approach is the most commonly used approach by
modern distributed operating systems due to its properties of complete .location
transparencyand location independency. Therefore, insubsequent sections of thischapter
we will concentrate on this naming scheme.
10.6.2 Partitioning a Name Space into Contexts
Name space management involves the storage and maintenance of naming information,
which consists of object names, object attributes, and the bindings between the two. Due
to reliability and space overhead problems in a distributed system, storing the complete
naming information at a centralized node or replicating it at every node is not desirable.
Therefore, thenaming information isusually decentralizedandreplicated. That is,instead
of one global name server, there are many name servers, each storing acopy of aportion
ofthe complete naming information. These name servers interact among themselves to
maintain the naming information and tosatisfy the name resolution requests of the users.
A basic problem is how todecompose the naming information database to bedistributed
among the name servers. The main goal ofa decomposition mechanism for this purpose
is to minimize the overhead involved in the maintenance of naming information and the
resolution of names.
The notion ofcontextisused forpartitioninganame space into smallercomponents.
Contexts represent indivisible units for storage and replication of information regarding
named objects. A name space is partitioned into contexts by using clustering conditions.
Aclusteringcondition isanexpression that, when applied toa name, returns either atrue
or false value, depending on whether the given name exists in the context designated by
the clustering condition. The three basic clustering methods are algorithmic clustering,
syntactic clustering, and attribute clustering. These are discussed below.
Algorithmic Clustering
In this method, names are clustered according to the value that results from applying a
function to them. Therefore, in algorithmic clustering, the clustering condition is a

Sec.10.6 • Human-Oriented Names 527
=
predicate ofthe formfunction(name) value. For instance, a hash function that clusters
names into buckets is a good example of a clustering condition for algorithmic
clustering.
As an illustration, a simple example of algorithmic clustering of names is given in
Figure 10.13. In this example, at first,func-l is applied to the names in the name space
to partition them into two contexts. One ofthe two contexts isstill found to betoo large.
Therefore, a second function (junc-2) is applied to the names of this context to further
partition thecontextinto smallercontexts. Thus starting with thecompletename space as
a single context, a sequence of clustering conditions can be applied to yield a group of
reasonably sized contexts.
London Paris Arizona
Poznan Oregon Berlin
Vienna Tokyo Kyoto
Nara Mexico Nebraska
Nagasaki
London Poznan
Berlin Vienna
Oregon Mexico Nara
Nagasaki Nebraska
London Poznan Vienna Oregon
Berlin Nara
Mexico Nebraska
Nagasaki
=
func-1(name):if(totalcharacters(name) even)
return(0)
elsereturn(1);
func-2(name):if(totalvowels(name)=even)
return(0)
elsereturn(1);
Fig.10.13 Anexampleof algorithmicclustering.

528 Chap.10 • Naming
The main advantage of algorithmic clustering mechanism is that it supports
structure-free name distribution. A structure-free name distribution is one that places no
restriction on the administrative control over parts of'a name space. The partitions of
such a name space do not correspond to the structure of the names, such as their sizes,
or the number of component names, or the order of the component names or characters
within a name, and so on. In particular, the owner of an object may choose its
authoritative name servers, subject to administrative constraints, independent of the
object's name. This permits maximum flexibility in the administration assignment (and
reassignment) of authoritative name servers to a given object. Algorithmic clustering
also has the advantage that it allows a healthy name resolution tree to be built even
for flat name spaces. However, the main drawback of the method is difficulty in
devising proper clustering functions. This is because the characteristics of the names to
be defined by the various users is normally unpredictable. Therefore, if the clustering
functions are not devised properly in the beginning, some contexts may become too
large while other contexts may be very small.
Syntactic Clustering
This is the most commonly used approach for partitioning a name space into contexts.
Syntactic clustering is performed using pattern-matching techniques. Patterns are
templates against which a name is compared. They range from names that may simply
contain wildcards, which are denoted byasterisks and match any sequence of characters,
toregularexpressions [Terry1986].Thus,asyntacticclustering condition thatismeantfor
a particular pattern, when applied to a name, returns TRUE if the name matches the
pattern. All names that match a particularpattern, such as names with a common prefix
*,
prefix are considered part of the same context.
Asanillustration, twosimple examples of syntactic clustering ofnames aregivenin
Figure 10.14. Figure 10.14(a) shows the partitioning of structured names and Figure
lO~14(b) shows the partitioning of flat names. It may be observed from Figure lO.14(a)
that in hierarchically structured name spaces pattern matching is usually performed on a
component-by-component basis and the resulting contexts usually contain only the
unmatched part of the names. Therefore, at subsequent levels, anew clustering condition
is applied only on the truncated part of the names. However, syntactic clustering
conditions are notrestricted to matching asingle additional componentineach step.This
leads to variable syntactic clustering in which a variable number of components can be
matched at a time.
The syntactic clustering mechanism allows names toberesolved inamanner similar
to their structure, as isdone by virtually all name management systems. This means that
simple matching suffices as a clustering technique.
Attribute Clustering
The method of attribute clustering is used by attribute-based naming conventions.
In this method, names are grouped based on the attributes possessed by the
names. An attribute has both a type and a value, where the type indicates the

Sec.10.6• Human-Oriented Names 529
/country/lndialDelhi Icountry/lndiaiBombay
/country/JapanlTokyo Icountry/Japan/Kyoto
luser/sinha/file1 luserlsinha/file2 /userlsinha/file3
Matchesthepattern
"/country/*"
sinhaIfile1
India/Delhi
sinhalfile2
India/Bombay sinhalfile3
JapanlTokyo pradeep/file1
Japan/Kyoto pradeep/file2
Delhi Tokyo
Bombay Kyoto
file1
file2
file2
file3
(a)
If'ig.10.14 Examplesof syntacticclustering: (a) partitioningof structurednames; (b)
partitioningof flat names. (Continuedon next page.)
format and meaning of the value field. Therefore, all names having the same
attribute (type, value) pair are placed in the same partition in the attribute
clustering mechanism.
As an illustration, a simple example of attribute clustering of names is given in
Figure 10.15. Observe that in a hierarchically structured name space, attribute
clustering is usually performed on an attribute-by-attribute basis, and the resulting
contexts usually contain only the unmatched attributes of the names. Therefore, at
subsequent levels, a new clustering condition is applied only on the remaining
attributes of the names. However, attribute clustering conditions are not restricted to
matching a single additional attribute in each step and several attributes may be
matched in a single step.

S30 Chap. 10 • Naming
Poppy Poster Sheep Silky
Powder Power Shark Silver
Silver Shark
Silky Sheep
(b)
."ig. 10.14 (Continued.)
= =
Type Object,Name P1
= =
Type Object,Name P2
Matchestheattribute
=
Type Source
I
Lang = Basic,Name = P1 Name=P1
Lang=C,Name = P2 Name=P2
Fig. 10.15 Example of attribute clustering.

Sec.10.6• Human-Oriented Names 531
10.6.3 Context 81nding
The contexts ofa name space are distributed among the various name servers managing
that name space. That is, a name server normally stores only a small subset ofthe set of
contexts of the name space. Therefore, when presented with a name to be resolved, a
server first looks in local contexts for an authority attribute for the named object. Recall
that an authority attribute contains a list of the authoritative name servers for a named
object. If the authority attribute for the named object is not found in a local context,
additional configuration data, called context bindings, must exist locally for the name
resolution to proceed. A context binding associates the context within which it is stored
to another context, that is more knowledgeable about the named object, and the name
servers that store that context. The two strategies commonly used to implement context
bindings in naming systems are table-based strategy and procedure-based strategy. These
are discussed below.
Table-Based Strategy
This is the most commonly used approach for implement.ing context bindings in
hierarchical tree-structured name spaces. In this method, each context is a table having
two fields; the first field stores a component name of the named object and the second
field eitherstorescontextbindinginformationor authority attributeinformation.Anentry
correspondingtothe lastcomponentofaname containstheauthorityattributeinformation
while other entries contain context bindings. The context bindings reflect the delegation
of authority for managing parts of the name space. Note that the amount of configuration
data that must be stored in context objects at the various levels of the hierarchy is
proportional to the degree of branching of the name space tree. The contexts of a table
based strategy are also known as directories.
As an illustration, an example of table-based strategy for implementing context
bindings is given in Figure 10.16. The figure also illustrates how an object can have two
different human-oriented names in this implementationstrategy. Therefore, the users ofa
shared object can specify their own suitable names for accessing it.
Procedure-Based Strategy
Name servers may also use procedure-based strategy rather than table-based strategy to
implementthe bindingsinagiven context. Inthis method,acontextbindingtakes theform
ofa procedure, which, when executed, supplies information about the next context to be
consultedforthenamedobject. For instance,intheexamplesofFigure 10.14,thesyntactic
clustering condition used by each contextcan also be used as theprocedurefor supplying
contextbindingsforthenamesdefinedwithinthatcontext.Noticethatnoconfigurationdata
isrequired by this schemeand hence thedata managementoverheadiszero. This scheme,
however, is less flexible than the table-based strategy because, if required, the context
bindings cannot be changed dynamically and independently for each object. Changing of
context bindings will require changes in the clustering procedures. Therefore, almost all
namingsystemsusethe table-basedstrategyfor implementingcontextbindings.

532 Chap. 10 • Naming
Rootdirectory
havingnameI/'
Contextbinding(CB)
Component or
name authorityattribute
(M)
user1 (CB)
user2 (CB)
Directoryhaving Directoryhaving
name'/user2' name'/user1'
Component CB/M Component CB/M
name name
x (CB) b (CB)
y (CB) d (M)
z (M)
Anauthoritativename
serverofobject'/user21z'
Directoryhaving
Directoryhaving
namesI/user1/b'
name'/user2ly'
andI/user21x'
Component
name CB/M Component CB/M
name
c (M)
Anauthoritativename
serverofobjecthaving
twonames'/user11b/c'
and'/user21xlc'
Fig. 10.16 Anexample of table-based strategy forimplementingcontext bindings.
10.6.4 Distribution ofContexts and Name Resolution
Mechanisms
Name resolution is the process of mapping an object's name to the authoritative name
servers of the object The process basically involves traversal of a resolution chain of
contexts (using the context binding information of each context in the chain) until the
authority attribute ofthenamedobject isfound. Obviously,thetraversal oftheresolution
chain ofcontexts for a name is greatly influenced by the locations of these contexts ina
distributed system. Therefore, the name resolution mechanism of a naming system
depends on the policy used fordistributing thecontexts of the name spaceof the naming
system. Some of the commonly used name resolution mechanisms are described below.

Sec.10.6 • Human-Oriented Names 533
Centralized Approach
In this method, a single name server in the entire distributed system is located at a
centralized node. This name serveris responsible for storing all contexts and maintaining
the name-mapping information in them up to date. The location ofthe centralized name
serverisknown toall othernodes. Therefore, name resolutionrequests from all nodes are
first sent to the centralized name server's node. The name server resolves a name by
traversing the complete resolution chain of contexts locally and finally returns the
attributes of the named object to the client node. An example of a system with a
centralized name service is the early DARPA Domain Name System [Mockapetris and
Dunlap 1988, Cerf"and Cain 1983].
The main advantage of this scheme is that it is simple and easy to implement. It is
also efficient because any name resolution request involves only two messages (arequest
and areply) to be passed across the network. The scheme, however, suffers from several
drawbacks. Forinstance, itdoes not scale well becausethe performanceofthecentralized
name server may become a serious bottleneck for large systems. Furthermore, the
reliability of this mechanism is also very poor. The inability ofa node to communicate
with the centralized name server's node due to a link failure of the network will create a
situation in which none of the name resolution requests ofthat node can be carried out.
An even worse situationoccurs if the centralized name server's node fails, in which case
none of the name resolution requests from any node of the system can be carried out.
Fully Replicated Approach
Inthis method, there is aname serverlocated on each node ofthe distributed system and
all contexts are replicated at every node. That is, the name space database is fully
replicated, and therefore each name server has the name-mapping information for all
objectnames. Obviously, all name resolutionrequests ofany node are servicedlocally by
the local name server. The Pup name service [Boggs 1983] and the DOLeN (Distributed
Double-Loop Computer Network) system [Lin et a1. 1982J use this approach.
The method is simple and efficient because all name resolution requests can be
serviced locally without the need to communicate with any other node. The method,
however, involves large overheadinmaintainingconsistencyof naming informationinall
servers and hence is not suitable for large name spaces.
Distribution Based on Physical Structure of Name
Space
This isthe most commonly used approachfor hierarchicaltree-structuredname spaces. In
this method, the name space tree is divided into several subtrees that are known by
different names in different systems, such as domainsin Sprite [Welch and Ousterhout
1986] andfile groupsin LOCUS [Walker et al. 1983]. Todifferentiate this concept with
otherterminologiesused before,the term zoneswillbeused here toreferto such subtrees.
In this scheme, there are several name servers in the distributed system, with each server
providing storage for one or more of these zones. An example is shown in Figure 10.17.

S34 Chap.]0 • Naming
--~
/ "."/a~ .... \
: b e\
~/" :
\c d "
,, ,I
.............._-"",,".
(a)
Zone identifier
Nameprefix
Specific zone
Serveridentifier
identifier
/ 51 12
/a 52 19
If/hlp 53 33
/m 52 87
/mlp 53 61
(b)
Fig. 10.17 Distributionandmanagementofcontexts basedonthephysicalstructure
ofthenamespace:(a) hierarchicalnamespacetreepartitioned intozones;
(b) samplenameprefixtable/cacheforthezonesofthenametreeof(a).
Note that each node of the name space tree corresponds to a context, and therefore the
contexts belonging to the same zone are grouped together in this scheme. Hence, in this
scheme, instead of contexts, zones represent indivisible units for storage and replication
regarding named objects.
Inthisscheme, theresolution ofanobject name basicallyinvolves thesending ofthe
name resolution request to the server that manages the subtree (zone) to which the last
componentofthenamebelongs.Forinstance,inFigure10.17,thenameresolutionrequests
for the object names laIble, tflh, /ftg, Iflhiplk, lmln, Imlplr, and Imlplrlsl must be sent to
servers S2' Sit SI' S3' S2' S3' and S3' respectively. To facilitate the mapping of names to
servers,eachclientmaintainsanameprefixtable(alsoknownasanameprefixcache)thatis

Sec.10.6• Human-Oriented Names 535
builtandupdateddynamically.Eachentryinthenameprefixtablecorrespondstooneofthe
zonesofthename spacetree.Itcontains thenameofthetopmost context (directory) inthe
zone(called thenameprefixforthezone)andazoneidentifier.Azoneidentifierconsists of
two fields-server identifier and specific zone identifier-where the specific zone
identifierisassignedtothezonebytheserver.Ithelps indifferentiatingamongthevarious
zonesmaintainedbythesameserver.Forexample, inFigure 10.17,serverS3maintainstwo
zonesbothofwhoseroots havethesamenamepbutdifferent zoneidentifiers33and61.A
server'sidentifierhasitslocationembeddedinit.
When a name is to be resolved, the client searches its own name prefix table for a
matching name prefix oftheobject name to beresolved. Notice that anyoneofI,la, lalb,
or/a/b/e isamatching nameprefix for anobjectnamed la/ble. Ifmore thanone matching
name prefixes are found, the longest one isselected and theclient uses thecorresponding
zone identifier information to send its request directly to the server that stores that zone.
The specific zone identifier of the table entry is also sent along with the request.
When the server receives the request, it uses the specific zone identifier associated
with the request to carry out the requested name resolution. It then maps the rest of the
componentnames ofthe named object with thecontexts (directories) oftheselected zone.
Ifalltheremainingcomponentsaremapped, the nameresolution operationcompletes and
the object's attributes are returned by the server to the client. On the other hand, if only
apartofthe remaining componentnames isresolved intheselected zone,thensomeother
zonehastobeused forfurther resolution ofthename;thatis,azonecrossing isnecessary.
In this case, the server returns the next zone's name prefix to the client for further
processing. The client then broadcasts the new prefix to obtain the zone identifier of the
next zone. The servers that store that zone respond to the broadcast message. Using the
responses, the new zone's details are entered into the name prefix table and the name
resolution proceeds as before by sending the request to a server of the new zone.
The name prefix table is built and updated dynamically with a broadcast protocol.
Initially,each client starts with anempty name prefix table. Entries areadded tothename
prefix tableofaclient only whenneeded.That is,the nameprefixofazonethathasnever
been accessed by a client will not appear in its name prefix table. In some systems, the
name prefix of the zone containing the root directory of the name space tree is known to
all nodes and is entered into a name prefix table at the time of its creation. With this
approach, a client never experiences a complete miss while searching a name prefix for
any name in the name prefix table.
Notice that in this scheme the name prefix of a zone may simultaneously exist in
multiple name prefix tables of the same or different nodes. Furthermore, the servers of a
zone may change dynamically, providing increased availability, security, and efficiency.
Therefore, an important issue of this scheme is the updating of entries in name prefix
tables to keep them consistent. Instead of informing all clients when the server of a zone
they are storing intheir name prefix tables changes, the consistency of name prefix table
entries ismaintainedby detecting and discarding/updating stale table entries on use.That
is, the server information stored in a name prefix table is made up of hints that are
corrected when they are found to be wrong. A hint is a piece of information that can
substantially improve performance if correct but has no semantically negative con
sequence if incorrect. For maximum performance benefit a hint should nearly always be

536 Chap.10 • Naming
correct. Of course, only information that is self-validating upon use is amenable to this
strategy. When a client tries to use a stale name prefix table entry, it ends up sending its
name requesttothewrong server.Iftheservernolongerexists, theclient receivesnoreply
and times out. Ifthe serverexists but no longer stores the given zone, the serverreports
that fact back tothe client. Ineithercase, theclient recognizes that its table entry isstale,
deletes the entry, and issues a broadcastfor up-to-date server information for the zone to
which the context corresponding to the named object's last component name belongs.
This scheme has the following advantages:
1. Matchingon aname prefix ratherthan on the full name allows a small numberof
table entries to cover a large number of names, resulting in good performance. For
example, in Figure 10.17, a single table entry for la allows every name ofobjects in this
zone to go directly to the correct server without the overhead of a broadcast or global
directory lookup. The zone having prefix name la can reasonably contain the attribute
information for hundreds or thousands ofobjects.
2. As comparedtoglobaldirectory lookup, in whichalldirectoriesstartingfrom the
root down to the last component name ofthe named object are searched one by one, the
name prefix table helps in bypassing part of the directory lookup mechanism. The
directory bypass has advantages in performance and reliability because a crash on one
name serverdoes not preventclients from resolving names in zones on otherservers. To
illustrate this, let us see the example given in [Welch and Ousterhout 1986]. In a system
withtwozonesIandla,theserver forIneed notbeavailableforresolvingthenamelalble.
If a client already has an entry for la in its name prefix table, then it will use it and
communicate directly with the server for that zone; otherwise it will broadcast the name
lalble and the server for la will respond directly to the client with zone identifier
information. In neither case is the server for I involved. This bypassing mechanism
reduces contention for the upper level directories. However, the bypassing of upper level
directories has consequenceson the system's security mechanism. Toillustrate this, once
again let us see the example given in [Welch and Ousterhout 1986]. Ifthere is a name
prefix la/ble and theclientlooks upthe namelalblcld, neitherlanorlalb isexamined; the
client communicatesdirectly with theservercontaininglalble.This means that any access
controlsin directoriesla orlalb will be ignored. The effect is that all programs implicitly
have search permission along the paths to all the directories. If access to a directory is to
be restricted, it must be restricted with access controls at the level of the directory or
below.
3. The on-use consistency checking of a name prefix table entry results in
considerable saving ofconsistency control overheads because there is no need to inform
all clients when a table entry they are storing becomes invalid.
Structure-Free Distribution of Contexts
Name services should be able to be reconfigured if the present servers become
overworked or overburdened with data or ifthe system scale changes due to addition or
removal of workstations (nodes). Reconfiguration often requires changing an object's

Sec.10.6• Human-Oriented Names 537
authoritative name server. In the lone-based context distribution approach, changing an
object's authoritative name server willrequire changing its name,storing allthecontexts
in the zone that corresponds to the object and is managed by the object's current
authoritativename server onthenewname server,orcreating anewzoneforstorage with
the object's new authoritative name server. The new zone will be a subzone of the zone
that corresponds to the object and is managed by the object's old authoritative name
server.The firsttwo solutions areobviously notacceptable.The thirdsolution mayrequire
duplication of some of the contexts in the old as well as the new zones, resulting in the
need to store such contexts on both the object's old and new authoritativename servers.
For example, in Figure 10.17(a), if the authoritative name server for the object named
la/ble is to be changed from 8 toS4' a new zone containing the contexts named la,lalb,
2
and lalble or the contexts named lalb and/alblc will have to be created for storage at S4.
Notice that inthe former case two contexts (Ia andlalb) willhave tobestored at bothS2
andS4' while inthe lattercase onecontext (/alb) needs duplication. Inlargenamespaces,
such changes may require the duplication of many contexts. Therefore, the lone-based
scheme has limited flexibility in the assignment/changing of an object's authoritative
name server.
To overcome this limitation of the approach of distributing contexts based on the
physical structure of the name space, some systems use the structure-free name
distribution scheme. Astructure-free name distribution isone that places norestriction on
the administrative control over parts of a name space. Therefore, in this scheme, any
context of the name space can be freely stored/moved at any name server independently
of any other context. That is, in thisapproach, each context may beconsideredto belong
to its own zone. A name service based on this approach permits maximum flexibility in
the assignment/changing of an object's authoritative name server. Furthermore, this
scheme also simplifies name management because name servers need not agree on what
zones make up the name space.
Insystems usingthisscheme, somepolicy isusedforthepartitioning anddistribution
of contexts among name servers. For example, in Galaxy [Sinha et al. 1991a], the
distributionandreplicationofcontexts among nameserversof variousnodesarebasedon
theideaofimprovingthereliability ofthe nameresolutionmechanism. The details ofthis
policy isgiven in[Sinha etal. 1992J.Also notice thatinthisscheme namespace structure
need not but can be exploited to partition and distribute the contexts among name
servers.
In this scheme, the name resolution operation foragiven object name iscarried out
bytraversingthecompleteresolution chain ofcontexts onacontext-by-contextbasisuntil
the named object's authoritative name server is encountered. For example, the pathname
lalble ofanobject isresolved byfirstconsultingtherootcontext (I) toobtain thelocation
of the pathname's next context (fa). Recall that this is stored in the root context as a
context-binding information. With the location information obtained, the context la is
consulted next to obtain the location of the pathname's next context (Ialb). Finally, the
named object's authoritative name server's information is obtained by consulting the
contextlalb. Notice that since each contextcan be freely placed at any server, so a name
resolution operation may migrate from server toserver, possibly hopping from one node
to another if the servers involved are located on different nodes.

538 Chap. ]0 • Naming
The are two important questions to be addressed in this type of name resolution
mechanism:
1. How should context objects be located?
2. How should one interact with name servers while resolving a name?
Methods to handle these questions are described below.
Locating Context ObjectsDuring Name Resolution. Recall that a qualified
name consists of a context/name pair. That is, a name is always associated with some
context. Therefore, when auser specifiesaname toberesolved, itsresolutionshould start
inthe contextto which itisassociated. Hence, the first step inthe resolution processisto
locate the context. However, locating a context involves resolving its name. That is, a
context-locatingoperationistriggeredbythe resolutionofaname inthe first place. Thus,
we observe an infinite recursion that must be solved using some special technique. Two
commonly used methods to tackle this problem are described below.
1. Usinga metaeontext. In this method, a special context, called a metaeontext, is
usedthatcontainsthename and authorityattributepairs forallcontextobjectsinthename
space. The size ofthe metacontextdependson the numberofcontexts inthe name space.
Ifit is small, it can be stored at all name server nodes. However, ifit is large, it is stored
only atsome name server nodes, and other name servers only store pointersto the servers
that store the metacontext. Therefore, given a name to be resolved in a context, the
metacontextis first consulted toobtainthe authority attributeof the context. The name is
then sent for resolution to one of the authoritative name servers of the context.
2. Always starting the resolution from the root context. This method is used in
those naming systems in which the name space is structured as a single global
hierarchical name tree. In this method, the root context is replicated at all name server
nodes. Given a name to be resolved in a context, we say that the name is relative to
that context. A relative name can be converted to an absolute name by prepending the
context name to it. Recall that an absolute name begins at the root context ofthe name
space. Therefore, in this method, given a name to be resolved in a context, the name
is first converted to its absolute form and then its resolution starts from the root context.
Since the root context is available at all name server nodes, the resolution of any name
can be started at any name server node.
Interacting with Name Servers During Name Resolution. We saw that the
various contextsofagiven pathname may be stored at different name servers. Therefore,
the resolutionofapathnameinsuch asituation will involveinteracting with all the name
servers that store one or more contexts of the pathname. During name resolution, a name
agent may interact with the name servers in one ofthe following manners:
1. Recursive. In this method, the name agent forwards the name resolution request
to the name serverthat stores the first context needed to start the resolution ofthe given
name. After this, the name servers that store the contexts of the given pathname are

Sec. 10.6 • Human-Oriented Names 539
recursively activated one after another until the authority attribute ofthe named object is
extracted from the context corresponding to the last component name of the pathname.
The lastname server returns theauthority attributetoitsprevious name server, which then
returns it to its own previous name server, and so on. Finally, the first name server that
receivedthe requestfrom the name agent returns theauthority attribute tothe name agent.
Figure 1O.18(a) illustrates the method of recursive interaction.
Client
7
Name
server
(a)
Client
2 7
3 6
(b)
Client
2 5
Fig. 10.18 Methodsofinteracting withname
servers duringname resolution:
(a) recursive; (b) iterative;
(c) transitive. (c)
As an example, if the name lalble is to be resolved, the name agent sends it to
the name server (say Sl) of the root context (I) and waits for a reply. Then S]
searches for the component name a in the root context, extracts the corresponding
binding information, sends the remaining pathname ble to the name server (say S2)of

540 Chap. 10 • Naming
the next context (/a), and waits for a reply. Then S2 extracts from context la the
binding information corresponding to the component name b, sends the remaining
pathname c to the name server (say S3) of the next context (Ialb), and waits for a
reply. Then S3 extracts from context lalb the authority attribute corresponding to the
component name c and returns it to S2' which in turn returns it to Sit and finally S,
returns it to the name agent.
Notice that in the recursive name resolution mechanism, the name agent.has little
work to do but the name servers may be involved in processing several requests at the
same time. Therefore, the name servers may get overloaded in situations where the
number of name agents is too large as compared to the number of name servers.
Hence, this mechanism is not suitable for use in those systems in which the ratio of
name agents to name servers is very high. Furthermore, to allow a name server to
start another job when waiting for a response, the name servers have to be
multiprogrammed.
2. Iterative. As shown in Figure 1O.18(b), in this method, name servers do not
call each other directly. Rather, the name agent retains control over the resolution
process and one by one calls each ofthe servers involved in the resolution process. As
in the recursive process, the name agent first sends the name to be resolved to the
name server that stores the first context needed to start the resolution of the given
name. The server resolves as many components of the name as possible. If the name
is completely resolved, the authority attribute of the named object is returned by the
server to the name agent. Otherwise, the server returns to the name agent the
unresolved portion of the name along with the location information of another name
server that the name agent should contact next..To continue the name resolution, the
name agent sends a name resolution request along with the unresolved portion of the
name to the next name server. The process continues until the name agent receives the
authority attribute of the named object.
3. Transitive. In this method, a name is resolved as follows. The name agent first
sends the resolution request to the name server that stores the first context needed to
start the resolution process. The server resolves as many components of the name as
possible. It then passes on the unresolved portion of the name to the name server that
stores the next context needed to proceed with the resolution process. This name
server resolves as many components of the name as possible and then passes on the
unresolved portion ofthe name to the next name server. The process continues and the
name server .that encounters the authority attribute of the named object returns it
directly to the name agent. The method is illustrated in Figure 10.18(c).
Notice that, as in the recursive method, in this method also the name agent
has little work to do. Also notice from Figure 10.18 that the transitive approach
requires the fewest number of messages. However, a sender does not receive any
acknowledgment message once it passes on the resolution operation to another
server. Therefore, this approach should be used in systems with reliable commu
nication. On the other hand, recursive and iterative approaches can be efficiently
supported by RPC-based communication systems because they use a "call-response"
model.

Sec. 10.7 • Name Caches 541
10.7 NAME CACHES
Readers might have observed that name resolution operations are not likely to be
especially cheap. Based on the measurements made by few researchers in the past, it
has been found that in operating systems that provide a flexible, hierarchical name
space, the system overhead involved in name resolution operations is considerably
large. For instance, Leffler et a1. [1984] attribute 40% of the system call overhead in
UNIX to file name resolution. Also, Mogul's measurements of the UNIX system call
frequency indicate that name-mapping operations (open, stat, lstat) constitute over 50%
of the file system calls [Mogul 1986]. Sheltzer et al. [1986] also made an observation
that in a large distributed system a substantial portion of network traffic is naming
related. Hence it is very desirable for a client to be able to cache the result of a name
resolution operation for a while, rather than repeating it every time the value is
needed.
Work has been carried out in the past by some researchers [Sheltzer et al. 1986,
Cheriton and Mann 1989] to investigate whether a distributed name cache is a suitable
solution to improve the performance of name service as wen as to reduce the overall
system overhead. The conclusion drawn by these researchers is that a simple distributed
name cache can have asubstantial positive effect ondistributedsystem performance.This
is mainly due to the following characteristics of name service related activities:
1. High degree oflocality ofname lookup. The property of "locality of reference"
has been observed in program execut.ion, file access, as well as database access.
Measurementsmade bySheltzeretal. [1986] andCheritonand Mann [1989]clearlyshow
thatahighdegree oflocality alsoexists inthe useofpathnamesforaccessing objects. Due
to this locality feature, a reasonable size name cache, used for caching recently used
naming information, can provide excellent hit ratios.
2. Slow update ofname information database. It has also been found that naming
data does not change very fast, so inconsistencies are rare. The activity of most users
is usually confined to a small, slowly changing subset of the entire name information
database. Furthermore, most naming data have a high read-to-modify ratio. This
behavior implies that the cost of maintaining the consistency of cached data is
significantly low.
3. On-use consistency ofcached information is possible. An attractive feature of
name service related activity isthat it is possible to find that something does not work if
one tries to useobsolete naming data, sothat itcan be attended to atthetime ofuse. That
is, name cache consistency can be maintained by detecting and discarding stale cache
entries on use.With on-use consistencychecking,there isnoneed toinvalidateallrelated
cacheentries when anaming data update occurs, yet stale data never causes aname tobe
mapped to the wrong object.
Some issues specific to the design of name caches are discussed in the next
section.

542 Chap. 10 • Naming
10.7.1 Types of Name (aches
Depending on the type of information stored in each entry,a name cache may be of one
of the following types:
1. Directory cache. In this type of name cache, each entry consists of a directory
page. This type of cache is normally used in those systems that use the iterative method
of name resolution. All recently used directory pages that are brought to the client node
during nameresolution arecached forawhile.Forexample,LOCUS [Sheltzeretal. 1986]
uses name caches of this type. The argument given by LOCUS designers in favor of
caching directory pages is that common tasks such as listing the contents of a directory,
expanding wildcardarguments,andaccessing parentdirectoriesalluseinformation found
in directory pages. However, this means that for only one useful entry of a directory an
entire page of the directory blocks a large area of precious cache space requiring large
sized name caches.
2. Prefix cache.This type of name cache is used in those naming systems that use
the zone-based context distribution mechanism. In this type of name cache, each entry
consists ofaname prefix and thecorresponding zone identifier.Recall that aname prefix
corresponds toa zone in the zone-based context distribution approach. Details of the use
ofthistypeofnamecachehasalready beenpresented inSection 10.6.4.Sprite [Welchand
Ousterhout 1986]and V-System [Cheriton and Mann 1989]use this type of name cache.
This type ofnamecache isnotsuitable forusewiththestructure-free context distribution
approach.
3. Full-namecache.Inthistypeofnamecache,eachentryconsistsofanobject'sfull
pathname and the identifier and location of its authoritative name server. Therefore
requests foraccessing anobject whose nameisavailable inthelocalcachecan bedirectly
senttotheobject'sauthoritativenameserver.Thistypeofnamecachecanbeconveniently
used with anynaming mechanism, although itismainly usedbythe naming systems that
use the structure-free context distribution approach. Notice that inaprefix cache anentry
usually servesasamapping information forseveralobjects, butinafull-name cacheeach
entry serves as a mapping information for only a single object. Therefore, full-name
caches are usually larger in size as compared to prefix caches. This type of name cache
is used in Galaxy [Sinha et al. 1991b].
10.7.2 Approaches for Name (ache Implementation
The two commonly used approaches for name cache implementation are as follows:
1. A cache per process
2. A single cache for all processes of a node
Both approaches have their own advantages and drawbacks. In the first approach, a
separate name cache is maintained for each process. Each cache is maintained in the
corresponding process's address space and is usually small in size. Therefore, accessing
ofcached information isfast and no memory area ofthe operating system isoccupied by

Sec.10.7 • NameCaches 543
the name caches. However, a process-oriented name cache vanishes with the process.
Therefore every new process must create its name cache from scratch. Hence, if the
processes are short lived, the caches will have short lifetimes, and the true hit ratio will
be fairly low for process-oriented caches due to start-up misses, which are initial misses
thatoccur when anew,empty cache iscreated. Furthermore,theuseof aprocess-oriented
cache is limited only to a single process due to which there isa possibility that the same
naming information is duplicated in several caches of the same node. To alleviate the
problem of start-up misses, V-System implementation [Cheriton and Mann 1989] uses
cache inheritancetogivecached dataalonglifetime. Inthismethod, eachprocessinherits
its initial cache contents from its parent process (usually the V-System command
interpreter or "shell").
In the second approach, a single name cache is maintained at each node for all the
processes ofthatnode.Ascomparedtotheprocess-orientednamecaches, thesecaches are
larger in size and are located in the memory area of the operating system. Accessing of
cached information is slower as compared to that of process-oriented caches. However,
cached information in a single-name cache is long lived and is removed only when the
cache replacement policy finds it suitable for being removed. Therefore the problem of
start-up misses is not associated withthe single-namecache approach, resulting in higher
average hitratioascomparedtotheprocess-orientedcaches.Thepossibility ofduplicating
cached information on the same node is also not there in this approach.
SpriteandV-Systemuseprocess-orientednamecaches, whereas LOCUS andGalaxy
use the single-name cache approach.
10.7.3 Multlcacha Conslstancy
When anaming data update occurs, related name cache entries become staleand must be
invalidated or updated properly.Twocommonly used met.hods formulticache consistency
of name caches are immediate invalidate and on-use update.
Immediate Invalidate
Inthis method, allrelated namecache entries are immediately invalidated whenanaming
data update occurs. There are two ways of doing this. In the first approach, whenever a
naming data update operation is performed, an invalidate message identifying the data to
be invalidated is broadcast to all the nodes in the system. Each node's name caches are
then examinedfor thepresence of the updated data, and ifitispresent, thecorresponding
cache entry is invalidated. Although this approach may work well for a system having a
few nodes, its use becomes prohibitive for very large networks having many nodes.
Toavoid the use of broadcast protocol, in the second approach, the storage node of
naming data (for example, the storage node of a directory) keeps a list of nodes against
each data thatcorresponds tothenodes on which thedata iscached. When astorage node
receives a request for a naming data update, only the nodes in the corresponding list are
notified to invalidate their corresponding cache entry. This method is acceptable only if
there is a low rate of update to naming data that are shared among nodes and only if a
small number of nodes share a naming data when that data is modified.

544 Chap. 10 • Naming
On-Use Update
This is themorecommonly used method formaintaining name cache consistency. In this
method, no attempt .ismade to invalidate all related cache entries when a naming data
updateoccurs. Rather,whenaclient usesastalecached data,itisinformed bythenaming
system that the data being used is either incorrectly specified or stale. On receiving a
negative reply,necessary steps aretaken (either by broadcasting ormulticasting arequest
or by using some other implementation dependent approach) to obtain the updated data,
which is then used to refresh the stale cache entry.
10.8 NAMING AND SECURITY
An important job of the naming system ofseveral centralized and distributed operating
systems is to control unauthorized access to both the named objects and the information
inthe naming database. Many different security mechanisms have been proposed andare
used by operating systems to control unauthorized access to the various resources
(objects) of the system. Chapter 11fully addresses security issues.This section describes
only those security issues that are pertinent toobject naming.Three basic naming-related
access control mechanisms are described below.
10.8.1 ObJ-ct Namas AsProt.ctlon Hays
Inthis method, anobject'sname actsasaprotectionkey fortheobject. Auser whoknows
the name of an object (i.e., has the key for the object) can access the object by using its
name. Notice that an object may have several keys in those systems that allow an object
to have multiple names. In this case, any of the keys can beused to access the object.
In systems using this method, users are notallowed bythe system to define a name
for an object that they are not authorized to access. Obviously, if a user cannot name an
object, heorshecannot operate on it.This scheme isbasedontheassumption thatobject
names cannot be forged or stolen. That is, there is no way fora user to obtain the names
ofotheruser'sobjects andthenamescannotbeguessedeasily.However,inpractice, since
object names are generally picked to be mnemonic, they can often be guessed easily.
Therefore, the scheme does not guarantee a reliable access control mechanism. Another
limitation ofthisscheme isthatitdoes notprovide theflexibility of specifying themodes
of access control. That is, a user having a name for an object usually has all types of
possible access rights for the object. For instance, providing only read access to a file
object toone user and both read and write accesses toanother userisnotpossible bythis
scheme alone.
10.8.2 Capabilities
This is a simple extension of the above scheme that overcomes its limitations. As shown
in Figure 10.19,a capability is a specialtype of object identifier that contains additional
information redundancy forprotection. Itmay beconsidered asan unforgeable ticketthat

Sec.10.8• Naming andSecurity S4S
Fig.10.19 Thetwobasicpartsofa
Objectidentifier Rightsinformation
capability.
allows its holders to access the object (identified by its object identifier part) in one or
more permission modes (specified by its access control information part). Therefore,
capabilities are object names having the following properties:
1. A capability is a system-oriented name that uniquely identifies an object.
2. Inadditiontoidentifyinganobject,itisalso used toprotecttheobjectitreferences
bydefiningoperationsthat maybeperformedontheobjectitidentifies.
3. A client that possesses a capability can access the object identified by it in the
modes allowed by it.
4. There are usually several capabilities for the same object. Each one confers
differentaccessrights toits holders. The same capabilityheld bydifferentholders
provides the same access rights to all ofthem.
5. All clientsthat have capabilitiesto agivenobjectcan share this object. The exact
mode ofsharing depends on the capability possessed by each client ofthe same
object.
6. Capabilitiesare unforgeableprotectedobjectsthat aremaintainedbytheoperating
system and only indirectly accessed by the users. Capability-based protection
relies onthe factthat thecapabilitiesare never allowedtomigrateintoanyaddress
space directly accessible by a user process (where they could be modified). If all
capabilities are secure, the objects they protect are also secure against
unauthorized access.
When aprocess wants toperformanoperationon anobject, itmust send tothename
server a message containing the object's capability. The name server verifies if the
capability provided by the client allows the type of operation requested by the client on
therelevantobject. Ifnot,a"permissiondenied" messageisreturnedtotheclientprocess.
Ifallowed,theclient'srequestisforwarded tothe manageroftheobject. Noticethat inthe
capability-based approach, there is no checking ofuser identity. If this is required, some
userauthenticationmechanism must beused. Chapter 11deals withthese issues ingreater
detail.
10.8.3 Associating Prot.ctlon withNama A.solutlon Path
Protectioncan be associatedeitherwith an objector with the name resolution path ofthe
name used to identify the object. The more common scheme provides protection on the
name resolution path.
Systems using this approach usually employ access control list (ACL) based
protection, which controls access dependent on the identity of the user (described in
Chapter 11).The mechanism based on AC:L requires, in addition to the object identifier,
anothertrustedidentifierrepresentingtheaccessingprincipal,theentitywith which access

S46 Chap.10 • Naming
rights are associated. This trusted identifier might be a password, address, or any other
identifierform that cannot be forged or stolen. AnACL is associated with an object and
specifies the user name (user identifier) and the types ofaccess allowed for each user of
thatobject. When auserrequestsaccess toanobject, theoperatingsystem checks theACL
associated with that object. If that user is listed for the requested access, the access is
allowed. Otherwise, aprotection violationoccurs and the userjobis denied access to the
object.
By associating anACL with each context (directory) of the name space, access can
becontrolledtoboth thenamed objects andtheinformationinthenamingdatabase.When
a name server receives an access request for a directory, it first verifies if the accessing
process is authorized.forthe requested access. With this approach, name servers do not
provideinformationtoclients thatare notauthorizedtohave it,andatthe same time name
servers do not accept unauthorized updates to naming information stored in the contexts
ofthe name space.
Regarding access control to an object, a client accesses an object by specifying its
name. A name is first resolved using the context objects to find out the named object's
authoritative name server. If theACL associated with any ofthe contexts that correspond
tothe name oftheobjectdoes notallow theclient toaccess contextinformation,the name
specifiedbytheclientcannotberesolved and theclient willautomatically not beallowed
to access the object by using that name.
Notice that, toallow access toanobjectinthisscheme, ausermust beallowedaccess
toboththedirectoriesoftheobject'spathnameand theobjectitself.Therefore,associating
protection with the name resolution path of an object name provides an additional layer
ofprotection to the named object. Also notice that, in systems where objects may have
multiple pathnames (such as acyclic or general graphs), a given user may have different
access rights to an object depending on the pathname used.
10.9 CASE STUDY: DCE DIREOORY SERVICE
Asacase study ofhow thenaming conceptsandmechanismsdescribedinthischaptercan
beused to build anaming system for adistributedcomputing system, the DCE Directory
Service is briefly described below.
Recall fromChapter1thatinaDCE system, users, machines, andother resourcesare
grouped into cells. Therefore, the DCE Directory Service mainly has the following types
ofcomponents for intracell and intercell naming:
1. Cell Directory Service (CDS), which controls the naming environment used
within a cell. Every cell has at least one CDS server.
2. Global DirectoryService (GDS), which controls the global naming environment
outside(between)cells. It links cells togethersothat any cell can belocated from
any other cell. GDS implementation is based on the X.500 directory service,
which is an international standard for naming defined by the CCITT and ISO
standards organizations. Therefore, X.500-style names (described later) are used
in DCE for naming cells.

Sec. 10.9 • Case Study: DeE DirectoryService 547
Since many DCE users use the Internet, DCE also supports the standard Internet
DomainNameSystem(DNS)forcell naming. Therefore, cell names in DeEcan also be
specified in DNS notation (described later).
10.9.1 DCE Name Space andNaming Convention
The DCE uses the single global name space approach for object naming. That is, the
DCE name space is a single, worldwide structure, with a global root denoted by the
symbol I.... Below this root appears the GDS name space, used to name each cell. If
DNS names are also used to name cells in the same DCE system, its name space also
appears below the DeE global root, sitting by the side of the GDS name space. Finally,
each cell contains its own internal name space, starting from the cell root. An example
of the DCE name space structure is shown in Figure 10.20.
I...
Fig. 10.20 The namespacestructureofa
DCE system having ncells that
usesbothX.SOOandDNS
notations forcellnaming.
Each object in DeE has a unique name that mainly consists of the following parts
each of which is separated by a slash (see Fig. 10.21):
1. Aprefix.This partindicates whetherthename islocaltothecurrent cellorglobal
to the entire DeE name space. The prefix I.: is used to denote a local name,
whereas the prefix I...is used to denote a global name.
Prefix Cellname Localname
/...meansglobal inX.500notationor UNIX-like
/.:meanslocal inDNSnotation hierarchicalname
Fig. 10.21 TheDCE namingconvention. Objectname=prefix/cellname/localname

S48 Chap.10 • Naming
2. Cellname.This is an optional part specified only when the prefix /... is used for
thefirstpartofthename.This partcan bespecified either inX.5ODnotation orin
DNS notation.A global name must contain this part, whereas a local name must
not contain this part.
3. Localname.This part uniquely identifies an object within acell. The UNIX-like
hierarchical naming scheme is used for local names.
The X.500 and DNS notations for cell naming are briefly described below.
The X.SOO Notation
The OSI X.500 is an international standard for naming people, computers, resources,
services, or anything else that needs a unique name. It uses the hierarchical, attribute
based naming scheme (see Fig. 10.22). Therefore, in the X.5OD notation, a name is
represented byasetofattributes separated byslashes.Eachattribute hasanattribute type
andoneormorevalues.The typeand valueofanattribute areseparated byanequal sign.
Therefore, a typical X.5OD name may be of the form
[Country»USIOrgType=COMIOrgName=JBMIDept=ENGIPerson=Nancyl
which uniquely identifies a person named Nancy who belongs to the ·engineering
department of a company named IBM in the United States. In X.500 terminology, each
Fig.l0.22 PartofanX.500directoryinformation tree.

Sec. 10.9 • CaseStudy: DCEDirectory Service 549
component of a name is called the relative distinguishedname(RDN)and the full name
is called the distinguished name(DN).
X.500 also provides the facility to define and use aliases, which are similar to
symbolic links in a file system.
The X.500 name tree is called the Directory Information Tree (DID and the entire
directory structure including the data associated with the nodes is called the Directory
Information Base (D/B). A DIB entry consists of a set of attributes described in the OSI
ASN.l notation (a standard notation for syntax definitions). For each attribute type, the
descriptionincludes atypedescription andasyntaxdefinition defining representationsfor
all permissible values of the type. New attributes can be defined as and when required.
X.500 uses an object-oriented information model for grouping DIB entries into
classes. Each DIB entry hasanObjectClassattribute thatdetermines theclass (orclasses)
of the object to which an entry refers. For instance, inthe DIT of Figure 10.22, Country,
Org'Iype, OrgName, Dept, andPersonareallexamples ofvalues ofObjectClassattribute.
Thedefinitionofaclassdetermines whichattributes arcmandatory andwhichareoptional
forentries ofthegiven class.TheObjectClassattribute isalways mandatory,whose value
must bethe name ofone ormoreclasses. IftheObjectClassattribute ofanobject hastwo
or more values, that object inherits the mandatory and optional attributes of each of the
corresponding classes.
X.500, being astandard, does not address implementation issues. Readers interested
in a more detailed description of X.500 and methods for its implementation may refer to
[Rose 1992].
The DNS Notation
The DNS isthe standard scheme for naming hosts and other resources on the Internet. It
uses a hierarchical, tree-structured name space partitioned into domains. In this scheme,
a name consists of one or more strings called labels separated by thedelimiter ".". There
isnodelimiteratthebeginning orendofaname. Names arewritten withthehighest level
domain on the right.
The Internet DNS name space is partitioned both organizationally and according to
geography. It divides the world up into top-level domains consisting of country names
such as us (United States) (this isthedefault top-level domain and isoften omitted inthe
qualified name of anobject belonging tothisdomain), uk(United Kingdom),fr(France),
andjp (Japan). Different countries then have their own organizational domains that form
the next level of domains of the name space tree. For example, in the United States, the
organizational domains are edu (educational institutions), com (commercial organiza
tions), gov (government agencies), mil (military organizations), net (network support
centers), int (internationalorganizations), andorg (organizationsnotincluded inanyofthe
above-mentioneddomains). The organizationaldomains inturn have subdomains such as
stanford.edu, ibm.com, anl.gov, and ieee.org. These subdomains in turn have sub
subdomains such as cs.stanford.edu.
Registration authorities responsible for the registration of names in a domain at a
particularlevelaredifferent. Forexample, thedomain cs.stanford.edu.us,whichstands for
the department of Computer Science at Stanford educational institution of the United

550 Chap. 10• Naming
States, can contain any name the department wishes. But the name cs in the domain
stanford.edu.ushas tobeagreed with the StanfordUniversityauthorities, who managethe
domain stanford.edu.us. Similarly, the name stanford in the domain edu.us has to be
agreed with the registration authorities who manage the domain edu.us, and so on.
In summary, Figure 10.23 shows the three different ways that may be used to refer
to an object in DCE.
I...IENG.IBM.COM.USlnancylletters/tollucy
(a)
I...ICountry=US/OrgType=COM/OrgName=IBM/Dept=ENG/nancy/letterslto/lucy
(b)
I.:/nancy/letters/to/lucy
(c)
Fig.10.23 Three different ways torefer tothesameobject inDeEDirectory
Service. Global object name withcell name specified in(a) Internet
format (DNS notation) and (b) ODS format (X.500 notation);
(c) localobject name withinacell.
10.9.1 Intrac.1I Naming InDCE
Allnameswithin acell aremanagedbytheCDS, whose functions and implementationare
briefly described below.
CDS Directories
The CDS ofacell basically manages the CDS directoriesofthe cell that are organized in
ahierarchicaltree structure.Thesedirectoriescollectivelycontainthenamesandattributes
ofall the objects within the cell. Each CDS directory has a number ofdirectory entries.
Each directory entry consists ofa name and a set ofattributes. For example, a directory
entryfor aprintermight containits name, itslocation, and itscharacteristics, such as type
and speed.
DCEuses the approachofassociatingprotection with the name resolutionpath ofan
object name. Therefore, each directory entry also has protection information associated
with it that specifies which users have what types ofaccess rights (such as read, delete)
for the entry. The CDS manages this protection information. Note that permission to
access adirectoryentry does not imply permissiontoaccess the named object. Protection
information for the named object is managed by the server that manages the object.

Sec. 10.9 • Case Study: DeE Directory Service 5S1
Therefore, the server knows which users have what types of access rights for the
object.
Replication of Naming Information
For better performance and reliability, CDS supports replication ofits information, with
the unit of replication being a directory. A collection of directories forms a
clearinghouse. A clearinghouse is a physical database managed by a CDS server. While
every DCE cell must run at least one CDS server, most will choose to run two or more,
with critical information replicated among them. Each CDS server maintains one or
more clearing houses. Each replica of a directory resides in a different clearinghouse.
The root directory is replicated in all clearinghouses to allow a search for any name
to be begun by any CDS server. This is because when a new directory is created, CDS
automatically creates an entry for this directory in its parent directory (the directory
immediately above the new directory in the hierarchical tree-structured name space).
This entry is used to track the location of the child directory even when the parent and
child directories are located in different clearinghouses. The root directory contains
entries for all its children directories. Those directories, in turn, contain entries for their
own children directories, and so on. Therefore, given the root directory, this path of
connectivity enables CDS servers to find every directory (and thus every entry) in the
name space.
Consistency of-Replicated Naming Information
To maintain the consistency of the naming information in replicated directories, DeE
uses the primary-copy protocol for directory update operations. That is, for each
replicated directory, one copy is designated as the primary copy and all others are
secondary copies. Read operations can be performed using any copy of a replicated
directory, primary or secondary. But all update operations are directly performed only
on the primary copy. One of the following two approaches is used to update the
secondary copies:
1. Update propagation. In this method, when the primary copy of a directory is
updated, the changes are immediately sent to all the secondary copies. This
method is used for updating the naming information that must be kept consistent
all the time.
2. Skulking. In this method, the changes made to the primary copy are accumulated
and are periodically sent together in a single message. This method is used for
updating less critical naming information.
CDS Implementation
The CDS implementation uses the client-server model. That is, there are CDS server
daemon processes and CDS client daemon processes. A CDS server runs on a server
machine, stores and manages one or more clearinghouses, and handles requests to

552 Chap. 10• Naming
create, modify, or look up names in its local clearinghouse. On the other hand, a CDS
client, called a CDS clerk, runs on every client machine that uses CDS. A CDS clerk
receives a request from a client application, interacts with one or more CDS servers to
carry out the request, and returns the resulting information to the client. A CDS clerk
also maintains a name cache in which it saves the results of name resolution requests
for future use. The cache is written to disk periodically so that the information in it can
survive a system reboot or the restart of an application.
How CDS Clerks Learn about CDS Servers
A CDS clerk learns about the existence of a CDS server in one of the following ways:
1. Bybroadcasting. CDS servers periodically broadcast their existence. CDS clerks
learn about CDS servers by listening to these broadcast messages. This method allows
CDS clerks to learn about all those CDS servers that reside on the same LAN as that of
the CDS clerk.
2. During a name resolution. During a name resolution, if a contacted CDS server
cannot completely resolve the name with the information in its local clearinghouse, it
returns to the requesting CDS clerk the location of another CDS server that has more
information about resolving thegiven name.Suchareply fromaCDS serverduring name
resolution helps a CDS clerk to learn the existence of another CDS server that it was
unaware of until now.
3. By management command. A DCE administrator can use the CDS control
program to create information about a CDS server in a CDS clerk's cache. This method
isnormally usedwhentheCDSclerk andtheCDSserverresideondifferent LANssothat
broadcast messages sent by the CDS server on its own LAN cannot be received by the
CDS clerk on a different LAN.
Name Resolution
Anameresolution operation (calledlookup inCDS) isperformedinthemanner described
below.The stepsof thisdescription correspond tothestepsofFigure 10.24,which shows
how a simple name resolution operation is performed in DCE:
1. A client application sends a lookup request to its local CDS clerk in an RPC
message.
2. The CDSclerkchecks itscache forthe name. Ifit isfound inthecache, theCDS
clerk returns a reply to the client and the name resolution operation completes.
3. If the name is not found in the cache, the CDS clerk does an RPC with a CDS
server that it knows about.
4. With the directories available in its local clearinghouse, the CDS server tries to
resolve as many components of the name as possible.
5. Ifthenamecanbecompletely resolved, theCDS serverreturns theresultofname
resolution to the CDS clerk.

Sec.10.9 • CaseStudy:DCEDirectory Service SS3
Clientmachine Servermachine
3
5
Name
cache
Directories
."ig.l0.24 Steps of asimple name resolution inDCE.Complex name resolutions
involving multiple CDS servers usethe iterative approach of
Figure JO.18(b).
6. The CDS clerk caches this information in its cache for future use.
7. The CDS clerk finally returns a reply to the client and the name resolution
operation completes.
If the name can only be partially resolved by the contacted CDS server in Step 4
above (remember that partial name resolution is always possible since the root directory
isreplicatedinevery clearinghouse),theCDS server returns totheCDS clerk thelocation
of another CDS server that has more information about resolving the given name. The
CDS clerk then interacts with this newly learned CDS server for getting the name
resolved. This stepisrepeateduntilthe namegels resolved and theCDS clerk receives the
result of name resolution from the last contacted CDS server. That is, the iterative
approach of interaction among the name agent and name servers is used during name
resolution. The CDS clerk caches the result of name resolution and the information of
newly learned CDS servers and returns a reply to the client.
Notice from the description above that CDS only performs the name resolution
operation. It does not perform an object-accessing operation. To access the object after
successful name resolution, a client must do an RPC with the server that manages the
named object.
10.9.3 IntereellNamingin DeE
Aclient in one cell may want to access an objectthat belongs to anothercell. Toresolve
namesthatpointtoobjects inothercells,CDSclerksmusthaveawaytolocateCDSservers
inothercells.WehavealreadyseenthataDeEsystemhasaGDSnamespaceforcellnames
stored inX.500 notation. This namespaceismanagedbyaGDSserver.Wealsosawthatin
addition to aODS name space forcell names, aDeEsystem may also have aDNS name
spaceforcell names inDNSnotation. Thisname space ismanagedbyaDNSserver. These
twoname spaces mapacellname toaCDSserverwithinthatcell.

554 Chap.10 • Naming
In addition, another component caJledGlobal DirectoryAgent (GDA) exists in any
cell that needs to communicate with other cells. It can exist either on the same machine
as aCDS server oron an independent machine. In the intercell name resolution example
of Figure 10.25, I have assumed that the GDA exists on an independent machine. A cell
can even have more than one GOA for increased availability and reliability. The CDS
servers of a cell have information about the location of the local GDA.
GOSmachine DNSmachine
DBofGOSnamespace DBofDNSnamespace
forcellnames(inX.SOO forcellnames(inDNS
Notation) Notation)
<; <, 8
...... ............
~------~"'6""",' ~~~~
...............
CDS
machine
2
Client Name
machine cache
12
CDSmachineoftheremotecell
towhichthenamedobjectbelongs
Fig.10.25 Intercellname resolution inDeE.
Now letusseethesteps involved intheresolution ofanamethatpoints toanobject
in anothercell. The steps in the following description correspond to the steps of Figure
10.25, which shows how intercell name resolution is performed in DCE:
1. A client application sends a lookup request to its local CDS clerk in an RPC
message.

sss
Sec. 10.9 • Case Study: DeE Directory Service
2. TheCDSclerkchecksitscachefor the name. Ifitisfound inthe cache,the CDS
clerkreturnsareplytotheclientand the nameresolutionoperationcompletes.
3. If the name is not found in the cache, the CDS clerk does an RPC with aCDS
serverasking for the location ofthe GOA. Recall that a global name must have
acell nameandalocalnamemustnot haveacellname.Therefore,by seeingthe
name supplied by the client, the CDS clerk knows that it is a global name and
the GDA must be contacted to resolve its cell name.
4. The CDS server returns the location ofthe GDA to the CDS clerk.
5. The CDS clerk then does an RPC with the GDA, sending it the cell name
embedded in the name to be resolved.
6. The GDA checks to see which notation has been used to name the cell. If it is
X.500 notation, the GOA does an RPCwith the GOSserver. On the otherhand,
if the notation used is DNS notation, the GOA does an RPC with the DNS
server.
7. 'The ODS or DNS server looksup the cell name in its database.
8. It returns to the GOA the address ofa CDS server in the named cell.
9. The GOA forwards this information to the CDS clerk.
10. The CDS clerk now uses this information to send its name lookup request to
the CDS server ofthe cell to which the named object belongs. The CDS server
resolves the name using the directories in its clearinghouse. In this example,
we assume that the contacted CDS server can completely resolve the name
with the directories in its clearinghouse. If this is not the case, an iterative
approach is used by the CDS clerk to interact with other CDS servers of the
remote cell.
11. The CDS server returns the result of name resolution to the CDS clerk.
12. The CDS clerk caches this information in its cache for future use.
13. The CDS clerk finally returns a reply to the client and the name resolution
operation completes.
10.9.4 User Interfaces to the DeE Directory Service
DeE Directory Service supports the following types of user interfaces for users having
different access privileges for the naming information:
1. Browsinginterface.This interface is for users having the least privilege for the
naming information. It allows the users to only view the content and structure of cell
directories. The interface takes the form ofabrowsertool that runs on workstations with
windowing software based on the OSFlMotif graphical user interface. The browser can
display an overall directory structure as well as show the contents ofdirectories. It can
also be customized so that it displays only specific kinds of entries. Recall that each
directory entry has protection information associated with it that specifies which users
have what types ofaccess rights for the entry. When a user uses the browser tool, only

556 Chap.10 • Naming
thosedirectory entries aredisplayed for whichthe userhasreadpermission. Other entries
are not displayed.
2. XDS application programming interface. Users can create, modify, and delete
directory entries by using the XDS (XlOpen Directory Server) application programming
interface to write an application that makes direct calls to the DeE Directory Service.
The XDS application programming interface is consistent with the standard interface to
X.5oo, called XOM (X/Open Object Management). The XDS interface has 13 calls for
manipulating directory objects and for setting up and initializing connections between
clients and directory servers. Some ofthe calls for directory manipulation are add_entry
for adding a new entry to a directory, remove_entry for deleting an entry from a
directory, list for listing all entries of a directory, read for reading the attributes of an
object, modify_entry for changing the attributes of an object, and modify_rdn for
renaming an object.
3. Administrativeinterface. This interface isforusers having maximum privilege to
the naming information. It allows the administrators to configure or reconfigure the
naming information within the system. That is, based on the idea of where a directory is
mostlikely tobeused,anadministratordecides thedistribution andreplication of various
directories in the clearinghouses of different CDS servers. For replicated directories,
administratorsalsospecifythemechanism (update propagation orskulking) tobe usedfor
consistency control of the information in each of these directories. Furthermore, an
administratorcan also change the access control rights associated with a directory entry
for controlling access to it.
10.10 SUMMARY
A naming system ofa distributed operating system enables users and programs toassign
character-string names to objects and subsequently use these names to refer to those
objects. Itprovides theusers withanabstraction of anobject thathides thedetails of how
and where an object is actually located in the network.
The desirable features ofagood naming system foradistributed system are location
transparency, location independency, scalability, uniform naming convention, provision
formultiple user-defined names forthesameobject, group naming facility,assignment of
meaningful names toobjects, good performance, faulttolerance, replication transparency,
locating the nearest replica, and locating all replicas.
Aname isastring composed of asetof symbols chosen froma finite alphabet. Two
basic classes ofnames widely used in operating systems are human-oriented names and
system-oriented names.
Anaming system employs one or more naming conventions for name assignment to
objects. The set of names complying with a given naming convention is said to form a
namespace.Anamespacemayeither beflatorpartitioned. Eachpartition ofapartitioned
name space iscalled a domain of the name space. A name defined in a domain iscalled
a simple name. A compound name is composed of one or more simple names that are
separated by a special delimiter character.

Sec.10.10 • Summary 557
A name server is a process that maintains information about named objects and
provides facilities that enable users to access that information. A name space of a
distributed system is usually managed by several name servers. Each name server
normally has information aboutonly asmallsubset of the setofobjects inthedistributed
system. The name servers that store the information about an object are called the
authoritative name servers of that object.
The distribution of the name service and thelocations ofthe name servers should be
transparent to the clients of a name service. This transparency is achieved through name
agents that act between name servers and their clients.
Names are always associated with some context. Acontext can be thought of as the
environment in which a name is valid. A "context/name" pair is said to form a qualified
name that uniquely identifies an object.
Nameresolution istheprocessofmapping anobject'snametotheauthoritativename
servers of that object.
For ease of use, a naming system usually provides the facility of abbreviations,
relative names, generic names, and multicast names for its human-oriented names.
Anaming system that supports descriptive/attribute-basednames allowsanobjectto
be identified by a set of attributes or properties that describe the object and uniquely
identify it among the group of objects in the name space.
Object locating is the process of mapping an object's system-oriented unique
identifier to the replica locations of the object. The commonly used object-locating
mechanisms in a distributed system are broadcasting, expanding ring broadcast,
encoding the location of an object within its identifier, searching the creator node first
and then broadcasting, using forward location pointers, and using hint cache and
broadcasting.
The four basic approaches for assigning systemwide unique human-oriented names
to the various objects in a distributed system are combining an object's local name with
its host name, interlinking isolated name spaces into a single name space, full/partial
replication of remote name spaces on explicit request, and a single global name
space.
Anamespaceispartitioned intocontexts byusingclustering conditions.Aclustering
condition is an expression that, when applied to a name, returns either a true or a false
value, depending on whether the given name exists in the context designated by the
clustering condition. The three basic clustering methods are algorithmic clustering,
syntactic clustering, and attribute clustering.
Acontext binding associates thecontext within which it isstored toanother context
that is more knowledgeable about the named object and the name servers that store that
context. Two commonly used strategies to implement context bindings are table-based
strategy and procedure-based strategy. The contexts of a table-based strategy are also
known as directories.
The name resolution mechanism of anaming system depends on thepolicy usedfor
distributing thecontexts ofthenamespaceof thenaming system. Some ofthecommonly
used approaches for the distribution of contexts for name resolution are the centralized
approach, the fully replicated approach, distribution of contexts based on the physical
structure of the name space, and structure-free distribution of contexts.

558 Chap. 10 • Naming
Research results have shown that a simple distributed name cache can have
substantial positive effect on distributed system performance. The three types of name
caches used in distributed systems are directory cache, prefix cache, and full-name
cache. The two commonly used approaches for name cache implementation are a cache
per process and a single cache at each node for all processes of the node. The two
commonly used methods for multicache consistency are immediate invalidate and on
use update.
An important job of the naming system of several centralized and distributed
operating systems is to control unauthorized access to both the named objects and the
information in the naming database. The three basic naming-related access control
mechanisms are object names as protection keys, capabilities, and associating protection
with the name resolution path of an object name.
EXERCISES
10.1. Whatarethemainjobs performedbythenamingsubsystemofadistributedoperatingsystem?
Is a naming subsystem also needed in an operating system for a centralized time-sharing
system?Ifno,explain why.Ifyes,inwhataspects doesthenaming subsystemofadistributed
operating system differ from that of a centralized time-sharing operating system?
10.2. Differentiatebetween theterms "locationtransparency"and"locationindependency."Which
is a more powerful feature and why?
10.3. The object-locating mechanism of a distributed operating system is designed to return the
locations ofallthereplicas ofanobject. Suggestamechanism thatcan beusedinthissystem
to allow a name agent to find out the relative distances of the replicas from the obtained
locations so that the object-accessing request can be sent to the nearest available replica
location.
10.4. Differentiatebetween human-orientedand system-orientednames used inoperatingsystems.
Isitpossible todesign asystem inwhichobjects are identified only bytheir system-oriented
names andtherearenohuman-orientednames forobjects? Ifno,explain why.Ifyes,explain
what problems may occur in such a system.
10.5. What is a name space? For a hierarchically structured name space, discuss the relative
advantages and disadvantages of using a fixed number of levels and allowing an arbitrary
number of levels for the hierarchy.
10.6. Give an example of each of the following types of name spaces:
(a) A flat name space having 10names.
(b) A nonhierarchical name space having four domains with each domain having three
names.
(c) A hierarchical name space having four domains with each domain having three names.
(d) A name space having two nonnested contexts and four qualified names.
(e) Anamespacehavingtwocontexts andfourqualified names.Oneofthecontexts isnested
within the other.
to.7.Give an example of a flat name space having 20 names. Use the following to partition this
name space into contexts such that none ofthe contexts contain more than eight names:
(a) The algorithmicclustering method
(b) The syntactic clustering method
Specify the clustering conditions used ineach case.

Chap. 10 • Exercises 5·59
10.8. The object-locating operation can be greatly simplified if the location of an object is
embeddedinitsunique identifier. Inspiteofthis advantage,why isitundesirabletoinclude
the location of an object in its unique identifier?
10.9. In what ways does the creation of system-oriented names in distributed operating systems
differ fromtheircreationincentralizedoperatingsystems?Give three differentmethods that
may be used to create system-oriented names in distributed operating systems.
10.10. Suppose a distributed system uses a single global hierarchical name space for its human
oriented names. The qualified names in this name space are likely to be long, having too
manycomponentnames.Therefore,itmaybeinconvenientforausertospecify thequalified
name of an object every time he or she wants to use it. Suggest two methods that may be
used to solve this problem and discuss the relative advantages and disadvantages ofboth
methods.
10.11. In what manner will the design of the naming systems ofthe following types ofdistributed
operating systems differ?
(a) One that does not permit object migration and object replication
(b) One that permits object migration butdoes not permit object replication
(c) One that permits object replication but does not permit object migration
(d) One that permits both migration and replication of objects
10.12. Suppose you have to design an object-locating mechanism for the naming system of a
distributed system. What factors will influence your design decision?
10.13. Which one or more of theobject-locating mechanismsdiscussed inthe chapter aresuitable
for a naming system for the following types of distributed systems:
(a) A LAN-based system having few nodes
(b) A LAN-based system having a large numberofnodes
(c) A WAN-based system
(d) A system that does not permit object migration and object replication
(e) A system that permits object migration but does not permit object replication
(f) A system that permits object replication but does not permit object migration
(g) A system that permits both migration and replication of objects
If more than one mechanism is suitable for a particularcase, which one will you prefer to
use and why?
10.14. What is a "meta-context"? Is it needed in the design of all naming systems that use name
spaces partitionedinto contexts'! Ifyes, explain why.If no,explain inwhat type of naming
systems it is needed.
10.15. For a distributed system. suppose you have to design a naming system that uses a single
global hierarchical name space foritshuman-orientednames anddirectoriesformaintaining
context-binding information. To simplify the design and programming, suppose that you
have decided to separate the part of the nanling system that deals with the managementof
directoriesand toimplementitintheform ofadirectoryserver.Specify thesetofprimitives
that you will provide to allow interaction with the directory server.
10.16. Supposeyouhave todesign aname-cachingscheme foranaming system.What arethemain
issues involvedinthedesign? What features ofthenaming system willinfluence themanner
in which you will handle each of these issues in your design?
10.17. Suppose a user inputs the command open (file_name, mode), wherefile_name is specified
asleduluserslsinhalprojectJ/filel.Assume thatthe naming subsystemofthissystem usesthe
following:

S60 Chap. 10• Naming
(a) Table-based strategy for context bindings
(b) Structure-freedistribution ofcontexts
(c) Transitive approach of interaction among name agent and name servers during name
resolution
(d) A full name cache for each process on a node
List allthemainstepsinsequentialorderthatthenaming subsystemoftheoperatingsystem
has toperform tocarry out this command. Youmay make anyotherassumptionsifneeded,
but state your assumptions.
10.18. Explaintheon-use consistencycontrol mechanism. Inwhatrespect isthismechanismbetter
thanothercommonlyused mechanismsforcacheconsistency?Can this mechanismbeused
to maintain the consistency ofall types ofcached data? Give reasons for your answer.
10.19. Inadistributedsystem, system-orientednames ofobjectsaretobeused tocontrol access to
objects. That is, in this system, a subject that possesses the system-oriented name of an
objectwill be allowed toaccess the object inall possible modes. An importantissue in the
design of this system is that system-oriented names must be hard to guess. Suggest some
methods that can be used to handle this issue.
10.20. In adistributed system, system-oriented names of objects are to be used as capabilitiesfor
controllingaccesstoobjects. Think ofamethod thatcanbeusedforcreatingsystem-oriented
names thatcan also be used ascapabilitiestogrant differentaccess permissionstodifferent
users ofthe same object.
818UOGRAPHY
[Birrelletal. 1982] Birrell, A.D., Levin, R., Needham, R. M., and Schroeder, M. D.,"Grapevine:
An Exercise in Distributed Computing," Communications of the ACM, Vol. 25, No.4, pp.
260-274 (1982).
[Boggs 1983] Boggs, D. R., "Internet Broadcasting," Ph.D. Dissertation, Stanford University,
Technical Report No. CSL-83-3, Palo Alto Research Center(1983).
[Brownbridgeet al. 1982] Brownbridge, D. R., Marshall, L. F.,and Randell, B., "The Newcastle
Connectionor UNIXes oftheWorldUnite!"SoftwarePracticeandExperience, Vol.12,No. 12,
pp. 1147-1162(1982).
[CerfandCain1983]Cerf, V.G.,andCain, E.,"The DoD InternetArchitectureModel,"Computer
Networks, Vol.7, No.5, pp. 307-318 (1983).
[Cheriton and Mann 1989]Cheriton, D. R., and Mann, T. P.,"Decentralizing aGlobal Naming
Service for Improved Performance and Fault Tolerance," ACM Transactions on Computer
Systems, Vol.7, No.2, pp. 147-183 (1989).
[ComerandMurtagh1986]Comer, D.,and Murtagh,T.P.,"TheTilde File Naming Scheme,"In:
Proceedingsofthe6thInternationalConferenceonDistributedComputingSystems,IEEEPress,
NewYork, NY,pp. 509-514 (1986).
(Comerand Peterson 1986]Comer, D., and Peterson, L. L., "A Model ofName Resolution in
Distributed Systems," In: Proceedings of the 6th International Conference on Distributed
Computing Systems, IEEE Press, New York,NY, pp. 523-530(1986).
[Coulouris et al, 1994JCoulouris, G. F., Dollimore, J., and Kindberg, T., Distributed Systems
Concepts and Design, 2nd ed., Addison-Wesley, Reading, MA (1994).

Chap. 10 • Bibliography 561
[Goscinski 1991] Goscinski, A., Distributed Operating Systems, The Logical Design, Addison
Wesley, Reading, MA (1991).
[Kille1992]Kille,S., Implementing X.400andX.SOO: ThePPand QUIPUSystems,ArtechHouse,
Norwood, MA (1992).
[Lampson 1986J Lampson, B.W.,"DesigningaGlobal NameService,"In: Proceedingsofthe5th
Annual Symposium on Principles ofDistributed Computing, Calgary, Canada, Association for
Computing Machinery, New York, NY,pp. 1-10 (August 1986).
[Lantz et al, 19851 Lantz, K., Edighoffer, J., and Hitson, B., "Towards a Universal Directory
Service,"In: Proceedingsofthe4thAnnualSymposium onPrinciples ofDistributedComputing,
Minaki, Canada, Association for Computing Machinery, New York, NY, pp. 250-260 (August
1985).
[Leachetal,1982]Leach,P.1.,Stump,B.L., Hamilton,J.A.,and Levine,P.H.,"UIDsasInternal
NamesinaDistributedFile System,"In:Proceedingsofthe /stAnnualSymposiumonPrinciples
ofDistributedComputing, Ontario, Canada, Association for Computing Machinery, New York,
NY,pp. 34-41 (August 1982).
[1.Jeach et al, 1983] Leach, P.J., Levine, P.H., Douros, B. P.,Hamilton,J. A., Nelson, D.L., and
Stumpf, B. L., "The Architecture of an Integrated Local Network," IEEE Journal on Selected
Areas in Communication, Vol.SAC-I, No.5, pp. 842-857 (1983).
[Leffler et al. 1984] Leffler, S., Karels, M., and McKusick, M., "Measuring and Improving the
Performance of4.2BSD," In: Proceedings ofthe /984 USENIX Summer Conference, USENIX
Association, Berkeley, CA, pp. 237-252 (1984).
[Levy and Silberschatz 1990] Levy, E., and Silberschatz,A., "DistributedFile Systems: Concepts
and Examples,"ACM Computing Surveys, Vol.22, No.4, pp. 321-374 (1990).
[Lin et al, 1982]Lin, M.T.,Tsoy, D.P.,and Lian,R.C., "Designof aNetworkOperatingSystem
for the Distributed Double-Loop Computer Network (DDLCN)," Local Computer Networks,
North-Holland Co., IFIP ()982).
[Lockhart, Jr. 1994J Lockhart, Jr., H. W., OSF DeE: Guide to Developing Distributed
Applications, IEEEComputerSociety Press, Los Alamitos, CA (1994).
[Mockapetris 1984] Mockapetris, P.Y.,"The Domain Name System," In: Proceedings IFIP 6.5,
International Symposium on Computer Messaging, Nottingham, England (May 1984).
[Mockapetris and Dunlap 1988] Mockapetris, P. V., and Dunlap, K. 1., "Development of the
Domain Name System," In: Proceedings ofthe SIGCOMM'88 Symposium on Communications
Architectures and Protocols, Stanford, CA, Association for Computing Machinery, New York,
NY, pp. ]23--133 (]988).
[Mogul 1986] Mogul, J.C., "Representing InformationAbout Files," Ph.D. Dissertation, Stanford
University, ComputerScienceTechnical Report, STAN-CS-86-1103 (March 1986).
[Mullender and Tanenbaum 1986] Mullender,S. J., and Tanenbaum, A. S., "The Design of a
Capability Based Distributed Operating System," The Computer Journal, Vol. 29, No.4, pp.
289-300 (1986).
[Mullenderet al, 1990]Mullender,S.1.,VanRossurn,G.,Tanenbaum,A.S.,VanRenesse,R.,and
VanStaverene, H., "Amoeba:A Distributed Operating System for the 1990s,"IEEE Computer,
Vol. 23, No.5, pp. 44-53 (1990).

562 Chap. 10 • Naming
[Needham1993]Needham,R.M.,"Names,"In:S.Mullender(Ed.),DistributedSystems, 2nded.,
AssociationforComputingMachinery,NewYork, NY, pp.315-327 (1993).
[Oppen and Dalal 1983]Oppen,D.C.,and Dalal,Y. K.,"The Clearinghouse:A Decentralized
AgentforLocatingNamedObjectsinaDistributedEnvironment,"ACMTransactionsonOffice
Information Systems, Vol. 1,No.3, pp.230-253 (1983).
[Peterson 1988] Peterson,L., "The Profile Naming Service," ACM Transactions on Computer
Systems, Vol. 6,No.4, pp.341-364 (1988).
[Rashid1987]Rashid,R.F.,"Mach:ANewFoundationforMultiprocessorSystemsDevelopment,"
In:ProceedingsofCOMPCON'87-DigestofPapers,IEEEPress,NewYork,NY,pp. 192-193
(1987).
[Rose 1992] Rose, M. T., The Little Black Book: Mail Bonding with OSI Directory Services,
Prentice-Hall,EnglewoodCliffs,NJ(1992).
[Rosenberry et al. 1992] Rosenberry, W., Kenney, D., and Fisher, G., OSF DISTRIBUTED
COMPUTING ENVIRONMENT, Understanding DCE, O'Reilly &Associates,Sebastopol,CA
(1992).
[Rozieret al, 1988)Rozier,M.,Abrossimov,V.,Armand,F.,Boule,I.,Gien,M.,Guillemont,M.,
Herrmann, F.,Kaiser,C., Leonard, P, Langlois, S., and Neuhauser,W., "Chorus Distributed
OperatingSystem,"Computing Systems, Vol. 1,pp.305-379 (1988).
[Saltzer 1982J Saltzer, 1. H., "On the Naming and Binding of Network Destinations," In:
ProceedingsofIFIPffC6InternationalSymposiumonLocalComputerNetworks,Florence,Italy,
pp.311-317 (April 1982).
[Sandbergetale1985]Sandberg, R.,Goldberg, D.,Kleinman, S.,Walsh,D.,andLyon,B.,"Design
and Implementation of the SUN Network File System," In: Proceedings of the USENIX
Conference, Portland,OR,USENIXAssociation,Berkeley,CA,pp. 119-130 (1985).
[Schantzetal,1986]Schantz,R.E.,Thomas,R.H.,andBono,G.,"TheArchitectureoftheCronus
Distributed Operating System," In: Proceedings of the 6th International Conference on
DistributedComputing Systems, IEEEPress,NewYork, NY, pp.250-259 (1986).
[Schickler1982JSchickler,P.,"NamingandAddressinginaComputerBasedMailEnvironment,"
IEEE Transactionson Communications, Vol. COM-30,No.1 (1982).
(Schroeder et al, 1984] Schroeder,R., Birrell,A. D., and Needham, R. M., "Experience with
Grapevine:TheGrowthofaDistributedSystem," ACMTransactionsonComputerSystems,Vol.
2,No.1, pp.3-23 (1984).
[Schwartzetal, 1987]Schwartz,M.,Zahorjan,1.,andNotkin, D.,"ANameServiceforEvolving
HeterogeneousSystems," In:Proceedings ofthe 11thACM Symposium on Operating Systems
Principles, Association for Computing Machinery, New York, NY, pp. 52-62 (November
1987).
[Sheltzeret al. 1986]Sheltzer,A. B.,Lindell,R.,andPopek,G.1.,"Name ServiceLocalityand
Cache Design in a Distributed Operating System," In: Proceedings ofthe 6th International
Conference on Distributed Computing Systems, IEEE Press, New York, NY, pp. 515-522
(1986).
[SilberschatzandGalvin 1994]Silberschatz,A.,andGalvin,P.B.,OperatingSystems, Concepts,
4thed.,Addison-Wesley, Reading,MA(1994).

Chap. 10 • Bibliography 563
[Sinhaetal, 1991a]Sinha,P.K.,Maekawa,M.,Shimizu,K.,Jia,X.,Ashihara,H.,Utsunomiya,N.,
Park,K.S.,andNakano,H.,"TheGalaxyDistributedOperatingSystem,"IEEE Computer, Vol.
24,No.8, pp.34-41 (1991).
[Sinha et al, 1991b] Sinha,P.K., Shimizu,K., Utsunomiya,N., Nakano,H., andMaekawa,M.,
"Network Transparent Object Naming and Locating in the Galaxy Distributed Operating
System,"Journal ofInformation Processing, Vol. 14,No.3, pp. 310-324 (1991).
[Sinha et al. 1992]Sinha,P. K., Mackawa,M., and Shimizu,K., "Improvingthe Reliabilityof
Name ResolutionMechanismin DistributedOperatingSystems," In: Proceedings ofthe 12th
International Conference on Distributed Computing Systems, Yokohama, Japan, pp. 589-596
(June 1992).
[Su and Postel 1982] Su,Z., andPostel,1., "The DomainNamingConventionforInternetUser
Applications,"NetworkInformationCenter,SRIInternational,RFC819(August1982).
[Terry 1984] Terry, D. B., "An Analysis of Naming Conventions for Distributed Computer
Systems," In: Proceedings of the 6th International Conference on Distributed Computing
Systems, IEEEPress,NewYork, NY, pp.502-508 (1984).
[Terry 1986] Terry, D. B., "Structure-Free Name Management for Evolving Distributed
Environments,"In:ProceedingsoftheA(:MSIGCOMM'84, Montreal,Quebec,Associationfor
ComputingMachinery,New York, NY,pp. 218-224 (June 1986).
[Tichy and Ruan 1984] Tichy,W. F., and Ruan,Z., "Towards a DistributedFile System," In:
Proceedingsof/heSummerUSENIX Conference, IJSENIXAssociation,Berkeley,CA,pp.87-97
(June 1984).
[Tripathi et al, 1987] Tripathi, A., Ghonami, A., and Schmitz, T., "Object Managementin the
NEXUSDistributedOperatingSystem," In:Proceedings ofJEJ!.E COMPCON'87, IEEEPress,
NewYork, NY, pp.50-53 (February1987).
[Walker et al. 19831Walker, B., Popek,G., English,R., Kline,C., andThiel,G., HThe LOCUS
Distributed Operating System," In: Proceedings of the 9th ACM SIGOPS Symposium on
OperatingSystems Principles,Association forComputingMachinery, NewYork,NY,pp.49-70
(1983).
[Watson 1981]Watson,R.,"Identifiers (Naming)inDistributedSystems,"In:B. W.Lampson,M.
Paul, and H. Siegert (Eds.), Lecture Notes in Computing Science: Distributed !iystems
Architectureand Implementation, Springer-Verlag, NewYork, NY, pp. 191-210 (1981).
[Welch and Ousterhout 1986] Welch, B., and Ousterhout, J. K., "Prefix Tables: A Simple
Mechanismfor LocatingFilesinaDistributedSystem,"In:Proceedingsofthe6th International
Conference on Distributed Computing Systems, IEEEComputerSociety Press,LosAlamitos,
CA, pp. 184-189 (1986).
[Wiebe 1986J Wiebe, D., "A Distributed Repository for Immutable Persistent Objects," In:
ProceedingsofoOPSLA'86,AssociationforComputingMachinery,NewYork,NY,pp.453-465
(1986).
POINTERS TOBIBLIOGRAPHIES ONTHE INTERNET
I could not find a bibliography dedicated only to Naming. However, the following
bibliographies contain references on the topics covered in this chapter:

564 Chap. 10• Naming
ftp:ftp.cs.umanitoba.calpub/bibliographies/Os/lMMD_IV.html
ftp:ftp.cs.umanitoba.calpub/bibliographieslMisc/misc.l.htmI
ftp:ftp.cs.umanitoba.calpublbibliographieslDistributedlrfc.html

11
CHAPTER
Security
11.1 INTRODUOION
Computer systems store large amounts of information, some of which is highly
sensitive and valuable to their users. Users can trust the system and rely on it only if
the various resources and information of a computer system are protected against
destruction and unauthorized access. Obviously, the security requirements are different
for different computer systems depending on the environment in which they are
supposed to operate. For example, security requirements for systems meant to operate
in a military environment are different from those for systems that are meant to
operate in an educational environment. The security goals of a computer system are
decided by its security policies, and the methods used to achieve these goals are called
security mechanisms. While designing the security of a system, it is often useful to
distinguish between security policies and security mechanisms because security
policies can be decided independent of the available technology but security mecha
nisms are influenced by the available technology. It may be difficult to implement the
desired security policies with a selected set of security mechanisms. But new security
mechanisms can be later added to the set to implement the desired security
policies.
565

S66 Chap. 11 • Security
Irrespective of the operation environment, some of the common goals of computer
security are as follows [Mullender 1985]:
1. Secrecy. Information within the system must be accessible only to authorized
users.
2. Privacy.Misuse ofinformation mustbeprevented.That is,apiece ofinformation
given to a user should be used only for the purpose for which it was given.
3. Authenticity. When a user receives some data, the user must be able to verify its
authenticity.Thatis,thedataarrivedindeedfromitsexpected sender andnotfrom
any other source.
4. Integrity. Information within the system must be protected against accidental
destruction or intentional corruption by an unauthorized user.
A total approach to computer security involves both external and internal
security. External security deals with securing the computer system against external
factors such as fires, floods, earthquakes, stolen disks/tapes, leaking out of stored
information by a person who has access to the information, and so on. For external
security, the commonly used methods include maintaining adequate backup copies of
stored information atplacesfarawayfrom theoriginal information, using security guards
toallow theentryofonlyauthorized personsintothecomputercenter,allowing theaccess
to sensitive information to only trusted employees/users, and so on.
Internal security, on the other hand, mainly deals with the following two aspects:
1. User authentication. Once a user is allowed physical access to the computer
facility, the user's identification must be checked by the system before the user can
actually use the facility.
2. Access control.Acomputer system contains many resources and several types of
information. Obviously, not all resources and information are meant for all users.
Therefore, even when a user passes the authentication phase and is allowed to use the
computer facility, a way is needed to prohibit the user from accessing those resources/
informationthatheorsheisnotauthorized toaccess.Infact,asecure system requires that
atanytimeasubject(personorprogram) shouldbeallowedtoaccessonlythoseresources
that it currently needs to complete its task. This requirement is commonly referred to as
the need-to-know principle or theprinciple ofleastprivilege.
Wesaw that the security needs of acomputer system are intricately linked with that
system'senvironment, use,and implementation. Therefore, inaddition tothe two aspects
mentioned above, internal security in distributed systems has a third aspect called
communication security.
3. Communication security. In a distributed system, the communication channels
that are used toconnect thecomputers are normally exposed to attackers who may try to
breach the security of the system by observing, modifying, or disrupting the
communications. Wireless networks are even more vulnerable tomonitoring byintruders

Sec.11.2 • Potential Attacks toComputerSystems 567
because anyone with a scanner can pluck the radio signals out of the air without being
detected. Communicationsecurity safeguards against unauthorizedtamperingofinforma
tion while it is being transmitted from one computer to another through the
communicationchannels. Twoother aspects ofcommunicationsecurity areauthenticityof
communicating entities and integrityofmessages. That is,the senderof a message wants
to know that the message was received by the intended receiver, and the receiver wants
toknow that the message was sent bythe genuine sender.Obviously, both the sender and
thereceiveralso want tobeguaranteedthatthecontents ofthe message werenotchanged
while it was in transfer.
Providing both external and internal security ismore difficult in distributed systems
than incentralized systems because of the lack of a single point ofcontrol and the useof
insecure networks for data communication. Although external security is as important as
internal security, the policies and mechanisms used at the operating system level for
computer security deal only with the internal security aspects. Therefore, this chapter
deals mainly with the commonly used mechanisms for providing different types of
internal security in distributed systems.
11.2 POTENTIAL ATTACKS TOCOMPUTER SYSTEMS
The first step in the provision of appropriate computer security is to identify the
potential threats/attacks to computer systems. The term intruder or attacker is
commonly used to refer to a person or program trying to obtain unauthorized access
to data or a resource of a computer system. An intruder may be a threat to computer
security in many ways that are broadly classified into two categories-passive attacks
and active attacks. A passive attack does not cause any harm to the system being
threatened, whereas an active attack does. Therefore, passive attacks are inherently
undetectable by the system and can only be dealt with by using preventive measures.
On the other hand, active attacks are combated by a combination of prevention,
detection, and recovery techniques. A description of these attacks and some other
related problems are presented below.
11.2.1 Passive Attacks
In passive attacks, an intruder somehow tries to steal unauthorized information from the
computer system without interfering with the normal functioning of the system. Some
commonly used methods of passive attack are described below:
1. Browsing. In this method, intruders attempt to read stored files, message packets
passing by on the network, other processes' memory, and so on, without modifying any
data. Access control mechanisms are used toprevent unauthorized reading of stored files
and other processes' memory contents, and message encryption is used to prevent
eavesdropping of messages transmitted over network links.

568 Chap.11 • Security
2. Leaking. In this method,an intruderuses an accomplice(a legitimateuser having
authority to access the information tobestolen) who leaks the informationto him or her.
Prevention ofleaking is a difficult problem to solve and requires preventing all types of
communicationbetweenthe accomplice and the intruder. The problemofensuring that it
is impossible for a potential accomplice to leak any information to the outside world is
called the confinement problem [Lampson 1973]. As described later, leaking of
information between processes that in theory cannot communicate at all is relatively
straightforward. Therefore, the confinement problem is in general unsolvable.
3. Inferencing, In this method, an intruder tries to draw some inference by closely
observing and analyzing the system'sdataor the activitiescarriedout by the system. For
example, ifinformation is encrypted to protect unauthorized access, an intruder may try
to derive the encryption key by analyzing several pieces of encrypted data. Since the
derived key can be used for stealing information from the system, it is valuable and may
be sold tootherintruders.Anotherexampleofinferencingistraffic analysisindistributed
systems.In this case, an intruderobserveswhen and where interprocessmessages flow in
the system, and by analyzing the frequency of message exchanges between various
communicating partners, the intruder tries to draw some inference. For example, in a
business environment, traffic analysis may provide useful clues to negotiations taking
place between different organizations.
4. Masquerading. In this method, an intruder masquerades as an authorized user
or program in order to gain access to unauthorized data or resources. For instance,
many systems have mechanisms for alJowing programs written by users to be used by
other users. These programs can improperly use the access rights of an executing user
and leak information. For example, an intruder may write an editor program that works
perfectly as an editor but also creates a copy of the edited file to a special area
accessible to the intruder. This editor program is then compiled and read into the bin
directory ofa user, whose files the intruder is interested in. From then on, the intruder
gets a copy of all the files edited by the user. The user is ignorant of the theft being
made because the editor program performs all his or her editing jobs in a perfectly
normal fashion.
Penetrating computer security in this manner is known as the Trojan horse attack.
That is, a Trojan horse program is a program that consists of clandestine code to do
nasty things in addition to its usual function but appears to be benign. It is often offered
as a gift or sometimes for a nominal price to prevent suspicion. A user normally accepts
it into his or her system because ofthe useful function performed by it. However, once
inside the user's computer, code hidden in the program becomes active and either
executes malicious acts or creates a way of subverting system security so that
unauthorized personnel can gain access to the system resources. Note that a Trojan
horse attack may either be passive or active depending on the activities performed by
the clandestine code. For example, if the clandestine code simply steals information,
then it is of the passive type. But if it does something more harmful like destroying/
corrupting files, then it is of the active type.
An intrudercan also masquerade as a trusted server to a client requesting a service
from the system. This action is known as spoofing.

Sec. 11.2 • Potential Attacks to ComputerSystems 569
11.1.1 Actlv.Attacks
Active intruders are more malicious than passive intruders. Unlike passiveattacks, active
attacks interfere with the normal functioning of the system and often have damaging
effects. The most common types of damage that active attacks cause are corrupting files,
destroying data, imitating hardware errors, slowing down the system, filling up memory
or disk space with garbage, causing the system to crash, confounding a receiver into
accepting fabricated messages, and denial/delay of message delivery. Some commonly
used forms of active attacks are described below. In the following, the description of
viruses, worms, and logic bombs isbased onthe material presentedin[Bowles andPelaez
1992].
Viruses
Acomputervirusisapiece ofcode attached toalegitimate programthat, when executed,
infects otherprogramsinthesystem byreplicatingandattachingitselftothem. Inaddition
to this replicatingeffect, a virus normally does some other damageto the system, such as
corrupting/erasing files. Therefore, due to its spreading nature, a virus can cause severe
damage to a system. Notice that virus attacks are active-type Trojan horse attacks.
A typical virus works as follows. The intruder writes a new program that performs
someinterestingoruseful function (suchassomegameorutility)andattaches thevirustoit
insuch a way that when the program is executed the viral code also gets executed. The
intrudernowuploads this infectedprogramtoapublic bulletin board system orsends itby
mailtoother usersofthesystemoroffersitforfreeorforanominalchargeonfloppy disks.
Now ifanyone uses theinfectedprogram, itsviralcode getsexecuted. When theviralcode
oftheinfectedprogramexecutes,itrandomly selectsanexecutablefileontheharddiskand
checks toseeifitisalready infected. Most viruses include astring ofcharactersthatactsas
amarkershowingthattheprogramhasbeeninfected. Iftheselectedfileisalready infected,
the virus selects anotherexecutable file. When an uninfected program isfound, the virus
infects it by attaching a copy of itselfto the end of that program and replacing the first
instruction of the program with ajump to the viral code. When the viral code isfinished
executing, itexecutes the instruction that had previously been first and then jumpsto the
second instruction so that the program now performs its intended function. Notice that a
virus spreads because every time an infected program is executed, it tries to infect more
programs.Alsonoticethatavirusdoesnotinfectanalready infectedfileinordertoprevent
an object file from growing ever longer. This allows the virus to infect many programs
withoutnoticeablyincreasingdiskspace usage.
Recovery from a virus infection is a difficult task that often requires partial or
complete shutdown for long periods of time of the computer system under attack.
Therefore, it is always better to take necessary precautions to prevent virus problems.
Some precautionary steps include (a) buying software only from respectable stores, (b)
refusing to accept software in unsealed packages or from untrusted sources, (c) avoiding
borrowing programs from someone whose security standards are less rigorous than one's
own, and(d)avoidinguploadingoffreesoftwarefrom public domain, bulletin boards, and
programs sent by electronic mail.

570 Chap.11 • Security
When acomputersystem suffersfrom virusinfection, ithastobecured.The simplest
way to cure acomputerfrom virus infection is to shut it down, purge its memory and all
its disks, and rebuild its files from scratch using the original manufacturer's copy.
Disinfection utilities may also be used to cure a computer from virus infection. These
utilities first identify the virus type with which the computer is infected by matching its
markeragainst the markers ofwell-known viruses. Once the type is known, the original
programsarerestored fromtheirinfectedversionsbyapplying adetailed knowledgeofthe
infectionmethod usedbythe virus. Forexample, inviruses that modifyjumpinstructions
at the beginning of the host program, recovering can be done simply by restoring the
originaljumptothestartofthehostprogram code. However,notice thatthese disinfection
utilities can only curespecific known viruses.They cannot cure anewly encounteredtype
ofvirus. A good disinfection utility can normally cure several hundred types of viruses
and its power can be regularly improved by frequently updating it as new viruses are
discovered.
Notice that thelonger avirus remains inasystem, themore time ithas tospread and
the tougherrecovery from it becomes. Therefore, it is important todetect a virus as soon
aspossible. Aneffective method todetect virusesistouseasnapshot programandacheck
routine. The snapshot program isused tologall critical system informationat the time of
the initial installation and the check routine is periodically executed to compare the
system's current state with the original snapshot. If signs of infection are detected, the
affected area of the computer is identified and the user is notified.
Curingadistributedsystem from virusinfection ismuch moredifficult because ifthe
infection is not removed from every workstation at the same time, reinfection will occur.
This is because an infected fileon the network server can infect every workstationon the
network.
Worms
Wormsareprogramsthatspread fromonecomputertoanother inanetwork ofcomputers.
They spreadbytaking advantage ofthe way in which resources are shared on acomputer
network and, in some cases, by exploiting flaws in the standard software installed on
network systems. A worm program may perform destructive activities after arrival at a
network node. Even when not directly destructive, worms often cripple a network by
subverting the operation of computers on the network to their own purposes,
monopolizing their resources, and saturating the communications links in a network.
Often, it is necessary to shut down the entire system (all computers of the network) to
recover from a worm problem.
Toillustratehow a worm propagates in a network, the famous Internet Worm attack
byaCornellgraduatestudent, Robert TappanMorris, on November2, 1988,that infected
thousands ofUNIX machines all over the Internet isdescribed here. This worm program
had two types of code, the bootstrap code and the code forming the main body of the
worm. The bootstrapcode wascompiledand executedon the system under attack. When
executed, the bootstrap code establishedacommunicationslink between its new host and
the machinefrom which itcame, copied the main body of the worm tothe new host, and
executed it. Once installed on a new host, the worm's first few actions were to hide its

Sec.11.2 • Potential Attacks toComputerSystems 571
existence. For this, it unlinked the binary version ofitself, killed its parent process, read
its files intomemoryandencryptedthem, and deletedthe files createdduringits entryinto
the system. After finishing its hiding operations, the worm's nextjob was to look into its
host's routing tables to collectinformationaboutotherhosts to which its currenthostwas
connected. Using this information, it then attempted to spread its bootstrapcodeto those
machines by trying the following three methods one by one:
1. The first method was to try to spawn a remote shell on the target machine using
the rsh command ofUNIX. This method sometimes works because some machines trust
other machines and willingly run rsh without authenticating the remote machine. If
successful, the remote shell uploaded the worm program on the target machine and
continued spreading to new machines from there.
2. If the first method failed, the second method was tried. This method took
advantageofa bug in thefingerprogramthat runs as adaemonprocessatevery BSDsite.
A user anywhere on the Internet can type
to obtain general informationabout aperson at a particularsite, such as the person's real
name, home address, office address, telephone number, and so on. Thefinger utility uses
the Clibraryfunctiongets to read input data. A problem with gets isthat itreadsthe entire
input string without checking for buffer overflows. The worm exploited this flaw and
calledfinger with a specially constructed 536-byte string as a parameter. The overflow
caused an area ofthe system stack to be overwritten, allowing the worm to put its own
suitable instructions (a procedure to execute Ibinlsh) on the stack. Now when thefinger
daemon returnedfrom the procedure it was in at the time it got the request, it returned to
and executed the procedure inside the 536-byte string on the stack instead ofreturning to
main. If this method succeeded, the worm had a shell running on the target machine.
3. Ifthe first two methodsfailed, the wormtried athird methodthat takesadvantage
ofa loophole in the UNIX electronic mail utility sendmail. The sendmail program has a
DEBUG option, which allows program testers to verify that mail has arrived at a site
without having to invoke the mailer's address resolution routines. Many vendors and site
administrators leave the debug option compiled into the sendmail code to facilitate
configuring the mailer for local conditions. What the worm did was to execute the
sendmail program with the DEBUG option and then enact a sequence of commands to
mail the bootstrap code to the target machine.
Once established on a new machine, the worm tried to break into user accounts by
exploitingthe accessibilityofthe UNIXpasswordfile andthe tendencyofusers tochoose
common words as their passwords. Each broken password allowed the worm to
masquerade as the user corresponding to that password and gain access to any remote
machine where that user had an account.
The worm was designed to act intelligently to prevent being spotted. It periodically
forked itself and killed its parent, so that its process ID was constantly changing. This

572 Chap.II • Security
prevented anyone process from accumulating a large amount of CPU time that might
create suspicion orcause its scheduling priority to bedegraded. Furthermore, afterevery
12 hours, the worm erased its record of the machines it had infected, so that already
infected hosts were put back on the list of potential targets.Whenever the worm gained
access to a new machine, itfirst checked to seeifthe machinealready had acopy of the
worm. If so, the new copy exited, except one time in seven. The use of one in seven
possibly was to allow the worm to spread even on a machine on which the system
administrator might have started its own version of the worm to fool the real worm.
Although virusesand worms bothreplicate and spread themselves, the twodiffer in
the following aspects:
1. A virus is a program fragment whereas a worm is a complete program in itself.
2. Since a virus isaprogram fragment, itdoes notexist independently. It resides in
a host program, runs only when the host program runs, and depends on the host
program for its existence. On the other hand, a worm can exist and execute
independently.
3. A virus spreads from one program toanother whereas a worm spreads from one
computer to another in a network.
Logic Bombs
A logic bomb is a program that lies dormant until some trigger condition causes it to
explode. Onexplosion, itdestroys data and spoils system software of the host computer.
A trigger condition may bean event such as accessing a particular data file, a program
beingrunacertain numberoftimes,thepassage ofagivenamountoftime,orthesystem
clockreaching somespecificdate(forinstance,Fridaythe 13thorAprilFool'sDay).The
triggercondition isnormallyselectedsothatthelogicbombexplodesatthemomentwhen
it can do maximum damage to the system. Logic bombs can be embedded in a Trojan
horse or carried about by a virus.
Active Attacks Associated with Message
Communications
Ina distributed system, communication channels are used tocarry information from one
node to another in the system in the form of messages. These communication channels
may be exposed to attackers who may try to breach the security of the system by
observing, modifying, deleting,inserting,delaying,redirecting,orreplaying themessages
that travel through the communication channels. The commonly known active attacks
associated with message communications are of the following types:
1. Integrity attack. For secure communication, the integrity requirement specifies
that every message is received exactly as it was sent or a discrepancy is detected.
However, an intruder maychange thecontents ofa message whileit istraveling through
acommunicationchannel and the receiver may not be aware of this and accept it as the
original message.

Sec.11.2 • PotentialAttackstoComputerSystems 573
2. Authenticity attack. An intruder may illegally connect his or her own computer
systemtoacommunicationchannelandimpersonatealegalnetwork site.Theintrudercan
then synthesize and insert bogus messages with valid addresses into the system so that
they aredelivered as genuine messages. Ifan integrityattack ispossible, an intruder may
alsocause anauthenticity attack bychanging theprotocolcontrol information (addresses)
of the messages so that they are delivered to wrong destinations.
3. Denialattack. Inthis case, the intruder either completely blocks thecommunica
tion path between two processes so that the two processes cannot communicate at all or
observes all messages exchanged between the two processes and prevents only selected
messages fromdelivery.That is,theintrudercausescomplete orpartialdenial ofmessage
delivery.
4. Delay attack. Several messages have time value. Therefore, instead of using a
denial attack, an intruder may simply delay the delivery of message passing in an
association between two communicating processes to fulfill his or her motive.
5. Replayattack. Inthiscase, anintruder retransmits old messages thatareaccepted
as new messages by their recipients.
Cryptography deals with the encryption of sensitive data to prevent its comprehen
sion and is the only practical means for protecting information sent over an insecure
channel, be ittelephone line, microwave, satellite, or any other transmission media.This
isbecause anencrypted message provides noinformation regarding theoriginal message,
hence guaranteeing secrecy; and an encrypted message, if tampered with, would not
decrypt intoalegal message, henceguaranteeing integrity.Cryptography canalsobeused
for secure identification of communicating entities, hence guaranteeing authenticity.
Furthermore, encryption, inconjunction withprotocols,canalsobeusedtopreventdenial,
delay, and replay of messages. For instance, replay of old messages can becountered by
using nonces or timestamps. Anonceisaninformation that isguaranteed tobefresh; that
is,ithasnotbeen used orappeared before.Therefore, areply thatcontains somefunction
of a recently sent nonce should be considered timely because the reply could have been
generated only after the nonce was sent. Perfect random numbers are suitable for use as
nonces. In summary, the only way to prevent attacks associated with message
communications is by the application of cryptographic techniques.
In a client-server model, a single server program may be shared by multiple clients. In
situations where programs are shared, a security problem is considerably more complex
than if only data objects are shared. One reason that we have already seen is a Trojan
horse. A Trojan horse is just one way in which a shared program could leak classified
information to other unclassified subjects. There may be several other ways in which a
shared program could leak confidential information to unauthorized subjects.
A program that cannot retain or leak confidential information is said to be
memoryless or confined, and the prevention of such leakage is called the confinement

574 Chap.II • Security
problem [Lampson 1973]. That is, the confinement problem deals with the problem of
eliminating every means by which an authorized subject can release any information
contained in the object to which it has access to some subjects that are not authorized to
access that information. According to Lampson, as long as a program does not have to
retain or output any information, confinement can be implemented by restricting the
access rights of the program. But if a program must retain or output information, access
control alone is not sufficient to ensure security.
Lampson identified thefollowing kindsofchannels thatcan beusedbyaprogram to
leak information:
1. Legitimate channels. Legitimate channels are those that the program uses to
convey the results of its computation, such as messages or printed output. The program
mayhideadditional information inthesechannels along withtheactualresult. Someform
of encoding that is meaningful to the person or process receiving the result is used to
convey the additional information. For example, in a printed output, two different space
lengths thatare notdiscernible tonormal persons butvisibleifobserved minutely may be
used between words to mean 0 and 1 bits. This type of printed output can be used to
convey additional information toaperson whoknows about the hidden bits between two
words.
2. Storage channels. Storage channels are those that utilize system storage such as
shared variables or files to leak information to other processes. Notice that when a
process (A) wants to leak information to another process (B) by using a storage
channel, it is not necessary that both processes must have access rights to the shared
object. For example, if the system provides a way of locking files, process A can lock
some file to indicate a 1 and unlock it to indicate a O. It may be possible for process
B to detect the status of a lock even on a file that B cannot access. Similarly, in UNIX,
process A could create a file to indicate a 1and remove it to indicate a O. Even though
process B has no permission to access the file created byA, itcan use theaccess system
call to see if the file exists.
3. Covert channels. Covert channels are paths that are not normally intended for
information transfer at all but could be used to send some information. For example, a
process may useone of the following methods to leak information tosome other process
that is carefully monitoring its activities [Tanenbaum 1992]:
• By modulating paging rate. For example, during a fixed time period, many page
faultscausedbytheprocess maybeusedtoconveya l, andnopagefaultsforaO.
• Bymodulating CPUusage.Forexample, usageofCPUforafixedtimeperiod by
the process may be used to convey a l, and sleeping of the process for the same
period may be used to convey a O.
• By acquiring and releasing dedicated resources. For example, the process may
acquire the resource to convey a 1and release it to convey a O.
To solve the confinement problem, it is important to block all channels that a
program may use to communicate with other processes. However, finding all such

Sec.11.3 • Cryptography S7S
channelsand trying toblock them isextremelydifficult. Inpractice, there islittle that can
be done. Therefore, the confinement problem is in general unsolvable.
11.3 CRYPTOGRAPHY
Cryptography is a means of protectingprivate information againstunauthorizedaccess in
those situationswhere itisdifficult toprovide physical security.The basicideabehind this
security technique isthat ifitisnot.possibletopreventcopyingofinformation, itisbetter
to prevent comprehension.
11.3.1 8aslc Concepts andTerminologies
Two primitive operations employed by cryptography are encryption and decryption.
Encryption (also called enciphering) is the process of transforming an intelligible
information (called plaintext or cleartext) into an unintelligible form (called ciphertext).
Decryption (also called deciphering) is the process oftransforming the information back
from ciphertext to plaintext. When cryptography is employed for protecting information
transmitted through communication channels, plaintext is also called a message.
Encryption is basically a mathematical function (encryption algorithm) having the
following form:
where P is the plaintext to be encrypted, K, is an encryption key, and C is the resulting
ciphertext. Decryption of C is performed by a matching function (decryption algorithm)
that has the following form:
P =D (C, K d)
where K is the decryption key. Note that the decryption function D is the inverse of the
d
encryption function E. Therefore we have
Toprevent the plaintext from being easily revealed, it must be possible to transform
a given plaintext into a large variety of possible ciphertexts selected by a specific
parameter. The keys K, and K serve as this parameter. That is,the function parts remain
d
the same but the keys are changed as often as necessary.
The above described general structure of a cryptosystem is illustrated with an
example in Figure 11.1,where a message is encrypted for secure transmission over an
insecure channel from a sender node to a receiver node.
11.3.2 SasieRequirements
To be practically useful, a cryptosystem must fulfill the following basic requirements:
I. It must be easy to use and its encryption and decryption algorithms should be
efficient for computer application.

576 Chap. 11 • Security
Ciphertext
•
Insecure
channel
Sendernode Receivernode
Fig. 11.1 General structure ofacryptosystem.
2. There are two methods to achieve security. In the first method, the encryption
algorithm iskept secret and israther complex to make itdifficult toguess. In the second
method, theencryption algorithm ismade public butthekeysarekept secret and theyare
long enough to make it practically impossible to guess a key. The second method is
preferred forpractically useful systems. That is,the security of thesystem should depend
only on the secrecy of the keys and not on the secrecy of the algorithms.
3. The system must becomputationally (practically) secure. That is, the determina
tion of K must becomputationally infeasible foranattacker (also called acryptanalyst).
d
Note that the strength of a cryptosystem is measured by the level of difficulty (usually
measured either by the time or number of elementary operations) of determining K d.
Depending on the amount of information available to an intruder, in a cryptosystem,
attacks are mainly of three types-ciphertextonly,known plaintext, and chosen plaintext
[Bright 1977].
Inciphertext-onlyattack,anintruder isable tointercept ciphertext andtriestoderive
K fromtheciphertext.Asystem whosesecurity isnotresistant toaciphertext-onlyattack
d
is considered to be totally insecure and is useless.
Inknown-plaintextattack,ailintruderhasconsiderableamountofbothciphertext and
corresponding plaintext and tries to derive K from them. A system that can resist a
d
known-plaintext attack is considered to be secure.
Inchosen-plaintextattack,anintruder hasaccess tociphertextforanyplaintext ofhis
or her choice. The intruder tries to derive K by examining several ciphertexts for the
d
carefully thought plaintexts of his or her choice. It is most appropriate nowadays to
evaluate cryptosystems by their ability to withstand chosen-plaintext attacks.
Anotherimportant way by which acryptosystemisdemonstrated tobesecure isthe
testoftime.Ifnoknownsuccessfulattacks havebeenreportedsinceasystem ispublished
and in use for a significant amount of time (measured in years), the cryptosystem is
considered to probably provide pretty good security.
11.3.3 Symmetric and Asymmetric Cryptosyst.ms
There aretwo broadclassesofcryptosystems,symmetric andasymmetric. Inasymmetric
cryptosystem,either boththeencryption key(K e)anddecryption key(K d) arethesameor
one is easily derivable from the other. Usually, a common key (K) is used for both

Sec. 11.3 • Cryptography 577
encipheringand deciphering. For security, it is importantthat the key of a symmetric
cryptosystembeeasilyalterableandmustalwaysbekeptsecret.Thisimpliesthatthekey
isknownonlytoauthorizedusers.Symmetriccryptosystemsarealsoknownasshared-key
orprivate-key cryptosystems.
Symmetriccryptosystemsare usefulin thosesituationswherebothencryptionand
decryption of information are performed by a trusted subsystem. For example, a
password-baseduserauthenticationsystemmayusethisschemeforsavingpasswordsin
encrypted form. When a user declares a password, the operating system uses the
encryptionkey for encryptingthe passwordbefore storing it internally. At the time of
authentication, the operating system again uses the same key to decrypt the stored
passwordto compareit tothe passwordsuppliedbythe user.
In an asymmetric cryptosystem, on the other hand, the decryptionkey (K d) is not
equal to the encryption key (K e). Furthermore, it is computationally impractical to
derive K d from K e. Because of this property, only K d needs to be kept secret and K,
is made publicly known. Asymmetric cryptosystems are also known as public-key
cryptosystems.
Public-keycryptosystemsarccomputationally expensiveandhencearenotsuitable
forbulkdataencryption.A typicaluseofapublic..keycryptosystemindistributedsystems
is for establishingconnection between two communicatingentities (A and B) for the
exchangeofmessagesusingasymmetriccryptosystem. LetussupposethatAandBwant
to establish a connection between themselves for initiating messagetransfersusing a.
symmetriccryptosystemwhosekeyisK. Notethatit is insecuretosendthekeyKover
a normalcommunicationchannelforthepurposeof sharingitwithA andB. Therefore,
a public-keycryptosystemis firstusedtoestablishaconnectionbetweenA andBinthe
following manner:
• EntityA poststheencryptionkey(K
e)
of a public-keycryptosystem on, say,an
electronic bulletin board, sothat it is obtainable by B.
• EntityB usesA'spublickey toencryptthe key K andtransmitsittoA.
• EntityA decryptsB's message usingthe decryptionkey (K d) of the public-key
cryptosystem. Only A can decrypt this message because K is available only
d
with A.
• NowthatA alsohasthekeyK, bothA andB can safelycommunicatewitheach
other using the symmetriccryptosystemscheme.Anyeavesdropperwouldonly
havethepublickey(K e), theencryptedformofkeyK, andtheencryptedformof
the messagesbeingcommunicatedbetweenA andB.
One pitfall here is that someone else could masqueradeas A or B. This can be
overcomeby usingdigitalsignatures(describedlaterinthischapter).
Therelativeadvantagesanddisadvantagesofsymmetricandasymmetric cryptosys
ternsare asfollows:
1. Symmetric cryptosystems require that both encryption and decryption of
informationbe performed by a trusted subsystem, whereas this is not necessary with
asymmetric cryptosystems. Therefore, general security policies need asymmetric
cryptosystems.

578 Chap.11 • Security
2. When employed for the security of messages in a communication system, a
symmetric cryptosystem requires a secure channel by which the sender can inform the
receiverofthekeyusedtoencipherthemessages.Theencryptedmessagesmay,however,
be transmitted through an insecure channel. On the other hand, in an asymmetric
cryptosystem, both the public-key and the messages can be transmitted through an
insecure channel. Therefore, there is no need for a special secure channel for key
transmission. Due to this reason, asymmetric cryptosystems are considered to be more
secure than symmetric cryptosystems.
3. Ingeneral, asymmetriccryptosystems are computationally much more expensive
than symmetric cryptosystems and are inefficient for bulk data encryption. Hence, in
practice, for data communications, asymmetric cryptosystems are often used only for
initialization/control functions, while symmetric cryptosystems are used for actual data
transfer.
The Data Encryption Standard (DES) cryptosystem [NBS 1977, Seberry and
Pieprzyk 1989]is the best known and most widely used symmetric cryptosystem today.
Ontheother hand,theRivest-Shamir-Adleman (RSA)cryptosystem [Rivestet al. 1978,
Seberry and Pieprzyk 1989] is the first published and practically the most satisfactory
asymmetric cryptosystem today.
11.3.4 Key Distribution Problem
Whencryptography isemployedforsecurecommunications indistributedsystems,aneed
forkeydistribution arisesbecausetwocommunicating entities cansecurelycommunicate
only when they obtain matching keys for encryption and decryption of the transmitted
messages. A matching pair of keys held by two communicating entities forms an
independent, private logical channel between them. The key distribution problem deals
with how tosecurely supply thekeys necessary tocreate theselogicalchannels. The key
distribution problem in symmetric and asymmetric cryptosystems are described below.
Key Distribution in Symmetric Cryptosystems
When two users (persons or programs) of two different nodes want to communicate
securely by using a symmetric cryptosystem, they must first share the encryption!
decryptionkey.Forthis,thekeymustbetransmittedfromoneofthetwouserstotheother
user.However, there is no special transmission medium for the key transfer and the key
must be transmitted using the same insecure physical medium by which all exchanged
messages are transmitted. This requires that the key must itself be encrypted before
transmission because if the key is compromised by an intruder while being transmitted
over the insecure medium, the intruder can decrypt all encrypted messages exchanged
between the two users. Therefore, a circularity exists in symmetric cryptosystems. This
circularity can only be broken through prior distribution of a small number of keys by
some secure means. The usual approach is to use a server process that performs thejob
of a key distribution center (KDC). Each user in the system shares with the KDC a
prearranged pair of unique keys.

Sec. 11.3 • Cryptography 579
The KDCisagenerally trusted entityandissharedbyallcommunicatingusersofthe
system. On request by a user, it generates a new secret key to be used by the user to
communicate with another user.Inactual implementation, there may be several KDCs in
the system. The three commonly used implementation approaches are as follows:
• Centralized approach
• Fully distributed approach
• Partially distributed approach
Below we describe how key distribution takes place ineach approach between two
users who want to communicate securely with each other.
CentralizedApproach. In this approach, a single centralized KDC is used that
maintains atableof secret keys foreach user(seeFig. 11.2).Auser'ssecret keyisknown
only to the user and KDe. Suppose that the secret keys of users A and Bare K a and K b,
respectively, and that a secure logical communication channel is to be established for
exchanging encrypted messages between them. The following protocol was proposed in
[Needham and Schroeder 1978]for performing this task (see Fig. 11.2):
ec= m1 m3
~ ===xv
m4
m2 ms
IDa K a
lOb Kb
Table ofsecretkeys
foreach user
m1 =(Ra,IDa,lOb)
whereRa;:.;:code fortherequest madebyuserA
IDa=identifierofuserA
IDb=identifierofuserB
m2=E ((Ra,IDa,Kab,C1),Ka}
=
where Kab secret keygeneratedbytheKDC.forsecure
communicationsbetweenusersAand B
C1=E((Kab,IDa),Kb)
=
where Kb private keyofuserB
Ka::private keyofuserA
m3=C1
m4=C2= EifJr'Kab)
where N,==arandom numbergeneratedbyuserB
m5==C3= E ifJt,Kab)
where N,==f(Nr)and tisapreviouslydefined function
Fig. 11.2 The method of key distribution inthe centralizedapproach.

580 Chap.11 • Security
1. User A sends a request message (ml) to the KDC indicating that it wants to
establish a secure logical communication channel with user B. The message containsa
codefor the request(Ra),the user identifierofA (IDa), and the user identifierofB(ID b).
This message is transmitted from user A to KDC in plaintext form.
2. On receiving ml' the KDC extracts from its table the keys K a and K b, which
correspondrespectivelyto the useridentifiers IDa and IDb inthe message. It then creates
a secret.key Kabfor securecommunicationsbetween users A and B.By usingkey K b,the
KDC encryptsthe pair(K abtIDa) togenerateaciphertextC
1
=E«K ab,IDa), K b).Finally,
it sends a message (m2)to user A that containsR a,IDa' K abtand C 1•The message m2 is
encrypted with the key K so that only user A can decrypt it.
a
3. Onreceivingm2'userAdecryptsit with itsprivatekey K and checkswhetherR
a a
and IDa ofthe message match with the originals to get confirmed that m2is the reply for
mI.Ifso, user A keeps the key K abwith it for future use and sends a message m3 to user
B.This messagecontains theciphertextC 1•Note that only user Bcan decrypt C
1
because
it was generated using key K b•
4. On receivingm3'userBdecrypts C
1
with itsprivatekey K
b
and retrievesboth K
ab
and IDa. At this stage, both users A and B have the same key K ab that can be used for
secure communications between them because no other user has this key. At this point,
user B needs to verify if user A is also in possession ofthe key K ab. Therefore, user B
initiatesan authenticationprocedurethat involvessendinganoncetouserA and receiving
a reply that contains some function ofthe recently sent nonce. Forthis, userB generates
arandomnumberN r,encryptsN;byusing thekey K
ab
togenerateaciphertextC
2
=E(N
n
Kab), and sends C
2
to user A in a message ms, The random number N, is used as a
nonce.
5. On receiving ms, user A decrypts C with the key K and retrieves N; It then
2 ab
transformsN, to a new value N,by a previously defined function (f). UserA encrypts N,
by using the key K ab to generate aciphertext C 3 =E(N I, K ab), and sends C 3 to user B in
a message m«.
6. On receiving ms, user B decrypts C 3, retrieves Nt, and applies the inverse of
functionf to N to check if the value obtained is N; Ifso, user B gets confirmed that a
I
secure channel can be created between users A and B by using the key K ab. This is
sufficient to achieve mutual confidence, and from now on, the exchange of actual
messages encrypted with key K can take place between users A and B.
ab
That there is a problem in the protocol was pointed out by Denning and Sacco
[1981]. They observed that during the transfer of message m3' if an intruder copies C
1
and by unspecified means came to know K ab, that intruder can in future always
pretend to B that it was A. The basic problem here is that B never had the chance to
offer a nonce to the KDC and, therefore, has no means of deducing the freshness of
the quantity (K ab) that came from the KDC. Note that only the KDC and A know that
K is fresh. One method to correct this problem is to add a timestamp (T) to the
ab
ciphertext C 1, so that it becomes E«K ab, IDa' T), K b). User B decrypts this message

Sec.11.3 • Cryptography 581
and checks that T is recent. This solution is adopted in the Kerberos system (described
later in this chapter).
Notice that in the centralized KDC approach, if there are n users in the system, n
prearranged key pairs are needed to provide secure communications. The approach is
simple and easy to implement. However, itsuffers from the drawbacks ofpoor reliability
andperformancebottleneck ofthesingle KDC.Thatis,freshkeydistributions cannot take
place ifthe node on which the KDC resides crashes, and the KDCmay get overloaded in
a large system with too many users. Twoother approaches described below may beused
to overcome these drawbacks of the centralized approach.
Fully DistributedApproach. In this approach, there is a KDC at each node of
the distributed system. The prior distribution of secret keys allows each KDC to
communicate securely withall other KDCs.That is,each KDC has atableof secret keys
having private keys of allother KDCs.Therefore, inasystem having nnodes,each KDC
keeps n-l keys, resulting in a total of n(n-l)/2 key pairs in the system.
Suppose that a secure logical communication channel is to be established between
userAofnodeN
1
anduserBofnodeN 2.Alsosuppose thatK
1
andK
2
aretheprivate keys
of the KDCs of nodesN 1 andN 2, respectively.The desired connection canbeestablished
in the following manner (see Fig. 1I.3):
m,=(Ral10a, lOb)
where Ra =
=
codefortherequest madebyuserA
IDa identifierofuserA
lOb=identifierofuserB
m2=(~, IDa, Kab)
=
where Kab secretkeygeneratedbytheKDCofnode N1for
securecommunicationsbetween usersAand B
m3=~=E ((~bl IDa,lOb),K2)
where K2=privatekeyofKDCofnode N2
=
m4 (KablIDa)
ms= C2=E(N" Kab)
where N,=arandom numbergeneratedbyuser B
m6=C3=E VVt, Kab)
=
where Nt t(N,)and tisapreviouslydefined function
Fig. It.3 The method of key distribution inthefully distributedapproach.

582 Chap.11 • Security
1. User A sends a request message (ml) to its local KDC. The message contains a
code for therequest (Ra),the user identifier ofA(IDa)' andtheuseridentifierof B(ID b).
It is assumed that all local communications are secure and hence local messages can be
securely transmitted in plaintext form.
2. Onreceivingrn),theKDCofnodeN)consults thenameservertogetthelocation
(N 2) ofthe user having identifier IDb' Itthen extracts theprivate key (K 2) of the KDCof
node N
2
from its table. Next, it creates a secret key Kab for secure communications
between usersAandB.ByusingkeyK 2,itencrypts thetriplet(KabtIDa'ID b) togenerate
aciphertext C. =E«K ab, IDa'ID b), K 2). Finally, it sends a message (m2) to userA that
contains R a, IDa' K ab and a message (m3) to the KDC of node Nzthat contains C].
3. On receiving m2' userA checks whether R and IDa of the message match with
a
the originals to get confirmed that m2is the reply formi. If so, userA keeps the key K
ab
with it for future use.
4. Ontheotherhand,onreceiving messagem3'theKDCofnodeN 2decrypts itwith
its private key K and forwards the pair (K IDa)to the user with identifier JD in the
2 abt b
form of message ms.
5. On receiving m4' user B initiates an authentication procedure and authenticates
userA exactly in the same way as done in the centralized approach.
Notice that in this approach the number of prearranged key pairs needed to provide
secure communications is independent of the number of users and depends only on the
number of nodes in the system. Another major advantage of this approach is that for
successful distribution of a key for secure communications between two users, only the
nodes of the two users must be properly functioning. Therefore, the approach is highly
reliable.
Partially DistributedApproach. In this approach, the nodes of the system are
partitionedintoregions and, instead ofhaving a KDCforeach node, there isa KDConly
for each region that resides on one of the nodes of that region-.The prior distribution of
secret keys allows each KDC to communicate securely with each user of its own region
and with the KDCsof allother regions. That is,each KDChas atable of secret keys that
contains private keys of all users of its own region and of all other KDCs.
In this approach, the distribution of a key for the establishment of a secure logical
communication channel between two usersA and Bdepends on the locations of the two
users. If both the usersA and B reside on nodes that belong to the same region, the key
is distributed exactly in the same manner as is done in the centralized approach. In this
case, the KDC of thatregion plays the roleofthecentralized KDC. On theother hand, if
the users A and B reside on nodes belonging to different regions, the key distribution is
performed in a manner similar to that in the case of the fully distributed approach. The
only difference is that, in this case, messages m2 and m4are also encrypted because they
aretransmittedfrom onenode toanother.That is,m2isencrypted with theprivate key of
userA, and m«is encrypted with the private key of user B.The complete process of key
distribution in this approach is shown in Figure 11.4. Notice that, in this approach, the

Sec.11.3 • Cryptography 583
Region R
1
Lettherebe 10nodesinthesystemthatarepartitionedintotworegions
R1 and R2asshown above. TheKDCsofregions R1and R2arelocated
onnodesN1 and N61respectively.
m1::(Ra,IDa, lOb)
=
where Ra codefortherequest madebyuserA
ID =identifierofuserA
a=
10b identifierofuserB
m2::C1 =E ((Ra, IDa, Kab), Ka)
where Kab=secret keygeneratedbytheKDCofregion R1for
secure communicationsbetween usersAand B
Ka=privatekeyforuserA
m3 =C2::E ((Kab, IDa, lOb),K2)
where K2:: private keyofKDCofregion R2
m« = C3 = E q<et» IDa), Kb)
where Kb =private keyofuserB
ms=C4=EtN" Kab)
where Nr =arandom numbergeneratedbyuser B
m6 =Cs=E f/Vt, Kab)
where N,=f(Nr)andfisapreviouslydefined function.
Fig.11.4 Themethodof key distribution in the partially distributedapproach.
failure of the KDC of a particularregion will only disrupt key distribution activities that
involve auserofthat region. Keydistribution involving users ofother regions can stillbe
carried out successfully. Therefore, the reliability of this approach lies in between the
reliabilities of the centralized and the fully distributed approaches.
Key Distribution in Asymmetric Cryptosystems
In an asymmetric cryptosystem only public keys are distributed. Since public keys need
not be kept secret, there is no problem of key transmission through an insecure physical
medium. Therefore, one might conclude that the key distribution problem does not exist
in asymmetric cryptosystems. However, this is not correct since the safety of an

584 Chap. II • Security
asymmetriccryptosystem depends critically on thecorrect public key being selected bya
user.A user whoreceives a public key wants tobesure that the received key is genuine.
Thus, in asymmetric cryptosystems, the key distribution process involves an authentica
tion proceduretoprevent anintruderfrom generating apair ofkeys and sending apublic
key to another user forestablishing a logical communication channel with that user.The
authentication procedure allows the two users of a logical communication channel to
identify themselves before starting the actual exchange of data.
Thecommonly usedkeydistribution approach inasymmetric cryptosystemsistouse
apublic-keymanager(PKM)process thatmaintains adirectory ofpublic keys ofallusers
in the system. There isakey pair (Pk,Sk)for the PKM also.The public key Pkisknown
toallusers andthesecret keySk isknown exclusively tothePKM.The protocol based on
this approach for establishing a logical communication channel between two users is
described below.
Let us assume that a logical communication channel is to be established between
usersA and B.Also assume that (Pa,Sa)is the key pair of userA and (Ph' Sb)is the key
pair ofuser B.The public keys Paand P
b
are stored withthePKM and the secret keys Sa
and Sbare known exclusively to usersA and B,respectively.The desired connection can
be established in the following manner (see Fig. 11.5):
1. User A sends a request message (mJ) to the PKM indicating that it wants to
establish a secure logical communication channel with user B. The message contains a
code for the request (R a), a timestamp (T.), the user identifier of A (IDa)' and the user
identifier of B (ID b). This message is encrypted by using P k• That is, m, contains the
=
ciphertext C} E «Ra, T}, IDa, JD b), P k).
2. On receiving mit the PKM decrypts C. by using Sk'It then extracts the public
keys P and P from its table that correspond respectively to the user identifiers IDaand
a b
ID
b
ofthemessag
=
e. ByusingkeyP a,thePKMencrypts thetriplet(R a,T., Ph)togenerate
a ciphertext C 2 E «R a, Tit Ph), P a). Finally, it sends C 2 to user A in the form of a
message (m2)'
3. On receiving m2' userA decrypts C by using Saand retrieves its contents. The
2
retrieved values of R
a
and T 1are compared with the originals to get confirmed that m2is
the reply for m. and that it is not a replay of an old message. User A then generates a
random numberNa,encrypts thepair(IDa'Na)byusinguserB'spublickeyPhtogenerate
=
a ciphertext C 3 E «IDa' N a), Pb), and sends C 3 to user B in a message (m3)'
4. On receiving m3' user B decrypts C by using Sband retrieves its contents. By
3
seeing IDa in the message, user B knows that user A wants to establish a logical
communication channel with it.In order toget the authentic public key of userA,user B
contacts the PKM. For this, it sends a message (m4) to the PKM requesting for userA's
public key.The message contains a code for the request (R b), a timestamp (T2), the user
identifierofA(IDa)'andtheuseridentifierofB(ID b).Thismessageisencrypted byusing
r; =
That is, m4 contains the ciphertext C 4 E «R b, T 2, IDa' ID b), P k).
5. On receiving ms, the PKM decrypts C by using Sk. It then extracts the public
4
keys Paand Ph that correspond respectively to the user identifiers IDaand ID
b
of the

Sec.11.3• Cryptography S8S
Directoryof
publickeys
IDa Pa
IDb Pb
=
P«=publickeyofPKM Pb publickeyofuserB
Sk=secret keyofPKM Sb = = secret keyofuser B
S P a a = = s p e u c b r l e ic t k k e e y yo o f f u u s s e er r A A I lO Da b = u u s s e e r r i i d d e e n n t t i i f f i i e e r r o o f f A B
m1 =C 1=E ((Ra,7;,IDa, lOb), Pk)
where
T
R
1
a= =
a
c
t
o
im
de
es
fo
ta
r
m
th
p
erequest madebyuserA
m2=C2 =E ((Ra, T 1, Pb), Pal
m3=C3=E ((IDa,Na),Pb)
where Na-=arandom numbergeneratedbyuserA
tru-=C4 -=E ((~, T2,IDa. lOb),Pk)
where Rb=code fortherequestmadebyuser B
T2
=atimestamp
m5=Cs =E ((Rb,T2,Pa),Pb)
= =
m6 C6 E ((Na,Nb), Pa)
where Nb=arandom numbergeneratedbyuser B
m7-=C7 =E rJb,Pb)
Fig. 11.5 Themethod ofkey distribution inan asymmetriccryptosystem.
message. By using key Pb' the PKM encrypts the triple (R b, Tz, Pa) to generate a
ciphertext Cs =E «Rb, Tz, P a), Pb)' Finally it sends C; to user B in the form of a
message (ms).
6. On receiving ms, user B decrypts Cs by using S/J, retrieves its contents, and
compares R and Tz with the originals to get confirmed that ms is really the reply of
b
m«. User B then generates a random number N b, encrypts the pair (N a, N b) by using
the key P a to generate a ciphertext C 6 = E «N a, N b), P a), and sends C 6 to user A in
a message (m6)'
7. On receiving m6' user A decrypts C 6 by using Sa' retrieves its contents, and
compares the received N with the original. Ifthey match, userA becomes sure that user
a

S86 Chap. II • Security
=
Bis authentic. UserAthen encryptsN by using the key P to generate a ciphertext C
b b 7
E (Nb, Pb) and sends C
7
to userB in a message (m7).
8. On receiving m-; user B decrypts C by using Sband compares the received N
7 b
with the original. Ifthey match, user Balso gets confirmed that userA is authentic. This
is sufficient to achieve mutual confidence and allows regular communication to start.
Note that the protocol paradigms described above for key distribution in symmetric
and asymmetriccryptosystems illustrate basic design principles only. A realistic protocol
is necessarily a refinement ofthese basic paradigms.
11.4 AUTHENTICATION
Authentication deals with the problem of verifying the identity of a user (person or
program) before permitting access to the requested resource. That is, an authentication
mechanism prohibits the use of the system (or some resource of the system) by
unauthorized users by verifying the identity ofa user making a request.
Authentication basically involves identificationand verification. Identification is the
process of claiming a certain identity by a user, while verification is the process of
verifying the user's claimed identity. Thus, the correctness ofan authentication process
relies heavily on the verification procedure employed.
The main types of authentication normally needed in a distributed system are as
follows:
1. User logins authentication. It deals with verifying the identity of a user by the
system at the time oflogin.
2. One-way authentication ofcommunicating entities. It deals with verifying the
identity ofone ofthe two communicating entities by the other entity.
3. Two-way authentication of communicating entities. It deals with mutual
authentication, whereby both communicating entities verify each other's
identity.
A description ofthe authentication mechanisms that are commonly used to perform
these types of authentication is presented below. Note that the authentication protocol
paradigms described below illustrate basic design principles only. A realistic protocol is
necessarily a refinement of these basic paradigms and addresses weaker environment
assumptions, stronger postconditions, or both.
11.4.1 Approaches toAuthentication
The basic approaches to authentication are as follows [Shankar 1977, Woo and
Lam 1992]:
1. Proof by knowledge. In this approach, authentication involves verifying
something that can only be known by an authorized principal. Authentication of a user

Sec.11.4 • Authentication 587
based on the password supplied by him or her is an example of proof by knowledge.
Authentication methods based on the concept of proof by knowledge are again of two
types-direct demonstration method and challenge-response method. In the direct
demonstration method, a user claims his or her identity by supplying information (like
typing in apassword)that the verifierchecks against prestoredinformation. On the other
hand, in the challenge-response method, a user proves his or her identity by responding
correctly to the challenge questions asked by the verifier. For instance, when signing up
asauser,the user picks afunction, forexample,x+ 18.When the user logsin,the system
randomly selects and displays a number, say 105, in which case the user must type 123
for authentication to be successful. For further security improvement, several functions
may be used by the same user.At the time of login, the function to be used will depend
onwhen the login ismade. Forexample, auser may useseven differentfunctions, one for
each day ofthe week. Inanothervariationof this method, alist ofquestions such aswhat
is the name of your father, what is the name of your mother, what is the name of your
street on which your house is located, is maintained by the system. When signing up as
a user, the user has to reply to all these questions and the system stores the answers. At
login, the system asks one of these questions at random and verifies the answer supplied
by the user.
2. Proof by possession. In this approach, a user proves his or her identity by
producing some item that can only be possessed by an authorized principal. The system
is designed to verify the produced item to confirm the claimed identity. For example, a
plastic card with a magnetic strip on it that has a user identifier number written on it in
invisible, electronic form may be used as the item to be produced by the user.The user
inserts the card in a slot meant for this purpose in the system's terminal, which then
extracts the user identifier number from the card and checks to see if the card produced
belongs to an authorized user. Obviously, security can be ensured only ifthe item to be
produced is unforgeable and safely guarded.
3. Proofby property. In this approach, the system is designed to verify the identity
ofauser bymeasuringSOInephysical characteristicsoftheuser that are hardtoforge.The
measured property must be distinguishing, that is, unique among all possible users. For
example, aspecial device (known asabiometricdevice) may beattached toeach terminal
of the system that verifies some physical characteristic of the user, such as the person's
appearance, fingerprints, hand geometry, voice, signature. In deciding the physical
characteristictobe measured,animportantfactor tobeconsideredisthat thescheme must
be phycologically acceptable to the user community. Biometric systems offer the greatest
degree of confidence that a user actually is who he or she claims to be, but they are also
generally the most expensive to implement. Moreover, they often have user acceptance
problems because users see biometric devices as unduly intrusive.
Of the three authentication approaches described above, proof by knowledge and
possession can be applied to all types of authentication needs in a secure distributed
system, while proofby property is generally limited to the authentication of human users
by a system equipped with specialized measuring instruments. Moreover, in practice, a
system may use a combination of two or more of these authentication methods. For

588 Chap.11 • Security
example, the authentication mechanism used by automated cash-dispensing machines
usually employ a combination of the first two approaches. That is, a user is allowed to
withdraw money only if he or she produces a valid identification card and specifies the
correct password corresponding to the identification number on the card.
11.4.2 Us.r login AutMfttlcQtlon
Asincentralized systems,ausergains access toadistributed system bylogging inahost
in the system. User identity is established at login;and all subsequent user activities are
attributed to this identity.Correct user identification at the time oflogin is crucial to the
functioning of a secure system because all access-control decisions and accounting
functions are based on this identity. Although any of the three basic approaches to
authentication can beemployed for user login authentication, the proofby knowledge is
themostwidelyusedmethod.Inparticular,mostsystemsemploythedirectdemonstration
method based on passwords.
In the authentication scheme based on passwords, the system maintains a table of
authorized users' login names and their corresponding passwords. When a user wants to
log in, the system asks the user to type his or her name and password. If the password
supplied by the user matches the password stored in the system against his or her name,
it is assumed that the user is legitimate and login is permitted; otherwise it is refused.
Toprovide good security and bepractically useful, apassword-based authentication
system must have mechanisms for the following:
1. Keeping passwords secret
2. Making passwords difficult to guess
3. Limiting damages done by a compromised password
4. Identifying and discouraging unauthorized user logins
5. Single sign-on for using all resources in the system
The commonly used mechanisms for dealing with these issues are described
below.
Keeping Passwords Secret
A user isresponsible forkeeping hisor herpassword secretoutside thecomputer system
(external world). Howtokeeppasswords secretintheexternal worldisnotofconcern to
operating system designers except for the fact that while a password is being typed in, it
shouldnotbedisplayed ontheterminal,topreventitfrombeingseenbypryingeyesnear
the terminal. The main concern, however. is to prevent an intruder from obtaining
somebody's password by having access to the system's password table. The password
tableisofcourseprotectedandisaccessibleonlytotheauthenticationprogram.However,
there is a great chance that the password table is exposed byaccident or that the system
administrator has access to the table. Therefore, instead of storing the names and
passwords inplaintext form,theyareencrypted andstoredinciphertext forminthetable.
Inthiscase,insteadofdirectlyusingauser-specifiednameandpasswordfortablelookup,

Sec.11.4 • Authentication 589
they are first encrypted and then the results are used for table lookup. Notice that for
implementing this scheme, the main requirement is that even if both the encryption
function and the passwordtable areknown, itisimpossibletofind the original password.
That is,anoninvertiblefunction isneeded forencryptionbecause inthe scheme wenever
need to decrypt a ciphertext. This is an example-of the use of one-way cipher. In
cryptography we use the function
C= E (P,K)
where K isakey,Pisaplaintext, and Cisthe resultingciphertext. Aone-way cipher can
be implemented by letting the plaintext P serve as the key to E such that
C= E (P,P)
In a one-way cipher, itis very difficult to invert the encryption function (E) even if
the intruder knows the key. Furthermore, if the key is not known (as in the case of
password security scheme in which a password itself is used as the key), the inversion
becomes much moredifficult because theintruder cannot know whichfunctions (andhow
many times of them) are used.
Tokeep passwordssecret inadistributedenvironment, itisimportantthatpasswords
should never be sent across the network in plain text form. Moreover, they should notbe
stored on normal servers but should only be stored on trusted servers that are well
protected. Notice that due to these requirements, authentication of a user by simply
sending his or her password to an authentication server for approval does not work in a
distributed environment. The Kerberos authentication system (described later in this
chapter) provides an interesting solution to this problem.
Making Passwords Difficult to Guess
Useofmechanismstokeep passwordssecret does notguarantee thatthesystem'ssecurity
cannot be broken. It only says that it is difficult to obtain passwords. The intruder can
always use a trial-and-error method. Virtually, in case of passwords, break-ins usually
consist of guessing a user name and password combination. How successful an intruder
can be in guessing passwords is obvious from the study made by Morris and Thompson
[1979] of passwords on UNIX systems. They compiled a list of likely passwords that
containedfirst and last names of persons, names ofcities and streets, words from a small
dictionary spelled correctly and backward, short strings of random characters, and license
plate numbers. These passwords were then encrypted using the known password
encryptionalgorithm toobtain alist ofencryptedguessed passwords. Then another listof
encrypted passwords was prepared by using entries in password tables of various UNIX
systems. The entries in the two lists were then compared to find matching entries.
Surprisingly, it was found that over 86% of actual passwords had a match in the list of
guessed passwords.
A test of only a limited set of potential strings tends to reveal most passwords
because there is a strong tendency for people to choose relatively short and simple
passwords that they can remember. Some techniques that may beused to make the task
of guessing a password difficult are as follows:

S90 Chap.11 • Security
1. Longerpasswords. The length ofa password determines the ease with which a
passwordcan befound byexhaustion. Forexample,athree-digitpasswordprovides 1000
variations whereas a four digit password provides 10,000 variations. Longer passwords
are less susceptibletoenumerationbecause the work involved inenumeratingallpossible
passwords increases by increasing the length of the password. Use of longer passwords
can beenforcedorencouraged by providingapasswordentry programthat asks a user to
enter a longer password if he or she enters a short one.
2. Salting the password table. Another technique to make the task of guessing
passwords difficult is to artificially lengthen passwords by associating an n-bit random
number with each password when it is first entered. The random number is changed
wheneverthe password is changed. Instead ofjust storing theencrypted password in the
password table, the password and the random number are first concatenated and then
encrypted together. This encrypted result is stored in the password table. In this scheme,
the password table has an additional field that contains the random number in its
unencrypted form. At the time of login, this random number is concatenated with the
entered password, encrypted, and then compared with the stored encrypted value.
Guessingofapassword becomes difficult because ifanintrudersuspects that Tokyomight
be the password ofa user, it is no longer enough tojustencrypt Tokyo and compare the
encryptedresult withthe value stored in thepassword table. Rather, foreach guess made,
the intruder has to tryout 2n strings, such as TokyoOOOO, TokyoOOOJ, Tokyo0002, and so
forth, for n=4. This increases the encryption and comparison time for eachguess made.
UNIX uses this method with n=12.
3. System assistance in password selection. A password can be either system
generatedoruserselected.User-selectedpasswordsareofteneasytoguess.Asystemcanbe
designed toassistusersinusingpasswordsthataredifficulttoguess.Thiscanbedoneintwo
ways. One way is tostore a listofeasy-to-guess passwords within the system and to first
compareauser-selected password withtheentries inthelist.Ifamatchisfound,thesystem
refuses to accept the selected password and asks the user to select another password
informinghimorherthattheselected passwordiseasy toguess.Another wayofproviding
systemassistanceistohaveapasswordgeneratorprogram thatgenerates random, easy-to
remember, meaningless words, such as mounce, bulbul, halchal, that can be used as
passwords. The userselects apasswordsuggestedbythesystem andusesitwith hisorher
ownchoice ofamixtureofupper-andlowercaselettersofthatword,suchashAICHal.
Identifying and Discouraging Unauthorized User
Logins
Some managementtechniques should be used toimprove thesecurity of asystem against
unauthorized user logins. Three such techniques are as follows:
1. Threat monitoring. This technique detects security violations by checking for
suspiciouspatterns ofactivity.Forexample, thesystem maycountthenumber ofincorrect
passwordsgiven whenauser istrying tologin,and after, say,three failed login attempts,
the system notifies security personnel by setting an alarm.

Sec. 11.4 • Authentication 591
2. Auditlogs. In this technique, the system maintains arecord of all logins.That is,
thetimeat which login wasdone, theduration oflogin, theaccessed objects andthetypes
of accesses made,etc., are recorded foreach login. When a user logs in successfully, the
system reports to the user some of the recorded information (such as time and duration)
of the previous login.This may be helpful for the user todetect possible break-ins.After
a security violation has been detected, the audit log can be used to detect which objects
were accessed by the intruder and the amount of damage done. This information can be
useful for recovery from the damages done by the intruder.
3. Baited traps. IntrudersITIayalsobecaught bylayingbaitedtraps.Forexample, the
system maymaintain somespecial loginnames witheasy passwords, suchaslogin name:
user, password: user. Whenever anyone logs in using any of these names, the system
security personnel are immediately notified.
Single Sign-on
In a distributed client-server environment, a user might have several client programs
running onhisor herhost node that access different server programs on remote nodes.In
such an environment, the servers must authenticate that the clients run on behalf of a
legitimate user. If a simple password-based authentication scheme is used, the user's
password must be presented each time a server wants to authenticate a client program
running on behalfof the user.This iscurnbersome and inconvenient because a user will
certainly not want to enter a password each time he or she accesses a new service. This
is also not desirable from the point of view of transparency, which aims to provide a
virtual single system image. A simple way to solve this problem is to cache the user's
password on hisor her hostcomputer and use itfrom thecache every time anew service
is accessed. However, this solution does not work because it is dangerous to keep
passwords incache from the point of view of keeping passwords secret. Once again, the
Kcrberos authentication system (described later in this chapter) provides an interesting
solution to this problem.
Limiting Damages Done bya Compromised Password
Itissuggested thatausershouldchange hisorherpassword frequently sothatanintruder
who has been successful in guessing the current password will have to guess a new one
after the current password is changed by the user. In this way, the damage done by the
intruder can be reduced. The extreme form of this approach is the use of one-time
passwords. There are three different methods to implement the idea of one-time
passwords. In the first method, the user gets a book containing a list of passwords. For
each login, the user mustusethe next password inthelist.Obviously, theuser mustkeep
thepassword book inasecure place. Inthesecond method,every timeauserlogsout,the
systemaskstoselectanewpassword forthenextloginandreplaces theoldpasswordwith
the new one. The user either remembers the new password or notes itdown somewhere
foruseatthetimeofnext login.The thirdmethod relieson theuseofaspecialequipment
such as smart cards or synchronized password generators. For example, a synchronized

592 Chap.11 • Security
passwordgeneratordevicegenerates a pseudorandom alphanumeric word or numberthat
changeseveryminuteor soand is time synchronizedtoadatabasestoredin thecomputer.
To log in, a user types in the word or numberdisplayed on the card at the time oflogin.
This results in a one-time password that is good only at that particularpoint in time and
for only one login. One such deviceisthe SecureIDfrom SecurityDynamics(Cambridge,
Massachusetts). This method has not been very successful thus far, primarily due to the
inconvenience and cost of the additional hardware required. It requires that all users
purchase the hardware device. However, it has the advantage that once set up, it is fairly
easy to administer(as opposed to a manual password list that requires frequent updating
for all users).
11.4.3 On••Way Auth.ntkatlon of Communicating Entltl.s
When an entityA wants to communicate with another entity B, B may like to verify the
identity ofA before allowing A to communicate with it. For example, a server may be
designed to first verify the identity ofany client that wants to communicate with it. The
commonly used authentication protocols for one-way authentication of communicating
entities are described below.
The followingdescriptionisbasedon the materialpresentedin [Wooand Lam 1992].
Sincethe authenticationprotocol paradigmsdirectly use cryptosystems,theirbasic design
principlesalso follow closelythe type of cryptosystem used. Therefore, the protocols can
be broadly classified into two categories-those based on symmetric cryptosystems and
thosebasedon asymmetriccryptosystems.Authenticationprotocolsofboth categoriesare
based on the proof-by-knowledge principle.
Protocols Based on Symmetric Cryptosystems
In asymmetriccryptosystem,the knowledgeofthe sharedkey allowsan entity toencrypt
or decrypt arbitrary messages. Without such knowledge, it is not possible for the entityto
encrypta messageor to decryptanencryptedmessage. Hence,inauthenticationprotocols
based upon symmetric cryptosystems, the verifier verifies the identity of a claimant by
checking if the claimantcan correctly encrypt a message by using a key that the verifier
believes is known only to an entity with the claimed identity (outside the entities used in
the verification process).
Let us assume that user A wants to communicate with user B but B wants to
authenticate A before starting the communication. Also assume that K is the key of a
symmetriccryptosystemthat issharedbetweenusersA and B.The authenticationprotocol
for this consists ofthe following steps:
1. UserA encrypts its identifier(IDa)by using key Kto obtain a ciphertext C =E
t
(IDa'K). It then sends a message m, to user B, which contains IDa and C,.
2. On receiving ml' user Bdecrypts C) by using key K and compares the obtained
result with IDa ofthe message. Ifthey match, user A is accepted; otherwise it is
rejected.

Sec.11.4• Authentication 593
One major weakness of this protocol is its vulnerability to replays. That is, an
intruder could masquerade as A by recording the message ml and later replay it to B.
Replay attackscanbecounteredbyusingnoncesortimestamps. Anonce-basedchallenge
response protocol thatovercomestheproblem ofreplay ofmessages worksasfollows (see
Fig. 11.6):
1. User A sends its identifier (IDa) to user B in plaintext form in a message m.,
2. On receiving nu,user Bgenerates arandom number N,and sends N,touserA in
plaintext form in a message m2'
3. On receiving m2' user A encrypts N, by using key K to obtain a ciphertext C =
1
E (N K). It then sends C to B in a message m-;
n 1
4. On receiving ms, user B decrypts C by using key K and compares the obtained
1
result withtheoriginal valueofN Iftheyareequal, userAisaccepted; otherwise
r
it is rejected.
m1=IDa =identifierofuserA
m2=Nr=arandom numbergreneratedbyuser B
m3=C1 =E Wr, K)
where Kisthesymmetric keysharedbetween usersAandB
BdecryptsC1using Kandcompares theresultwithoriginalN,.
Iftheyareequal, A isaccepted,otherwiserejected.
I'-'ig. 11.6 One-way authenticationprotocol based onsymmetric cryptosystem.
Inthis protocol, the freshness ofN, (whose value isdifferent foreach conversation)
guarantees that an intruder cannot masquerade as A by replaying a recording of an old
authentication conversation between A and B.
Although the above-described protocol functions correctly, it is impractical for a
general large-scale system due to the following reasons:
1. Notice that the scheme requires thateach user must store the secret key forevery
otheruser itwould ever want toauthenticate. This may notbepractically feasible
in a large system having too many users and in which the number of users keeps
changing frequently.
2. The compromise of one user can potentially compromise the entire system.

594 Chap. )) • Security
Toovercome theseproblems, theuseofacentralized authenticationserver(AS) was
proposed in [Needham and Schroeder 1978].Each user in the system shares with theAS
a prearranged secret key. The AS is a generally trusted entity and is shared by all
communicating users of the system. When the AS is used, the authentication protocol
takes the form shown inFig. 11.7(ithas beenassumed thatK and K are the secret keys
a b
of users A and B, respectively, that are shared with the AS).
1114 ==a== m,
~ ~
m2
ms
IDa Ka m3
lOb Kb
AS=authenticationserver
=
IDa identifierofuserA
IDb=identifierofuserB
Tab
f
l
o
e
r
o
e
f
a
s
c
e
h
c
u
re
s
t
e
k
r
eys Ka=secretkeyofuserA
Kb=secretkeyofuserB
m,=
IDa
=
m2=N, arandom numbergeneratedbyuser8
m3=C1=E IN" Ka)
m4=C2=E «IDa,C 1),Kb)
TheASretrieves N,byfirstdecryptingC2withKbandthendecrypting
C,with «;
ms= C3=E 'N" Kb)
BdecryptsC3withKbandcomparestheresultwithoriginalN,.
Iftheyareequal, thenA isaccepted,otherwiserejected.
Fig.11.7 One-wayauthentication protocol basedonsymmetric cryptosystem and the
useofacentralized authentication server.
1. User A sends its identifier (IDa)to user B in plaintext form in a message nu,
2. On receiving mJ, userBgenerates arandom numberN,andsends N,to userAin
plaintext form in a message m2'
3. On receiving m2' user A encrypts N, by using its secret key K to obtain a
a
ciphertext C 1 = E (N n K a). It then sends C 1 to B in a message m«.
4. On receiving m3' userBencrypts thepair (IDa'C 1) by using its secret key K b to
generate a ciphertext C 2 = E «IDa' C 1), K b). It then sends C 2 to the AS in a
message m4.
5. Onreceiving m4'theASdecrypts C
2
withkeyK
b
andretrieves thepair(IDa' C)).
Itthenextracts fromitsdatabase thekey(Ka)thatcorresponds toIDaanddecrypts

Sec.11.4• Authentication S9S
C with key K and retrievesN; Next, itencryptsN,byusing key K togenerate
1 a b
a ciphertext C 3 =E (N r, K b). Finally, it sends C 3 to B in a message ms.
6. On receiving m«,user B decrypts it by using its secret key K b, retrieves N n and
compares it with the original value ofNy- If they are equal, user A is accepted;
otherwise it is rejected.
As compared to the previous protocol, the key distribution and storage problems are
greatly alleviated because now each user needs to keep only one key. Moreover, the
system's security can be greatly improved simply by tightening security for the AS
becausethe risk ofcompromise ismostly shifted totheAS.The centralizedAS, however,
suffers from the same drawbacks as the centralized KDC. These drawbacks can also be
solved byusing fully/partiallydistributedASs inasimilarmannerasdescribedforthekey
distribution problem in Section 11.3.4.
Protocols Based on Asymmetric Cryptosystems
In an asymmetric cryptosystem, the public key of each user is published while the secret
key of each user is known only to the user and no one else. Hence, in authentication
protocols based upon asymmetric cryptosystems;the verifier verifies the identity of a
claimant by checking if the claimant can correctly encrypt a message by using the secret
key of the user whose identity is being claimed.
Let us assume that user A wants to communicate with user B but B wants to
authenticate A before starting the communication. Also assume that P and Saare the
a
public and secret keys of userA, and Phand Sbare the public and secret keys of user B.
The authentication protocol for this consists of the following steps (see Fig. 11.8):
1. User A sends its identifier (II)ll) to user B in plaintext form in a message mi.
2. On receiving m, userBgenerates arandom numberN, and sends N, to userA in
plaintext form in a message m-:
m3
m1=IDa==identifierofuserA
m2==Nr==arandom numbergeneratedbyuser B
m3==C1==E tN" Sa)
where Sa=secret keyofuserA
BdecryptsC1byusing thepublickeyofuserAandcomparestheresultwith
original N; Iftheyareequal, A isaccepted,otherwiserejected.
Fig. 11.8 One-way authentication protocol based on asymmetriccryptosystem.

S96 Chap.11 • Security
3. On receiving m2' user A encrypts N, by using its secret key Sa to obtain a
ciphertext C) = E(Nn Sa)' It then sends C
1
to B in a message m-,
4. On receiving m«, user B decrypts C 1 by using the public key ofuser A (P a) and
comparesthe obtainedresultwith the original value ofNr: If they are equal, user
A is accepted; otherwise it is rejected.
As in the case ofprotocols based upon symmetriccryptosystems, in this case also a
centralizedAS may beused to greatly alleviatethe key distributionand storageproblems.
In this case, theAS maintainsadatabaseofall publishedpublic keys and each user in the
system keeps a copy of the public key. (P s) of the AS. When the AS is used, the
authentication protocol takes the following form (see Fig. 11.9):
1. User A sends its identifier (IDa) to user B in plaintext form in a message mi.
2. On receivingm), user Bgenerates arandom numberN, and sends N,to userA in
plaintext form in a message m2'
Pa=publickeyofA
Sa=secret keyofA
Pb=publickeyofB
Sb=secret keyofB
Ps =publickeyofAS
Ss=secret keyofAS
Databaseof =
publickeys IDa identifierofA
1Db=identifierofB
m,=IDa
m2=Nr: arandomnumbergeneratedbyB
m3=C1=E ttJr,Sa)
=
m4 (Rb,IDa)
where Rb = code forrequestingthepublickeyofauser
=
m5=C2 E ((IDa, Pa),5s)
Bretrieves NlibYfirstdecryptingC2withPsandthendecrypting
C1with Pa.T evalue ofNr obtainedinthiswayiscomparedwith
theoriginalvalue ofN; Iftheyareequal,thenA isaccepted,other
wise rejected.
Fig.11.9 One-way authenticationprotocol based onasymmetric cryptosystemand the
useofacentralized authenticationserver.

Sec.11.4 • Authentication 597
3. On receiving m2' user A encrypts N, by using its secret key Sa to obtain a
ciphertext C = E (N" Sa)' It then sends C to B in a message m-;
1 1
4. On receiving m-, user Bsends the pair(R b, IDa)to the AS in plaintextform in a
message ms, where R is a request code for requesting the public key ofthe user whose
b
identifier is specified in the second element ofthe message.
5. On receiving m«, the AS extracts from its database the public key (P a) of the
user whose identifier is IDa. It then encrypts the pair (IDa' Pa) by using its own secret
key S, to generate a ciphertext C
2
= E «IDa' Pa), Ss)' Finally, it sends C
2
to B in a
message m«.
6. On receiving m«, userB decrypts C 2 by using the public key ofthe AS (P s) and
retrieves the pair (IDa' Pa). Now by using the key Pa, it decrypts C
1
and compares the
obtained result with the original value of N,. If they are equal, user A is accepted;
otherwise it is rejected.
Notice that in this case each user needs to keep only one public key, the public key
ofthe AS.Also notice that in the above protocol it has been assumed that the asymmetric
cryptosystem is commutative. That is, the public and secret keys function in eitherorder.
Therefore, if the public key is used for encryption, then the secret key can be used for
decryption, whereas ifthe secret key is used for encryption, then the public key can be
used for decryption.
11.4.4 Two-Way Authentication of Communicating Entities
In adistributedsystem, tasks are often distributed overmultiple hosts to achievea higher
throughputor morebalancedutilizationofresourcesthan centralizedsystems.Correctness
ofsuch adistributed task depends on whetherpeerprocesses participatingin the taskcan
correctly identify each other. Two-way authentication protocols allow both communicat
ing entities to verify each other's identity before establishing a secure logical
communication channel between them.
Obviously, mutual authentication can be achieved by performing one-way authenti
cation twice. That is, if two communicating users A and B want to authenticate each
other, A can first authenticate B by performing one-way authentication, and then Bean
authenticate A by repeating the same process, but with the roles ofA and B reversed.
However, this may turn out to be costlier than a protocol designed specially for two
way authentication. For example, if the protocol of Figure 11.9 is repeated twice for
performing two-way authentication, a total of 10 messages will be required. However,
a protocol for mutual authentication of communicating entries that requires only seven
messages and that is also based upon an asymmetric cryptosystem and uses a
centralizedAS is described below. In the description, it has been assumed that two users
A and B want to authenticate each other. Here, Pa and Saare the public and secret keys
of user A, and P and Sbare the public and secret keys of user B. Moreover, P, and
b
Ss are the public and secret keys of the AS. The authentication protocol consists ofthe
following steps (see Fig. 11.10):

598 Chap. 11 • Security
Database of
publickeys
IDa Pa
lOb Pb
Ps=public keyofAS Pb=public keyofuser8
P 5 a s = = p s u e b cr li e c t k k e e y y o o f f u A s S erA I S D b a = = = u se se cr r e i t de ke n y tif o ie f r u o s f e A r 8
5a=secretkeyofuser8 lOb useridentifierof8
m,=
(Ra,IDa, lOb)
where Ra=codefortherequest madebyuserA
m2=C1=E ((/OJ,,Pb),Ss)
=
m3=C2 E ((IDa,Na),Pb)
where Naisarandom numbergeneratedbyuserA
= =
m« C3 E ((Rb, IDa,lOb,Na),Ps)
mS=(C4,C6)
where C4=E«IDa,Pa),5s)and
C6 =E (Cs,Pb)
where Cs=(lOb,K,Na),5s)
where Kisasession keygenerated
bytheASforusersAand B.
m6= C7=E ((Cs, Nb),P a)
where N»isarandom numbergeneratedbyuser8
m7= Ca=E (Vb,K)
BdecryptsCawiththesession keyKandcomparestheresultwiththe
oriQinalvalue ofNb.Ifthet areequal,thisissufficienttoprovethatthe
10(;Jlcalcommunicationchannel establishedbetweenAandBwithkey
Klsanewlyestablishedchannel andissecure.
Fig. 11.10 Two-way authenticationprotocol based on asymmetriccryptosystemand
the useofacentralizedauthenticationserver.

Sec.11.4 • Authentication 599
1. UserAsendsarequestmessage(m}) totheASindicatingthatitwantstoestablish
asecurelogicalcommunicationchannelwithuserB.Themessagecontainsacodeforthe
request(R a),theidentifierofuserA(IDa)' andtheidentifierofuserB(ID b).Thismessage
is sent in plaintext form.
2. On receiving ml, the AS extracts from its database the public key Ph that
corresponds to the user identifier ID ofthe message. By using its secret key Ss' the AS
b
=
encrypts the pair (IDb,Pb) to generate a ciphertext C. E «IDb,Pb), Ss)' It then sends
C 1 to A in a message m2'
3. On receiving m-; userA decrypts C 1 by using the public key of the AS (P s) and
retrieves its contents. Itthen generates arandom number N a, encrypts the pair (IDa'N a)
=
by using the public key ofuser B(P b) togenerateaciphertextC 2 E«IDa' Na),Ph), and
sends C to B in a message m«.
2
4. On receiving m«, user B decrypts C by using its secret key Sband retrieves its
2
contents. Itthen sends amessagem4totheASrequestingfor the public key ofuserAand
a session key for the secure logical communication channel between A and B. The
message containsthe requestcode (R b), IDa'ID b, and Na.Before being sent, themessage
is encrypted with the public key of the AS (P s)' That is, the message contains the
ciphertext C 3 = E «R b, to; ID b, Na), Ps)'
5. On receiving ms, the AS decrypts C with its secret key (Ss) and retrieves its
3
contents. Itgeneratesanewsession keyK forAand B.Next itgeneratesthreeciphertexts:
C 4 = E«ID,l' Pa),Ss),
c,=
E«ID b, K, Na),Ss), and C 6
=
E(Cs,Pb)' Finally, it sends
C and C to B in a message m«.
4 6
6. On receiving m«, user B decrypts C and C with P, and Sb' respectively, and
4 6
retrieves their contents. It then generates a random number Nb and creates a ciphertext
C
7=E
«Cs,N b),P a).Finally, itsends C
7
toA inamessageIn6' Notice that Csisobtained
by decrypting C 6.
7. On receiving m6' user A first decrypts C by using its secret key Saand then
7
decrypts C« by using the public key of the AS (P s)' Now both users A and B have the
session key K. User A next generates a ciphertext ("'18 =E (N b, K) and sends C g to B in a
message m7'
8. Onreceivingm-; userBdecrypts C
g
byusing thesessionkeyK andcomparesthe
result with the original value of N b. If they are equal, this is sufficient to prove that the
logical communication channel established between A and B with key K is a newly
established channel and is secure. Both users A and B are now sure of each other's
identity.
Notice that although the authentication protocol is based upon an asymmetric
cryptosystem, the actual communications between users A and B, after the secure logical
communication channel is established between them, take place by a symmetric
cryptosystem.

600 Chap. 11 • Security
11.4.5 (ase Study: H.rberos Auth.ntkQtlon System
The needforsecureauthentication indistributed computing systemshasledtothedesign
of authentication standards and systems for this purpose. For example, X.509 identifies
theCCITIX.500directory authentication framework standard[Smart 1994],Kerberos is
a network authentication system developed at MIT [Neuman and Theodore 1994,
Stallings 1994], and SPX is an experimental authentication system developed by the
Digital Equipment Corporation [Khanna 1994]. Of several authentication systems
developed to date, the Kerberos system is the most popular one and is continuing to
evolve. IthasbeendeclaredtheInternetstandardandhasbecomethedefactostandardfor
remote authentication in networked client-server environments. Therefore, a description
of this system is presented here.
Kerberos was developed at MIT as part of its project Athena [Champine et al.
1990]. It is named after the three-headed dog of Greek mythology that guards the
entrance to Hades. Its design is based on the ideas of Needham and Schroeder [1978]
for key distribution and authentication that we have already seen in Sections 11.3.4and
11.4.3, respectively. It is now available in both commercial and public-domain
implementations and is widely used at MIT and elsewhere to provide secure access to
resources in distributed environments. It is used by Transarc's AFS file system and is
the underlying component of the OSP's DCE Security Server (described later in this
chapter). Several vendors, including DEC, Hewlett-Packard, and IBM, now offer
Kerberos implementations as part of their standard middleware offerings on commercial
UNIX and midrange server platforms. Version 5, the most recent version of Kerberos,
is described here.
Kerberos System Architecture
Thesystemarchitecture ofKerberosisshowninFigure 11.11.Itconsists ofthefollowing
basic components:
1. Kerberos server.The key component of a Kerberos system is a Kerberos server
thatactsasakeydistributioncenter.EachKerberosserverhasanauthentication database,
anauthentication server,and aticket-granting server.Theauthentication database hasthe
user ID and password of all users of the system. Moreover, the Kerberos server shares a
unique secret key with each server in the system. Therefore, the authentication database
also has the server ID and secret key for all servers in the system. The passwords and
secret keys are distributed physically or in some other secure manner as part of the
Kerberos installation. Kerberos uses the DES algorithm togenerate the keys and encrypt
messages, butthisisimplementedasaseparatemodulethatcanbeeasily replaced byany
other suitable algorithm.
The authentication server performs the task of verifying user's identity at the time
of login without requiring the password to travel over the network. Kerberos has
single sign-on facility. Therefore, a user has to enter his or her password only once at
the time of login no matter how many different resources are accessed by the user
after that.

Sec. 11.4 • Authentication 601
Kerberosservernode(KeyDistribution Center)
Clientnode Application
servernode
109=ticket-grantingserver's identifier Kg=ticket-granting server'ssecretkey
= =
10c clienfsidentifier K, ticket-granting ticketsessionkey
IDs
=
applicationserver'sidentifier
K2=
service-grantingticketsessionkey
N; = anonce Ts;=startingtimeofvalidityofticket
=
Kc =clienfssecretkey Tei endingtimeofvalidityofticket
Ks = applicationserver'ssecretkey T; =atimestamp
m,
=(l0c' N, )
= = =
m
2
C2 E((N" K" G,l. Kc)'whereG, E((f0c'109'Ts,' Te" K,),Kg)
=
m
3
(IDs'Nz C" C
3),
where C
3
=E((f0c'T,),K,)
m 4 =Cs = E((N 2,K 2,G 4),K,),whereC 4 =E((IDc,IDs' T S2' T e2,K 2),Ks)
= = o;
m s (C4' C e) ,where C e E((I T 2),K 2 )
= = =
me C E(T ,K ),whereT T +1
7 3 2 3 2
Fig.11.11 Kcrberosauthentication protocol.
The ticket-granting server performs the task of supplying tickets to clients for
permitting access toother servers inthesystem.These tickets areusedtoestablish secure
logical communication channels between clients and servers by performing mutual
authentication.
Since the Kerberos server has valuable information in its authentication database
that must be kept secret, it is extremely important that it be installed on a carefully
protected and physically secure machine. Although there is no technical problem in
installing the Kerberos server on the same machine that has another application, it is
always better for security reasons to have a dedicated machine for the Kerberos server.
The number of users having access permission to this machine should be extremely
limited.

602 Chap. II • Security
2. Client. The second component of a Kerberos system is comprised of client
processes that usually run on workstations located in effectively public places where
their consoles are available to whatever user happens to be physically in front of
them. Therefore, they are completely untrusted. Users (on whose behalf client
processes run) must first get their identification verified by the Kerberos server before
attempting to access any other server in the system. Once a user's identity has been
verified, each client process running on his or her behalf must obtain a ticket from the
ticket-granting server for communicating with a server that it wants to access.
3. Application server. The third component of a Kerberos system is the applica
tion server, also known simply as the server. A server provides a specific type of
service to a client upon request only after verifying the authenticity of the client. A
server usually runs on a machine that is located in a moderately secure room.
Therefore, Kerberos ensures that a compromise of one server does not compromise
another server.
Kerberos Authentication Protocol
Withthe idea of the basiccomponents and their functions, we will now see the Kerberos
protocol that explains how these components interact with each other to perform user
loginauthenticationandmutualauthentication ofclientandserverprocesses.Theprotocol
isdescribed below and is summarized in Figure 11.11.In the description, A,G, C,and S
stand respectively for the authentication server, the ticket-granting server, the client, and
the application server. Moreover, let K a, Kg, and K, be the secret keys of A, G, and S,
respectively, andK, bethesecretkeyof C(thiskeyisgenerated fromtheuser'spassword
by using a one-way function).
1. When a user logs on to a workstation by typing his or her login name, the login
program sendsarequesttotheauthentication serverforwhatisknownasaticket-granting
ticketinamessagemi.Messagem, contains theuser'sID(loginname)(JD e) andanonce
N} that is used to check the validity of the reply. This message is sent in plaintext
form.
2. On receiving mit the authentication server extracts the password of this user
from the authentication database. It then generates a random number for use as a
session key (K.). After this, it creates a ticket-granting ticket that contains the user's
ID (ID c)' the ticket-granting server's ID (ID g), the starting time for validity of the ticket
(~I)' the ending time for validity of the ticket (T eJ) (typically on the order of 8 hours),
and a copy of the session key (K}). Making the ticket-granting ticket time sensitive
prevents an unauthorized user from capturing it and using it at a later time. Now it
encrypts this ticket by using the ticket-granting server's secret key (Kg) to generate a
ciphertext C. = E«ID c' IDg, TsJ' Tel' K)), Kg). This encryption ensures that no one (not
even the client) can tamper with the ticket-granting ticket and only the Kerberos server
can decode it. Next it uses the client's secret key (K c) (generated from the user's
password) to generate another ciphertext C 2 =E«N b K J, C 1), K c). It then returns C 2
to the login program in a message m2.

Sec.11.4• Authentication 603
3. On receivingm-, thelogin programpromptsthe user for hisorher password.The
entered password is run through aone-way function that generates the client's secret key
(K e) from the password. Immediately after obtaining K c' the password is removed from
the computer's memory to minimize the chance of password disclosure in the event of a
client crash. The login program then attempts to decrypt C 2 by using K c• If the user
suppliedthecorrect password, C issuccessfullydecryptedand the login programobtains
2
thenonce, the sessionkey,and theencryptedticket from the message. Itchecks the nonce
for validity of the reply and stores the session key andthe encryptedticket for subsequent
use when communicating with the ticket-granting server. When this has been done, the
client's secret key can also be erased from memory, since the ticket now serves to
authenticate the user. A login session is then started for the user on the user's
workstation.
Notice that user authentication isdone without requiring the passwordtotravel over
the network. Moreover, if an intruder intercepts the reply message, it will be unable to
decrypt it and thus be unable to obtain the session key and the ticket inside it.
4. Now when a client process running on the client workstation on behalf of the
authenticated user wants to access the application server, it requests the ticket-granting
server for a service-granting ticket that can be used to communicate with the application
server. For this, the client creates an authenticator that contains the client's ID (IDe) and
a timestamp (T}). It encrypts this authenticator by using the session key (K 1) to obtain a
=
ciphertext C 3 E«ID c' T 1), K 1). Unlike the ticket-granting ticket, which isreusable, this
authenticator is intended for one-time use and has a very short life span (typically on the
order of a few minutes). The client next sends the encrypted authenticator (C 3), the
encrypted ticket-granting ticket «(',), the ID of the application server (IDs), and a nonce
(N 2) to the ticket-granting server in a message m«.
5. On receiving m«, the ticket-granting server decrypts C. by using its secret key
(Kg) and makes sure that it has not expired by comparing Tel with the current time. It
extracts the session key (KI) from it and uses it to decrypt C
3
to obtain IDeand T}. The
obtainedID iscomparedwith the valueofIDeintheticket-grantingticket toauthenticate
c
the source of the request. On the other hand, T is used to ascertain the freshness of the
1
request. If all verifications pass successfully, the ticket-granting server gets assured that
the sender of the ticket is indeed the ticket's real owner.
Notice here that it is the authenticator and not the ticket-granting ticket that proves
the client's identity. Therefore, the use of a ticket-granting ticket is merely a way to
distribute keys securely. Moreover, since the authenticatorcan be used only once and has
a very short life span, it is nearly impossible for an intruder to steal both the ticket
granting ticket and an authenticator for later use. Each time a client applies to the ticket
granting serverforanew service-grantingticket, itsends itsreusableticket-grantingticket
plus a fresh authenticator. Also notice that the reusability of the ticket-granting ticket
allows support of a single sign-on facility in which a user need not enter his or her
password every time it needs to access a new server.
After successful authentication of the client, the ticket-granting server generates a
new randomsession key(K 2) andthencreates areusableservice-grantingticket foraccess
to the requested server. This ticket contains the client's ID (IDe)' the application server's

604 Chap.II • Security
ID (IDs)'the starting time for validity of the ticket (1's2), the ending time for validity of
the ticket (4.2), and a copy of the new session key (K 2). It then encrypts the service
granting ticket withthe secret key of the application server (K s) toobtain aciphertext C 4
=
E«JDe,IDs,Ts2''T e2, K 2), Ks)'This encryption ensures thatnoone (noteven theclient)
cantamper withtheservice-grantingticketandonlytheapplication serverortheKerberos
server can decode it. Next ituses the old session key (KI)to generate another ciphertext
Cs=E«N 2,K2, C 4), Kt). It then returns Cs to the client in a message ms.
6. On receiving m4' the client decrypts Cs by using the old session key (K 1) and
obtains thenonce,thenewsessionkey,andtheencrypted service-grantingticket. Itchecks
the nonce for validity ofthe reply andstores the new sessionkeyandtheencryptedticket
for subsequent use when communicating with the application server. The client is now
ready to issue request messages to the application server. However, if mutual
authenticationisdesired, before proceeding withitstransaction or request for service, the
client creates an authenticatorthat contains theclient's ID(IDe) and a timestamp(T 2). It
=
encrypts this authenticator by using the session key (K 2) to obtain a ciphertext C 6
E«JD c' T 2), K 2). The client next sends to the application server, in a message ms, the
encryptedauthenticator(C 6),theencryptedservice-grantingticket(C 4),andarequest that
theserver reply withthevalueofthetimestamp from theauthenticator, incrementedby 1,
and encrypted in the session key.
7. On receiving ms,the application server decrypts C by using its secret key (K...)
4
and extracts the copy of the session key (K 2). It then uses it to decrypt C 6 to obtain
IDe and T 2. Next, it increments T 2 by 1and encrypts the obtained value (T 3) with the
=
session key K 2 to obtain a ciphertext C 7 E(T 3, K 2). It returns C 7 to the client in a
message m6'
8. On receiving m6' theclient decrypts C 7 byusing the session key K 2 toobtain T 3.
=
IfT T +1,theclientgetsassured thattheserverisgenuine.Thismutualauthentication
3 2
procedureprevents anypossibility ofanintruder impersonating aserverinattempt togain
access information from a client.
At the conclusion of this process, the client and server are assured of the
establishmentofasecure communicationchannel inbetween them. They also now share
a session key (K 2), which they can use (if required) to encrypt future messages.
Interrealm Authentication in Kerberos
In a system that crosses organizational boundaries, it is not appropriate for all users and
servers to beregistered with asingle Kerberos server.Therefore, insuchanenvironment,
multiple Kerberos servers exist, each responsible for a subset ofthe users and servers in
the system. A subset of users and servers along withthe Kerberos server with which they
are registered is called a realm in Kerberos. In a typical implementation, networks of
clients andservers belonging todifferentorganizationsusuallyconstitute different realms.
In such an environment, interrealm authentication facility is needed to allow a client to
securely interactwith a server belonging to adifferent realm. Forthis, a Kerberos server

Sec.11.4 • Authentication 60S
of one realm is registered (shares a secret key) with the Kerberos servers of all other
realms. The interrealm authentication protocol is shown in Figure 11.12and is described
below:
1. Aclientthat wants toaccess aserver inadifferent realm first makesarequest for
a service-granting ticket from its own ticket-granting server to access the ticket
granting server of the other realm.
2. With the obtained service-granting ticket, the client then makes a request for
another service-granting ticket from the ticket-granting server of the other realm
to access the application server of that realm.
RealmA
----------------------------------------------------------I
I
I
I
I
I
I
I
Realm B
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I----------------------------------------------------------
ml =requestforticket-grantingticket
m2=replyform1
m3=requestforservice-grantingticket toaccess theremoteticket-grantingserver
=
m4 replyform3
ms=
requestforservice-grantingtickettoaccess theremoteapplicationserver
me=replyforms
m-=access request
Fig.1l.12 lnterrealm authentication protocolof Kerberos.

Chap. 11 • Security
3. With the newly obtained service-granting ticket, the client now sends its request
to the desired application server of the other realm. The remote server then
chooses whether to honor the client's request.
Intheabove approachforinterrealmauthentication,ifitisdesired thateach Kerberos
realm beable tooperate with allotherKerberos realms, inasystem having nrealms, there
must be n(n-l)/2 secure key exchanges. Therefore, this approach does not scale well. It
creates a lot ofoverhead on the network and the Kerberos servers themselves. Thus, it is
suggested that Kerberos implementations should use a few relatively large realms rather
than too many small realms. The latest version of Kerberos (Version 5) also supports
muJtihop interrealm authentication, allowing keys to be shared hierarchically [Neuman
and Theodore 1994].
Within a single realm, the Kerberos server is a critical component for smooth
functioning. Therefore, to ensure reliability of the Kerberos server, Kerberos supports
replication of the Kerberos server. When replicated, a simple master-slave technique is
used to keep the authentication databases of all the replicas of a Kerberos server
consistent. That is, all changes are only applied to the master copy by a single Kerberos
database management server (KDBMS) that runs only on the master Kerberos server's
machine. Administrative operations such as adding or deleting users or requests for
password change from users are handled by the KDBMS. Changes made to the master
Kerberos server's authentication database are periodically propagated to the authentica
tion databases ofslave Kerberos servers.
Some Limitations of Kerberos
For all itspopularity, however, Kerberos isnotacompletesecurity solutionbecause ofthe
following limitations [Bellovin and Merritt 1990, Neumann and Theodore 1994]:
1. Kerberos is not effective against password-guessing attacks. If a user chooses a
password that is easy to guess, an intruder guessing that password can impersonate the
user.Anotherwaybywhichanintrudercangettoknow auser'spasswordisbymodifying
thelogin program(aTrojan horse) that resides ontheuser'sworkstation. Inthiscase, also,
the intruder may obtain sufficient information to impersonate the user. To address these
limitations, it is suggested that Kerberos should be combined with a one-time password
technique. Commercial products that combine a one-time password technique with
Kerberos are available.
2. The Kerberos protocol dependsupon loose synchronizationofclocks ofthenodes
presentin asystem. It may not be very difficult tomeet this requirement, but the problem
is that the synchronization protocol used for this purpose must itself be secure against
security attacks. A Kerberos protocol that does not rely on synchronized clocks has been
presented in [Kehne et al. 1992].
3. Another problem with Kerberos is that it is difficult to decide the appropriate
lifetime ofthe tickets, which is generally limited to a few hours. For bettersecurity, it is
desirable that this lifetime should be short so that users who have been unregistered or

Sec. 11.5 • Access Control 607
downgraded will not be able to continue touse the resources for a long time. However,
for user transparency, the lifetime should beas long as the longestpossiblelogin session,
since the use of an expired ticket will result in the rejection of service requests. Once
rejected, the user must reauthenticatethe login session and then requestnew servertickets
for all the services in use. Interrupting an application at an arbitrary point for
reauthentication might not be acceptable in commercial environments.
4. Finally,client-serverapplicationsmust bemodifiedtotakeadvantageofKerberos
authentication.This processiscalledKerberization.Kerberizinganapplicationisthemost
difficult part of installing Kerberos. Many large organizations may find it almost
impossible to Kerberize their applications and turn to other solutions. Fortunately, the
availabilityof Kerberizedapplicationshasimprovedwith time andisexpected toimprove
further. More and more vendors are now producing new Kerberized versions of their
popular products.
11.5 ACCESS CONTROL
Once auseroraprocesshas been authenticated, the next step insecurity istodevise ways
to prohibit the user or the process from accessing those resources/information that he or
sheor it isnot authorizedtoaccess. This issue iscalled authorizationand isdealt with by
using access control mechanisms. Access control mechanisms (also known as protection
mechanisms) used in distributed systems are basically the same as those used in
centralizedsystems. The main differenceisthatsince allresources arecentrallylocated in
acentralized system, access control can beperformed byacentral authority. However, in
adistributedclient-serverenvironment,each server isresponsibleforcontrollingaccess to
its own resources.
When talking about access control in computer systems, it is customary to use the
following terms:
1. Objects.An objectisanentity to which access must becontrolled. Anobject may
be an abstract entity, such as a process, a file, a database, a semaphore, a tree data
structure, or aphysicalentity, such as aCPl], amemory segment, aprinter, acard reader,
a tape drive, a site of a network.
Eachobjecthasauniquenamethatdifferentiatesitfromallotherobjectsinthesystem.
An object is referenced by its unique name. In addition, associated with each object is a
"type"thatdeterminesthesetofoperationsthatmaybeperformedonit.Forexample,theset
of operations possible on objects belonging to the type "data file" may be Open, Close,
Create,Delete,Read,and Write,whereas forobjects belongingtothe type"programfile,"
thesetofpossibleoperationsmaybeRead,Write,andExecute.Similarly,forobjectsoftype
"semaphore,"the setofpossibleoperations may be Upand Down,and forobjects of type
"tape drive,"thesetofpossibleoperationsmaybeRead,Write,andRewind.
2. Subjects.Asubjectis an active entity whose access to objects must becontrolled.
That is, entities wishing to access and perform operations on objects and to which access
authorizations are granted are called subjects. Examples of subjects are processes and

608 Chap. 11 • Security
users. Note that subjects arealso objectssincethey toomust beprotected.Therefore,each
subject also has a unique name.
3. Protection rules.Protection rules define the possible ways in which subjects and
objects are allowed to interact. That is, protection rules govern the subjects' access to
objects. Therefore, associated with each (subject, object) pair is an access right that
definesthe subsetofthe setofpossibleoperationsfor theobjecttype that the subjectmay
perform on the object. The complete set of access rights of a system defines which
subjectscan performwhatoperationson which objects. Atany particularinstanceoftime,
this set defines the protection state ofthe system at that time.
The exact mannerin which the protection rules are imposed to control the subjects'
access to objectsdepends on the access control model used by the system. The following
access control models have been proposed in the literature:
1. The access matrix model [Lampson 1971,Graham and Denning 1972,Harrisonet
al. 1976]
2. The information flow controlmodel [Denning 1976, Bell and LaPadula 1973]
3. The security kernel model [Ames et al. 1983, Rushby and Randell 1983]
Of these, the access matrix model is the most popular one and is widely used in
existing centralized and distributed systems. The other two models are mainly of
theoretical interest. A description ofthe access control mechanisms based on the access
matrix model is presented below.
11.5.1 Prot.ctlon Domains
Wesaw that the principle ofleast privilege requires that at any time a subject should be
able to accessonly those objects that itcurrently requires tocomplete its task. Therefore,
from time to time, subjects may need to change the set of access rights they have to
objects, depending on the particulartask that they have todo at any time. The conceptof
a domain is commonly used to provide this type of flexibility of access control in a
security system.
A domain is an abstract definition of a set of access rights. It is defined as a set
of (object, rights) pairs. Each pair specifies an object and one or more operations that
can be performed on the object. Each one of the allowed operations is called a
right.
Asecuritysystem based on theconceptofdomaindefines asetofdomainswitheach
domain having its own set of(object, rights) pairs. At any instance oftime, each process
executesinone oftheprotectiondomainsofthesystem. Therefore,ataparticularinstance
oftime, the access rights ofaprocessare equal to the access rights defined in thedomain
in which it is at that time. A process can also switch from one domain to anotherduring
execution.
Figure 11.13shows anexampleofasystem having three protectiondomainsD),D 2,
D 3•The following points may be observed from this example:

Sec. 11.5 • Access Control 609
1. Domainsneed not bedisjoint. That is,the same access rights may simultaneously
exist in two or more domains. For instance, access right (Semaphore-l, {Up, Down}) is
in both domains D} and D 2.This implies that a subject ineitherof the two domains can
perform Up and Down operations on Semaphore-I,
2. The same object can exist in multiple domains with different rights in each
domain. For instance, rights for f"'i[e-J and File-2 are different in D} and D 2, and rights
for Tapelsrive-I are different in D 2 and D 3• Considering File-I, a subject in domain D}
can Read, Write, and Execute it, but a subject in domain D can only perform Read
2
and Write operations on it. Therefore, in order to execute File-I, a subject must be in
domain D 1•
3. If an object exists only in a single domain, it can be accessed only by the
subjects in that domain. For instance, File-S can only be accessed by the subjects in
domain D 3•
(File-1,{Read, Write,Execute})
(File-2,{Read})
(File-3 {Read, Write,Execute})
J
(TapeDrive-1,{Read, Write,Rewind})
(File-1,{Read, Write})
(File-2,{Read, Write,Execute})
(TapeDrive-1,{Read})
."ig. 11.13 Asystem having three protectiondomains.
Since a domain is an abstract concept, its realization and the rules for
domain switching are highly system dependent. For instance, some ways of
realizing a domain may be the following:
1. Each useras a domain. In this case, processes are assigned to domains
according to the identity ofthe user on whose behalfthey are executed.
A domain switching occurs when a user logs out and another user logs
in.
2. Each processasa domain. Inthiscase, the access rights ofaprocess are
limited to the access rights of its own domain. A domain switching
occurs when one process sends a message to another process and then
waits for a response.
3. Each procedureas adomain. Inthis case, each procedurehasitsown set
of access rights and a domain switching occurs when a procedure calls
another procedure during the course ofits execution.

610 Chap. 11 • Security
In the protection scheme of UNIX, both the concepts of user-oriented domains
and procedure-oriented domains are employed. Let us first see the use of the concept
of user-oriented domains. In UNIX, a user-id (uid) and a group-id (gid) are associated
with each user. A (uid, gid) pair identifies the list of objects and the types of
operations that can be performed on these objects. The domain of a process depends
on the user on whose behalf it is executed. Therefore, the domain of a process is
defined by its (uid, gid) pair. That is, two processes with the same (uid, gid) pair will
have exactly the same access rights and two processes with different tuid, gid) pairs
will have different access rights. In the latter case, several of the access rights may be
the same for the two processes. A process switches domains by acquiring a new uid
or gid by executing the setuid or setgid commands. Furthermore, it is also worth
mentioning here that the superuser in a UNIX system is not a person but a name for
a special domain within which the most privileged processes, needing access to all of
the objects in the system, may run.
Now let us see the use of the concept ofprocedure-oriented domains in UNIX. In
UNIX, all the procedures are grouped into two classes-user procedures and kernel
procedures. These.two classes form two domains called the user mode and the kernel
mode. A process running in the kernel mode has a different set of access rights as
compared to a process running in the user mode. For example, in the kernel mode, a
process can access all pages in the physical memory, the entire disk area, and all other
protected resources. In this two-domain architecture, when a user process does a system
call,itswitches fromtheusermodetothekernelmode.Therefore, ataparticular instance
oftime,thedomainofaprocessandhenceitsaccessrightsdependonwhethertheprocess
is executing a user procedure or a kernel procedure at that time.
Multics [Schroeder et al. 1977] used a more generalized form of the procedure
oriented domains. Unlike UNIX, which uses twodomains (user mode and kernel mode),
the Multics architecture hadthe flexibility to support up to64 domains. Each domain of
Multics was called a ring. As shown in Figure 11.14,the rings were concentric and the
proceduresintheinnermostring,theoperating systemkernel,weremostpowerful,having
maximum access rights. Moving outward from the innermost ring, the rings became
successively lesspowerful havingfeweraccessrights.Infact,theaccessprivileges ofring
i were a subset of those for ringj for all i >j when the ring numbers started from the
innermostringandincreasedforeachringaswemovedoutward.Thatis,j<iimpliedthat
the procedures in ringj had more access rights than the procedures in ring i.A process
couldoperateinmultipledomains(rings)duringitslifetime.Adomainswitchingoccurred
when a procedure in one domain (ring) made a call to a procedure in another domain
(ring). Obviously, domain switching was done in a controlled manner; otherwise, a
process could start executing in the innermost ring and no protection would be
provided.
11.5.2 Acca.s Matrix
In the domain-based protection approach, the system must keep track of whichrights on
whichobjects belongtoaparticular domain. Inthe access matrix model,thisinformation
is represented as a matrix, called an access matrix, that has the following form:

Sec.11.5 • AccessControl 611
Ring0(mostpowerful)
Ring1
Ringn-1
Ringn(leastpowerful)
Fig. 11.14 The ring architectureof Multics protectiondomains.
1. The rows represent domains.
2. The columns represent objects.
3. Each entry in the access matrix consists of a set of access rights.
4. The (i, j)thentry of the access matrix defines the setofoperations that aprocess,
executing in domain Di, can perform on object OJ<
Atany instance of time, theprotection state ofthe system isdefined bythecontents
of the access matrix. The access matrix of Figure 11.15shows the protection state of the
system in Figure 11.13.
When an access matrix is used to represent the protection state of a system, the
following issues must be resolved:
1. How to decide the contents of the access matrix entries.
2. How to validate access to objects by subjects.
3. How to allow subjects to switch domains in a controlled manner.
4. How to allow changes to the protection state of the system in a controlled
manner.
The normally used methods to handle these issues are described below.

612 Chap. 11 • Security
~
F1 F2 F3 81 T1
Domain (File-1) (File-2) (File-3) (Semaphore-1)(Tapedrive-1)
01 Read Read Up
Write Down
Execute
Read Read
D2 Up Read
Write Write Down
Execute
Read Read
D3 Write Write
Execute Rewind
Fig. 11.15 Theaccess matrixfortheprotection stateof thesysteminFigure 11.13.
Deciding the Contents of the Access Matrix Entries
Policy decisions concerning which rights should be included in the (i, j)th entry are
system dependent. However, in general, the contents of the access matrix entries
corresponding to user-defined objects are decided by the users, whereas the contents
of the entries corresponding to system-defined objects are decided by the system. For
example, when a user creates a new object OJ'columnj is added to the access matrix
with suitable entries as decided by the user's specification of access control for the
object.
Validating Accessto Objects by Subjects
Given the access matrix, how can access to objects by subjects be allowed only in the
manner permitted by the protection state of the matrix? For this, an object monitor is
associated with each type of object, and every attempted access by a subject to an object
is validated in the following manner:
1. A subject Sindomain D initiates raccess to object 0, where rbelongs tothe set
ofoperations that may be performed on O.
2. Theprotectionsystem forms thetriple(D, r,0)andpasses ittotheobject monitor
ofO.
3. The object monitor of 0 looks for the operation r in the (D, O)th entry of the
access matrix. Ifpresent, theaccess ispermitted;otherwise, aprotection violation
occurs.

Sec.11.5 • AccessControl 613
Allowing Controlled Domain Switching
The principle of least privilege requires that a process should be allowed to switch from
one domain to another during its lifetime so that at any instance of time the process is
given only as many rights as are necessary for performing its task at that time. However,
domain switching byprocessesmustbedone inacontrolledmanner; otherwise, aprocess
may switch to a powerful domain and violate the protection policies of the system.
Domain switchingcanbecontrolledbytreating domains asobjects onwhichtheonly
possible operation is switch. Therefore, for allowing domain switching in a controlled
manner, the domains are also included among the objects of the access matrix.
Figure 11.16 shows the access matrix of Figure 11.15 with the three domains as
objects themselves. In the modified access matrix, domain switching from domain D; to
domain D,is permitted ifand only ifthe right switch ispresent in the (Di~ Dj)th entry of
the access matrix. Thus in Figure 11.16,a process executing in domain D) can switch to
domain D 2 ortodomain D 3, aprocess executingindomain D 2 can switchonly todomain
D 3, and a process executing in domain D 3 cannot switch to any other domain.
!
F1 F2 F3 51 T1 01 02 03
Domain
Read Read Up Switch Switch
01 Write Down
Execute
Read Read Up Read Switch
02 Write Write Down
Execute
Read Read
03 Write Write
Execute Rewind
14'ig.11.16 The access matrixof Figure 11.15withthedomains includedasobjects.
AllowingControlled Change to the Protection State
Inaflexible design, the protection system should also allow thecontent ofadomain tobe
changed. However, this facility is not essential because ifthe content of adomain cannot
bechanged, the same effect can be provided by creatinga new domain withthe changed
contents and switching tothat newdomain when wewant tochange thedomain contents.
Although notessential, thefacility ofallowing controlledchange totheprotection state is
normally provided in a protection system for greater flexibility.

614 Chap.11 • Security
Inanaccessmatrixmodel, this facility can beprovided bytreating theaccess matrix
itselfas an object to beprotected. In fact, since each entry in the access matrix may be
individually modified, each entry must be considered as an object to be protected. For
allowing controlled change, the possible rights defined for this new object are copy,
owner, and control. To simplify the description, we will categorize the changes to be
allowed to the content of the access matrix entries into two types-allowing changes to
the column entries and allowing changes to the row entries.
Allowing Changes to the Column Entries. The copy and ownerrights allow
a process tochange theentries in acolumn. The ability tocopy an access right from one
domain (row) to another is denoted by appending an asterisk (*) to the access right. For
example, Figure 11.17shows the access rights of only the first three objects (F l, F 2, and
F 3) of the access matrix of Figure 11.15with some copy rights. A process executing in
domain D, cancopy theReadand Writeoperations on F, toanyother domain (anyother
entry in column F whereas a process executing in domain D can copy the Read
J), 2
operation on F) andReadandExecute operations on F toanyother domain. Noneof the
2
operations on F can be copied to any other domain.
3
.:S:
F1 ~ Fa
Domain
Read· Read
01 Write·
Execute
Read· Read*
02 Write Write
Execute·
Read
03 Write
Execute
Fig.11.17 Anaccess matrix withcopy rights.
The copy right may have the following three variants:
1. Transfer.In thiscase, when a right iscopied from the (i.j)thentry to the (k,j)th
entry of the access matrix, it is removed from the (i,j)th entry.
2. Copy withpropagationnotallowed. Inthiscase, whentherightR*iscopied from
the (itj)thentry to the(k,j)thentry,only therightR(notR*) iscreated in the(k,
j)thentry.The implication ofthisisthataprocess executing indomain D, cannot
further copy the right R.

Sec. ]1.5 • AccessControl 61S
3. Copy withpropagationallowed. Inthiscase,whentherightR* iscopiedfromthe
(i,})thentrytothe(k,})thentry,therightR*iscreatedinthe(k,j)thentry,sothat
a processexecutingindomainD, can furthercopy the rightR* or R.
On theother hand,theowner rightis usedtoallowthe adding/deletingofrights to
columnentriesinacontrolledmanner. If theownerrightis includedinthe(i, j)th entry
oftheaccessmatrix,aprocessexecutingindomainD;canaddanddeleteanyrightinany
entryincolumn). Figure11.18showstheaccessmatrixofFigure11.17withownerrights
includedforthethreeobjects.Inthefigure,domainsD},D 2,andD
3
havetheownerrights
forobjectsF 1, F 2, andF 3,respectively. Therefore,aprocessexecutingindomainD}can
add and delete any valid right of F) in any entry in column F). Similarly,a process
executingin domainD can addanddeleteany validrightof F inanyentryincolumn
2 2
F 2, andaprocessexecutingindomainD] canaddanddeleteanyvalidrightofF 3 inany
entry incolumnF3.
Is
F1 ~ Fa
Domain
Read*
Read
01 Write*
Execute
Owner
Read* Read*
02 Write Write
Execute*
Owner
Read
Write
03
Execute
Fig. 11.18 An access matrix with owner Owner
rights.
Allowing Changes to the Row Entries. The control right, which is only
applicableto domain objects,is used to allow a process to change theentriesin a row.
If the control right is present in the (D;, Dj)th entry of the access matrix, a process
executing in domain D; can remove any access right from row D j. For example,
Figure 11.19shows the access matrix of Figure 11.16with control rights included. In
this figure, since the entries (Db D 2) and (D}, D 3) have control rights, a process
executing in domain D] can delete any right from rows D 2 and D 3. Similarly, since
the entry (D 2, D 3) has control right, a process executing in domain D 2 can delete any
right from row D 3•

616 Chap. J] • Security
~
r,
F2 F3 51 t, 01 02 03
Domain
Read Read Up Switch Switch
01 Write Down
Control Control
Execute
Read Read Up Read Switch
02 Write Write Down Control
Execute
Read Read
D3 Write Write
Execute Rewind
Fig. 11.19 An access matrix withcontrol rights.
Inpractice,anaccess matrixislargeandsparse. Mostdomains havenoaccess atalltomost
objects, that is, most of the entries are empty. Therefore, a direct implementation ofan
access matrix as a two-dimensional matrix would be very inefficient and expensive
(wastage ofdisk space). Moreover, in distributed systems, subjects and objects may be
located on different sites, which further complicates the implementation issue. The two
most commonly used methods that have gained popularity in contemporary distributed
systems forimplementinganaccess matrixareaccesscontrol lists(ACLs)andcapabilities.
For instance, Andrew [Satyanarayanan 1989], Apollo [l.evine 1986], and Butler
[Dannenberg and Hibbard 1985] use ACLs and Accent [Rashid and Robertson 1981],
Amoeba[MullenderandTanenbaum 1986],andMach[Sansometal.1986]usecapabilities.
These twomethods aredescribedbelow.
AccessControlLists
Inthismethod, theaccess matrixisdecomposedbycolumns, andeachcolumnofthematrix
is implemented as an access list for the object corresponding to that column. The empty
entries ofthe matrix are not stored in the access list. Therefore, for each object, a list of
orderedpairs(domain, rights)ismaintained,whichdefinesalldomains withanonempty set
ofaccess rights for that object. Furtherdetails of the working and propertiesofa security
system basedonACLs arepresentedbelow.
AccessValidation. Wheneverasubject indomain Dexecutes anoperationronan
object0,theaccess listforobject0 isfirstsearched forthelistelement whosedomainfield
isD.Then therightsfieldofthiselementissearched forr.Iffound,theoperationisallowed
tocontinue;otherwise,aprotectionviolation occurs.
In this method, the access list is checked on every access. This is a very desirable
feature from a security point of view.However, consulting theaccess Jiston every access

Sec. 11.5 • Access Control 617
couldcausesubstantialoverhead,especiallywhen theaccesslistislong.Thisdrawbackcan
beovercomebymaintainingacachefortheaccesslistentriesofonly the active domains.
Granting Rights. Access right r for object 0 is granted to domain D in the
following manner:
1. The access list for object 0 is first searched for the list element whose domain
field is D.
2. If found, right ris added to the rights field of this list element. Otherwise, a new
o.
listelementisaddedtotheaccess listfor theobject The domainfield and rights
field of this list element are set to D and r,respectively.
PassingRights. Access right rforobject0 ispassed(propagated)from adomain
D to another domain D in the following manner:
1 2
1. Access list for object 0 is first checked to ensure that DI possesses eitherowner
right for object 0 or copy right for access right r.
2. IfD possesses any of the above two rights, access right rfor object0 is granted
1
to domain D in the manner described above. Otherwise, a protection violation
2
occurs.
Rights Revocation. Accessright rfor object0 isrevokedfrom domainDsimply
o.
by deleting r from the rights set ofdomain D in the access list for
For file protection, several systems use user-oriented domains like that of UNIX in
which a(uid, gid) pair forms adomain. Revocationofauser's accessright becomesmore
complicatedin these systemsbecauseaccessrevocation requiresthe deletionofthe user's
uid(if present) from the access list ofthe object in question and also the cancellation of
the user's membership from all the groups that belongto adomain that has access to that
object. In a large distributed system, the process of discovering all groups thatthe user
should be removed from and performing the actual removal operation may take a
significant amount of time that may be unacceptable in emergencies. The concept of
negative rights is used to overcome this problem [Satyanarayanan 1989, 1990]. This
conceptis basedon the idea that to revoke a user's access right to an object, the user can
be given negative rights on that object. Negative rights indicate denial of the specified
rights, with denialoverridingpossessionincaseofconflict.With this extension,theACLs
may contain negative rights, so that it is possible to express facts, such as every user of
a domain except user-iand user-j may exercise. right ron object O.The union of all the
negative rights specified for a user subtracted from his or her positive rights gives his or
her actual total rights. Negative rights thus act as a mechanism for rapid and selective
revocation and are particularly valuable in a large distributed system.
The main advantage of the method of ACLs is that for a given object the set of
domains from which itcan be accessedcan be determinedefficiently. However, the main
drawback of the method is that for a given domain the set of access rights cannot be
determined efficiently.

618 Chap. 11 • Security
Capabilities
Rather than decomposing the access matrix bycolumns, inthis method the access matrix
isdecomposedbyrows,andeachrowisassociated withitsdomain. Obviously,theempty
entries arediscarded. Therefore, foreach domain, alistofordered pairs (object, rights) is
maintained, which defines allobjects forwhich thedomain possesses some access rights.
Each (object, rights) pair is called a capability and the list associated with a domain is
called a capability list.
A capability is used for the following two purposes:
• To uniquely identify an object
• To allow its holder to access the object it identifies in one or more permission
modes
Therefore, asshowninFigure 11.20,acapability iscomposed oftwobasicparts-an
object identifier part and a rights information part, The object identifier part usually
contains apointer totheobject andactsasaglobally uniquesystem-oriented nameforthe
object. Ontheotherhand,therights information partisusuallyasetofbitsthatdetermine
which operations are allowed on the object with this capability. Further details of the
working andproperties ofasecurity systembasedoncapabilities arepresented below.For
ease of presentation, in the following description we will assume that each user/process
forms a domain of the system.
I
Objectidentifier Rights information Fig. 11.20 The two basicpartsofa
~ ....L-.. ......J capability.
AccessValidation. Acapabilityisconsidered asanunforgeable ticketthatallows
its holder to access the object (identified by its object identifier part) in one or more
permissionmodes (specified by its rights information part). A process that possesses a
capability can access the object identified by it in the modes permitted by it. There are
usually several capabilities for the same object. Each one confers different access rights
to its holders. The same capability held by different holders provides the same set of
access rights to all of them.
Since simple possession of a capability means that access ispermitted in the modes
associated withthecapability,toexecute anoperation r onanobject 0, aprocess executes
the operation r, specifying the capability for object 0 as a parameter. When the monitor
for object 0 receives the access request (with the capability), it need only verify that the
rights information part of the capability has permission for operation r.
Notice that in acapability-based security system, there is no need to search a list to
verify that access is allowed. Rather, the security system need only verify that the
capability supplied byaprocessisvalidforthedesired operation ontheobject.Therefore,
once a process establishes the possession of a capability for an object, it can access the
object in one of the modes allowed by the capability without any further check by the

Sec.11.5 • AccessControl 619
security system. For these reasons, capability-based security systems are more efficient
than security systems based on ACLs.Alsonotice that in the capability-based approach,
there is no checking ofuser identity. If this is required for access validation, some user
authentication mechanism must be used.
GrantingandPassing ofRights. Inthe capability-basedsecurity scheme,each
user maintains a list ofcapabilities that identifies all objects that the usercan access and
the associated access permissions. But how does a user get a capability in the first
place?
In acapability-based system, there are usually one or moreobjectmanagersfor each
type ofobject. A request to create an objector to perform some operationon an objectis
sent to one ofthe object managers of that object type. When a new object is created, the
object manager that creates the object generates (as a part ofthe objectcreation process)
acapability with all rights ofaccessfor the object.Thegeneratedcapabilityis returnedto
the owner for use. Now the owner may give the capability to other users with whom the
object is to be shared. However, the owner may want to restrict the access modes in a
different manner for different users who share the object. Therefore, before the owner
gives the capability to other users, it may be necessary to restrict the capability by
removing some ofthe rights. The usual way to do t.hisis to have a function in the object
managerfor restrictingcapabilities. Whencalled,this function generates anew capability
for the object with only the desired access permissions and returns the newly generated
capability to the caller. This capability is then given to the usens) for whom it was
generated.
Protecting Capabilities against Unauthorized Access. To ensure that in a
capability-basedsecuritysystemthe objectsare protectedagainstunauthorizedaccess,the
following basic requirements must be met:
1. A capability must uniquely identify an object in the entire system. Even afterthe
objectassociatedwith agivencapability isdeleted,itisimportantthat thecapabilityisnot
reused because some users may retain obsolete capabilities. Use ofan obsoletecapability
should produce an error instead of allowing access to a different object.
2. Capabilities must be protected from user tampering. For this, it is necessary that
capabilities be treated as unforgeable protected objects that are maintained by the
operating system and only indirectly accessed by the users.
3. Guessing ofa valid capability should be impossibleor at least very difficult. This
is because in the capability-based scheme the degree of protection is probabilistic and
proportional to the difficulty of guessing a valid capability.
The following methods are normally used to meet these requirements:
1. Tagged architecture. In this method, each object has a tag to denote its type as
either a capability or an ordinary accessible data such as integer, pointer, character, or
instruction.Tags are normally implementedin hardwareby associatingatag with units of
memory, usually words. In this case, each memory word has a tag field that tells whether

620 Chap.11 • Security
the word contains a capability or not. The tag field is not directly accessible by an
application program, anditcan bemodified only byprograms running inthe kernel mode
(i.e., the operating system). Although only one bit is necessary for the tag field to
distinguish between capabilities and other objects, tagged architecture machines typically
use n bits for the tag field to allow the software to distinguish among 2n different types
ofobjects (memory contents).
2. Partitioned implementation. Another method to preserve the integrity of
capabilities is to store them separately from data in special segments that can only be
accessed by the operating system. One way to implement this is to partition the address
space of a user process into two parts-one accessible to the process and the other
accessibleonlytotheoperating system.Theformercontainstheprocess'snormaldata and
instructions, whereas the latter contains its capabilities.
3. Encryption of sparse capabilities. The tagged and partitioned methods of
protectingcapabilities fromunauthorizedaccessareoriented towardcentralizedcomputer
systems. These methods are not suitable for use in a distributed system because in a
distributed system the security mechanism used should allow capabilities to be safely
transferred from one node to another. The third method, which does not require
capabilities to be distinguished from other objects either by separation or by tagging, is
particularly suited to distributed systems. In addition topreventing capabilities from user
tampering, this method also makes capabilities unique and difficult to guess.
In this method, a large name space that is sufficiently sparse is used for the
capabilities. Uniqueness is achieved by using the methods described in Section 10.4 for
thecreation ofunique system-orientedidentifiers thatformtheobject identifierpartofthe
capabilities, On the other hand, to make the capabilities difficult to guess or forge, the
rights information part of each capability is combined with an extra field containing a
random number,therebyrending thetaskofamalicious userwishingtogenerate anyvalid
capabilitysolengthy astobeimpractical. Furthermore, therightsinformation partandthe
random-numberpart are encrypted by theobject manager before acapability is issued to
auser.The secret key isavailable only with theobject manager.When a process presents
the capability along with a request for object accessing, the object manager uses the key
to decrypt the encrypted part of the capability before using it. In this way, the rights
informationpart ofacapabilitythatconfers restricted permissions cannot beforged byits
possessors to convert it to one with more permissions.
Rights AmplifICation. The concept of rights amplification was introduced in
Hydra [Cohen and Jefferson 1975].Wesaw that in a capability-based system objects are
typed and recognize a set of predefined operations. The set of predefined operations for
an object type is known as auxiliary rights in Hydra. In addition to the auxiliary rights,
eachobject typehasasetofkernelrights,suchasget,put,andaddtomanipulatethedata
part of an object and load, store, append, delete, copy, and create to manipulate the
capability list part of an object. The kernel rights are implemented within the kernel and
are transparent to the user processes.
Arequest toperform an operation on an object is sent to the object manager of that
object type. The request contains the capability for the object as a parameter. This

Sec.11.5• AccessControl 621
capabilitymay includeanauxiliary right toinvoke someoperationontheobjectbut would
notincludeanyofthekernel rights fortheobject. Aproblemarises herebecausetheobject
manager is itselfan ordinary program. It is essential that the object manager be able to
invoke kernel operations in order to perform the requested operation successfully. The
rights amplificationtechniqueof Hydra solves this problemby giving arights templateto
objectmanagersthat gives them more rights toan objectthan the capability itselfallows.
Additionalrights given totheobject managers allow them toperformkernel operationson
the objects.
When a process P invokes an operation r on an object 0, the capability C supplied
by P may get amplifiedto C when theobjectmanagerM starts performingthe operation
a
r on the object. This may be necessary in order to allow Mto access the storage segment
representing 0 for performing operation r on it.That is, M is allowed to perform kernel
operationson0 directly, even though thecalling processPcannot.Aftercompletionofthe
operation ron 0, the capability Ca for 0 is restored to its original, unamplified state C.
This is a typical case in which the rights held by a process for access to a protected
segment must change dynamically, depending on the task to be performed.
Notice that in the rights amplification scheme, the object managers are treated as
"trustworthy" procedures and are allowed to perform kernel operations on the objects of
aspecifiedtype, on behalfofany process thatholds anauxiliary right foranobjectofthat
type. Therefore, the rights held by an object manager are independent of and normally
exceedthe rights held by the subjects that access the object. However, an object manager
is not a universally trustworthy procedure because it is not allowed to act on other types
of objects and cannot extend its rights to any other procedure.
Rights Revocation. In a security system based on ACLs, revocation of rights is
easy because for a given object it is possible to easily and efficiently determine which
subjects have what rights for the object. However, in a security system based on
capabilities, revocation of rights is a much more difficult problem because for a given
object it is difficult to determine which subjects have what rights for the object. This is
because the capabilities for an object may be stored in several capability lists that are
distributedthroughoutthesystem, andwemustfirstfindthem before wecanrevoke them.
Some commonlyused methods forimplementingrevocation forcapabilitiesaredescribed
below.
1. Backpointers. One way is to keep track of all the capabilities for an object and
tochange/delete them selectively depending on the desired revocationof access rights. A
simple method to keep track ofall the capabilities for an object is to maintain a list of
pointers with the object, pointing to all capabilities associated with the object. This
methodhas been used in the Multics system. The method isquite general but very costly
to implement.
2. Indirection. Another approach is to use indirect addressing. In this method each
capabilitypoints toanindirectobject(such asatable entry) rather than totheobjectitself.
The indirectobjectinturnpoints totherealobject. Revocationisimplementedbydeleting
the indirectobjectto break the connectionbetweenthe real objectand the capabilitiesfor
it.When an access is attempted with acapability whose indirectobject has been deleted,

622 Chap. 11 • Security
access operation failsbecause therealobject isunknown duetothebrokenconnection. A
drawback of this method is that it does not allow selective revocation.
3. Use of keys. In this method, in addition to the object identifier and rights
information fields, each capability has a field that contains a unique bit pattern. The
contents of this field is called a "key." Acapability's key is defined when the capability
iscreatedanditcannotbemodified orinspected byaprocessowning thatcapability.Each
object hasamasterkey associated withitthatcanbedynamically definedorchanged with
a special set_key operation. Normally, only the owner of an object is given the right to
invoke the set_key operation for changing the master key of the object.
When a new capability for an object iscreated, the key field ofthe capability is set
tothecurrent masterkeyfortheobject.Whenanaccess isattempted withacapability, the
capability'skeyiscompared tothemasterkeyofthecorresponding object. Ifthetwokeys
match, access to the object is allowed; otherwise, a protection violation occurs.
Revocationinvolves replacementofthemaster key withanewvaluebyusing theset_key
operation, invalidating all previous capabilities for this object.
This revocation scheme is used in Amoeba, in which random numbers are used as
keys.Adrawback ofthis scheme isthat itdoes notallow selective revocation, since only
one master key isassociated with each object. However, thisdrawback can be overcome
by associating a list of keys with each object.
Hybrid Approach
As compared to the capability-based scheme, the scheme based on ACLs is more suited
tothe implementationofsecurity systems becauseACLscorrespond directly to the needs
of the users. When users create objects, they can specify which domains can access the
objects as well as the operations allowed. However, we saw that a security system based
onACLs is normally less efficient than capability-based security systems because of the
need to search the access Jiston every access. To overcome the drawbacks of the two
schemes and to combine their advantages, most systems use a hybrid approach for
designing their security system.
In the hybrid approach, both ACLs and capabilities are employed along with the
concept of a session. A session is a logical concept of a period during which a process
accesses an object. When a process first tries to start a session for accessing an object,
it specifies the access modes (types of operations) that it may perform on the object
during the session. The ACL of the object is searched for the types of operations
desired. If access is denied, a protection violation occurs. Otherwise, the system
creates a new capability for the object and attaches it to the process. After this, all
accesses to the object by this process during the session are made using the capability
so that an access control check can be performed efficiently. After the session is over,
the capability is destroyed.
The UNIX filesystememploys thisscheme, withasessionbeing theperiod between
the open and close operations of a file by a process. Each file has an associated ACL.
When a process opens a file, its ACL is checked for the mode specified in the open
command. If access in the specified mode is permitted, a newentry is allocated in a file

Sec.11.6• Digital Signatures 623
table, the access mode is recorded in this entry, and an index to this entry is returned to
the process. All subsequentoperations on the file are made by this process by specifying
the index into the file table. The entry inthe file table points tothe file. When the process
executes the close operation on the file, the session is closed by deleting the file table
entry.Anew session must be started after this ifthe process wants to access the same file
at some later time.
The file table is maintained by the operating system, so that it cannot be corrupted
by the user.Security isensured because access isvalidatedatthe time asession isstarted
and a process can access only those files for which a session has been started but not yet
closed.
11.6 DIGITAL SIGNATURES
Message integrity, which guarantees that the contents of a message were not changed
when itwas intransfer, isalso an importantsecurity requirement inadistributed system.
The concept of digital signature, which is based upon asymmetric cryptosystems, is the
most commonly used method to handle this issue.
Recall that in an asymmetric cryptosystem the secret key of a user isknown only to
that user and no one else. Therefore, the sender of a message can use its secret key for
signing the message by encrypting itwith the key.That is, the sendercan uniquely "seal"
the message with his or her own signature (secret key). The sealed message can be sent
toanyone with the corresponding public key.Using digital signatures assures the receiver
of the message not only that the message content has not been manipulated but also that
themessage was indeed sent bytheclaimedsender.Thus, digital signaturesareapplicable
to both user authentication and message integrity.
A digital signature is basically a code, or a large number, that is unique for each
message and to each message originator. It is obtained by first processing the message
with ahash function (called adigestfunction) toobtain asmall digest dependenton each
bit of information in the message and then encrypting thedigest by using the originator's
secret key.Toavoidduplicity problems, adigest function (D) must have theproperty that
D(M) is different from D(M') for all possible pairs ofM and M'. Rivest [1992] proposed
a message digest function (known as,WD5)for use in secure mail and other applications
on the Internet.
To illustrate how the digest of a message can be obtained from the message, the
example given in [Adam 1992] is presented here. In this example, it is assumed that a
message isadigital string of O's and I'8 and isdivided into blocks of64bits.The bitwise
Exclusive-OR of the first two blocks is performed to obtain a new block of 64 bits. The
newly obtained block isagain Exclusive-ORed with the third block ofthe message, again
resulting inanew block of64 bits.This process iscontinuedone byone withallthe other
blocks of the message. The end result is a 64-bit digest that depends on each bit of data
in the whole message stream. In other words, to alter the message, even by 1bit, would
alter the 64-bit digest. Moreover, it should beessentially impossible to forge a message
that would result in the same digest.

624 Chap.11 • Security
A protocol based on a digital signature for ensuring message integrity works as
follows:
1. Asender(A) computesthedigest (D)ofamessage(M). Itthenencryptsthedigest
D by using its secret key (Sa)to obtain a ciphertext C =E(D, Sa).A signed message is
l
then created that consists ofthe sender's identifier, the message M in its plaintext form,
and the ciphertext C,. The signed message, which has theform (IDa' C J, M),isthen sent
to a receiver.
2. On receivingthe signed message, thereceiverdecrypts C by using thepublic key
l
ofthe senderto recoverthe digestD. It then calculates adigest forM (by using the same
digest function) and compares the calculated digest· with the digest recovered by
decrypting Cl' Ifthe two are equal, message M is considered to be correct; otherwise it
is considered incorrect.
Notice that the protocol does not require a message to be hidden from unauthorized
users. Rather, itallows amessage to beread openly byanyone who receives or intercepts
it. But a forged message is successfully detected by the protocol.
An application may require that the first receiver retransmit the signed message to
another receiver, which may have to subsequently retransmit it to other receivers. In
such a situation, it is important that each ofthe recipients should be able to verify that
the signed message indeed originated from the claimed originator and that its contents
were not changed by any of the intermediate recipients or by an intruder. A digitally
signed message meets these requirements because it has the originator's identifier
included in it and the digest of the message can only be decrypted by using the
originator's public key.
In the actual implementation, a key distribution server may be used that maintains a
databaseofthe public keys ofall users. If the receiverofadigitally signed messagedoes
not already have the public key ofthe message originator, it can request it from the key
distributionserver.This avoids the need tosend anew user'spublic key toallother user's
inthesystem. The newuser'spublic keycan besimply registered withthekeydistribution
server.
Privacy EnhancedMail (PEM) scheme, designed for adding privacy to Internet
mail applications, is a good example of use of cryptography and digital signature
techniques. PEM offers confidentiality, authentication, and message integrity. These
features are intended to provide sufficient trust so that the general Internet user
population will feel comfortable using the Internet for business correspondence and
sending messages that contain sensitive information. PEMis completely implemented
at the application level by end systems so that it can be incorporated on a site-by-site
or user-by-user basis. This approach imposes no special requirements on message
transfer systems at intermediate relay sites or endpoints. That is, network routers and
mail relays treat PEM messages as an ordinary piece of mail. How PEM provides
privacy to electronic mails is briefly described below. Readers interested in its detail
description may refer to [Linn 1993, Kent 1993b, Balenson 1993, Kaliski 1993, Kent
1993a].

Sec. 11.6 • Digital Signatures 625
PEM assumes that the network is not trusted but that each user of PEM trusts his or
her own local computer. Mail users obtain a public/secret key pair from a local PEM
program and publish their public keys with their mail addresses. The PEM program
maintains a database of the secret keys of its local users and the public keys of remote
users. Currently, the Rivest-Shamir-Adleman (RSA) algorithm is used to generate the
public/secret key pairs for users. PEM provides the following types of facilities:
1. Confidentiality. Sending a message in encrypted form so that sensitive
information within it cannot be read by an int.ruder.
2. Message integrity. Sending a signed message so that the receiver can beensured
that the contents of the message were not changed.
Both facilities also possess an authentication feature because the encryption and
decryption of the message or the digital signature can only be done by a user having the
proper key.
Let us first see how PEM sends a secret message (M) for ensuring confidentiality:
I. The PEM program of the sender's computer first generates a random secret key
=
(K) and encrypts the message (M) by using this key toobtain aciphertext C. E(M, K).
Currently, the DES algorithm is used for this purpose, but others may as well be used in
the future. The secret key (K) is then encrypted by using the recipient's public key (say,
P r) to obtain a ciphertext C 2 = E(K, P r). Now C. and C 2 are sent to the recipient in a
message mI'
2. On receiving m., the PEM program of the recipient's computer fetches the
recipient's secret key (Sr) from itsdatabase anddecrypts C byusingS,toobtain K. Now
2
by using K, it decrypts CI to obtain the original message M, which it then stores in the
recipient's mailbox.
Notice that thePEM scheme retains theefficiency ofsymmetric cryptography forthe
bulk encryption but avoids the need for a secure key distribution server.
Now let us see how PEM sends a signed message (M) for ensuring message
integrity:
1. The PEM program of the sender's computer computes the digest (D) of the
message (M) by using a message digest function. The digest (D) is then encrypted by
usingthe sender's secret key (Ss) toobtain aciphertextC 1 =(D, Ss).The sender'sID, C 1,
and M are then sent to the recipient in a message m.,
2. On receiving mi, the PEM program of the recipient's computer fetches the
sender'spublic key (P s) from itsdatabase and uses ittodecrypt C 1 toobtain thedigest D.
It then applies the same message digest function toM and compares the result withD. If
the two are equal, message M is considered to be correct; otherwise it is considered
incorrect.The message M isthenstored intherecipient'smailbox withaproper notefrom
the PEM program's side about the result of its integrity check.

626 Chap. 11 • Security
11.7 DESIGN PRINCIPlES
Based on their experience with Multics, Saltzer, and Schroeder [1975] identified some
design principles thatcan be usedas aguide todesigning secure systems. Although these
design principles were proposed for centralized systems, they hold good for distributed
systems as well [Kent 1981]. These and some other design principles are summarized
below. Designers of security components of a distributed operating system should use
them as basic guidelines.
1. Leastprivilege. The principle of least privilege (alsoknown as the need-to-know
principle) states thatanyprocess should begiven only thoseaccess rights that enable itto
access,atanytime, whatitneedstoaccomplish itsfunctionandnothing moreandnothing
less. That is, the security system must be flexible enough to allow the access rights of a
process togrow andshrink withitschanging access requirements. This principle servesto
limitthedamage whenasystem'ssecurity isbroken.Forexample, ifaneditor isgiven the
right to access only the file that has to beedited, even if theeditor has aTrojan horse, it
will not be able to access other files of the user and hence cannot do much damage.
2. Fail-safe defaults. Access rights should be acquired by explicit permission only
and the default should beno access. This principle requires that access control decisions
should be based on why anobject should beaccessible toaprocess rather than on why it
should not be accessible.
3. Open design. This principle requires that the design of the security mechanisms
should not be secret but should be public. It is a mistake on the part of a designer to
assume that the intruders will not know how the security mechanism of the system
works.
4. Built in to the system. This principle requires that security be designed into the
systems at their inception and be built in to the lowest layers of the systems. That is,
security should not be treated as an add-on feature because security problems cannot be
resolved very effectively by patching the penetration holes detected in an existing
system.
5. Checkfor current authority. This principle requires that every access to every
object must be checked using an access control database for authority.This is necessary
tohave immediate effect of revocation of previously given access rights. For instance, in
some file systems, acheck for access permission ismade only when a file isopened and
subsequent accessestothefileareallowed withoutanycheck.Inthesesystems, ausercan
keep a file open for several days and continue to have access to its contents, even if the
owner of the file changes the access permission and revokes the user's right to access its
contents.
6. Easy granting and revocationofaccess rights. For greater flexibility, a security
system must allow access rights for an object to be granted or revoked dynamically.
It should be possible to restrict some of the rights and to grant to a user only those
rights that are sufficient to accomplish its functions. On the other hand, a good security

Sec. 11.8 • Case Study: DeE Security Service 627
system should allow immediate revocation with the flexibility of selective and partial
revocation. With selective revocation facility, it is possible to revoke access rights to
an object only from a selected group of users rather than from all users who posses
access rights for the object. And with partial revocation facility, only a subset of the
rights granted to a user for an object can be revoked instead of always revoking all its
rights for the object.
7. Never trust otherparties. For producing a secure distributed system, the system
componentsmust bedesignedwith the assumptionthat otherparties(peopleor programs)
are not trustworthyuntil they aredemonstratedtobetrustworthy. Forexample,clientsand
servers must always be designed to view each other with mutual suspicion.
8. Always ensure freshness ofmessages. To avoid security violations through the
replay of messages,the securityof adistributedsystemmustbedesignedtoalways ensure
freshness ofmessages exchanged between two communicating entities.
9. Buildfirewalls. To limit the damage in case a system's security is compromised,
the system must have firewalls builtinto it. One way to meet this requirementisto allow
only short-livedpasswords and keys inthe system. For example, a sharedsecretkey used
to build a logical communication channel between a client and a server should be fairly
short-lived, perhaps being changed with every communication session between them.
10. Efficient, The security mechanisms used must execute efficiently and be simple
to implement.
11. Convenient to use. To be psychologically acceptable, the security mechanisms
must be convenient to use. Otherwise, they are likely to be bypassed or incorrectly used
by the users.
12. Cost effective. It is often the case that security needs to be traded offwith other
goals of the system, such as performance or ease of use. Therefore, in designing the
security of a system, it is important to come up with t.he right set of trade-offs that take
into accountthe likelihood that the systemwill becompromisedwith thecost ofproviding
the security, both in terms of money and personnel experience.
11.8 CASE STUDY: DCE SECURITY SERVICE
As a case study of how the various securit.y concepts described in this chapter can be
integrated to provide security in a single system, the DCE Security Service is briefly
described below.
In DeE, a user or a process (client or server) that needs to communicate securely is
calledaprincipal. For convenienceof accesscontrol, principalsare assigned membership
inone or more groups and organizations.All principalsof the same group or organization
have the same access rights. Groups generallycorrespondto work groups ordepartments,
and organizations typically include multiple groups having some common properties.
Typically, a principal is a member of one organization but may simultaneously be a
member of multiple groups. Each principal has a unique identifier associated with it.

628 Chap. II • Security
Together, a principal's identifier, group, and organization membership are known as the
principal'sprivilege attributes.
The main components of the DCE Security Service for a single cell are shown in
Figure 11.21. These components collectively provide authentication, authorization,
message integrity,andsecurity administrationservices. Letusconsider these services one
by one.
Securityserver node Physically
protected
Administrator
Clientnode Applicationservernode
Fig. 11.21 MaincomponentsofDeE SecurityServiceforasinglecell.
11.8.1 Ruth.ntlcatlon In DCE
TheDCEauthentication service usestheKerberossystemdescribed inSection 11.4.5.The
authentication server, ticket-granting server, and authentication database of Kerberos are
respectively called authentication server, privilege server, and registry database in DCE.
Theinformation registeredintheregistrydatabase includeseachprincipal'ssecretkeyand
privilege attributes. The protocols for authenticating a user at the time of login and for
mutualauthenticationofaclientandaserverarethesameasthatofKerberos(theintercell
client-serverauthentication protocol in DCE is the same as the interrealm authentication
protocol of Kerberos).The only difference isthat theservice-granting ticket in DCE also
contains thegroup andorganization membership information ofaclient.This information
is used bythe application server to verify the access rights of theclient before providing
the requested service.

Sec. 11.8 • Case Study: DCE Security Service 629
The establishment of a secure logical communication channel between a client and
aserverbyusing theauthenticationprotocol isknown asauthenticatedRPCinDeE.This
is because in DeE clients and servers communicate by using RPCs. Once authenticated
RPC has been established, it is up to the client and the server to determine how much
security is desired. That is, subsequent RPC messages mayor may not be encrypted
depending on the security needs of the application.
11.8.2 Authorization inDCE
Authorization in DCE is based on ACLs. Associated with each application ·serveris an
ACL and an ACL manager. The ACL contains complete information about which
principals have what rights for the resources managed by the server. When a client's
request comes to the server, it extracts the client~s ID and its group and organization
membership information from thereceived encrypted ticket. Itthenpassestheclient'sID,
membership, and the operation desired to the ACL manager. Using this information, the
ACLmanagerchecks theACLtomakeadecision iftheclient isauthorizedtoperformthe
requested operation. Itreturns anaccess granted ordenied reply tothe server,afterwhich
the server acts accordingly.
Note that in DCE groups are effective only within cells. Therefore, a principal
belongingtoadifferent cellcanbegrantedaccessbased solelyonitsuniqueidentifier,not
ongroupmembership. That is,ifaccessistobegranted toprincipals ofremotecells, their
unique identifiers have to be entered in the ACL along with the access rights.
11.8.3 Message IntegrityInDCE
Asalready mentioned above, once authenticated RPC has beenestablished, itisuptothe
client and server to determine how much security is desired. Therefore, if message
integrity is desired, itcan be ensured by the use of a digital signature technique. That is,
applications canensure dataintegritybyincludinganencrypted digestofthemessagedata
passed between clients and servers. The digest must beencrypted anddecrypted byusing
the session key that a client and a server share for secure communication between
them.
11.8.4 S8curlty Administration inDeE
The administrator, registry server,andACL manager jointly perform security administra
tion tasks. Two programs are used by the administrator for performing administration
tasks. One is the registry editor program and the other is theACL editor program.
The registry editor program may be used by the system administrator to view,add,
delete,andmodify information intheregistry database. Evensystemadministratorsdonot
have direct access to the registry database, and they access the registry database only by
making requests to the registry server.This is much safer, for although an administrator
can change any password, he or she cannot obtain the password of any user.
On the other hand, the ACL editor program may be used by an application
administrator to view,add, delete, and modify entries inACLs for applications orACLs

630 Chap.11 • Security
for objects (resources) controlled by them. Once again, all requests for updates toACLs
are sent to the ACL manager and not performed directly on ACLs.
InDCE, systemadministrators useorganization membership toapply global security
policies, such asdeciding thelifetime oftickets andpasswords ofdifferent principals. For
example, the lifetime of tickets and passwords is kept smaller for principals of an
organization that handles highly sensitive information as compared to the lifetime of
tickets and passwords of principals of an organization that does not handle sensitive
information.
11.9 SUMMARY
Computer security deals with protecting the various resources and information of a
computer system against destruction and unauthorized access. The main goals of
computer security are secrecy, privacy, authenticity, and integrity.
A total approach to computer security involves both external and internal security.
The three main aspects of internal security in distributed systems are authentication,
access control, and communication security.
An intruder isaperson orprogram that tries toobtain unauthorizedaccess todata or
aresource ofacomputersystem.Anintruder maybeathreattocomputersecurity inmany
ways that are broadly classified into two categories-passive attacks and active attacks.
In passive attacks, an intruder somehow tries to steal unauthorized information from the
computer system without interfering with the normal functioning of the system. Some
commonly used methods of passive attack are browsing, leaking, inferencing, and
masquerading. On the other hand, active attacks interfere with the normal functioning of
thesystem andoften havedamaging effects. Somecommonly usedformsofactiveattacks
are viruses, worms, and logic bombs. Active attacks associated with message
communications are integrity attack, authenticity attack, denial attack, delay attack, and
replay attack.
Three kinds of channels that can be used by a program to leak information are
legitimate channels, storage channels, and convert channels. The confinement problem
deals with the problem of eliminating every means by which an authorized subject can
release anyinformationcontained intheobjecttowhichithasaccess tosome subjectsthat
are not authorized to access that information. The confinement problem is in general
unsolvable.
Cryptography is a means of protecting private information against unauthorized
access in those situations where it isdifficult to provide physical security. There are two
broad classes of cryptosystems-symmetric and asymmetric. When cryptography is
employed for secure communications in distributed systems, a need for key distribution
arises. The mechanisms and protocols for key distribution in symmetric and asymmetric
cryptosystems have been described in the chapter.
Anauthenticationmechanism prohibits theuseofthesystem(orsomeresource ofthe
system) by unauthorized users by verifying the identity of a user making a request. The
main types of authentication normally needed in a distributed system are user login
authentication, one-way authentication of communicatingentities, and two-way authenti-

Chap. 11 • Exercises 631
cation of communicating entities. The three basic approaches to authentication are proof
by knowledge, proof by possession, and proof by property. The proof-by-knowledge
method based onpasswords isthemostwidely usedmethod foruserlogin authentication.
For one-way and two-way authentication of communicating entities, the protocols based
oncryptosystemshave been described inthechapter.The Kerberos authentication system
has also been described as a case study.
Access control deals with the ways that are used in a computersystem to prohibit a
user (or a process) from accessing those resources/information that he or she is not
authorized to access. The three access control models proposed in the literature are the
access matrix model, the information flow control model, and the security kernel model.
Of these, the access matrix model is the most popular one and is widely used inexisting
centralized and distributed systems.
In the access matrix model, the access rights of each subject to each object are
definedasentries inamatrix,called theaccessmatrix.Thetwomostwidelyusedmethods
that have gained popularity in contemporary distributed systems for implementing an
access matrix are ACLs and capabilities.
The concept ofdigital signatures, which isbased upon asymmetric cryptosystems, is
the most commonly used method to handle the issue of message integrity in distributed
systems.
Some design principles that can be used as a guide to designing secure systems are
leastprivilege, fail-safe defaults, opendesign, security builtintothesystem,checking for
current authority,easy togrant and revoke access rights, nottotrust other parties, always
ensuring freshness of messages, building of firewalls, cost effective, efficient, convenient
to use, and right set of trade-offs.
EXERCISES
11.1. List some of the common goals of computer security.
11.2. What are the additional security problems that a distributed operating system designer must
deal with as compared to the designer of an operating system for acentralized time-sharing
system? Can we ensure the same degree of security in a distributed system as we have in a
centralized time-sharing system? Give reasons for your answer.
11.3. What isthe"need-to-know"principle incomputersecurity?Think ofsomesecurity problems
that may occur if this principle is not taken care of in the design of the security component
of a computersystem.
11.4. Differentiate between passive and active attacks. Which of the two is more harmful and
why?
11.5. What are some of thecommonly used methods forpassive attack? Commenton the relative
complexity of each of these methods from the point of view of the following:
(a) An intruder
(b) The designerof asecurity system
11.6. What isaTrojan horseprogram?Give anexample(inpseudocode)ofbothapassive typeand
an active type Trojan horse program.

632 Chap. 11 • Security
11.7. Listnecessaryprecautionsforpreventingacomputersystemfromvirusinfection.Howcan
an already infected computer system be cured of virus infection? Why is curing of a
distributed system from virus infection much more difficult than curing a centralized
system?
11.8. List the importantdifferencesbetweencomputervirusesand worms.How do they each
reproduce?Asecuritysystemistobedesignedthatpreventsvirusprogramsfromreplicating
inanuncontrolledmannerbutallowswormprogramstoberun.Suggestasuitablesecurity
schemeforthis.
11.9. Whatarethecommontypesofactiveattacksassociatedwithmessagecommunicationsina
distributedsystem?Commentontherelativecomplexitiesofthesetypesofattacksfromthe
pointofviewofthefollowing:
(a) Anintruder
(b) Thedesignerofthesecuritysystemofadistributedsystem
11.10. Whatisanonce?Giveexamplesofsomeitemsthatcan beusedasanonce.
11.11.Whatisaconfinementproblemincomputersecurity?Explainwhythisproblemisingeneral
unsolvable.
11.12. List the differenttypes of channelsthat may be used by a programto leak information.
Explainhowthesechannelsareusedtoleakinformation. Isitpossibletototallypreventthe
leakageof information?
11.13. Discusstherelativecomplexityofdesigningthesecuritycomponentofasystemthatallows
onlydataobjectstobesharedandasystemthatallowsbothdataobjectsandprogramobjects
tobeshared.
11.14. Whatiscryptography?Whataresomeof itscommonusesinadistributedsystem?
11.15. Whataresomeofthebasicrequirementsthatagoodcryptosystemmustfulfill?
11.16. Differentiate among ciphertext-only, known-plaintext, and chosen-plaintextattacks with
respecttoacryptosystem.
11.17. Explain how symmetric and asymmetric cryptosystems work. Discuss their relative
advantagesanddisadvantages. Whichofthetwoismoresuitableforeachofthefollowing
cases(givereasonsforyouranswer):
(a) Where both encryption and decryption of information are performed by a trusted
subsystem
(b) Wheredifferentsubjectsperformtheencryptionanddecryptionofinformation
(c) Wherebulkdataencryptionisinvolved
(d) For establishing connection between two communicating entities in a distributed
system
(e) For exchange of messages between two communicating entities in a distributed
system
11.18. What is a key distributionproblem?How does it differ for symmetricand asymmetric
cryptosystems?
11.19. Describetwomethodsforsolvingthekeydistributionproblemforasymmetriccryptosystem
anddiscusstheirrelativeadvantagesanddisadvantages.

Chap. 11 • Exercises 633
11.20. Describe a method for solving the key distribution problem for an asymmetric crypto
system.
11.21. What are the commonly used approaches for user authentication in computer systems?
Explain how a user is authenticated ineach of these approaches.
11.22. Explain the password-basedapproach foruser logins authentication. What aretheproblems
associated with this approach? Suggestsolutions to overcomethese problems.
11.23. In adistributed system it isdesired that aserver process should serve anyclient that needs
its service only after verifying the identity of the client. Describe how to implement this
authentication requirement.
11.24.In the preceding exercise, suppose that both the client and the server should verify each
other'sauthenticity before acommunication session can be started between them. Describe
how to implement this authentication requirement.
11.25. The passwordmechanismisused inadistributedsystem toauthenticateusers atlogintime.
State the most suitable locations (according to you) for storing the login program and the
password file in the following cases:
(a) The distributed system is based on the workstation-servermodel witheach workstation
having a small hard disk of about 20 megabytes capacity.
(b) The distributed system is based on the workstation-server model. Some of the
workstations are diskless and others have a small hard disk of about 20 megabytes
capacity.
(c)The distributed system is based on the processor-pool model.
Assume that any user is free to use any of the user terminals or workstations.
11.26. What is an access matrix? Explain how the following issues can be handled in a security
system that uses access matrix for access control:
(a) Decidingthe contents of the access matrix entries
(b) Validating access to objects by subjects
(c) Allowing subjects to switch domains in acontrolled manner
11.27. What are the commonly used methods for implementing an access matrix? Explain their
relative advantages and disadvantages.
11.28. Whatisadomain?State threedifferent waysinwhichdomains canberealized inthedesign
ofthesecurity componentofanoperatingsystem andexplain howdomain switching willbe
done ineach case. Explain howdomains areformed and howdomain switching takes place
in UNIX and Multics operating systems.
11.29. What is a capability? Answer the following questions for a capability-based security
system:
(a) When asubject accesses anobject, howisthevalidation check madewhether thesubject
is allowed to access the object in the requested mode?
(b) How does asubject get acapability for an object? Considerboth thecase in which the
subject istheowneroftheobject and another one inwhich thesubject isnottheowner
of the object.
(c) The owner of an object wants to share the object with another subject allowing it
restricted access rights to the object. How can this bemade possible?
(d) Howcanitbeensured thatacapabilityisnever reused foridentifyingsomeother object
in the system?
(e) How can capabilities be protected against unauthorized access?
(t) How can capabilities be made difficult to guess?
(g) How can capabilities be made difficult to forge?
(h) How can selective revocation of capabilities be performed?

634 Chap. I1 • Security
11.30. Answer the following questions for anACL-based security system:
(a) Whenasubjectaccessesanobject, howisthevalidation check madewhetherthesubject
is allowed to access the object in the requested mode?
(b) How is access right for an object granted to a subject?
(c) The owner of an object wants to share the object with another subject allowing it
restricted access rights to the object. How can this bemade possible?
(d) How areACLs protected against user tampering?
(e) How can an access right given to a subject be revoked?
11.31. Inadistributedsystem, itisdesired that servers should check the access right ofclients for
every access request made. Explain how can this be implemented by using the following:
(a) Only ACLs
(b) Only capabilities
(c) Both ACLs and capabilities
Which approach is preferable and why?
11.32. Adistributed operating system uses theACL-based access control mechanism butdoes not
use the negative rights concept. What type of access control activity isdifficult to perform
in this system? Why is this difficulty not faced ina centralized system that uses theACL
based access control mechanism?
11.33. A system has a large number of users. In this system, it isdesired that a file be accessible
to all except five users. Describe how this security requirementcan be specified:
(a) If the system uses theACL-based security scheme without negative rights facility
(b) If the system uses theACL-based security scheme with negative rights facility
(c) If the system uses thecapability-based security scheme
11.34. What is adigital signature?What are its uses in the security of adistributed system? Give
a method to create a digital signature. Describe how digital signature can be used for
ensuring message integrity in a distributed system.
11.35. What are the important design principles that should normally be used as a guideline to
designing secure computersystems? Explain why these design principles are important.
BIBLIOGRAPHY
[Abramsetal,1995]Abrams, M.D.,Podell, H.J.,andJajodia, S.(Eds.), InformationSecurity:An
Integrated Collection ofEssays, IEEE ComputerSociety Press, LosAlamitos, CA (1995).
[Adam 1992] Adam, 1. A., "Cryptography = privacy," IEEE Spectrum, pp. 29-35 (August
1992).
[Ak11983]Akl, S.G.,"Digital Signatures: ATutorial Survey," IEEEComputer,Vol.16,No.2,pp.
15-24 (1983).
[Ames et al, 1983]Ames, S. R., Gasser, M., and Schell, R. R., "Security Kernel Design and
Implementation:An Introduction," IEEE Computer, Vol. 16,No.7, pp. 14-22 (1983).
[Amoroso 1994] Amoroso, E., Fundamentals of Computer Security Technology, Prentice-Hall,
EnglewoodCliffs, NJ (1994).

Chap. 11 • Bibliography 635
[Anderson 1994]Anderson, R.1.,"Why Cryptosystems Fail," Communications ofthe ACM, Vol.
37, No. 11,pp.32-40 (1994).
[Balenson1993]Balenson,D.M.,"Privacy Enhancementfor InternetElectronicMail. Algorithms,
Modes, and Identifiers," Internet RFC1423 (Part 3)(1993).
[Baueret al. 1983] Bauer, R. K., Berson, T.A., and Feirtag, R. 1.,"A Key Distribution Protocol
Using Event Markers," ACM Transactions on Computer Systems, Vol. 1,No.3, pp.249-255
(1983).
[Bell and LaPadula 1973] Bell, D. E., and LaPadula, L. 1., "Secure Computer Systems:
Mathematical Foundations," ESD-TR-278, I, ESO/AFSC, Hanscom AFB, Bedford, MA
(1973).
[Bellovinand Cheswick 1994] Bellovin, S. M., and Cheswick, W.R., "Network Pirewalls,"IEEE
Communications Magazine, pp. 50-57 (September 1994).
[Bellovin and Merritt 1990] Bellovin, S. M., and Merritt, M., "Limitations of the Kerberos
Authentication System,"ACM Computer Communications Review, Vol.20, No.5, pp.119-132
(1990).
[Bowlesand Pelaez]992]Bowles,1.B.,andPelaez,C.E.,"BadCode,"IEEESpectrum,pp. 36-40
(August 1992).
[Bright 1977] Bright,H.S.,"CryptanalyticAttackand Defense:Ciphertext-Only,Known-Plaintext,
Chosen-Plaintext," Cryptologia,Vol. I, No.4, pp. 366-370 (1977).
[Brown1994]Brown,P.W.,"DigitalSignatures:AreTheyLegal for ElectronicCommerce?"IEEE
Communications Magazine, pp. 76--80 (September 1994).
[Burrows et al,1990] Burrows,M.,Abadi, M., and Needham, R.M., "A Logic ofAuthentication,"
ACMTransactions on Computer Systems, Vol.8, No.1, pp. 18-36 (1990).
[Champine et al, 1990) Champine, G. A., Geer, Jr., D. E., and Ruh, W.N., "ProjectAthena as a
Distributed ComputerSystem," IEEEComputer, Vol.23, No.9, pp. 40-51 (1990).
[Chess 1989JChess, D. M., "Computer Viruses and Related Threats to Computer and Network
Integrity," Computer Networks and ISDN Systems, Vol. 17, pp. 141-148 (1989).
[Clark and HotTman 1994] Clark, P. C., and Hoffman, L. J., "BITS: A Smartcard Protected
Operating System," Communications ofthe ACM, Vol. 37, No. 11,pp. 66-70 (1994).
[Cohen 1987] Cohen, F.,"ComputerViruses: Theory and Experiments," Computers andSecurity,
Vol.6, pp. 22-35 (1987).
[Cohen and Jefferson 1975] Cohen, E., and Jefferson, D., "Protection in the Hydra Operating
System," In: Proceedings of the 5h ACM Symposium on Operating Systems Principles,
Association for Computing Machinery, New York, NY,pp. 141-160(November 1975).
[Dannenberg and Hibbard 1985] Dannenberg, R. B., and Hibbard, P.G., "A Butler Process for
ResourceSharingonSpiceMachines,"ACMTransactionsonOffice InformationSystems,Vol.3,
No.3, pp. 234-252 (1985).
[Denning1976]Denning, D.E.,"A LatticeModel for SecureInformationFlow,"Communications
ofthe ACM, Vol. 19, No.5, pp. 236-243 (1976).
[Denning and Sacco 1981] Denning, D. E., and Sacco, G. M., "Timestamps in Key Distribution
Protocols," Communications ofthe ACM, Vol.24, No.8, pp. 533-536 (1981).
[Dolevand Yao1983]Dolev, D., and Yao,A.C., ,,'Onthe SecurityofPublicKey Protocols,"IEEE
Transactions on Information Theory, Vol.IT-30, No.2, pp. 198-208 (1983).

636 Chap. 11 • Security
[Ganesan and Sandhu 1994]Ganesan, R., and Sandhu, R., "SecuringCyberspace," Communica
tionsoftheACM, Vol.37, No. 11,pp. 29-11 (1994).
[Glasgowet al. 1992]Glasgow, J., McEwan, G., and Pananageden, P., "A Logic for Reasoning
about Security," ACM Transactions on Computer Systems, Vol. 10, No.3, pp. 265-310
(1992).
[Graham and Denning 1972]Graham, G. S., and Denning, P.J., "Protection-Principles and
Practice,"In:AFIPSProceedingsoftheSpringJointComputerConference,Vol.40, pp.417-429
(1972).
[Harrisonetal. 1976]Harrison, M.A.,Ruzzo, W. L., and Ullman, J. D., "ProtectioninOperating
Systems," Communications oftheACM, Vol. 19, No.8, pp. 461-471 (1976).
[Hellman 1978]Hellman,M.E., "An OverviewofPublic-KeyCryptography,"IEEE Transactions
on Computers, Vol.C-16, No.6, pp. 24-32 (1978).
[Hendry 1995]Hendry, M., Practical Computer Network Security, Artech House, Boston, MA
(1995).
[Hruska 1993] Hruska, J., Computer Viruses and Anti-Virus Warfare, 2nd ed., Prentice-Hall,
Englewood Cliffs, NJ (1993).
[Hu 1995] Hu, W.,DeESecurity Programming, O'Reilly, Sebastopol, CA (1995).
[Huttetal. 1995]Hutt,A. E.,Bosworth,S., and Hoyt, D. B.(Eds.), ComputerSecurityHandbook,
3rd ed., Wiley, New York, NY (1995).
[Kak1983]Kak,S.C.,"DataSecurityinComputerNetworks,"IEEEComputer,Vol.16,No.2,pp.
8-10(1983).
[Kaliski1993]Kaliski,B.S.,"PrivacyEnhancementforInternetElectronicMail. KeyCertification
and Related Services," Internet RFC1424 (Part 4) (1993).
[Kehneet al, 1992]Kehne,A.,Schonwalder,J., and Langenderfer, H.,"A Nonce-Based Protocol
for Multiple Authentications," ACM Operating System Review, Vol. 26, No.4, pp. 84-89
(1992).
[Kent 1981]Kent, S. T.,"Security in Computer Networks," Protocols and Techniquesfor Data
Communication Networks"Prentice-Hall, Englewood Cliffs, NJ, pp. 369-432 (1981).
[Kent1993a]Kent, S.T.,"InternetPrivacyEnhancedMail,"CommunicationsoftheACM, Vol.36,
No.8, pp. 48-60 (1993).
[Kent 1993b]Kent, S. T., "Privacy Enhancement for Internet Electronic Mail. Certificate Based
Key Management," Internet RFC 1422(Part2) (1993).
[Khanna 1994J Khanna, R. (Ed.), Distributed Computing: Implementation and Management
Strategies, Prentice-Hall, Englewood Cliffs, NJ (1994).
[Kluepfel 1994] Kluepfel, H. M., "Securing a Global Village and Its Resources," IEEE
Communications Magazine, pp. 82-89(September 1994).
[Lampson1971]Lampson,B.W.,"Protection,"In:Proceedingsofthe5thPrincetonSymposiumon
Information Sciences and Systems, Princeton University, Princeton, NJ, pp. 437-443 (March
1971).
[Lampson 1973]Lampson,B.W.,"A Note on the ConfinementProblem,"Communications ofthe
ACM, Vol.6, No. 10, pp. 613-615 (1973).

Chap. 11 • Bibliography 637
[Lampson1993JLampson,B.W.,"AuthenticationinDistributedSystems,"In:S.Mullender(Ed.),
Distributed Systems, 2nd ed., Association for Computing Machinery, New York, NY, pp.
543-580 (1993).
[Lampsonetal, 1992]Lampson, B.W.,Abadi,M.,Burrows,M.,and Wobber, E., "Authentication
inDistributedSystems:Theoryand Practice,"ACM TransactionsonComputerSystems, Vol.10,
No.4, pp.265-310 (1992).
[Levine 1986] Levine, P.,"The Apollo Domain Distributed File System," Distributed Operating
Systems: Theory and Practice, NATOAdvanced Study Institute, Turkey, Springer-Verlag, New
York, NY(1986).
[Linn 1993]Linn, 1., "Privacy Enhancement for Internet Electronic Mail. Message Encipherment
and Authentication Procedures," Internet RFC 1421 (Part 1) (1993).
[Lockhart, Jr. 1994] Lockhart, Jr., H. W., OSF DCE: Guide to Developing Distributed
Applications, IEEE ComputerSociety Press, Los Alamitos, CA (1994).
[Milenkovic 1992] Milenkovic, M., Operating Systems: Concepts and Design, 2nd ed., McGraw
Hill, New York, NY (1992).
[Mitchelletale1992]Mitchell,C.1.,Piper, F.,andWild, P.,"DigitalSignatures,"In:G.1.Simmons
(Ed.), Contemporary Cryptology, IEEE, NewYork, NY (1992).
[Morris and Thompson 1979] Morris, M., and Thompson, K., "Password Security: A Case
History," Communications oftheACM, Vol.22, No. 11,pp. 594-597 (1979).
[Mullender 1985] Mullender, S. J., "Principles of Distributed Operating System Design," Ph.D.
Dissertation, Mathematisch Centrum,Amsterdam (1985).
[Mullender and 'Ianenbaum 1984] Mullender, S. J., and Tanenbaum, A. S., "Protection and
Resource Control in Distributed Operating Systems," Computer Networks, Vol.8,pp. 421-432
(1984).
[Mullender and Tanenbaum 1986]Mullender, S. 1., and Tanenbaum, A. S., "The Design of a
Capability-BasedDistributedOperatingSystem,"ComputerJournal,Vol.29,No.4,pp.289-300
(1986).
[NBS1977]National BureauofStandards, Federal Information ProcessingStandards,Publ. p.46,
Washington, DC (1977).
[Needham 1993]Needham, R.M.,"Cryptography and SecureChannels," In: S. Mullender(Ed.),
DistributedSystems,2nded.,AssociationforComputingMachinery,NewYork,NY,pp.531-541
(1993).
[Needham 1994]Needham,R.M.,"DenialofService:An Example,"CommunicationsoftheACM,
Vol.37, No. 11,pp. 42-46 (1994).
[Needham and Schroeder1978] Needham, R. M., and Schroeder, M. D., "Using Encryption for
AuthenticationinLarge NetworksofComputers,"CommunicationsoftheACM,Vol.21, No. 12,
pp. 993-999 (1978).
(Needham and Schroeder 1987] Needham, R. M., and Schroeder, M. D., "Authentication
Revisited,"ACMOperating System Review, Vol.21, No.1, p. 7 (1987).

638 Chap. 11 • Security
(Nessett 1983] Nessett, D. M., "A Systematic Methodology for Analyzing Security Threats to
Interprocess Communication ina Distributed System," IEEE Transactionson Communications,
Vol.COM-31, pp. 1055-1063 (1983).
[Nessett1987]Nessett,D.M.,"FactorsAffectingDistributedSystemSecurity,"IEEETransactions
on Software Engineering, Vol.SE-13, No.2, pp. 233-248 (1987).
[Neuman and Theodore 1994]Neuman, B. C., and Theodore, T.,"Kerberos: AnAuthentication
Service for Computer Networks," IEEE Communications Magazine, pp. 33-38 (September
1994).
[Otway and Rees 1987]Otway,D., and Rees, 0., "Efficient andTimely Mutual Authentication,"
ACMOperating System Review, Vol.21, No. I, pp. 8-10(1987).
[Pfteeger 1989) Ptleeger, C. P., Security in Computing, Prentice-Hall, Englewood Cliffs, NJ
(1989).
[Rashid and Robertson 1981] Rashid, R. F.,and Robertson, G. G., "Accent: A Communication
Oriented Network Operating System Kernel," In: Proceedings ofthe 8h ACM Symposium on
Operating Systems Principles,Association forComputing Machinery,New York,NY (1981).
[Rivest 1992)Rivest, R.L., "The MD5 Message-Digest Algorithm," TechnicalReport RFC 1321,
available for anonymous ftp from the Internet Network Information Center, Internet host:
nic.ddn.mil, directory/usr/pub/RFC (1992).
(Rivestetal.1978]Rivest,R.L.,Shamir,A.,andAdleman,L.M.,"AMethodforObtainingDigital
Signatures and Public-Key Cryptosystems," Communications ofthe ACM, Vol.21, No.2, pp.
120-126(1978).
[Rosenberry et al, 1992) Rosenberry, W., Kenney, D., and Fisher, G., OSF DISTRIBUTED
COMPUTING ENVIRONMENT, Understanding DCE, O'Reilly, Sebastopol, CA (1992).
[Rushbyand Randell1983)Rushby,J.M.,andRandell,B.,"ADistributedSecureSystem,"IEEE
Computer, Vol.16,No.7, pp. 55-67 (1983).
[Saltzerand Schroeder1975]Saltzer,J.H.,andSchroeder,M.N.,"TheProtectionofInformation
inComputer Systems," In:Proceedings ofthe IEEE, Vol.63, pp. 1278-1308 (1975).
[Sandhu and Samarati 1994]Sandhu, R. S., and Samarati, P.,"Access Control: Principles and
Practice," IEEE Communications Magazine, pp.40-48 (September 1994).
[Sansometal.1986]Sansom,R.D.,Julin,D.P.,andRashid;R.F.,"Extending aCapability Based
System into a Network Environment," Technical Report No. CMU-CS-86-115, Computer
Science Department, Carnegie-Mellon University,Pittsburgh, PA(1986).
[Satyanarayanan1989]Satyanarayanan, M.•"Integrating SecurityinaLargeDistributedSystem,
n
ACMTransactionson Computer Systems, Vol.7, No.3, pp. 247-280 (1989).
[Satyanarayan8n1990]Satyanarayanan, M.,"Scalable, Secure, andHighlyAvailableDistributed
FileAccess," IEEE Computer, Vol.23, No.5, pp.9-21 (1990).

Chap. 11 • Bibliography 639
[Schneier1996]Schneier,B.,AppliedCryptography:Protocols,Algorithms, andSource CodeinC,
2nded., Wiley, New York, NY (1996).
[Schroederet al.1977]Schroeder, M. D., Clark,D. D., and Saltzer,1.H., "TheMULTICS Kernel
Design Project," In: Proceedings ofthe6thACMSymposium on Operating Systems Principles,
Association for Computing Machinery, New York, NY,pp.43-56 (November 1977).
[Seberry and Pieprzyk 1989] Seberry, 1, and Pieprzyk, 1., Cryptography: An Introduction to
ComputerSecurity, Prentice-Hall, EnglewoodCliffs, NJ(1989).
[Shankar 1977] Shankar, K. S., "The Total Computer Security Problem: An Overview," IEEE
Computer, Vol. 10, pp. 50-62,71-73 (1977).
[SilberschatzandGalvin1994]Silberschatz,A.,and Galvin,P.B.,OperatingSystemConcepts,4th
ed., Addison-Wesley, Reading, MA (1994).
[Simmons 1992] Simmons, G. J. (Ed.), Contemporary Cryptology, IEEE, New York (1992).
[Simmons 1994] Simmons, G. 1., "Cryptanalysis and Protocol Failures," Communications ofthe
ACM, Vol.37, No. 11,pp. 56-65 (1994).
[SinghalandShivaratri1994]Singhal,M., and Shivaratri,N.G.,AdvancedConceptsinOperating
Systems, McGraw-Hill, New York (1994).
[Skardhamar1996]Skardhamar,R.,VirusDetectionand Elimination,AcademicPress, San Diego,
CA (1996).
[Smart 1994] Smart, R. K., "The X.509 Extended File System," In: Proceedings ofthe ISOC
SymposiumonNetwork and DistributedSystem Security, InternetSociety,Reston,VA(February
1994).
[Stallings 1994] Stallings, W., "Kerberos Keeps the Enterprise Secure," Data Communications
Magazine, pp. 103--111 (October 1994).
[Stallings 1995] Stallings, W., Network and Internetwork Security: Principles and Practice,
Prentice-Hall, Englewood Cliffs, NJ (1995).
[Stallings 1996] Stallings, W. (Ed.), Practical Cryptography for Data lnternetworks, IEEE
ComputerSociety Press, Los Alamitos, CA (1996).
[Steineretal.1988]Steiner,1.G.,Neuman, B.C., and Schiller,1.I.,"Kerberos:AnAuthentication
Service for Open Network Systems," In: Proceedings ofthe Winter 1988USENIX Conference,
USENIX, Berkeley, CA, pp. 191-202 (February 1988).
[TardoandAlagappan 1991] Tardo, J.1.,and Alagappan, K., "SPX: GlobalAuthentication Using
Public Key Certificates," In: Proceedings ofthe IEEE Symposium on Research in Security and
Privacy, IEEEPress,New York, NY, pp. 232-244 (1991).
[Tanenbaum 1987] Tanenbaum, A. S., Operating Systems: Design and Implementation, Prentice
Hall, Englewood Cliffs, NJ (1987).
[Tanenbaum 1992] Tanenbaum, A. S., Modern Operating Systems, Prentice-Hall, Englewood
Cliffs, NJ (]992).

640 Chap. 11 • Security
[Tanenbaumet al. 1986]Tanenbaum,A.S.,Mullender,S.J.,andvanRenesse,R.,"UsingSparse
Capabilities in a Distributed Operating System," In: Proceedings of the 6th International
ConferenceonDistributed Computing Systems, IEEEPress,NewYork,NY,pp.558-563 (May
1986).
[lSaietal.1990]Tsai,C.,Gligor,V.D.,andChandersekaran,C.S.,"OntheIdentificationofCovert
StorageChannelsinSecureSystems,"IEEE TransactionsonSoftwareEngineering, Vol. SE-16,
No.6, pp,569-580(1990).
[Whiteet al, 1996]White,G.B.,Fisch,E.A.,andPooch,U.W.,Computer System and Network
Security,CRC,BocaRaton,FL(1996).
[Wobberetal,1994]Wobber,E.,Abadi,M.,Burrows,M.,andLampson,B.,"Authenticationinthe
TaosOperatingSystem,"ACMTransactionson Computer Systems, Vol. 12,pp. 3-32 (1994).
[Wooand Lam 1992]Woo,T.Y.C., andLam,S. S., "Authenticationfor DistributedSystems,"
IEEE Computer,Vol. 25,No.1, pp. 39-52 (1992).
[Worm 1989]"SpecialSectionontheInternetWorm,"Communications oftheACM, Vol. 32,No.
6,pp.677-710 (1989).
POINTERS TO81BlIOGRAPHIES ONTHE INTERNET
Bibliographies containing references on Computer Security can be found at:
ftp:ftp.cs.umanitoba.calpublbibliographieslMisc/security.htm}
ftp:ftp.cs.umanitoba.calpublbibliographies/Misc/security.l.html
ftp:ftp.cs.umanitoba.calpublbibliographieslMisc/security.2.htm}
http:julmara.ce.chalmers.se/Security/sec_bib.html
http:www.telstra.com.au/pub/docs/security
A list ofbooks on Security in Computer Networks can be found at:
http:www.crpht.lu/CNS/htmllPubServ/Securitylbibliography.html
Bibliography containing references on Cryptography can be found at:
http:mnementh.cs.adfa.oz.au/htbin/bib_lpb
(This is agateway to the index into Lawries Cryptography Bibliography, which contains
references on various aspects of cryptography and computer security.)
Anindex of bibliographiescontainingreferences on Cryptography, InternetSecurity, and
Kerberos can befound at:
http:www.comp.vuw.ac.nz/.....sai/docs

Chap. 11 • Pointers toBibliographiesontheInternet 641
COAST (Computer Operations, Audit, and Security Technology) is a multiple project,
multiple investigator laboratory in computer security research in the Computer Science
Department at Purdue University.A list of documents on various COAST projects, and
some other security-related papers by COAST personnel can be found at:
http:www.cs.purdue.edu/coast/coast-library.html

12
CHAPTER
Case Studies
11.1 INTRODUmON
This chapter aims to consolidate the reader's understanding of the various concepts
described in the preceding chapters of this book by describing real distributed operating
systems.The systemsdescribed areAmoeba, V-System,Mach,andChorus. The firsttwo
are university research projects. The latter two also started as research projects but have
now beencommercialized. These systems are still in various stages of development and
refinement.Therefore, anexhaustiveanddetailed descriptionofeachsystem hasnotbeen
presented. For each system, only the design goals, system architectures, and most
important and noteworthy aspects and features have been highlighted. Furthermore, the
order of presentation is not strictly chronological and does not reflect the relative
importanceofthesystems.Thebibliographyprovidesreferences forseveralothersystems
that may be of interest to researchers performing research activities in the area of
distributed operating systems.
642

Sec.12.2• Amoeba 643
12.2 AMOEBA
Amoeba is a microkernel-based distributed operating system developed at Vrije
University and the Center for Mathematics and Computer Science in Amsterdam, The
Netherlands. It was started in 1981 by Andrew S. Tanenbaum as a research project in
distributed and parallel computing. Since then, it has evolved over the years to acquire
several attractive features. The following description of Amoeba is based on
[Tanenbaum 1995, Coulouris et a1. 1994, Mullender et a1. 1990, Mullender and
Tanenbaum 1986].
12.2.1 Design Goals and Main Features
Amoeba's design was influenced by the research and design goals given below.
Transparency
Providing a single-system image to the users was one of the main goals ofAmoeba. The
most important design decision that was taken to achieve this goal was to use the
processor-pool model inwhichthere isnoconcept ofa"homemachine" andallresources
belong to the system as a whole.
Parallel Programming Support
Although transparency is a useful feature for most users of a distributed system, some
usersareinterested inusingthesystem asatestbed forexperimentingwithdistributed and
parallel algorithms, languages, tools, and applications. Amoeba supports such users by
making the underlying parallelism available to them. For this, the Orca language [Bat et
al. 1992] has been designed and implemented on Amoeba.
Capability-Based, Object-Oriented Approach
Another major goal ofAmoeba was to investigate the possibility of using the capability
based, object-orientedapproach forbuilding anoperationaldistributedsystem.'fa achieve
this goal, objects and capabilities are used in a uniform way in the design ofAmoeba. In
particular, theAmoeba software isbased on objects, and objects are named and protected
by using capabilities.
Small-Kernel Approach
Anothergoal inAmoeba'sdesign wastominimizethe kernel sizeandenhance flexibility.
To achieve this goal, its design was based on the microkemel approach. That is, in
Amoeba, severalofthestandard services, suchasafileservice, arebuiltoutside thekernel
in user space. This helps in enhancing. flexibility because these services can be easily

644 Chap. 12 • Case Studies
modified,andmultipleversionsofaservicecanbesimultaneouslyrunonthesamesystem
to suit the needs of different users.
High Performance
Highperformance wasalsoagoalinAmoeba.Threedesigndecisionsthatwereinfluenced
by this goal are the useoftheprocessor-pool model, the useof multithreaded processes,
and the use ofa bullet file service that stores immutable files as contiguous byte strings
both on disk and in its cache.
High Reliability
Amoeba hasalsobeendesigned tohaveahighdegreeofreliability.The followingdesign
decisions helped in improving the overall reliability ofthe system:
1. Use ofthe processor-pool model. In the processor-pool model, processors can
be dynamically added to the pool or removed from the pool. Therefore, when a few
processors crash, some jobs may have to be restarted and the computing power of the
system is temporarily lowered, but otherwise the system continues to function
normally, providing a high degree of fault tolerance.
2. Support for reliable interprocess communication. RPC is the basic IPC
mechanism inAmoeba.Amoeba's RPC supports at-most-once semantics, so thatanRPC
is never carried out more than once, even in the face of server crashes and rapid
reboots.
3. Making the directory service reliable. The directory service, whose primary
function is to provide a mapping from human-oriented object names to system-defined
capabilities, is a critical component in Amoeba because almost every application
depends on it for finding the capabilities it needs. If the directory service stops,
everything else will also come to a halt. So that no single-site failure can bring it
down, it has been implemented in a fault-tolerant way by using the stable-storage
technique.
UNIX Emulation
Amoeba was developed from scratch rather than starting with an existing system (e.g.,
UNIX). The motivation behind this design approach was to experiment with new ideas
without having to worry about backward compatibility with any existing system.
However, the result of this design was that Amoeba's interface turned out to be quite
different from that of UNIX. Therefore, to avoid writing hundreds of utility and
application programs for Amoeba from scratch, a UNIX emulation package was later
added to Amoeba. This package, which is in the form of a library, allows most UNIX
programs to run on Amoeba with little or no modification. Further details of UNIX
emulation inAmoeba can befound in [Mullender et al. 1990].

Sec.12.2• Amoeba 64S
12.1.2 SystemArchitecture
HardwareArchitecture
As Figure 12.1 shows, the Amoeba hardware consists of the following principal
components:
Processor
r--QQ~---..
Wide-area
Localareanetwork network
I
I
t
I
1 1
1' 1I
Specializedservers
Fig. 12.1 Hardwarearchitectureof the Amoebasystem.
1. Processor pool. Amoeba is based on the processor-pool model. An Amoeba
system can have one or more processor pools. The CPUs in a pool belong to the system
as a whole and have no specific "owners."Therefore, auserdoes notlog ontoaspecific
machine buttothesystemasawhole.Whenauserhasanapplication torun,theoperating
system dynamically allocates one or more CPUs from thepool tothat application. When
the user's application completes, theCPUsarereturned tothe pool forallocation toother
work. Ifno CPU isfree when auserhas anapplication to run, theCPUs aretime shared,
and a newly arrivedjob is assigned tothe most lightly loaded CPU at that time.Amoeba
has been designed to support heterogeneity. Therefore, the CPUs in a pool can be of
different architectures.
2. Terminals. Terminals provide an interface to the users for accessing the system
resources. A terminal may either be an ordinary X terminal or a personal computer or
workstation running X windows. When a personal computer or a workstation is used as
a terminal, the processes that require intense user interaction (such as command
interpreters andeditors) areexecuted atthe terminals. Mostapplications, however,donot

646 Chap. 12 • CaseStudies
interact much with the user and are run on one or more of the CPUs of a processor
pool.
3. Specialized servers. Specialized servers are machines for running dedicated
processes with unusual resource demands. For example, it is natural to run a file server
process on a machine having one or more disks and a print server process on a machine
having one or more printers. The main servers are directory, file, and block servers,
database servers, and boot servers.
4. Gateways. Gateways are used to link two or more Amoeba systems over wide
area networks into a single, uniform system.
Software Architecture
The Amoeba software consists of the following principal components:
1. A microkernel.The microkernel that runs on all machines of an Amoeba system
provides low-level memory management support, manages processes containing multiple
threads, supports interprocess communication, and handles low-level I/O.
2. A collection of servers. All other services (functions not included in the
microkernel) areprovided by user-level processes called servers. Servers provide mostof
thetraditional operating system functionality. Servers aretypically written bythesystems
programmers, but users are free to write their own servers if they wish. Some standard
servers inAmoeba are the bullet server, which manages files; the directory server, which
handles naming of files and other objects; the replication server, which takes care of
automatic replication of objects; and the run server, which decides which process should
run on which processor.All standard servers have stub procedures in the library. To use
a server, a client normally just calls the stub, which marshals the parameters, sends the
message, and blocks until the reply comes back. This mechanism hides all the
implementation details of a server from its clients.
Amoeba isanobject-based systeminwhichtheentire software isstructured asobjects.An
object is like an abstract data type that consists of some encapsulated data with certain
operations defined on it.Amoeba objects are passive in nature. That is, they cannot do
anything on their own. Therefore, each object is managed by a server process that
performs the operations defined on the objects it manages. Typical examples of objects
supported in this manner are files, directories, memory segments, screen windows,
processors, disks, and tape drives.
Object Naming and Protection
Each object in Amoeba is both identified and protected by a capability. As Figure 12.2
shows, a capability inAmoeba is composed of the following fields:

Sec.12.2• Amoeba 647
Number
ofbitsI 48 24 8 48
I
Fields Serverport Objectnumber Rights Check
Fig.12.2 AnArnoehacapability.
1. Serverport.This isa48-bit logical address that identifies the server thatmanages
theobject referred to bythecapability. InAmoeba, theonly way aservercan beaccessed
isviaitsport. Itmay be noted here that aserver port isalogical address thatisassociated
not with a specific machine but with a particularserver (or a set of servers providing the
same service). Therefore, when a server isrelocated on anew machine, ittakes its server
port with it. A server can choose its own port address.
2. Object number. A server typically manages several objects of the same type. For
example, a file server usually manages hundreds of files. The object number field is a
24-bit identifier used by the server to identify the specific objectinquestion from among
the objects managed by it. The server port and object number fields together uniquely
identify an object in the entire system.
3. Rights. This 8-bit field is a bitmap telling which of the allowed operations the
holder of this capability can perform on the object identified by the capability. The
meaning ofthisfield isdifferent foreach object typesince thelegaloperations themselves
also vary from one object type to another.
4. Check.This field isused for validating thecapability. Itcontains a48-bit number
that is used to protect the capability against forging.
Capabilities are managed entirely by user processes and are protected crypto
graphically. In particular, to create an object, a client sends a request message to the
appropriate server.The server thencreates theobject and returns acapability totheclient.
This capability iscalled theownercapability, and all itsrights bits are initially on. When
creating the capability for the newly created object, the server picks a 48-bit random
number and stores itboth in the check field of the capability and also inits own internal
table. The client must send thiscapability along with any request foranoperation onthis
object. Before performing therequested operation, theservercomparesthecontents ofthe
check field inthecapability withthatstored initsinternal table forvalidating thesupplied
capability.
When the owner of an object wants to pass restricted access rights for the object to
other users,itsendsamessagetotheserverrequesting tocreate acapability withrestricted
rights.This message, among other things, contains theowner capabilityfortheobject and
a bit mask for the new rights. The server EXCLUSIVE-DRs the new rights with the
original value of the check field from its internal table and then encrypts the obtained
=
result (x) by using a one-way function [y !(x)], which has the property that given x it
is easy to find y,but given y,x can only be found by an exhaustive search of all possible

648 Chap.12 • CaseStudies
valuesofx.Theserver thencreates anewcapability whoseserverportandobject number
fields havethesamevaluesastheownercapability fortheobject, therights fieldcontains
the new rights bits, and thecheck fieldcontains theoutput of the one-way function. This
newcapability isreturned tothecaller, which thensendsittoanother process forpassing
to it restricted access rights for the object.
It may benoted here that the capability-based naming scheme of Amoeba is fully
location transparent because to perform an operation on an object, it is not necessary to
know the location of theobject or the location of the server that manages the object. The
protocol for talking to a server, whether local or remote, is identical in all cases. Thus a
client is entirely concerned with what it wants to do, not where objects are stored and
where servers run.
Thecapability-basedprotection schemeofAmoeba hastheadvantage thatitrequires
almost noadministrative overhead. However, notice that it ispossible tocreate an object
inAmoeba and then lose itscapability.Therefore, some mechanism is needed to identify
and destroy those objects that are no longer.accessible, For this, each server inAmoeba
periodically runs a garbage collector, which removes all objects that have not been used
in n garbage collection cycles. Furthermore, in an insecure environment, to keep
capabilities from getting disclosed to intruders on the network, additional cryptographic
measures (e.g., link encryption) will be necessary [Tanenbaum 1995].
Server-Locating Mechanism
Toperform anoperation on anobject, aclient process presents acapability for theobject
to the system. The system must forward this request to the appropriate server for
performing theoperationontheobject.However,noticethat,although theserverportfield
in the capability specifies the server that manages the object, it does not carry any
information about the whereabouts of theassociated server process. Therefore, a locating
mechanism is necessary to find the location of the appropriate server.
The mechanism used inAmoeba for locating a server isbased on broadcast queries.
That is, forlocating aserver,amessage "where are you" thatcontains theportaddress of
the server is broadcast on the network. The location of the server is known only after a
reply for the broadcast message is received from the server whose port address was
contained in the broadcast message. The reply message contains the network addressof
the server.The kernel doing the broadcasting records the (port, network address) pair in
a cache for future use. Therefore, broadcasting is done only when a server's portis not
found in the cache.
Name Resolution Mechanism
Capabilities are low-level, system-oriented object names. They are hard for people to
rememberanduse.Therefore, objectsinAmoebaalsohavehuman-orientedASCIInames.
The process of mapping a human-oriented object name to its system-oriented name
(capability) is known as name resolution. In Amoeba, directory servers are used to
perform the name resolution operations. Directory servers manage directory objects for
this purpose. In its simplest form, a directory is a set of (ASCII name, capability) pairs.

Sec.12.2• Amoeba 649
Lookup, enter, anddelete arethethreebasicoperationsallowedondirectoryobjects.The
firstoperationlooksupanobject'sASCIInameinadirectoryandreturnsitscapability.
The other two operationsare meantfor enteringand deletingentries from a directory.
Since directories themselves are objects, a directory can contain capabilities for other
directories,thus allowingusers tobuildhierarchicaldirectorytrees and othermore general
graph structures [Mullender et ale 1990].
The directory-based object-naming scheme of Amoeba is very flexible and can be
used to implementsharingof objects in variousdifferentways. For instance, considerthe
following examples:
1. Differentusers of asharedobjectcan use their own suitableASCIInamesfor the
object. For this, each user can enter his or her own (ASCII name, capability) pair in a
directory. Note that the (ASCII name, capability) pairs of different users need not
necessarilybeenteredinthesamedirectory.They canbeplacedintwoormoredirectoriesto
createmultiple links to theobject. Moreover, the capabilitiesfor differentusers may have
different rights so that different users have different access permissions for the shared
object.
2. Objects shared among a group of users can be stored in a directory whose
capability isgivenonly to the group members, makingthe directory and hence the shared
objects accessible only to the group members. In this manner, a directory capability can
serve as a capability for many other capabilities [Mullender et a1. 1990].
3. Instead ofimplementing a directory as a two-column table, each row of which is
an (ASCII name, capability) pair, a directory can be implemented as an (n+l)-column
table with ASCII names incolumn 0and capabilities in columns 1through n [Mullender
et al. 1990]. In this generalized model, each of the n capabilities of an object forms a
different protection domain, having different access rights for theobject. For example, a
directory may have one column for the owner capability, one for the owner's group
capability, and one for public capability, to simulate the UNIX protection scheme
[Mullender et al. 1990].
Amoebadesignersfelt that thedirectory serviceisacriticalcomponentin the system
because almost all applications depend on it for finding the capabilities they need. If the
directory servicefails, the entire system will stop functioning. Therefore, in orderthat no
single-site failure can cause the directory service to fail, it has been implemented in a
fault-tolerant way. In particular, directory servers come in pairs, each with its own array
ofcapability pairs (on differentdisks), to preventloss ofinformationif the disk ofone of
the directory servers is damaged. The two servers communicate to keep synchronized.
12.2.4 Process Management
Process Model
A process in Amoeba consists of a segmented virtual address space and one or more
threads. Therefore, the process abstraction in Amoeba is realized by three kinds of basic
objects-process, segment, and thread. They are briefly described below.

650 Chap. 12 • Case Studies
1. Process. A process is defined by its state. Each process has aprocessdescriptor
associated with it that defines its state at any instance of time. The four main parts of a
processdescriptorarethehostdescriptor,thecapabilities,thesegmentdescriptors,and the
thread descriptors.
The host descriptordescribes the processor type(s) suitable for running the process
and the memory requirements of the process, Processor type(s) can be specified as a
particularCPU architecture, agroup ofCPUs, or apredefinedclass ofCPUs. The process
can be run only on a processor whose processor type matches the processor type(s)
specifiedintheprocess'shost descriptorand which hassufficientmemoryforrunning the
process.
The capabilitiespart mainly consistsofaprocess capability and ahandlercapability.
Every client manipulating the process must possess the process capability. On the other
hand, the handlercapability is needed tocommunicate theexit status ofthe process toits
owner. That is, when the process terminates or is stunned (process states are explained
later), the handlercapability is used to do an RPC with the ownerprocess to report this
event.
The segmentdescriptorspart describes the segmentsoftheprocess. It has asegment
descriptor for each segment in the process's address space. A segment's descriptor
contains information such as its virtual address, its length, and its access control.
The thread descriptors part describes the threads of the process. It has a thread
descriptor for each thread in the process. Among other things, a thread's descriptor
contains the thread's program counter and stack pointer. The exact contents ofa thread
descriptor-is CPU architecture dependent.
2. Segment. Asegmentisanamed linear section ofmemory.Aprocesscanhave one
or more segments. The numberofsegments can keep changing during the lifetime ofthe
process as it executes.
To create a segment, a process makes a request to the kernel specifying the initial
size of the segment and optionally specifying a segment or a file whose contents
should be used as an initial value of the segment. The kernel then allocates the
necessary amount of memory and returns to the requester a capability for the newly
created segment. This capability is used by the requester to perform any subsequent
operation on the segment. The initial size of the segment may change during process
execution.
System calls are provided to allow processes to create, read, write, map, unmap,
and destroy their segments. Of these, the map and unmap operations can be used for
various different purposes. For instance, they can be used by a process to add new
segments to its address space by mapping them and to remove segments from its
address space by unmapping them. An unmapped segment remains in memory (but is
not a part of theaddress space of any process) and can be either read or written like
a file or can even be remapped to any part of the same process's address space or to
a different process's address space. For this, the unmap operation returns a capability
for the segment. Due to this feature, the map and unmap operations can also be used
for interprocess communication; the sender process unmaps a segment of its address
space and passes the capability returned by the unmap operation to the receiver

Sec.12.2• Amoeba 651
process, which then maps the segment to its own address space. Finally, these two
operations can also be used to allow two or more processes to operate on shared
memory by simultaneously mapping a segment into the address spaces of all these
processes.
3. Thread. A process has one or more threads, all of which share the process's
address space. Each thread has its own stack and stack pointer, its owncopy ofthe CPU
registers, and its own set of"glocal" variables. A thread's glocal variables are global to
all its procedures but are not accessible to otherthreads. Amoeba has library procedures
to allow threads to create and use glocal variables.
A process can create new threads and can terminate existing threads as it executes.
Hence, the number of threads in a process can keep changing during its lifetime. The
parametersto the call for creatinga new threadspecifythe initial procedureto be run and
the size ofthe initial stack.
Threads are managed by the kernel, and a priority-based threads-scheduling scheme
is used in which kernel threads have higher priority than user threads.
The three different mechanisms that may be used for threads synchronization are
signals, mutex variables, and counting semaphores. Signals are used to send an
asynchronous interrupt from one thread to another in the same process. Mutex variables
are like binary semaphores, and they have the same purpose as described in Chapter 8.
Counting semaphores are useful for more general synchronization requirements that
cannot be handled by using mutex variables.
InAmoeba, a processcreates anew (child) processby first makingaCreateSegment
request three times for the three segments (text, data, and stack) of the child process and
gets back one capability for each segment. It then performs write operations for each
segmenttofilleach one with that segment'sinitial data(this may not bedone asaseparate
step if the pointer to the initial data for each segment is provided as a parameter in the
CreateSegment requests). Finally, the process does a MakeProcess request with the
capabilities ofthe child's segments as parameters. Makel'rocess creates the child process
and returns acapability for it.The parentprocesscontinuestoexecute inparallel with the
newly created child process. It can use the child process's capability to suspend, restart,
or destroy the child process. Note that a process can create an arbitrary number of
children, which in turn can create their own children. This flexibility allows the creation
ofan arbitrarily large tree of processes.
At any time, a process is in one oftwo states-running or stunned (suspended). A
running process can be stunned by executing a library procedure called stun, with the
capability of the process as a parameter. A process can be stunned directly by its parent
or by any other process that has a capability for the process with rights to stun it. For
example, a process being interactively debugged switches between running and stunned
states. For this, the parent gives the process's capability to the debugger process, which
can then change the process's state as it likes during interactive debugging. When a
process is in the stunnedstate, for all attemptsto communicate with the process, the low
levelcommunicationprotocols in the kernel respond with a message saying "thisprocess
is stunned."

652 Chap. 12 • CaseStudies
Choosing a Processor for a NewProcess
Intheprocessor-poolmodel ofAmoeba, when anew process hastobecreated,the system
must choose from the pool the most suitable processorfor runningthe process. There are
special servers called runserversto make this decision. Each run servermanages one or
more processorpools. For simplicity, letusassume thateach run servermanagesonly one
processor pool.
When the shell wants to run a program, itdoes an RPC, with the run serversending
to it the process descriptors for the program and asking it topick a processorfor it from
thepool. The run serveruses the following method toselect aprocessorfor executingthe
program:
1. The process descriptors contain information about the CPU architectures on
which the corresponding process can run. Therefore, the run servertakes the intersection
of the CPU architectures in the process descriptors and the CPU architectures in the
processor pool that it manages. To facilitate this, a processor pool is represented by a
directorycalledapooldir, which containssubdirectoriesforeach oftheCPU architectures
in the pool. The subdirectories contain capabilities for accessing the process servers on
each ofthe CPUs in the pool. The processors that result from the intersection are chosen
as possible candidates for running the process.
2. Next, the run server checks to see which of the selected processors have
sufficient memory for executing the program. For this, the run server maintains a table
containing the speed, current load, and amount of currently free memory for each
processor. The information in this table is continuously refreshed by executing a
library procedure called getload for each processor in the pool. Those processors that
do not have enough memory for the program are removed from the set of possible
candidates.
3. Finally, for each of the remaining processors, the run server estimates the
computing power that the processor can devote to the new program. For this, it uses a
heuristic that takes as input the known total computing power of the processor and the
number of currently active threads running on it. The processor that can deliver the
maximumcomputingpowerisfinally selectedasthemostsuitable candidateforexecuting
the program.
The run serverreturns the capability of the process serverofthe selected processor
to the caller. The caller then uses this capability to create the process on the selected
processor.
Process Migration Mechanism
Although Amoeba does not currently support process migration, the designers have
thought about it, and it has been mentioned in the literature that a process migration
mechanism for migrating a process from its old node to a new node can be easily
implemented in the future in the following manner [Mullender et al. 1990]:

Sec.12.2 • Amoeba 653
1. At first, the process server at the old node of the migrant process will stun it.
2. It will then pass the process descriptor of the process to the process server at the
new node in a RunProcess request.
3. Theprocess serveratthenewnodewillthenfetchtheaddressspaceoftheprocess
from the old node by making a series of ReadSegmentrequests.
4. After this, it will start the process atthe new node with a CreateProcess request.
With this, the process starts executing on its new node.
5. The process server of the new node then returns a "success" reply for the
RunProcessrequest madebytheprocess serveroftheoldnode.Onreceiving this
reply, the process server at the old node will delete the process by making a
DeleteProcess request to the kernel. With this, the migration of the process will
complete.
Note that while the migration ofaprocess isinprogress, itisfrozen(stunned) on its
old node.Therefore, processes communicating withthe process willreceive "this process
is stunned" replies to their attempts until the process on the old node iskilled.They will
then get a "process not here" reply for their attempts to communicate with the deleted
process. Asaresult ofthis reply,asender willstartalocate operation tofindtheprocess.
After itfinds theprocess on its new node, itcan resume communication withtheprocess
in normal fashion.
Amoeba is designed to support multiple file systems at the same time. It has a built-in
standard filesystem, butthose users whodonot likeitarefreeto writeandusetheirown
file system. Therefore, in the same Amoeba system, files of different users may be
managed by different and incompatible file systems. The main reason for this flexibility
is that a file system inAmoeba runs as a collection of server processes. Adescription of
the standard file system of Amoeba is presented below.
ThestandardfilesystemofAmoebaiscomprisedofthefollowingtypesofservers:
1. Bullet server. It deals with the storage and manipulation of files.
2. Directory server. It takes care of object naming (including files) and directory
management.
3. Replication server. It handles automatic replication of objects (including files)
managed by the directory server.
A description of t.he directory server has already been presented in Section 12.2.3.
Therefore, only the bullet server and the replication server are described below.
Bullet Server
The bullet server is so called because it was designed to be very fast. In fact, high
performance and simple design werethe twomain goals inthedesignofthestandard file
system of Amoeba. The following design decisions were made to achieve these goals:

654 Chap. 12 • Case Studies
1. Useofimmutablefiles. A file cannot be modified once ithas been createdexcept
to be deleted. Therefore, the bullet server supports only three principal file operations
read,create, and delete. To modify an existing file, a client makes a readrequest to the
bullet serverspecifyingitscapability. Inreply tothisrequest, theserversends totheclient
the entire file that is loaded into the client's memory. The client now performs all the
modifications on the local copy ofthe file. It then makes a create request to the bullet
server, sending a pointer to the memory location that contains the modified file as a
parameter. The server creates a new file and returns a new capability to the client. Either
theoriginalfilecanbeleftfor useasabackup copy ortheclient can make adeleterequest
to the bullet server to destroy it.
Notice from the description above that for an immutable file a file modification
typically requires a client to perform three RPCs (one each for read, create, and delete
operations)with thebullet server.This makes filemodificationscomplicatedand slow.To
solve this problem, the bullet server supports two kinds of files-uncommitted and
committed.An uncommittedfile is one that is in the process ofbeing created and can be
directly modified by sending modification requests (such as to change, insert, or delete
bytes) to the bullet server. When all the changes have been completed, the file can be
committed, at which point it becomes immutable. Notice that an uncommitted file is
transient in nature and hence cannot be read. On the other hand, a committed file is
permanent in nature. It can beread but cannot be modified because it is immutable. A
create request must specify whether the file is to becommitted immediately or not. In
eithercase, acopy of the file is made at the server and acapability for the file is returned
to the client.
2. Storing files as contiguous byte strings. The final size of an immutable file is
always known atcreationtime because itcannot bemodifiedafter itscreation. Due tothis
property, files in Amoeba are stored contiguously, both on the disk and in the main
memorycache. Althoughthis strategy wastes space due toexternal fragmentation, both in
memory and on disk, it helps in achieving the goals of high performance and simple
design. This isbecause acontiguouslystored filecan beread into memory inasingle disk
operation and can be sent to users in a single RPC reply message. Furthermore, in this
strategy, afilecan besimply representedbyaninitial address andlength, which simplifies
the administration ofstorage space, both in memory and on disk.
As Figure 12.3shows, the bullet servermaintains afile table that isentirelymemory
resident while the bullet server is running. The entire file table is loaded into memory
when the bullet server is loaded.
The file table has one entry for each file being managed by the bullet server. The
entriesare indexed by a file numberfield. Since the initial address and size are sufficient
to represent a file, each entry ofthe file table also has the following three fields:
• A length field that has the file size.
• A disk address field that contains the initial address ofthe file on disk.
• Amemoryaddress field. Ifthefile iscurrently present inthemain-memorycache,
this field contains the starting address ofthe file in main memory.

Sec. 12.2 • Amoeba 6SS
Disk File Other Memory
addressnumber Length fields address
0 LO Null
L1
2 L2 Null ::n
:n CD
3 L3 CD
CN
Disk Filesin
cache
Filetable
Bulletserver'smemory
.Fig.12.3 Bulletserver implementation.
In additionto these fields, there are otherfields containing informationthat are used
for access control and administration purpose.
To read a file, a client does an RPC with the bullet server, sending it the capability
for the file. The server then extract.sthe file number from the object number field ofthe
capability and uses it to reach at the appropriate file table entry. Each file table entry also
has a field containing the random number used in the check field of the capability. This
information is used to validate the capability. If the capability is found to be valid, the
server first checks the information in the memory address field ofthe file table to see if
the file is currently present in the main-memory cache. If not, the server next uses the
information in the disk address and length fields of the file table to fetch the entire file
from the disk into the main-memory cache. Finally, it returns the entire file to the client
as a reply of the RPC message, The least recently used (LRU) scheme is used to manage
the cache space.
If a file's capability is lost, it will remain forever in the system as an inaccessible
file. Uncommitted and committed files are handled differently to prevent this situation.
For an uncommitted file, if ithas not been accessed for 10 minutes, it is simply
deleted and the corresponding entry in the file table is freed. If a request for accessing
the deleted file is received after the deleted file's entry is reused for another file, the
check field will detect the fact that the file has changed, and the operation on the
already deleted file will be rejected. This approach is based on the designers'
assumption that files normally exist in the uncommitted state for only a few
seconds.
For committed files, a garbage collector is run periodically, removing all files
that have not been used in n garbage collection cycles. This idea is implemented as
follows. The tile table entry has a counter field for each file that is initialized to a
predetermined constant MAX_I__IFETIME. Now the following two calls are made
periodically to detect and remove committed files that can never be accessed:

656 Chap. 12 • Case Studies
1. Age. The age call starts a new garbagecollectioncycle and decrements the value
ofthe counterfield ofeach file table entry by 1.Any file whose counter value becomes
zero isdeletedand thecorrespondingdisk space, cachespace (ifany), and file table entry
are freed.
2. Touch. The touch call prevents those files from being removed that are in use.
Unlike age, which applies to all files, touch is for a specific file. When called for a
particular file, it resets the value of the counter field of that file's entry to MAX_
LIFETIME. Touch is calledperiodically for all files listed in any directory, to keep them
from getting deleted by the age call.
Replication Server
Thereplicationserverhandlesautomaticreplicationofobjects(includingfiles) managedby
thedirectoryserver.Italwayskeeps runninginthebackgroundandusesthelazyreplication
approach for creating object replicas. It scans specified parts of the directory system
periodicallytocheckifreplicacreationforany objectinthedirectorysystemisneeded. In
particular,ifthereisadirectoryentry that issupposedtocontainncapabilitiesbutcontains
only m(m<n), then n- madditionalcopiesofthecorrespondingobjectmust becreated.
Therefore, when the replicationserverfinds such adirectoryentry, itcontactsthe relevant
serversandarrangesforthecreationofadditionalreplicasofthecorrespondingobject.
In addition to creating object replicas, the replication server is also responsible for
garbage collection. That is, it periodically makes a touch call for each object in the
directory systemto refresh their lifetimeso that they win not be removed by the garbage
collector. Periodically,italsosends theage messagetotheservers ofdifferentobjecttypes
tocausethem to make the age call todecrementthe value ofthecounterfield for all their
objects and to remove those objects whose counter value reaches zero.
12.2.6 Int.rproc.ss Communlcetlon
Basic fPC Mechanism
Amoeba uses the client-server model for operations on objects. That is, client processes
send requeststoserverprocesses(objectmanagers)forcarryingout operationsonobjects.
A server accepts a client's request message, carries out the requested operation, and
returns a reply to the client. Each standard server is defined by a set ofstub procedures
thatclientscan call.The stubprocedurespreciselydefinethe servicesand their parameters
that the server provides. When a stub procedure is called by a client, it packs the
parametersinto amessageand invokes thekernel primitivestoactuallysend themessage.
The kernel provides the following three basic IPC primitives:
• trans is used by a client to send a request message to a server for getting some
work done by the server.
• get_request is used by a server to announce its willingness to accept messages
addressed to a specific port.
• send_reply is used by a server to send a reply to a client.

Sec. 12.2 • Amoeba 657
Although these primitives are actually related to message passing, the procedural
interface provided by stub procedures makes the basic IPC mechanism appear as RPC
(Remote Procedure Call) to the programmers. Therefore, the basic IPC mechanism of
Amoeba is also referred to as RPC. This RPC mechanism has the following properties:
1. Itsupports only synchronoustypeofcommunication.That is,aftermakingatrans
call, a client thread blocks until the corresponding reply comes back from the called
server. Similarly, after making a get_request call, a server goes to sleep, waiting for an
incoming request message. The server is unblocked only when a request message
arrives.
2. Messages are unbuffered. Hence a message is simply discarded if its receiver is
not in a state ready to reeeive it. In this case, the sending kernel will time out and
retransmit the message. Flexibility is provided to the users to specify the maximum
duration for retransmissions, after which the kernel should give up and report failure.
3. It supports at-most-once semantics. That is, the system guarantees that an RPC
will never be carried out more than once, even if the server crashes and is rapidly
rebooted.
4. Stateless servers are used.Therefore, each RPC iscompletely self contained and
does not depend on any previous information stored in the server's memories.
Group Communication
In addition to the RPC mechanism, Amoeba also supports group communication facility.
A group consists of multiple processes that cooperate to provide some service. Amoeba
uses the concept of closed groups. That is, the information regarding the size and the
member processesofagroup arenotknown toprocesses outside thegroup.Therefore, for
accessing a service provided by a group, a client process simply performs an RPC with
one of its members. That member then uses group communication within the group to
service the client's request in cooperation with other members of the group.
Processes can join and leave a group dynamically and can be members of multiple
groups at the same time. The system provides primitives for creating a new group,
allowing aprocess todynamically join orleave agroup,sending amessagetoallmembers
of agroup, and receiving a message from agroup. The group communication mechanism
of Amoeba has the following properties:
1. Itensures ordered delivery of messages. That is, if two processes send messages
to agroup almost simultaneously, the system ensures thatall group members willreceive
the messages in the same order.A sequencerprocess is used for properly sequencing the
messages received by a group. The sequencer process is chosen by using an election
algorithm.
2. It ensures reliable delivery of messages. That is, when a user process broadcasts
a message, the message is correctly delivered to all members of the group, even though
the hardware may lose packets. The basic mechanisms used to ensure reliable message

658 Chap. 12 • Case Studies
delivery aretimeout-basedretransmissionsforretransmitting lostmessages; useofunique
message identifiers to detect duplicate messages; and the useof a history buffer to store
messages for which acknowledgments have not yet been received.
3. Itcan withstand the loss of an arbitrary collection of k processors (including the
sequencer), wherek(thedegreeofresilience) isspecified bytheuserasaparameterinthe
primitive for creating a group [Tanenbaum 1995].The larger is the value of k,the more
redundancy isrequired, andtheslower thegroup communication becomes. Therefore, the
user must carefully choose the value of k.
Further details of the group communication facility of Amoeba may be found in
[Kaashoek and Tanenbaum 1991].
Low-Level Communication Protocols
Both RPC and the group communication facilities of Amoeba use a custom protocol,
called FLIP(FastLocalInternet Protocol), foractualmessage transmission. Thedetailsof
FLIP have already been presented in Chapter 2 and hence win not be repeated here.
FLIP is used in Amoeba for achieving high performance. However, there are
occasions when Amoeba users need to use the standard TCP/IP instead of FLIP. For
instance, TCP/IP must be used to communicate with X terminals, to send and receive
mails to non-Amoeba machines, and to interact with other Amoeba systems via the
Internet [Tanenbaum 1995].To facilitate communications of these types, Amoeba has a
TCP/IP server. For TCP/IP-based communications, a client performs an RPC with the
TCPIIP server giving it a TCP/IP address. The TCPIIP server establishes a connection
with the desired process and returns a capability to the client process. This capability
allows the client to use the established connection. Once the connection is established,
subsequent RPCs between the two processes can be done without the Amoeba process
having to know thatTCP/IP is being used.
Communication Security
Ensuring Genuine Clients. Amoeba has two levels of protection to ensure the
genuinenessofaclient-portsforprotectingaccesstoserversandcapabilitiesforprotecting
accesstoindividualobjects[Mullenderetal.1990].Thatis,theknowledgeofaserver'sport
andthepossessionofavalidcapabilityistakenbythesystemassufficientevidencethatthe
senderhasarighttocommunicate withtheserverandtoperformoperationsallowedbythe
capability on the object identified by the capability. The ports used by an ordinary user
processwill,ingeneral,beknownonlytotheprocessesthathavetherighttocommunicate
withtheuserprocess.However,theportofaserverprocessprovidingsomepublicservice,
suchas a fileserver,will beknown toall users. Therefore, forpublic servers, capabilities
serveasthebasicmechanismtoensurethegenuinenessofaclient.
Ensuring Genuine Servers. Since ports for public servers are known to all
users, itiseasy foranintruder toimpersonateaserverjustbydoing agetrequestonthe
server's port. A one-way encryption technique is used inAmoeba to solve this problem.

Sec.12.3• V-System 659
In this approach, each port is defined as apair of ports, get-port and put-port, related by
=
put-port F(get-port). The function Fis a publicly known one-way encryptionfunction.
Therefore, it is easy to compute put-port for a given get-port, but finding get-port for a
given put-port is not practically feasible. Here, F need not be the same one-way function
as that used for protecting capabilities since the two concepts are unrelated.
As apart of its creationprocess, aserverchooses aget-portthat itkeeps secret with
it.Itcomputes the corresponding put-portand makes itpublicly known. When the server
is ready to accept client requests, it makes a get_request (get-port, ...) call. The kernel
computes the corresponding put-port and stores it in a table of ports being listened to.
When a client wants to send a message to the server, it makes a trans (put-port, ...) call.
Thekernel oftheclient machinethensendsamessage containingput-port inaheaderfield
to the server. On the server side, the kernel compares the put-port in the message header
totheput-ports initstable foramatchandthenforwards themessage tothecorresponding
server.
This scheme ensures that only genuine servers can receive client requests because
get-portsnever appear onthenetwork andcannotbederivedfromthepublicly known put
ports.
12.3 V-SYSTEM
V-System is a microkernel-based distributed operating system designed for a cluster of
workstationsconnectedbyahigh-performance network. Itstarted asaresearch project on
distributed systems at Stanford University under the direction of David Cheriton. It is
basically an outgrowth of the experience acquired with an earlier system called Thoth
[Cheriton et al. 1979]. Its development was motivated by the growing availability and
functionality of relatively low-cost, high-performance workstations and local area
networks.
V-Systemhas been inactive useatStanfordUniversityforthelastseveral years.The
V-System environment. at Stanford mainly consists of a collection of powerful Sun and
VAX workstations connected by Ethernet. V-System is also distributed under license by
Stanford and is in use at several other universities, research laboratories, and companies.
The following description of V-System is based on [Cheriton 1984, 1987, 1988,Cheriton
et al. 1990, Berglund 1986].
12.3.1 Design Goals Qnd MainFeatures
V-System's design was influenced by the research and design goals mentioned below.
High-Performance Communication
V-System designers were of the opinion that high performance of interprocess
communication is necessary in a distributed system for better performance and simpler
design ofresulting applicationsystems and also fortrue network transparency. Inasystem
with fast communication facility, better performance of application systems is obvious.

Chap. 12 • Case Studies
Simplerdesign ofapplication systems·is due to the reasonthat there is no need to highly
optimize the use of communication to prevent performance degradation. Finally, true
network transparency can be achieved because fast communication allows resource
accessing without concern for location.
Fast interprocesscommunicationinV-System isachievedbythe following [Cheriton
1988]:
1. Using relatively simple and basic interprocess communication primitives
2. Using atransportprotocolcalledVMTP(Versatile MessageTransactionProtocol)
that is carefully designed to support these primitives
3. Optimizing for the performance-critical common cases
4. Internally structuring the kernel for efficient communication
Uniform Interface and Protocol
.V-System designers were also of the opinion that the protocols and interfaces, not the
softwaremodules,define the system. That is,adevice-and network-independentuniform
interface and protocol can be defined to build an open-system architecture. Thus,
V-System designers mainly focused their work on designing the protocols and interfaces.
The result was a set of protocols for data transport, naming, 110, remote execution,
migration, and so on, which provides a basis for standardization.
Relatively Small Kernel
Anotherimportantdesign goal ofV-System was to have arelatively small kernel that can
provide a software backplanefor distributed systems analogous to what a good hardware
backplaneprovidesforhardwaresystemsand toimplementtherestofthesystematauser
level inamachine-and network-independentfashion. Consequently,most ofthe facilities
found in traditional operating systems, such as a file system, resource management, and
protection, are provided in V-System by servers outside the kernel.
High Performance
High performance was also an importantgoal in V-System becausethe designers were of
the opinion that no one will use a slow system, independent ofits elegance. Someofthe
important techniques used in the design of V-System for high performance are as
follows:
1. Making the interprocess communication fast by already mentioned methods
2. Allowing application programs to make use of concurrency by using multi
threaded processes
3. Maintaining a name prefix cache for each program for efficient resolution of
object names

Sec.12.3 • V-System 661
4. Using the problem-oriented approach for shared-memory coherence that imple
ments a relaxed form of consistency to improve performance, with the potential
inconsistencies handled in an application-specific fashion
Network Transparency
Similar to other distributed operating systems, network transparency was also a goal in
V-System. To achieve this goal, the naming facility of V-System was based on a three
level model, structured as character-string names, object identifiers, and entity
identifiers.
Support for Conventional Programming Models
Another goal in V-System design was to provide support for conventional programming
models so that applications programmers can access the system services through a set of
procedural interfaces. To achieve this goal, each system-provided service procedure is
made a part of one of the runtime libraries of V-System. When an application process
invokesaprocedure foraccessing somesystem service, ifpossible, theinvokedprocedure
itselfperforms therequestedoperation. Otherwise, itusesthekernel-providedinterprocess
communicationtocommunicatewith theproper service module(s) ofV-Systemtogetthe
operation performed. Ineither case, itis the invoked procedure that returns areply to the
application process, and hence the actual mechanism used to perform the operation is
transparent to the application process.
UNIX Emulation
A relatively new goal of V-System was to provide binary compatibility with a UNIX
system so that existing UNIX programs can be run on V-System. For this, a UNIX
emulation package was later added to V-System. Its details can be found in [Cheriton et
a1. 1990].
12.3.2 System Architecture
Hardware Architecture
As Figure 12.4 shows, the hardware architecture of V-System consists of a collection of
workstations interconnected by a communications network, such as an Ethernet. The
workstations are broadly classified into the following categories:
1. User machines. A workstation belonging to this category supports an interactive
user.It has most of the processing resources for its user in addition todisplay, keyboard,
and mouse.
2. Server machines. A workstation belonging to this category functions as a
dedicated server providing services such as file service, print service, authentication
service, and other kinds of services. Each workstation of this category may provide one

662 Chap. 12 • Case Studies
User
machines
Communicationnetwork
Fig.12.4 Hardware architecture of theV-System.
ormore typesofservices. Forinstance, aworkstation withsecondary storage mayrun the
V-Systemfileserver software andexclusively offerfile service.The kernel's interprocess
communication makes this service and others available in a network-transparent fashion
to all workstations on the network.
SoftwareArchitecture
From a software point of view, V-System architecture mainly consists of the following
layers (Fig. 12.5):
1. The kernel layer. The V-System kernel forms the lowest layer of the software
architecture. Tokeep it small, the kernel isdesigned using a minimalist philosophy.That
is, only those facilities that are just sufficient and powerful enough to build all other
system functions and features are placed in the kernel. Hence, the kernel performs only
such functions as management of lightweight processes, management ofaddress spaces,
and interprocess communication. A separate copy of the kernel executes on each
workstation, andallofthemcooperate toprovide asingle system image attheapplication
process level.
2. Service modules layer.The modules atthislayer usethebasicaccess tohardware
resources provided bythekernel forimplementingvarioustypes ofservices fortheusers.
Forinstance, thefileservermoduleatthislayerimplements aUNIX-likefilesystemusing
theraw disk access supported bythe kernel. Some other service modules of thislayerare
a pipe server that implements UNIX-like pipes, an Internet server that implements the

Sec.12.3• V-System 663
Applicationcode
Runtimelibraries
Servicemodules
User
space
Fig. 12.5 Software architectureof the V-System.
TCPIIP suite, aprinterserver that supports spoolingofprintjobs, andadisplay server that
implements multiwindow facilities using a bitmap display. In this way, V-System allows
many traditional kernel-based functions to be implemented as user-level server
processes.
3. Runtime libraries. This layer implements conventional language or applica
tion-to-operating-system interface. Most V-System applications and commands are
written in terms of these conventional interfaces and are unaware of the distributed
nature of the underlying system. Therefore, the software at this layer causes
V-System to appear to the applications programmers as a set of procedural interfaces
that provide access to the system services. It also allows many programs that
originated in nondistributed systems to be ported to V-System with little or no
modification by simply linking the original source with the runtime libraries at this
layer.
4. Application code layer. This layer contains the code for the application
programs.
11.3.3 Object Naming
The object-naming facility of V-System is based on a three-level model structured as
character-string names, object identifiers, and entity identifiers. The usage and
management of the names and the method of locating an object given its name are
described below.

664 Chap. 12 • Case Studies
Character-String Names
Character-string names are human-oriented names that are mainly used for naming
permanent objects such as files. In V-System, each object is managed by an object
manager that usually implements and manages several objects of the same kind. For
example, a file manager implements and manages several files. Each object manager
maintainsone ormore directories for itsown setofobjects. Adirectoryentry contains an
object'scharacter-string name and apointerto the object. Each directory forms acontext
ofthe name space ofcharacter-stringnames. Therefore,character-stringnames are always
interpreted in some context, and each object manager manages its own contexts.
Each object manager picks a unique global name for a context that it manages and
adds itselfto the name-handling (process) group. A context name acts as a name prefix
for the character-stringnames defined in that context. Because all character-stringnames
are interpreted relative to some context, a (context name, character-string name) pair
forms aqualifiedname that can beused for uniquely identifyinganobject. Both theparts
ofa qualified name may either form a hierarchical or a flat name space.
Toaccess an object, a client process specifies the qualified name ofthe object. The
appropriateobject manageristhen located bymulticastingthename along with theaccess
request to all the object managers in the name-handling group. Each object manager
extractsthecontextname part ofthequalifiedname tocheck ifthe named objectbelongs
to a context managed by it. Only the appropriate object manager then performs the
requested access operation on the named object and responds to the client.
Toreduce the numberofmulticastqueries mentionedabove, each process maintains
a name prefix cache that has context-name-to-object-manager bindings. The cache is
initializedon process initiation toavoid start-up name cache misses. Furthermore, an on
use update mechanism is used for the consistency ofname prefix caches.
The name management approach described above takes advantage ofthe fact that a
name is generally only mapped as part of an operation on the corresponding object.
V-System designers mentioned that, in general, integrating naming with object
management in this way has following advantages [Cheriton and Mann 1989]:
1. It leads to efficiency because when an operation on an object is requested by
specifyingitsname, boththename resolutionoperationandtherequestedoperationonthe
object can be completed in a single set ofmessage exchange, This contrasts with those
systems that use separate name servers, in which case one set of message exchange is
needed for name resolution to locate the object (its manager) and then another set of
message exchange is needed for performing the desired operation on the object.
2. It leads to consistentreliability ofboth the name resolution and object-accessing
operations. That is, in this scheme, an object's name can always be resolved when the
object (its manager) is available for use. This contrasts with those systems that use
separatename servers, inwhichcase itisquite likely thatanobjectisavailableforusebut
cannot be accessed because the name server is down.
3. It leads to simplerdesign because the availability ofall information ofan object
(names, properties, data, etc.) in one servermakes iteasierto maintain their consistency.

Sec.12.3 • V-System 66S
Forexample, when anobject isdeleted, all itsrelated information can bedeleted without
the need to communicate with any other server.
4. The design also allows the automatic application of the security mechanism for
communicating with the object manager and controlling access to information to the
naming operations as well.
Object Identifiers
Object identifiers are system-oriented names. They are used to identify transient objects
such as open files, address spaces, and contexts or directories to avoid the overhead of
character-string name handling andlookup each time anobject isaccessed. Forexample,
a process uses the character-string name of a file at the time of opening it for use, when
the system assigns an object identifier to the opened file and returns the object identifier
to the process. The process uses the object identifier to refer to the file in subsequent
operations on it.
Anobjectidentifierisstructuredasa(manager-id, local-object-idspair.Themanager
id identifies the object manager that implements the object, and the local-object-id
identifiestheobjectamongalltheobjectsmanagedbythisobjectmanager.Themanager-id
partofanobjectidentifier isusedtoefficientlysendaclient's requesttothecorrectobject
managerforoperation ontheobject.Ontheother hand,thelocal-object-idpartisusedby
the object manager to perform the requested operation on the correct object.
Notethatthelifeofanobject(transientobject) identified byanobjectidentifiermust
not exceed the lifetime of the embedded manager-ide This is because the manager-idof
an object manager is invalidated when the corresponding process crashes (an object
manager is assigned a new manager-idon reboot).
When an object manager is replicated or distributed across multiple nodes, the
process group mechanism (described later) is used to identify the entire group of server
instances, implementing the object manager, by a single group identifier. In this case, a
particular server of the group can be addressed by using the coresident addressing
(qualifier) facility of the group addressing mechanism (described later).
Entity Identifiers
Entity identifiers are used to identify processes, process groups, and transport-level
communication endpoints. Therefore, manager-id and group-id are basically entity
identifiers.
Entity identifiers are fixed-length (64-bit) binary values that are host address
independent (host addresses are network or internetwork dependent). Hence, when a
process migrates from one host to another,there is no need to change itsprocess-ide The
mapping of an object's identifier to its host address is done by the kernel. For this, the
kernel uses a cache of such mappings along with a multicast mechanism to query other
kernels for mappings notfoundinthecache. When anentity identifier isused toidentify
a process group, the entity identifier contains an embedded subfield that, when hashed to
a base multicast address, generates the multicast host address for the group.

666 Chap. ]2 • CaseStudies
11.3.4 Process Management
The three basicabstractions used inV-Systemforprocess management areprocess, team,
andteamspace, whichrespectivelycorrespond totheconceptsof"thread,""process,"and
"a process's address space" presented in this book. To avoid any confusion, in the
descriptionthatfollows,thetermsthread,process, andaddress space willbeusedinstead
ofprocess, team, and team space, except in V-Systemcalls.
In V-System, process management activities are mainly performed by two
modules-the kernelprocess serverandtheprogram manager. Each node of the system
has these two modules.The kernel process serverexecutes inside thekernel, whereas the
program managerexecutes intheuserspace. Oneofthegoals ofV-Systemdesigners was
to minimize the kernel process management activities so that the kernel can be kept as
small as possible. Therefore, the kernel process server performs only the basic low-level
process management functions and the rest are performed by the user-level program
manager. The role of these two modules in process creation, destruction, scheduling,
migration, and so on, are described below.
Process Creation and Termination
Threads and processes are dynamically created and destroyed by using the primitives
Createl'rocess, Create'Ieam,andDestroyProcess. Whenathreadiscreated, itiscreated as
part ofthe same process as that of itscreator.When a thread isdestroyed, all the threads
created byitarealsodestroyed. Creation ofaprocessissimilartothecreation ofathread,
except that in this case the newly created thread is created as a separate process and not
asapartofitsparentprocess.Alsonoticethatthereisnoexplicit operation fordestroying
aprocess becauseaprocessisautomatically destroyed whenthelastthreadinthatprocess
is destroyed.
To minimize the kernel's job in the process creation activity, V-System separates
process initiation from address space creation and initialization. Therefore, as far as
the kernel process server is concerned, creation of a new process simply involves
allocating and initializing a new process descriptor. The address space allocation and
initialization is performed by using V-System's virtual memory system, which is
described below.
In V-System,anaddress space isarangeofaddresses, called regions, boundtosome
portion of-anopenfileorauniforminputoutput (VID)object. (AVIDobject corresponds
to an open file inconventional systems.) Accessing a memory address that falls within a
region corresponds to accessing the corresponding data in the open file bound to the
region. The kernel memory server module manages the physical memory as a cache of
pages from open files.Apage faultoccurs whenaportion ofaregion thatcorresponds to
an uncached portion of the bound object is accessed. To handle a page fault, the kernel
maps from the virtual address to a block in the bound VIO or open file and then either
locatesthatblockinthekernelpageframecacheorelsecauses thefaultingprocess tosend
areadrequestrequesting thedatablocktotheserverimplementing theopenfile.File-like
read/write access may also be performed on address spaces by using the standard VID
interface.

Sec.12.3 • V-System 667
Creation and initialization of address space of a process becomes a simple task by
using the virtual memory system. Forthis,anaddress spacedescriptorisallocated andthe
program file of the process isbound to tbis address space. Now as the process references
portions of this address space, theprogram filepages aretransferredand mapped intothe
address space dynamically on demand. Hence, the V-System kernel has no special
mechanism for program loading.
The kernel'sjob inprocesstermination activity issimplified inthefollowing manner.
In V-System, most operating system resources such as open files are managed by user
level server modules. Therefore, there are few resources at the kernel level to reclaim
when a process terminates. Furthermore, when a process terminates, the kernel does not
inform the concerned servers because each server is responsible for keeping track of the
resources it has allocated to a client process and for checking periodically whether the
client.exists, reclaiming the resource if not. For example, the file server has a garbage
collector process that closes files that are associated with processes that do not exist any
more.
Remote Program Execution
In V-System, one of the following primitives may be used at the command interpreter
level to execute a program on a remote machine [Theimer et a1. 19851:
• <program-name> <arguments> @ <machine-name>
• <program-name> <arguments> @ *
In the former case, the specified program is executed on the specified machine, but
inthelattercase, thesystem becomes responsible forselecting anappropriate machine for
executing the specified program. The selection of an appropriate machine isdone by the
method described below.
In V-System, the program managers of all the nodes are grouped into a well-known
program manager (process) group. When a user uses the latter primitive for making a
request for remote program execution, the processor allocation module on the user's
machine multicastsa"remoteprogramexecution request" messagetotheprogrammanager
group. Only program managers whose machines have permission from their users for
remote program execution and sufficient amounts of processor and memory resources
available replytothismessage.Theprocessorallocation modulethenusesapolicytoselect
themost appropriatemachine from thereplies received. Inthecurrent policy,themachine
whoseprogram managerrespondsfirstisselectedsincethisisgenerallytheleastloadedone
andalsobecause thispolicyissimpleandinexpensivetoimplement.
Except fortheselectionofaprogram manager,remote program execution isthesame
as local program execution because processes in V-System are provided with a network
transparent execution environment. Furthermore, the kernel and the program manager of
a machine provide identical services to both locally originated programs and remote
programs executing on this machine.
Initiation of a program execution (either local or remote) involves sending a
request to the appropriate program manager (of the local or remote machine) to create

Chap. 12 • CaseStudies
a new address space and load the image file of the specified program into this address
space. The program manager, in cooperation with the kernel process server and kernel
memory server, sets up the address space and creates an initial process that awaits reply
from its creator. The program manager then turns over control of the newly created
process to the requester by forwarding the newly created process to it. The requester
initializes the new program space with program arguments, default 110, and various
"environment variables." Finally, it starts the execution of the program by replying to
its initial process.
Note that a program can be executed remotely only ifit does not require low-level
access to the hardware devices of the machine on which it originated. That is, the
program should not access such hardware devices as disks, frame buffers, network
interfaces, and serial lines by directly accessing a device server of its originating
node.
Process Scheduling
In V-System, processes are scheduled using a priority discipline. To simplify the task
of the kernel, two-level scheduling is used to allocate the processor of a node to the
processes assigned to that node. A kernel-level scheduler provides a very simple and
efficient priority-based scheduling. It simply takes the highest priority process in the
ready state and allocates it to the processor. On the other hand, a process-level
scheduler that runs outside the kernel manipulates priorities to effectively implement
time slicing among interactive and background processes. V-System gives special
treatment to real-time processes by reserving a number of high-priority levels for
them.
Process Migration
ProcessMigration Policy. InV-System, aprocess maybeselectedfor migration
either by the system or on explicit request by a user. In the former case, processes are
selectedfor migrationbetweenprocessorsbyaperiodicallyinvokedkernel procedurethat
attempts to balance the processing load across the processors. On the other hand, in the
lattercase, alltheprocessescorrespondingtoaprogramaremigratedtogetherbyinvoking
[Theimeret ale 1985]
migrateprog I-n] [<program~name>]
The selection of an appropriate destination node for a migrant process (or group of
processes) is done in exactly the same manner as is done for selecting a machine for
remote execution of a program when the meta machine name "*,, is specified. The
processes ofthe specified program are not migrated if an appropriate destination node
cannot befound for the program. However, ifthe "-n" option is present, the processesof
the program are destroyed if they cannotbe migrated. Furthermore, if the program name
is not specified, an attempt is made to migrate the processes of all remote programs that
are executing on this node.

Sec.12.3 • V-System 669
Process Migration Mechanisms.
1. Addressspace transfermechanism. V-Systemuses thepretransfermechanismfor
address space transmission. This mechanism is described in Chapter 8.
2. Message-forwardingmechanism. The threetypes ofmessages(type 1,type 2,and
type3)tobeforwarded tothemigrant process'sdestination nodearedescribed inChapter
8. V-System uses the mechanism of resending the message to forward messages of all
three types. This mechanism is described in Chapter 8.
3. Mechanism for handling coprocesses. V-Systern ensures that when a parent
process migrates, its children processes will be migrated along with it. Therefore, the
method of disallowing separation of parent and child processes is used in V-System's
process migration to handle coprocesses.
Exception Handling
InV-System,allexceptionsarehandled byauser-level server process called theexception
server.When anexceptioncondition occurs, thekernel causes thefaulting process tosend
a message describing itsproblem totheexception server.The exceptionserver then takes
over and initiates necessary actions todeal with theproblem by using thefacilities of the
kernel andotherhigher levelservers.The mainadvantageofthisapproach isthatitallows
the implementation of a powerful, flexible, and network-transparent exception-handling
mechanism with very little kernel complexity [Cheriton 1988].
12.3.5 Device Management
InV-System,most ofthedevice management activities areperformedbyuser-level server
processes. However, some device support must be provided in the kernel because device
interrupts go to the kernel, some device control operations are privileged, and kernel
control of some device operations is required for kernel integrity [Cheriton 1988].
Therefore, the V-System kernel has a kernel device server module designed to provide
efficient, reliable, machine-independent, and secure access to a wide variety of devices.
The kernel device server provides only minimum device support for a particulardevice
and has device-independentcode that interfaces between the user-level processes and the
driver modules for the individual devices.
The interface provided by the kernel device server is called the UfO interface. It
allows client processes to use the standard 110runtime support for device 110. It also
allows the user-level server processes to implementextended abstractions of devices for
application programmers. For example, the kernel device server provides access to each
disk drive as araw block device, andthe user-level file server implementsfiles using this
basic interface. Similarly, for network connections, the kernel device server provides a
block interface to the Ethernet, providing the ability to read and write raw Ethernet
packets, and the user-level Internet server implements TCPIIP, UDP, and X.25
protocols.

670 Chap. 12 • Case Studies
Device 1/0isaccomplished bycreatinga VIOobject thatisviewed as a sequence of
data blocks that are read or written. The VIO interface defines the syntax and semantics
ofread, write, query, and modify operations that can be performed on VIO objects. With
the supported operations and the use of a block-oriented data access model, the VIO
interface isgeneral enough tohandle awide variety ofdevices such as disk, tape, printer,
network interface, serial line, terminal, and even a mouse.
11.3.6 Int.rproc8ss Communication
Interprocess communication is an important facility provided by the V-System kernel.
Special care has been taken in the design of interprocess communication facility to
provide fast exchange of messages between client and server processes using RPC-like
semantics. The three basic forms ofinterprocess communication provided by the kernel
are communication for fixed-length message transfer, communication for passing access
to a data segment, and multicast communication. These are described below.
Fixed-Length Message Transfer
This form ofcommunication is used by processes to send, receive, and reply to requests
by using fixed-length messages. The three primitives provided for this purpose are Send,
Receive,and Reply.The Sendprimitive is used by aclient process to pass the equivalent
of a procedure argument. On the other hand, the Receive,and Replyprimitives are used
respectively by a server process to receive a client's request and to return the result ofa
request execution to the client. All messages are exchanged in a strictly synchronous
manner,Thatis,theprocess that has sent amessage isblocked awaitingfor thereply until
the messageitsent hasbeen receivedand replied tobythereceivingprocess. On theother
hand, after replying to aclient's request, a serverperforms aReceiveand blocks waiting
for a message to be sent by a client. All these messages are offixed length (32 bytes).
PassingAccessto a Data Segment
In this form ofcommunication,aprocess uses the Sendprimitivetosend apseudopointer
to one of its memory segments (a contiguous range of addresses) in a fixed-length
messagetoareceiverprocess. The segmentsizeandaccess modes arealso specifiedinthe
message. On receiving this message, the recipient process can access this segment for
readingand/orwriting, dependingon the access mode specifiedinthe message, while the
senderis awaiting reply from the recipient. Such reads and writes are handled by kernel
primitives CopyFrom and CopyTo (Fig. 12.6).The receivercan execute these primitives
several times before replying. Furthermore, the receiver may forward the message to
another process, passing to it the segment access and the right to reply. The sender is
blockeduntil itreceives areplyforthemessagethatitsent.Thisfacility allows parameters
to bepassedby reference.
The communication semantics for exchange of messages in the two forms of
communicationdescribedabove isillustratedinFigure 12.6.Notice that in both cases the
senderand receiverinteractin a strictly synchronous manner. Moreover, the receivercan

Sec.12.3• V-System 671
Sender Receiver
process process
Receive
I
Send I
I I I I I
I
I
I
Copyto
Copyfrom
Time
Reply
Blocked state
Executingstate
Fig. 12.6 Communication semanticsforexchange ofmessages intheV-System.
receive and queue multiple messages and can reply to a message when it wants. This
flexibility allows writing of applications that need sophisticated scheduling of message
handling andreplies.
Multicast Communication
The third formof interprocesscommunicationinV-Systemprovides group (one-to-many)
communication facility. V-System supports the notion of a process group (or simply a
group), which is a set of processes identified by a group identifier. A process can
simultaneously belong to multiple groups and can freely join or leave groups.
Any process, including one that isnot a member ofagroup, can senda message toa
group by specifying a group identifier inst.eadof a process identifierin the parameterfor
Send.After sending, the sender may block orcontinueexecutingdepending onwhetherit
selectstoreceive zero,one,ormorethanonereplymessageforthemessagethatithassent.

672 Chap. 12 • Case Studies
The zero..replycase is usedfor making an unreliablemulticastto thegroup becausethe
senderprocessdoesnotblockaftersendingthe message.Thesingle..replycaseisusedfor
reliablemessagedeliverytoatleastonememberofthegroupbecausethesenderisblocked
until it receives one reply message. Further replies are simply discarded without any
indicationofhowmanyotherprocessesreceivedthemessageorrepliedit.Finally,inthe
caseinwhichthesenderselectstoreceivenreplies(n>1),allthereplymessagesstarting
fromthesecondmessageanduptothenthmessagearequeuedinthekernelforthesenderto
retrieveuntilthestartofthe nextmessagetransaction,thatis,the nextSend. Aprimitive
GetReplyisprovidedtoallowthesendertoreceivesubsequentrepliesthatarequeuedinthe
kernel.Thesendingprocessdecideshowlongtowaitforthereplymessages.
In a group Send, a message can also have a qualifier, indicating that the message
should only be deliveredto those membersof the destinationgroup that are coresident
withtheprocessspecifiedinthequalifier.Forexample,tosuspendaprocessP,amessage
containing a requestfor suspendcan be sent to the group of process managers with the
coresident qualifierspecifyingprocessP, therebydeliveringthe request messagetoonly
the manager in charge of process P, not the entire group.The kernel simply routes the
requesttothehostaddressforprocessP. Noticethataqualifierneednotalwaysidentify
asingleprocessbutmayalsobeaprocessgroupidentifier,inwhichcase the messageis
delivered to all members of the destination group that are coresident with the group
specified by thequalifier.
ThemulticastcommunicationfacilityisusedinY..Systeminanumberofways,such
astotransmitclocksynchronizationinformationtothetimeserversinthekernelofeach
node,torequestaswellasdistributeloadinformationaspartofthedistributedscheduling
mechanism, and in the replicated file updateprotocol.
The kernel of Y..System has been carefully structured to minimize the cost of
communicationoperations.ForhandlingIPC,thekernelconsistsoftwomodules-alocal
IPCmoduleandanetworkIPCmodule.WhenasenderprocessmakesaSend calltosend
a message to a receiverprocess,thecall istrappedinto the kerneland processed by the
localIPCmoduleifthereceiverprocessislocal.Otherwise,itisprocessedbythenetwork
fPC module using the YMTP to communicate with the remote kernel and the remote
receiverprocess.It maybenotedherethatfast IPCinY..Systemisachieved notonlyby
usingrelativelysimpleand basicIPCprimitivesbutalsoduetotheuseof YMTP,which
is optimized for request..response behavior. YMTP supports multicast, datagrams,
forwarding, streaming, security, and priority of messages. The details of YMTP have
already been presentedinChapter 2 and hence will notbe repeatedhere.
11.3.7 V-System Servers
Wesaw that the V-Systemkernel provides only minimum functionalitythat serves as a
basic framework for implementingmanytraditionalkernel..basedfunctionsas user..level
serverprocesses.Thisallowstheuserstodesignandimplementtheirownserverprocesses
fortheirapplications.However,thereareseveralservicesthatarecommonlyusedbymost
applications.V-Systemprovidesserversforsuchservicessothattheusersneednotdesign
theirownserversfortheseservices.Theservicesprovidedbysomeofthemostimportant
V-Systemserversare brieflydescribed below.

Sec.12.3 • V-System 673
Program Manager
The program manager (also known as the teamserver) performs most of the process
management activities. It performs jobs that are normally performed by the kernel in
traditional time-sharing operating systems, such as process creation and termination,
process scheduling,andmaintainingresource consumptionstatistics forvariousprocesses.
It also manipulates priorities of processes to efficiently implement time slicing among
interactive, background,andguestprocesses (thosethatoriginatedonsomeother nodebut
are being executed on this node). In addition, it serves as a user-level exception handler
and invokes an interactive debugger on faulting programs. Moreover, the program
managers ofdifferentnodes cooperate for implementingautomatic load-balancingfacility
and for process migration activities.
File Server
The file server performs most of the file management activities performed by the kernel
in traditional operating systems. In particular, it provides an abstraction of the physical
storage devices by allowing the users to store and access data from these devices in the
form of read and write operations on files. The file server usually runs on a dedicated
server machine withmassdisk storage. Since mostworkstationsinV-Systemarediskless,
the file server also provides file access facility for clients on the network.
The fileserver ismultithreadedforefficiency,sothatwhenonethreadblocks waiting
for a disk block, some other thread can be executed. Furthermore, all threads belonging
tothefileserverprocess shareacommon buffercache, whichisusedtokeepheavily used
blocks in main memory.
Internet Server
The Internet server implements the TCP/IP suite on top of the basic network interface
device provided by the kernel device server. Ithas amultithreaded, modular structure for
efficiency and flexibility. Moreover, for betterperformance, it makesdirect useofseveral
kernel facilities such as real-time scheduling, accurate timing, and fast IPC. 'Itis not
permanently configured in the standard system and is loaded only when required.
Printer Server
The printer server spools print files. In general, print files are submitted to the printer
server byusing theIPCfacilityandVIOinterface ofV-System.However,whentheprinter
server runs an instance of the Internet server, print files can also be submitted by using
TCP connections.
Display Server
The display server implements multiwindow facilities using a bitmap display. It also
provides a high-level graphics representation at the client interface, allowing commonly
used operations to be performed local to the display server, rather than relying on

674 Chap. 12 • CaseStudies
applicationfacilities. Forinstance,the displayserversupportsmultipleviews,zooming,
and redraw,makingthesefacilitiesavailablefor all applications.
11.4 MACH
Machisa microkemel-basedoperatingsystemdevelopedatCarnegie-MellonUniversity
u.s.
(CMU)undertheleadershipofRichardRashidandwiththesupportofDARPA,the
Departmentof DefenseAdvancedResearchProjectsAgency. It is basedon a previously
developed operating system at CMU called Accent [Rashid and Robertson 1981].
Therefore,manyof thebasicconceptsinMachare basedonAccentwork.However,as
comparedtoAccent,Machhasmanyimprovedfeatures,includingfinergrainedparallelism
by the use of threads, multiprocessor support, a better interprocess communication
mechanism,andamoreflexibleandefficientmemorymanagementscheme.
ThefirstversionofMachwasreleasedin 1986fortheDECVAX computerfamily,
includingtheVAX 11n84, afour-CPUmultiprocessor. By1987,versionsfortheIBMRT
PC,SUN3,PERQ,Sequent,andEncoremachineswerealsoavailable.Atthistime,Mach
was mainly considered to be an operating system for shared-memory multiprocessor
systems, rather than a distributed operating system for a collection of machines
interconnectedbyanetwork.ThiswasbecausemostofthemachinesrunningMachwere
tightlycoupledshared-memorymultiprocessorsystems.Anextendedversion(Mach2.5)
was later released that was also suitable for loosely coupled distributed-memory
multiprocessorsystems.Thisversionalsoprovidedcompatibilitywiththe4.2BSDUNIX
by includingmost of the 4.2 BSDUNIXinto the Mach kernel. Due to the presence ofa
largeamountof BSDUNIXcodeinthekernel,theMach2.5kernelwasquitelargeand
monolithic.In 1989,anewversion(Mach3.0)wasreleasedin whichalltheBSDUNIX
codewasremovedfromthekernelandputintheuserspace.Therefore,Mach3.0hasa
microkernelthatconsistsof pureMach.
In 1989,theOpenSoftwareFoundation(OSF),a consortiumofcomputervendors,
selected Mach as the basis of its first operating system, called OSF/l. OSF has such
important companies as IBM, DEC, and Hewlett-Packard as its members.The NeXT
workstationalsousesMachas its operatingsystem.
The followingdescriptionof Mach is basedon [Rashid 1987,Accettaet al. 1986,
Jones and Rashid 1986, Black 1990, Fitzgerald and Rashid 1986, Rashid et al. 1988,
Coulouriset al. 1994,Tanenbaum1995,SilberschatzandGalvin 1994].
11.4.1 Design Goals and Main Features
Mach's design wasinfluencedby the researchanddesigngoalsdescribedbelow.
Open-System Architecture
Oneof the maingoalsofMach wastodesignanopensystemthatcouldprovidea base
forbuildingnewoperatingsystemsandemulatingexistingones.Toachievethisgoal,the
designphilosophyusedinMachwasto havea minimalmicrokernelthatwouldprovide

Sec.12.4• Mach 675
a small set of basic abstractions sufficient for deriving other functionality and to
implement many traditional kernel-based functions as user-level servers. With this
approach, it is both possible and rational to thinkoftraditional operatingsystems such as
UNIX and MS-DOS not as operating system kernels but as application programs
servers or a set of servers that can provide client programs with specific programming
abstractions. Note that the microkemel-based design ofMach allows multiple emulators
to be run simultaneously. This makes it possible to run programs written for different
operating systems such as UNIX and MS-DOS on the same machine at the same time.
Compatibility with BSD UNIX
From the very beginning, an important goal of Mach was to provide full compatibility
with BSD UNIX systems so that it could receive wide acceptance in the research and
academic communities. This goal was initially achieved by combining Mach and the 4.2
BSD UNIX into a single kernel. Although this design guaranteed absolute compatibility
with the4.2 BSDUNIX, itledto alarge kernel.The open-systemdesignstrategywas later
employed, and all the BSD UNIX code was removed from the kernel and put in the user
space. This redesignreducedthe Machkernel toaminimalmicrokernelconsistingofpure
Mach that could be used as a base for emulating not only BSD UNIX but also other
existing operating systems and designing new operating systems on it.
Network Transparency
Similar to any other distributed operating system, network transparency was also a goal
in Mach. To achieve this goal, a higher level networkwide naming system is used in
Mach. It is implemented by user-level servers called network message servers that are
involved in transparent sending of messages between two processes that are located on
different nodes of the system. This facility allows transparent access to networkwide
resources.
Flexible Memory Management
Another important goal of Mach was to support a powerful and flexible memory
management system. To achieve this goal, Mach provides an elaborate virtual-memory
system that is implemented in terms of fixed-size pages. Some ofthe attractive features
ofMach's memory management system are as follows:
1. It clearly separates the machine-independent parts of the memory management
system from the machine-dependent parts, making the memory management system far
more portable than in.other systems.
2. It is integrated with the communication system, allowing the realization of fast
local IPC.
3. It has a copy-on-write mechanism for efficient sharing of data between two or
more processes. In this mechanism, data are physically copied only when they are

676 Chap. 12 • Case Studies
changed. Hence, itprovides the potential to eliminate muchdata movement between the
kernel, block 110devices, clients, and servers.
4. It has an inheritance mechanism that allows a parent process to declare which
regions of memory are to beinherited by its children and whichare to be read-writable.
This mechanism provides for various sharing policies to enforce protection between the
parent and its children processes.
5. Ithasanexternalmemorymanagerconcept, whichallowstheimplementationand
use of multiple user-level memory managers for handling different memory managers.
Each user-level memory manager can implement itsownsemantics andpaging algorithm
suitable totheobjectitisbacking.Theexternal memorymanagerconcept alsolendsitself
well to implementing a page-based distributed shared-memory system.
Flexible IPC
Another goal of Mach was to provide a flexible IPC system that can allow processes to
communicate in a reliable and efficient manner. To achieve this goal, Mach uses a
message-based IPC that is based on ports, which are kernel objects that hold messages.
This IPC system has the following features:
1. It supports both synchronous and asynchronous message passing.
2. It guarantees reliable and sequenced delivery of messages.
3. It ensures secure message communication by using a capability-based access
control mechanism for controlling access to ports. All messages are sent to and
received from ports.
4. It supports network transparency by allowing a sender to send a message to a
receiver on another node without the necessity to know the receiver's location.
5. When boththe sender and receiver of amessage arelocated onthe same nodeof
the system, it provides a way to transfer bulk data without doing any copying.
6. It supports heterogeneity by translating data types from one machine's
representation toanother's when thedata istransferred between two machines of
different types.
High Performance
High performance was also a goal of Mach. However, the use of the microkemel-based
design approach in Mach is subject to a performance penalty because message passing
between serving processes and the microkernel requires context switches, slowing down
the system. Some of the important techniques used in the design of Mach to obtain high
performance are as follows:
1. Use of multithreaded processes to take advantage of fine-grained parallelism for
multiprocessing.

Sec.12.4• Mach 677
2. Useofhand-offthreadschedulingpolicyforfast localIPC.Forexample, aclient
can"hand off' toaserverandaservercan "hand back" atthecompletion ofalocalIPC.
With this approach, although the context switch is unavoidable, the path through the
scheduler is optimized or avoided.
3. Use ofcopy-on-writemechanismtominimize thecopying ofdata.Notice thatthe
largest CPU cost of many operations ina traditional kernel is the copying of data (in and
out of buffers, for instance). Mach's copy-on-write mechanism can be used to greatly
reduce the data-copying cost.
4. Use of a transparent shared library in the user space to perform server work
in the client address space. In Mach, many kernel activities are effectively moved out
of the kernel directly into the client's address space by being placed in a transparent
shared library. With this approach, to the extent possible, the requirements of a server
are implemented in the library, thus avoiding the need for either a message or a
kernel trap.
SimpleProgrammer Interface
Another goal of Mach was to provide a simple interface tothe programmers. Toachieve
this goal, Mach provides an interface generatorcalled Mach Interface Generator(MIG).
MIG isbasically acompilerthat generates stub procedures from aservice definition. The
stub procedures for all services are placed in a transparent shared library. It is these
procedures that are described in the manuals and called by application programs. This
approach allows the application programmers to use a service by simply making a
procedure call, rather than making asystem callor writingcode forsending andreceiving
messages.
11.4.2 System Architecture
As Figure 12.7 shows, the Mach system architecture mainly consists of the following
layers:
1. The microkernel layer. The lowest layer is the minimal microkernel that is
replicated on each node of the system. It is minimal because itconsists of a small set of
basic abstractions that are just sufficient and powerful enough to derive all other
functionality and features. In particular, this layer is concerned with IPC, memory
management, process management, and I/O services. To carry out these functions, it
supports five basic abstractions-tasks, threads, memory objects, ports, and messages.
Tasks and threads are abstractions for process management, memory objects is an
abstraction used in memory management, and ports and messages are abstractions for
interprocess communication.
2. User-level server layer. In Mach, many traditional kernel-based functions are
implementedasuser-level servers.These servers form thenext layeroftheoverall system
architecture. The servers at this layer may be broadly classified into two categories-

678 Chap. 12 • CaseStudies
Applicationcode
Transparentsharedlibrary
Generic
servers
File 5erv~rsspecificto~her
server operatingsys~msemulated~usingMach User
space
,.....----,
05-1 OS-n
Fig. 12.7 Mach system architecture.
generic and specific. Generic servers provide services of general interest. For instance,
there may begeneric servers for services such as user authentication, distributed file
management, transparentnaming ofobjects, networkprotocols, anddevice allocation. On
the other hand, specific servers are used to provide particular behavior in a node. For
instance, a set of specific servers may be used to deliver the functionality of an existing
operating system,suchasUNIX.AsshowninFigure 12.7,multiplesetsofspecificservers
may be used at a single node to emulate the functionality of different operating systems
at the same node.
3. Transparent shared-library layer. This layer contains the stub procedures
generated byMIG for all services, so that the application programmers can use a service
by simply making aprocedure call, rather than making a system call or writing code for
sending and receiving messages.
4. Application code layer. This layer contains the code for the application
programs.
11.4.3 Process Management
BasicAbstractions
The two basic abstractions used for process management in Mach are task and thread,
whichbasicallycorrespondtotheconcepts of"process" and"thread" presented inChapter
8.That is,ataskisanexecution environmentand athreadisthe basic unit ofexecution.
Resources are allocated to and owned by a task, and all threads of a task share its

Sec.12.4• Mach 679
resources. Toavoidanyconfusion, theterm"process"willbeusedinsteadof"task" inthe
following description of Mach.
Process and Thread States
At any instance of time, a thread may be in one of the following states:
1. Running. A thread that is in the running state either is executing on some
processor or iseligible for execution and waiting in the run queue for the allocation of a
processor to it.Athread that isblocked within the kernel (for example, while waiting for
a page fault to be satisfied) is also considered to be in the running state.
2. Suspended. A thread that is in the suspended state is neither executing on a
processor nor waiting in the run queue for processor allocation. The thread will not
execute until it is returned to the running state.
Similar toathread, aprocesscanalso beintherunningorsuspended state.Aprocess
can berunning orsuspended, independentof thestateof itsthreads. However,thestateof
a process affects all threads of that process. Therefore, if a process is suspended, its
threads cannot execute, irrespective of their current state. That is, a thread can execute
only when both it and its process are in the running state.
Processes and threads can be suspended and resumed under program control. For
this, each process and each thread has a suspend counter associated with it. Primitives
process_suspend and thread_suspend are provided to increment a suspend counter and
process_resumeand thread_resumetodecrement it.Whenthesuspendcounterofathread
(or process) is positive, it is suspended, and when it becomes zero, its state becomes
running. This mechanism allows multiple suspend calls to be executed on a thread (or
process), andonly when anequal number of resume calls occur isthe thread (orprocess)
resumed. Therefore, usingacounter provides greater flexibility than usingabitandhelps
avoid race conditions.
Operations on Processes and l'hreads
Eachprocess owns aprocessportandeachthreadownsathreadport. Theseportsprovide
themechanism tooperate onprocesses andthreads. Inparticular,anoperation onathread
(or process) is invoked by sending a message to the thread port (or process port) of the
thread (orprocess). Accesstoaprocess port indirectly permits accesstoallthreads within
that process, but not vice versa.
Process management primitives provided in Mach include those for creating a
process, killing aprocess, suspending orresuming aprocess, controlling whichthreads of
a process can run on which processor or group of processors, setting the priority of a
process for scheduling of its current and future threads, getting a list of all threads in a
process, and getting statistical information about a process.
InMach, threads are managed bythe kernel. That is,thread creation anddestruction
are done by the kernel and involve updating kernel data structures. The basic kernel

680 Chap. 12 • Case Studies
interfaceprovidesavariety ofprimitivesforoperationson threads.They providethe basic
mechanisms for handling multiple activities within a single address space. However,
rather than making programmers work with these low-level primitives, Mach provides
many higher level interfaces for programming in C and other languages. One such
interface is the C-threads package that is briefly described below.
The C-threads Package
The C-threads package allows the programmers to use the kernel thread primitives in a
simple and convenient manner. Although itdoes not provide the full powerofthe kernel
interface, it is good enough for the average programmers. Inparticular, ithas routines for
directlycontrollingthreads, forenforcingmutual exclusionforcriticalsectionsofthreads,
and for general synchronization ofthreads.
The routines for direct thread manipulation are given below:
• cthreadfork isused tocreateanew thread inthesame addressspaceasthecalling
thread. The thread executes concurrently with its parent thread. However, instead
ofexecutingtheparent'scode, itexecutesaprocedurethat isspecifiedas an input
parameter to this routine.
• cthreadexit is used to terminate the calling thread. This routine is called by a
thread when it has finished doing its work.
• cthreadjoin is used to cause the calling thread to suspend itself until a specific
child thread terminates.
• cthread_detachisused toannouncethat aparticularthread will neverbecthread_
joined(waited for). If that thread evercalls cthreadexit, its stack and other state
information is immediately deleted. Normally, this cleanup takes place after the
parent has done a successful cthreadjoin.
• cthreadyield is used by a thread to voluntarily relinquish the CPU to the
scheduler when it has nothing to do. The scheduler can then schedule another
thread to run on that CPU.
Since all the threads ofa process share a common address space, the execution of
critical regions by the threads must be mutually exclusive in time. In Mach, the mutex
variable techniqueis used for this purpose; the associatedC-threadspackageroutines are
given below:
• mutex_alloc is used to dynamically create a mutex variable.
• mutexfree is used to deallocate a mutex variable.
• mutex_lock is used to lock a mutex variable. If the mutex variable is already
locked, the thread keeps trying to lock it until it succeeds. Notice that adeadlock
will result if a thread with a lock tries to lock the same mutex variable. The
C-threadspackagedoes notguaranteeboundedwaiting. Rather, itisdependenton
the hardware instructions used to implement the mutex routines.
• mutex_unlock is used to unlock a mutex variable.

Sec.12.4 • Mach 681
Finally, the condition-variables technique is used in Mach for general synchroniza
tion of threads; the associated C-threads package routines are given below. Recall from
Chapter 8 that a condition variable is associated with a mutex variable:
• condition_alioe is used to dynamically allocate a condition variable.
• conditionfree is used to delete a condition variable that was previously
allocated.
• conditionwaitisused tounlock the mutex variable associated with thecondition
variable and block the calling thread.
• condition_signalisused toindicate toathread blocked onacondition variablethat
the event being waited for may have occurred. The associated mutex variable is
then locked, and the thread starts executing in the critical region. Note that a
condition_signal does not guarantee that the condition still holds when the
unblocked thread finally returns from its condition_wait call, so the awakened
thread must keep executing the condition wait routine in a loop until it is
unblocked and the condition holds.
Threads Scheduling
The threads-scheduling scheme of Mach is illustrated in Figure 12.8.It uses the concept
ofprocessorsets, in which all the processors of the system are grouped intodisjoint sets
by software. Depending on the computation needs of a thread and workload on each
: ;
,-
P
-
rio
-
r
-
ity
-
0
-------------------~,
\ ~ I
(,--------
0
-
t
-
-
-
-
-
--
-
-
-
-
-
1
---------~,
\
: (high) I I
(
I
I
I
I
I
I
: Threadshaving
I priority=6 31 _
: Priority31"'-'_
__.__I
: (low) Globalrunqueue Globalrunqueue
I counts7 I hint::1 count=6,hint=O
I
I
I
,: I 0 0 0 I o
I
I
I I
I I
I I
I I
I I
,I I
I ,I
I
I
I I
:31 31 31 I 31 31
I Localrun Localrun Localrun : Localrun Localrun
: queuefor queuefor queuefor : queuefor queuefor
I CPU·1 CPU-2 CPU·3 I CPU·4 CPU-5
I count=O count-n count-z I counteo count=3
" ,---------------- h - in - t - =2 ------~~I " ,---------------- h - in - t: - ::O ----~-,~I
Processorset 1 Processorset2
Fig. 12.8 Threads scheduling inMach.

682 Chap. 12 • Case Studies
processorset,each thread isassigned toone of theprocessor sets bysoftware. Thus each
processor set has a collection of CPUs and a collection of threads. For the purpose of
scheduling, theCPUsandthethreadsofaprocessorsetaretotallyindependentofallother
processorsets. That is, the scheduling algorithm is mainly concerned with assigning the
threads ofa processor set to theCPUs of the processor set ina fair and efficient manner.
For fairness and efficiency, the scheduling algorithm uses a priority-based scheme with
dynamically variable quantum size. Its details are given below.
InMach, eachthread hasanassociated priority from0to31,with0being thehighest
priority and 31being thelowest priority.Moreover, asFigure 12.8shows, associated with
each CPU is a local run queue and associated with each processor set is a global run
queue. Each of these queues is an array of 32queues, with each queue corresponding to
priorities 0-31. Both the local and global run queues have two variables associated with
them-a count and a hint. The count variable contains the number of threads on all the
32queues ofthat run queue, and the hint variable contains the number of the queue out
ofthe32queues thatcurrently hasthehighestpriority thread.The hintvariable allows the
search forthehighest priority thread tobeperformed efficiently byavoiding thechecking
of higher priority empty queues. Inaddition tothese two variables, each global runqueue
has a mutex variable associated with it that is used to lock the queue to ensure that only
one CPU at a time manipulates it.
The global run queue of a processor set holds those threads that can be assigned to
anyoftheCPUs oftheprocessorset.Ontheother hand,each local runqueue holds those
threads that are permanently bound tothecorrespondingCPU. For instance, athread that
isadevice driver foradeviceconnected toanindividual CPUmustrunonlyonthatCPU.
Putting such a thread on the global run queue is incorrect because in that case it may be
picked up for execution by some other CPU of the same processor set. With respect to a
particularCPU, threads in its local run queue have higher priority than the threads in the
global run queue of its processor set.
With this arrangement ofqueues and threads inaprocessor set,the basic scheduling
algorithm works as follows:
1. When thestate ofathread withpriority nbecomes running,itisputattheend of
queue n of either the local run queue of a particularCPU (if it is permanently bound to
that CPU) or the global run queue. Notice that a thread that is not in the running state is
not present on any run queue.
2. When a thread that is currently running on a CPU blocks, exits, yields, or uses
up its quantum, the CPU first inspects its local run queue for the highest priority thread.
For this, it first checks the count variable. If it is nonzero, it takes the value of the hint
variable and begins searching the queue for the highest priority thread, starting at the
queue specified by the hint. If the local run queue is empty (the value of its count
variable is found to be zero), the CPU searches the global run queue in the same
manner. However, this time it first locks the global run queue before starting the search
operation.
3. Ifno runnable thread is found on either queue, a special idle thread is run until
some thread becomes ready to run.

Sec.12.4• Mach 683
4. On theotherhand, iftheCPU finds arunnable thread, itrunsthethreadforone
quantum. Whenthequantum finishes, acheckismadetoseeifany otherthreads having
higherorequalprioritythanthejust-runthreadhavebecomerunnable.Thecheckinvolves
searchingboth the local and global runqueues. Ifsuch athread isfound, the CPU istaken
away from the current thread and assigned to that thread, and the currentthread is put at
the end ofits priority queue in the appropriate run queue. Otherwise, the same thread is
run for another quantum.
In addition to the basic scheduling algorithm described above, Mach's scheduling
scheme has several other important features:
1. To provide high efficiency when the system is lightly loaded and good response
time to small jobs when the system is heavily loaded, the size of the time quantum is
varied inversely with the total number ofthreads that are runnable.
2. Threads may also be scheduledpreemptively. Forexample, ifthequantum size is
suddenly reduced due to sudden increase in the load of a processor set, the currently
runningthread(that was given the previously valid long quantum size) can be preempted
before it has fully utilized its allocatedquantum. This feature allows fair CPU utilization
in a situation in which the system load suddenly increases. A thread may also relinquish
the CPU voluntarily before the quantum expires if it currently has no use for the CPU
(e.g., the cthreadyield routine).
3. The priority value of a thread is dynamically changed to prevent the
monopolization ofaCPU by a high-priority thread. This feature allows fair utilization of
the available computing power by all threads.
4. Handoffscheduling facility may be used to bypass the run queues and todirectly
switch the CPU to the thread specified by the currently running thread.
5. On a multiprocessor, Mach can also be configured to do affinity scheduling.
Further details of the threads-scheduling scheme of Mach can be found in [Black
1990].
Exception Handling
The exception-handling facility of Mach classifies the uses of exceptions into the
following categories:
1. For error handling
2. For debugging
The two classesofexceptionsare handledindifferentways. Exceptionsoftheformer
class are handled on a per-threadbasis, whereas exceptionsofthe latter class are handled
by a singleexception handlerthat has aspecial port, calledthe exceptionport, associated
with it. When an exception occurs, the kernel sends a message describing the exception

Chap. 12 • Case Studies
to the appropriate exception handler.The two types of exceptions are handled differently
because it makes little sense to try to debug only one thread or to have exceptions from
multiple threads invoking multiple debuggers.
In debugging, the exception-handling facilities of a process are inherited by its
children processes, allowing adebuggertomanipulate anentire treeof processes. On the
other hand, in error handling, no exception-handling facility of a process is inherited by
its children processes, and the default assumption is that a process has no exception
handling facility.
In Mach, if exceptions of both types occur simultaneously, error handlers take
precedenceoverdebuggers.Thisisbecauseerrorhandlersarenormally partoftheprocess
and.therefore should execute normally even in the presence of a debugger.
12.4.4 MemoryManagement
Virtual-Memory Management
InMach,thememoryismodeledasalinearvirtualaddressspacesupportedbypaging.Each
process has its own virtual address space within which its threads execute. The virtual
address space ofa process is generally sparse, consisting of holes of unallocated space
betweensectionsofthevirtualaddressspaceinuse.ThisisbecauseMachmakesnoattempt
tocompresstheaddressspacetoremovetheholesofunallocatedmemorythatappearinthe
address spaceasnewitemsaremappedorremoved fromtheaddressspace.
The commonly usedapproach to maintain the virtualaddress space foreach process
istokeepalinearpagetablefrom0tothehighest usedpageinthekernel.Sincethevirtual
address space of a process in Mach is sparse, for a reasonable page size, this approach
would require excessive amounts of memory for fairly large page tables, making it
expensive. Toovercome thisproblem, theconcept of regions is used in Mach, which isa
contiguousareaofanaddressspacerepresented asabaseaddressandasize.Aregionmust
bealigned on a page boundary.The size of the table used to maintain the virtual address
space of a process is kept manageable by keeping information about only currently
allocated regions in it. A virtual address is valid only if it falls in an allocated region.
Accessing an address that is in an unallocated region (unused virtual address space) will
result in a trap, which, however, can be caught by the process if it so desires.
Mach provides a number of calls for manipulating virtual address spaces. For
instance, the vm_allocate call allocates a new region of virtual memory. The caller can
specify eitherbothabaseaddress and asizeoronly asize.In theformer case, thesystem
allocates the indicated region, whereas in the latter case, the system finds and allocates a
suitable region and returns its base address to thecaller.The vm_deallocate call removes
aregion from a process'saddress space and makes itnolonger validfor the process.The
vm_read and vmwrite calls allow a process to access the virtual memory of another
process. The vm_copy call causes a memory region to be copied onto a new region. In
addition to these calls, there are other cans that may be used for such purposes as to
control access protection ofaregion ofmemory,tocontrol inheritance ofmemory regions
by a child process from its parent process when new processes are created, and to get
information about a region in a process's address space.

Sec.12.4 • Mach 685
External Memory Managers
In traditional operating systems, management of secondary storage (such as a disk) is
usually based around a kernel-supplied file system that determines the paging scheme,
sharing semantics, andother implementationdetails ofthecorrespondingobjects (such as
file).The schemes built into the system for managing such objects may be suitable for a
largevarietyofobjects but maynotbethebest forall typesofobjects.Therefore, Mach's
memory managementsystem is designed insuch a waythat itallows users toimplement
and use their own special-purpose memory managers, having their own object
management scheme,for objects with special requirements.These special-purpose, user
level memory managers are called external memory managers.
Fortherealization oftheideaofexternal memorymanagers, animportantabstraction
usedinthememory managementsystem ofMach isthememory object.Amemoryobject
is an abstract object that represents a collection of data on which a set of operations are
defined (for example, read and write). It can be mapped into an unused portion of the
virtual address space of a process, forming a new region.
Memory objects are created and managed byexternal memory managers.As Figure
12.9shows, for every memory object that is mapped in a process's address space, there
is an external memory manager that controls it.The Mach kernel simply acts asa cache
manager for these memory objects.Itmaintains acache of memory-resident pages of all
mapped objects,asinother virtual-memory implementations. However.sincetheexternal
memory managers are user-level processes, it is possible to have different memory
managers forhandling different classes ofmemoryobjects.This provides theflexibility to
have different sets of operations, different sharing semantics, and different rules about
what becomes of objects after they are mapped out for different classes of memory
objects. This flexibility also allows user-written memory managers to have their own
paging scheme for the memory objects they manage. Each memory manager can
Sparseaddress Mapped-in External Backing
spaceofaprocess memory memory storage
(unmappedareas objects managers
areshaded)
Fig. 12.9 Externalmemorymanagersandmemoryobjects.

686 Chap.12 • CaseStudies
determine on its own where to store pages that are not in memory and whether to write
back any changed pages to secondary storage when a memory object is destroyed. Note
that memory objects are independent of the kernel in the sense that no assumptions are
made by Mach as to the content or importance of memory objects.
To make it possible for users to write and use their own memory managers, Mach
provides a well-defined interface between the kernel and the memory managers. This
interface takes the form of a communication protocol that consists of a set of message
typesthatcanbeexchangedbetweenthekernelandthememorymanagers.These message
types can be classified into two categories-those that the kernel sends to memory
managersandthosethatmemorymanagerssendtothekernel.Someexamples ofthosethe
kernel sends to memory managers are as follows:
• memory_object_init.This message issentwhenamemory object istobemapped
for the first time. When a memory manager receives this message, it initializes
itself. The message contains the control and name ports (these port types are
described later) for the memory object being mapped. These ports are used later
by the memory manager to send messages to the kernel in connection with the
memory object.
• memory_object_data_request. This message is sent to a memory manager to
request data of a memory object managed by it when a page fault occurs on the
memory object. The message contains the range of the desired data.
• memory_object_data_write.Thismessageissentwhenthekernelneedstoremove
one or more dirty pages of a memory object from resident memory,for instance,
due to page aging. The updated data to be written on the secondary storage is
included in the message.
• memory_object_data_unlock. This message is sent to request the memory
manager of a memory object to unlock a locked page of the object so that it can
be used for another process.
• memory_object_lock_completed. This message is sent to a memory manager in
reply to its memory_object_data_lock request (described below) made to the
kernel.
• memory_object_terminate. This message is sent to a memory manager to inform
it that the memory object named in the message is no longer in use and can be
removed from memory.
Some examples of message types the memory managers send to the kernel are as
follows:
• memory_object_set_attributes. This message is sent in response to the memory;
object_init message received from the kernel. It indicates to the kernel that the
memory manager is now ready to accept requests for the newly mapped-in
object.
• memory_object_data-provided. This message is sent inresponse to the memory_
object_data_request message to return the requested page to the kernel.

Sec.12.4• Mach 687
• memory_object_data_unavailable. This message is sent in response to the
memory_objecl_data_request message when the requested data is not available.
• memory_object_data_lock.This message issentto makearequest tothekernel to
change the protection mode on pages. The message contains a lock_value that
specifies the new protection mode (read, write, execute) on the data specified in
the message.
• memory_object_data_clean. This message is sent by a memory manager to make
a request to the kernel to send it the pages specified in the message so that they
can be made clean by writing them to disk.
• memory_object_data_cache.This message issenttotellthekernel whether itmay
retain cached data of the memory objecteven when no process has it mapped in
to its address space.
• memory_object_destroy. This message is sent to tell the kernel that a certain
memory object is no longer needed and its information in the kernel can be
destroyed.
All messages from the kernel to the memory managers are sent asynchronously
because it is not reasonable for the kernel to block any of its threads waiting for a user
process that may not reply. The messages are exchanged between the kernel and the
memory managers by using the port-based interprocess communication mechanism of
Mach (described later). For this, the following types of ports are used:
1. Objectport. Every memory object hasan object port associated with it.This port
is created by the memory manager that manages the object. It is used by the memory
manager for receiving messages from the kernel about page faults and other events
relating t.o the object.
2. Control port. For each memory object, a control port is created by the kernel. It
is used by the kernel toreceive messages related to theobject from the memory manager
of the object.
3. Name port. Foreach memory object, thekernel alsocreates anameport.Itisused
as a kind of name to identify the object. Name ports are not used for receiving messages
but rather are used as a point of reference. For example, each region of an address space
maybetreated asanobject, inwhichcase thekernel hasanameportforeachregion. Now
when a thread gives an address to the kernel asking it which region the address belongs
to, the kernel returns the name port of that region as an answer. Notice that all addresses
belonging to the same region will be identified by the same name port.
Now let us look at how a memory manager and the kernel interact to manage a
memory objectand to satisfy useraccess requests for data inthe memory object. The call
vm_mapprovidedinMach (along withothercalls formanipulating virtualaddress spaces)
isusedtomapamemory object intothe virtualaddress spaceofthecalling process. When
a thread needs access to data in a memory object, it makes a vm_map call. If this is the
first vmmap call on the memory object, the kernel creates thecontrol and name ports for

Chap. 12 • CaseStudies
thememory object andsendsamemory_object_initmessage totheobject portincluded in
the vm_map call. The memory manager that manages the memory object provides this
object portas a part of its support of the object. In reply, the memory manager sends a
memory_object_set_attributes message telling the kernel what the object's attributes are.
Initially,allthepages oftheobject aremarked asunreadable/unwriteable, sothatthefirst
access to the object will result in a trap. The thread that made the vm_map call is now
unblocked and allowed to execute.
When thethreadattempts toread/write somedataofthememory object, apage fault
occurs, and the kernel sends a memory_object_data_request message to the memory
managerviatheobjectportofthememory object. Onreceiving thismessage, thememory
managerfetches thepage bywhatever method isappropriate fortheconcernedobject and
returns a pointer to the page in the memory_object_data-providedmessage or returns an
appropriate error to the kernel. The kernel then maps the page into the faulting thread's
address space and unblocks the thread allowing it to continue with its execution.
In addition to the user-written specialized memory managers, there is a default
memory manager provided by the Mach system. This memory manager is needed for
several reasons, such asfortakingcare of the system'sown memory needs, for managing
those regions of an address space that do not have a memory managerassigned to them,
and for managing a memory object in the default manner when the specialized memory
manager of that object crashes due to a bug in it. The default memory manager has an
interface identical tothatofuser-written memory managers. However,itusesthestandard
filesystem tostoredata thatmust be written todisk. Unlike UNIX, whichusesaseparate
swap area, thedefault memory manager uses atemporary file forswap space. Notice that
when a process makes the vm_allocate call to allocate a region of virtual address space,
it is in fact mapping an object managed by the default memory manager that provides
zero-filled pages in response to the call.
To ensure that there is always sufficient free-page frames, Mach also provides a
pageoutdaemon that isapart of thekernel. Itruns asathread within the kernel and uses
a first-in, first-out (FIFO) algorithm to select victims for page replacement. It wakes up
fromtimetotimeandchecks thenumberoffree-page frames. Iftherearenotenough free
pageframes, itselectsoneormorepagestobereplaced. Ifapageselectedforreplacement
is dirty, it is sent to the memory manager in charge of the page's object in a memory_
object_data_write message. On receiving this message, the memory manager writes the
page to disk and informs the kernel when it is done.
Memory Sharing
TheuseofthreadsinMach automatically allowsthethreadsofaprocess tosharethesame
address space. Therefore, no special mechanism is needed to share memory among the
threads of asingle process. However, for sharing memory among two or more processes,
Mach provides the three mechanisms described below.
Copy-on-WriteSharing. Wehave seen thateach process inMach, including the
kernel, has its own paged address space managed by the kernel. Therefore, two
processes of the same node can share a memory region by having entries for the

Sec.12.4• Mach 689
memory region in the page tables of both processes. This technique is used in the
copy-on-write memory-sharing mechanism. In this mechanism, two or more processes
share common memory pages by having entries for these pages in the page table of
each of these processes and making them read-only. The sharing continues until one of
the processes attempts to write into its own logical copy of a shared page; this causes
an actual copy of the page to be created for that process, so that it has its own version
of the page that is marked writable in the page table of the process. Other processes
that were sharing that page continue to use the old version of the page in the read
only mode.
Controlled Inheritance ofAddress Space. This mechanism allows selective
memory sharing at the time of process creation. As in UNIX, a new process in Mach is
basically created as a copy of an existing process. However, Mach's process creation
mechanism is different from that of UNIX in the following aspects:
1. InUNIX, thechild process isalways aclone of theprocess thatexecutes thefork
systemcall, but inMach thechild process can beaclone of adifferent processcalled the
prototype.
2. InUNIX, thechild process inherits theentire address spaceof itsparentprocess,
but Mach allows controlled inheritance of address space. For controlled inheritance of
address spaces, Mach allows a process to assign one of the following three inheritance
attributes to each region in its address space:
(a) Noaccess. Aregion withthisattribute isnot inherited bythechild process.That
is, it is not a part of the child's address space.
(b) Shared access. A region with this attribute is shared between the prototype
process and the child. That is, the pages in the region are present in the address
spacesofboththeprototypeandchildprocesses. Changes madebyeitherprocess
are visible to the other process.
(c) Copy access. A region with this attribute is copied and mapped into the child's
address space. The fork system call of UNIX can be simulated by using this
option. However,inMach, theregion isnotcopied whenthechild iscreated, but
the copy-on-write mechanism is used for efficiency.
Distributed Shared Memory. Mach does not provide a direct mechanism for
sharing memory among processes on separate machines that do not have any common
shared memory. However, the external memory manager concept of Mach allows users
to implement their own distributed shared-memory system. For example, to implement
a page-based distributed shared-memory system, a new memory object, the shared
page, can be defined. To manage these objects, one or more special memory managers
can be implemented. The memory managers could be as simple or as complicated as
needed, depending on which coherence protocol and data-locating mechanism, among
those described in Chapter 5, are used in the implementation.

690 Chap. 12 • Case Studies
12.4.5 Int.rproc8ss Communication
Basic Abstractions
The two basic abstractions used for interprocess communication in Mach are ports and
messages. A port in Mach is a one-way communication channel, logically a bounded
message queue. A message is a typed collection of data objects. To communicate with
anotherprocess, aprocess sends amessage toaport associated with thereceiverprocess.
The message isqueued at theport until the receiver retrieves itfrom the port. The sender
and receiver must have permission to access the port. This permission takes the form of
acapability. Forinstance,asendermusthaveacapability withsendrightforaporttosend
a message to the port, and a receiver needs a capability with receive right to retrieve
messages from the port. Ports support reliable, sequenced, message streams in the sense
that the system guarantees the delivery of messages sent to a port in the order sent.
Management ofPorts
Portsaremanagedandprotectedbythekernel.Theyarekepttrackofonaper-process basis
rather than per thread. Therefore, all ports created by the threads of a process are unique
withintheprocess.Thekernelkeepsnorecordofwhichthreadcreated whichport.
Protection is ensured by keeping ports information safely inside the kernel, where
userprocesses cannot modify it.Furthermore, thethreadsofaprocess can sendorreceive
messages from a portonly if the process possesses the appropriate port capability.Aport
capability consists of a port identifier (pointer to the port) and a rights field telling what
access the holder of the capability has to the port. The three types of rights that may be
defined onaportaresend, sendonce,and receive.Acapability withsendrightallows the
holder to send messages to the specified port any number of times. The sendonce right
allows the capability holder to send only one message to the port, after which the kernel
destroys thecapability (thismechanism isusefulforrequest-replyprotocols). The receive
rightallows thecapability holder toread messages from theport.Atany instance oftime,
there can be only one process with receive right to a port, but many processes may
simultaneouslyhavesendright.Aprocess having aportcapability with receive right may
send that capability in a message to another process. However, by doing so, the sender
loses its receive right for the port and the receiver gains that right. For each process, the
kernel maintains acapability listthatcontains complete information about whatrights the
process possesses fordifferent ports. Since ports are kept track of on aper-process basis,
all the threads in a process are equally considered holders of the process's port
capabilities.
When aprocess holding aportcapability withreceive rightexits oriskilled, theport
can no longer beused and is therefore destroyed by the kernel, even if it contains any
undeliveredmessages.Thekernelthensearches forallthesendcapabilitiesoftheportand
marksthemasdead.Anyattempttosendamessage withacapability thathasbeenmarked
asdead bythe kernel fails, and thekernel returns anappropriate error code tothe sender.
On the other hand, when there are no processes having a capability with sendright for a
port,thekernel (optionally) sendsamessage totheprocess having receive rightcapability
for the port, notifying it that there are no senders left.

Sec.12.4• Mach 691
'Themessage queue associated with a port is of finite length and may become full.
Several options are providedforhandling problems associatedwith message transmission
to a full queue. For instance, if a queue is full, a sender may abort the send, block until
aslot becomes availableinthequeue, or have thekernel deliverthe message for it.Inthe
latter case, thekernel acts asasingle-messagequeue andcannotaccept anymessage from
the sender until it has delivered the already queued message ofthe sender.
Mach provides the facility to group a number of ports into a port set. A port may
belong toatmost one port setatatime. Itispossible toonly receive messages fromaport
set; sending messages to a port set is not possible. Furthermore, a port that belongs to a
port set cannotbe used directly to receive messages. This is because all messages sent to
any portinaport setarequeued inacommonqueue, theport set'squeue. When areceive
isperformed onaport set, thekernel returns oilemessage from theport set'squeue. Ifthe
queue is empty, one of the options described above may be selected to handle this
problem. The facility tocreate aport setisparticula.rlyuseful forwriting server processes
that can service requests coming in on multiple ports, such as for a server that supports
multiple objects. Such a server can maintain adifferent port foreach ofthe many objects
that it supports and get messages for any of them without having to dedicate a thread to
each one.
Message Passing
The basic interprocess communication mechanism involves the sender process sending
a message to a port and the receiver process receiving it from the same port. Messages
may be sent and received either synchronously or asynchronously. An RPC mechanism
is also implemented in which the sender blocks after sending a message until a reply
comes back.
A message consists of a fixed-length header and a variable number of typed data
objects. The header contains such informationascapability name forthedestination port,
capability name for the replyport to which return messages should besent, message size,
and several types of options such as whether synchronous, asynchronous, or RPC type
communication isdesired, what todo ifthe send or receivecannot complete successfully,
and what to do if the message queue at the port is full.
The data partof a message consists of a variable number of typed data objects. Each
data object may be represented either as an in-line data or an out-of-line data. In the
former representation, the data object is included in the body of the message, whereas in
thelatterrepresentation,themessage bodycontains apointer tothedataobject.Associated
with each data object in the message isadescriptorthat contains such information as the
type ofdata objectand itssize.This information isneeded sothatthereceiver can unpack
the data correctly. It isalso useful todo conversions between machines, when the source
anddestinationmachines havedifferent internal representations.Adata fieldofamessage
may also contain a port capability when a process wants to send a capability to another
process.
The facility to use out-of-line data representation in a message body provides the
means to transfer the entire address space of a process in a single message.
Furthermore, when both the sender and receiver processes are on the same node,

692 Chap. 12 • Case Studies
Mach uses the copy-on-write mechanism to transfer out-of-line data from the sender to
the receiver. That is, instead of copying the data physically from the sender's address
space to the receiver's address space, the kernel simply updates the receiver's page
table by placing information about all the pages of the data object in it and making
them copy-on-write. Depending on the bit in the descriptor for the data object, the
region corresponding to the data object is either removed from the sender's address
space (by deleting the corresponding entries from the sender's page table) or kept
there. In the latter case, a page of the data is physically copied in the receiver's
address space when the receiver attempts to write on it, and the corresponding entry
in the receiver's page table is appropriately updated. This is done to ensure that any
modifications do not affect the original version of the data that the sender is still
using.
With the use of copy-on-write mechanism, message passing becomes very efficient
because no copying of data is required in most cases. In essence, message passing is
implemented by using"the virtual-memory management mechanism.
Networkwide IPC
For networkwide IPC, Mach uses user-level servers called network message servers and
networkportsthatareportsforreceiving messages fromother nodesofthesystem.There
isanetwork message serverateach nodeof the system.Allthe network message servers
work together tohandle internode messages inatransparent manner.The three mainjobs
performed by network message servers are as follows:
1. Making the networkwide IPC mechanism network transparent.
2. Supporting heterogeneity bytranslatingmessagedata fromthesender'scomputer
format to the receiver's computer format. The type information in the descriptor
for each data object in the message is used for this purpose.
3. Performing authentication of other network message servers to prevent message
data from falling into the hands of unauthorized users.
Processes of a node automatically inherit send right to a port created by the local
network message server for receiving messages from local processes. The network
message serverofanode allows local processes toregister network ports with it,andthe
network message servers of all the nodes communicate with each other to maintain a
distributed database of all network ports in the system.A process can gain send right to
a network port by asking its local network message server to look up a name in its
database or by receiving a portcapability in a message.
The basic method by which a message is sent from a process PIon node A to a
process P on node Bis illustrated in Figure 12.10.It involves the following steps:
2
1. Process PI prepares the header and body of the message to be sent and
executes the messagesend system call for sending the message, just as in the case of
a local IPC.

Sec.12.4• Mach 693
NodeA NodeS
Kernel Kernel
3
Communicationnetwork
Fig. 12.10 Networkwide IPCmechanisminMach.
2. The system call causes atraptothekernel.The kernel extracts thereceiver'sport
number from the message header, and after determining that this port is nonlocal, it
forwards the message to the local network message server.
3. Thenetwork messageserverofnodeAthenconsultsthedatabase ofnetworkports
to find out the node to which the message should be forwarded. It then constructs a
network message containing the message ofprocess PI and sends itacross thenetwork to
the network message server of node B. In some cases, the message between the two
network message servers is encrypted for security.
4. The network message server of node B looks up the network port number
contained in the message and maps it to its equivalent local port number by using a
mapping table that it maintains. When needed, it also translates message data from the
representation of the computer at node A to its own computer representation. It then
executes the message_send system call for sending this message to the extracted local
port.
5. The system call causes atraptothekernel. Thekernel extracts thereceiver'sport
number fromthemessage header,andafterdeterminingthatthisportislocalwithprocess
P having receiveright for it, it provides the message to process P2 when it executes a
2
messagereceive call. In this way, the message is transferred from the sender to the
receiver in a transparent manner.
Notice that since network message servers are user-level processes, they can be
designed by the users to allow for a flexible choice of data type representations, the
amount or type of security to be used on a network, and the use of a specific protocol
depending onthenetworktowhichtheyareattached. However,thisflexibility isachieved

694 Chap. 12 • CaseStudies
at the cost of performance because pure kernel implementation (which most other
distributed systems use) has better performance than the approach having user-level
servers forward internode messages.
11.4.6 UNIX Emulation
Mach emulates 4.3B5D UNIX.The basicapproach used for UNIXemulation inMach is
briefly described below.Its further details can be found in [Golub et at 1990].
As shown in Figure 12.11,the two software components used for UNIX emulation
in Mach are the UNIX emulation library and UNIX server.The UNIX emulation library
islinked asadistinct regionintoevery process emulating aUNIXprocess. This region is
inherited from/etc/init byall UNIXprocesses whentheyareforked off.The exec system
call has been changed so that it does not replace the emulation library but just the user
program part of the process's address space. The UNIX server, which contains a large
amount of UNIX code, has the routines corresponding to the UNIX system calls. It is
implemented as a collection of C threads.
T
Applicationcode
layer
3
+yer
User-levelserver's
Kernel Microkernellayer
"----_----------J-L
1Traptothekernel
2UNIXemulationlibrarygetscontrol
3RPetotheUNIXserver
4Systemcallperformed
5Replyreturned
6ControlgivenbacktotheuserprogrampartofUNIXprocess
Fig.12.11 UNIXemulationinMach.
The trampoline mechanism [Tanenbaum 1995] is used to invoke the code in the
emulation library. In this mechanism, the code in the emulation library is not directly
invoked byuserapplications, buttheemulation library getscontrol from thekernel when

Sec.12.4• Mach 695
a system call causes a trap to the kernel. For this, Mach provides acall named task_set_
emulation, which assigns the address of a handler in the emulation library to a given
system call number. At the time of system initialization, this is called for each UNIX
system call so that all ofthem get registered ina system call redirection table maintained
by the kernel.
With this setup, when a UNIX process makes a system call and traps to the
kernel, the kernel uses the system call redirection table and immediately transfers
control back to the emulation library of the same process. When the emulation library
gets control, it examines the machine registers to determine which system call was
invoked (note that at this time all machine registers have the same values that they
had at the time of the trap). The emulation library then packs the system call number
and necessary parameters into a message and does an RPC with the UNIX server. On
receiving the RPC message, the UNIX server' extracts the system call number and
parameters from it, carries out the system call, and sends back a reply to the
emulation library. On receiving the reply, the emulation library directly transfers
control to the user program without going through the kernel. The entire process is
summarized in Figure 12.11.
12.4.7 Mach Interface Generator
A programmer can write applications for the Mach system by directly using the
system call interface (several system calls of Mach were described in the sections
above). However, working at the system call level makes many programming tasks
tedious and repetitive. For instance, any client program that requests for a service
from a server process must have code to create and send messages to the server and
to wait to receive a reply. Similarly, any server program that provides service to client
processes must have code to accept messages, unpack them, dispatch them to the
proper routine, and reply after the routine finishes processing the data. A pro
grammer's task can be greatly simplified by providing stub procedures (procedures
that generate most of the repetitive code) for all services and placing them in a
transparent shared library. With this approach, an application programmer can use a
service by simply making a procedure call, rather than making a system call or
writing code for sending and receiving messages. To facilitate this, Mach provides an
interface specification language and compiler called Mach Interface Generator (MIG).
MIG generates stub procedures from a service definition.
The MIG language allows interfaces between cooperating computing entities to be
specifiedand maintained independent of specific languages or machine architectures. On
the other hand, the MIG compiler translates these specifications into interface code for
each oftheprogramminglanguages supportedwithin theMach environment,includingC,
C()MMON LISP,Ada, and Pascal. The interface code generated bythecompilerhascode
forcommunication,runtime support fortypechecking, typeconversions,synchronization,
and exception handling.
Over the years, MIG has proved to bea valuable tool because of thefollowing main
advantages:

Chap. 12 • Case Studies
1. It eases the task of programming distributed applications by relieving the
programmers from concerns about message data formats, operating system peculiarities,
and specific synchronization details.
2. It improves cooperationbetweenprogrammers working indifferent languages by
allowingboth clientand servers tobe written inanyofthelanguagessupportedwithin the
Mach environment.The MIGcompilerautomaticallytakescare ofdifferencesinlanguage
syntax, type representations, record field layout, procedurecall semantics, and exception
handling semantics.
3. It enhances system standardization by providing a uniform message-level
interface between processes.
4. It reduces the cost ofreprogramming interfaces in multiple languages whenever
a program interface is changed.
11.5 CHORUS
Chorus is a microkernel-based distributed operating system that started as a research
project in 1979 at INRIA (Institute National de Recherche en Informatique et
Autornatique), a government-funded laboratory in France. Until now Chorus has passed
through four major versions (Versions 0-3). Version 0 (1979-1982) was designed to
model distributed applications as a collection ofcommunicating processes called actors.
Version 1 (1982-1984) was aimed at porting the design of Version 0 from a shared
memory multiprocessor system to a distributed-memory multiprocessor system. It also
had additional features of structured messages and some support for fault tolerance. The
main goal of Version 2 (1984-1986) was to add the UNIX source code compatibility
feature to the system so that existing UNIX programs could be run on Chorus after
recompiJation. In 1986, the Chorus team left INRIA and formed a new company, named
Chorus Systems, to make Chorus acommercial product. They started Version 3 in 1987,
with the main goal ofchanging the research system into a commercial product. For this,
the first goal was to provide binary compatibility with UNIX so that UNIX programs
could be run on Chorus without the need to recompile them. Many key concepts from
other distributed operating systems were also included in Version 3. In particular, a
message-based interprocess communication mechanism was borrowed from V-System;
some ofthe conceptsoffast interprocesscommunication,distributed virtual memory, and
external pagers were borrowed from Mach; and the idea ofusing capabilities for global
naming and protection was borrowed from Amoeba. Version 3 also has RPC facility,
support for real-time operations, and a multithreading feature. It is available as a
commercial product for a wide range of hardware, such as the Intel 80x86 family, the
Motorola 68000 and 88000 families, and the Inmos Transputer.
The detailsofVersion3are presented below.The following descriptionofChorus is
based on [Pountain 1994, Armand et al. 1986, Rozier et al. 1988, Guillemont 1982,
Abrossimovet al. 1992,Batlivalaetal. 1992,Leaetal. 1991,Lea etal. 1993,Tanenbaum
1995, Coulouris et al. 1994].

Sec.12.5 • Chorus 697
12.5.1 hsl9n Goals Qnd Main Faaturas
Chorus's design was influenced by the research and design goals given below.
UNIX Emulation and Enhancements
One of the main goals of Chorus was to provide a UNIX compatibility feature so that
existing UNIXprograms could berunonChorus.This wasnotaninitialgoalbutwaslater
realized to be important for the commercial success of the system. Therefore, Version2
ofChorus wasdesigned toprovide UNIXsourcecodecompatibility.Toachievethisgoal,
the original kernel of Chorus was redesigned and converted to a microkernel by moving
as much functionality as possible from it to user address space. Then several processes
wereadded inthe user address space todo UNIX emulation. Later,in Version3,a UNIX
emulation subsystem, called ChoruslMiX (MiX stands for Modular UNIX), was built on
top of theChorus microkernel toprovide binary compatibility with UNIX System V.The
microkernel ofVersion2wasfurtherrefined by movingoutthepartaddedtoitforsource
code UNIX emulation and placing this part in the new UNIX emulation subsystem. A
4.3BSD UNIX emulation is also being currently implemented.
Inaddition toUNIXemulation, Chorus design alsoprovides UNIXenhancements to
allow users of the lJNIX emulation t.o use enhanced facilities provided by Chorus from
withinUNIXprocesses. Twosuchenhancements aretheuseofmultiplethreadsinasingle
process and the ability to create a new process at a remote node.
Open-System Architecture
Another important feature ofChorus isitsmicrokernel support, whichprovides abasefor
building new operating systems and emulating existing ones in a modular way.Withthis
feature, multiple operating systeminterfaces, suchasUNIX System V,BSDUNIX, OS/2,
and MS-DOS, can simultaneously exist on the same machine. Therefore, it will be
possible to run several existing applications that now are run on different machines on a
single machine.
Efficient and Flexible Communication
The basic communication paradigm used in Chorus is message passing. Since message
passing has a reputation of being less efficient than shared memory, Chorus's designers
have made great efforts tooptimize the IPC system.The IPC system alsoprovides ahigh
degree of flexibility in handling different types of communications. This IPC system has
the following features:
1. It provides both asynchronous message passing and request/reply type
interactions.
2. It hasRPC facility that provides at-most-once semantics. It also has lightweight
RPC facility for communication between two kernel processes.

698 Chap. 12 • CaseStudies
3. It has group communication facility with the flexibility to choose whether a
message should be sent to all members, to anyone member, to a particular
member,ortoanyonemember withtherestriction thatthemember should notbe
on a specified node.
4. It ensures secure message communication by using a capability-based access
control mechanism that is similar to the one used inAmoeba.
5. When both the sender and receiver of amessage are located on the same node, it
uses Mach's copy-on-write mechanism to transfer bulk data without doing any
copying.
Transparency
Chorus provides two types of transparency-network transparency and service reconfi
guration transparency. Network transparency isimplementedbytheuse ofasingle global
name space and user-level servers called network managers that are similar to Mach's
network message servers. On the other hand, service reconfiguration transparency, which
allows services to be reconfigured dynamically without being noticed by the users
interacting with them, is implemented by supporting port group and port migration
facilities. Note thatChorus isnotperfectly network transparent because some systemcalls
work only for local cases.
Flexible Memory Management
Two of the main facilities provided in Chorus for flexible memory management are
support formultiple user-level memorymanagersandsupport forpageddistributed shared
memory.The former facility is implemented by using Mach-style external pagers, which
are called mappers in Chorus.
Support for Real-Time Applications
Another important goal of Chorus was to support real-time applications. To achieve this
goal, Chorus provides for flexible allocation of thread priorities and also allows for
customizedthread-schedulingpolicies.Moreover,real-time programs canpartly runinthe
kernel mode and can have direct access to the microkernel without any in-between
software.
Object-Oriented Programming Interface
Arelatively newgoalofChorusistoprovide system-level support forfine-grained object
oriented languages and applications and to do so in such a way that new object-oriented
programs andold UNIXprograms canberunonthesamemachine without interfering. To
achieve this goal, Chorus designers have designed a subsystem, called Chorus Object
Oriented Layer (COOL), on top of the Chorus microkernel.

Sec.12.5 • Chorus 699
12.5.2 System Architecture
AsshowninFigure 12.12,theChorussystemarchitecturemainly consistsofthefollowing
layers:
1. The microkemellayer. The lowest layer is the microkernellayer, which is called
the "nucleus" in Chorus. This layer is present on each node ofthe system. It consists of
four components, three ofwhich are machineindependentand one is machinedependent.
The machine-dependent part, called the supervisor, manages the raw hardware and
catches traps, exceptions, and interrupts. It also handles context switching. This part has
to be rewritten for each new hardware to which Chorus is ported. Of the three machine
independent components, one is for process and thread management, one is for virtual
memory management, and one is for handling interprocess communications, The four
componentsofthe kernel areconstructedinamodularway sothat anyonecan bechanged
without affecting the others.
Applicationcodelayer
(applications,utilities,andlibraries)
+T
Subsystemlayer User
space
System UNIXSystemV Object-oriented Othersubsystems
processesof subsystem . subsystem
(CalledChO~~MIX) (calledCOOL)
SUbSY
Kernel
processesof
subsystems
--L....----------------------4
Kernel
1space
~
Supervisor(machinedependent)
."ig. 12.12 Chorus system architecture.
2. The subsystem layer. The microkemel layer provides a base for building new
operating systems and emulating existing ones in a modular way. Each such newly built
or emulated system is called a subsystem, and all these subsystems together form the
subsystem layer on top of the microkernel layer. A subsystem presents a well-defined
interface to its users. For example, one such subsystem is the UNIX System-V
emulator, called MiX, which provides UNIX interface and allows UNIX programs to

700 Chap. 12 • Case Studies
be run on Chorus. Another such subsystem is the object-oriented subsystem, called
COOL, which provides system-level support for fine-grained object-oriented languages
and applications.
Each subsystem is a collection of Chorus processes. These processes are of two
types-kernelprocesses and system processes. Kernel processes run in the kernel mode
andsystem processesrunintheusermode. Kernelprocessescancall oneanother andcan
invoke the microkerneI for obtaining services. On the other hand, system processes can
send messages to each other and to kernel processes and can also make calls to the
microkernel.
The basic idea behind using kernel processes in the subsystem layer was to provide
a way to extend the functionality of the microkemel without permanently increasing its
sizeandcomplexity.Thisisachievedbyproviding theflexibilitytodynamically loadand
remove kernel processes during system execution. With this facility, it is possible to
dynamically configure the system software to match the hardware components of a
particular node of the system without having torecompile or relink the microkernel. For
instance, a disk server, implemented as a kernel process, need not beloaded on diskless
workstations.
The kernelprocesses sharetheaddress spacewiththemicrokemel. Hence, theymust
berelocated after being loaded.
3. Application code layer. The layer above the subsystem layer is the application
code layer. This layer contains user processes that include applications, utilities, and
libraries.The userprocesses ofthislayerandthesystemprocesses ofthesubsystem layer
share the user address space.
A userprocesscannot makedirectcallstothemicrokernel. Itcanonly make system
callsoffered bythe subsystem that itisusing.Toensure this,themicrokernel keeps track
ofwhichuserprocess isusingwhichsubsystemanddisallowsauserprocess frommaking
system calls offered by other subsystems. However, real-time processes have special
privilege inthe sense thattheycan runas system processes rather than as userprocesses.
This allows them to make direct access to the microkernel without any software in the
way.
11.5.3 K.y Abstractions
The key abstractions used in the design of Chorus are as follows:
1. Actor.An "actor'tinChorus corresponds to theconcept of a "process" presented
in this book. It provides an execution environment for one or more threads and has an
address space and acollection of ports used to send and receive messages. Toavoid any
confusion, in the description that follows, the term "process" will be used instead of
"actor," except in Chorus system calls.
2. Thread.A "thread" inChorus is the same as the concept of a "thread" presented
in this book. In Chorus, threads are managed by the kernel. That is, their creation,
destruction, and scheduling are done by the kernel and involve updating kernel data
structures.

Sec. 12.5 • Chorus 701
3. Region. Each process has an address space that consists of one or more regions.
Aregion isanareaofcontiguousvirtual address thatisassociatedwithsome piece ofdata,
such as a program or a file. Regions of a process do not overlap. Only those portions of
an address space that are occupied by the regions are accessible by the threads of the
owningprocess. Inpaged virtual-memorysystems, aregion isalignedonapage boundary
and consists ofone or more pages.
4. Segment. Asegmentisacontiguoussequenceofbytes identifiedandprotectedby
a capability. To make the bytes of a segment accessible to the threads ofa process, the
segment is mapped on to a region of the process's address space. A segment can be
simultaneouslymappedintomultiple regions thatmay even beindifferent address spaces.
Once mapped, the threads access the segment's bytes simply by reading and writing
addresses inthe region. Note that tomap asegmenton toaregion, itisnot necessarythat
thesegmentbeexactlythesame sizeastheregion. Ifthesegmentislargerthantheregion,
only aportion ofthesegmentequal insizetotheregion willbeaccessible. Aninaccessible
portion ofthe segmentcan be made accessiblebyremappingthisportion on tothe region.
On the other hand, if the segment is smaller than the region, the result of reading an
unmapped address depends on the mapper (mappers are similar to Mach's external
memory managers and are described later). The mapper may be designed to raise an
exception, return 0, or extend the segment in this case. Another way to access the bytes
of a segment without the need to map it on to a region is to use traditional system calls
that are used for I/O operations on files.
5. Port. A port is a unidirectional communication channel, logically a bounded
message queue. Each port belongs to a single process and only one process can read its
messages. To communicate with another process, a process sends a message to a port of
the receiverprocess. The message isqueued at the port until the receiverretrieves itfrom
the port. As in Mach, ports can be migrated between processes and can also be grouped
together to form port groups.
6. Message. In Chorus, the basic communication paradigm is message passing.
Hence, two processes communicate with each other by exchanging messages between
them. A message is addressedto aport of the receiving process. Amessage has aheader,
an optional fixed part (of 64 bytes), and anoptional variable-sizedbody (of maximum64
kilobytes). For the kernel, both the fixed part: and the body are untyped byte arrays.
Therefore, the semantics of the contents of a message can be decided by user
applications.
7. User identifier (UJ). In Chorus, most kernel resources, such as ports and
processes, are assigned a 64-bit UI that is guaranteed to be globally unique within a
systemin itsentire lifetime. For uniqueness, UIsare formed ofthree fields-(a) the IDof
the machine(node) onwhich thetJI wascreated,(b)anepoch numberthat isincremented
each time the systemisrebooted, and (c)acounterthat isvalidintheepoch.The first field
of a VI is used as a hint by the object-locating mechanism to locate the corresponding
object (resource). The VIs may be freely passed in messages and files from one process
to another.

702 Chap. 12 • CaseStudies
8. Local identifier (LI). The systemwide unique VIs are longand expensive to use.
Therefore, forefficiency,resources (suchasthreadsandports)withinasingleprocess are
identified byLIs, whichare32-bit integers.TheLIsarevalidonly withintheprocess that
uses them.
9. Capability. Capabilities are used both as resource identifiers and for restricting
access to resources. They are mainly used for identifying resources managed by a
subsystem, but in some cases they are also used to identify resources managed by the
kernel. A capability consists of the following two fields-(a) a 64-bit VI that is
normally the identifier of the port to which messages are sent to request operations on
the object and (b) a 64-bit key. The key has two fields. The first field serves as an
index into a resource table to identify a resource from among multiple resources
accessed via the same port. The second field contains a random number that makes it
difficult to guess a valid capability. Processes can send capabilities to other processes
or store them in files.
10. Protection identifier (PI). For authentication purposes, each process in Chorus
hasaPI associated withit.API isa bitstring having nosemantics associated with it.By
default, a process's PI isthat of the process thatcreated it but can bechanged by kernel
or system processes. When a process receives a message, it can request the kernel to
specify the PI of the process that sent the message. This mechanism can be used in the
design of a server process to implement access control for the resources that it manages.
For instance, to emulate UNIX protection semantics, the equivalent of a UNIX user
identifier (UID) canbeassociated witheach process,andthentheChorus PIscanbeused
to implement the UIDs.
Of the key abstractions described in Section 12.5.3, the two basic abstractions used for
process management are process (actor) and thread.
Types of Processes
In Chorus, processes are classified into the following types:
1. Kernel processes. All kernel processes reside in the kernel space and share the
sameaddress space witheach other and withthemicrokernel. Theyexecute inthekernel
mode. They are trusted, which means that they are allowed to make direct calls to the
microkemel. They are also privileged, whichmeans that they are allowed toexecute I/O
and other protected instructions for making direct access to system resources. For high
performance, ChorusprovidesthefacilityoflightweightRPC,whichkernelprocessescan
use to communicate with each other.This communication facility is not available to the
other two types of processes.
2. System processes. System processes reside in the user space and execute in the
user mode. Each system process has its own address space. These processes are not

Sec. 12.5 • Chorus 703
privileged but trusted. That is, they can make direct calls to the microkernel but cannot
execute privileged instructions meant for direct access to system resources.
3. Userprocesses. User processes reside in the user space and execute in the user
mode. Eachuser processhas itsown addressspace. Theseprocessesare neitherprivileged
nor trusted. Therefore, they are the least powerful ones.
The properties of the three types of processes in Chorus are summarized in Figure
12.13.
Process Space Execution Private Privileged Trusted
type mode address
space
Kernel
process Kernel Kernel No Yes Yes
System User User Yes No Yes
process
User User User Yes No No
process
Fig. 12.13 Thethree typesofprocesses inChorus.
Process and Thread States
In Chorus, a process may be in one ofthe following states:
1. Active. A process is inthe active state when itsthreads can change states and can
be scheduled to run.
2. Stopped. A process is in the stopped state when it is frozen. All the threads ofa
stopped process are in the stopped state.
On the other hand, a thread may be in one ofthe following states:
l. Active. A thread in the active state is either executing on some processor or
eligible for executing and waiting in the run queue for the allocation of a
processor to it.
2. Waiting.When athread blocks and has to wait forsome event tooccur, the thread
is put in the waiting state until the event occurs.
3. Suspended. A thread that is in the suspended state is neither executing on a
processornorwaitingforaneventtooccur. Ithasbeen intentionallysuspendedby
either another thread or itselfby issuing a.kernel call to suspend the thread.
4. Stopped. All threads of a process enterthe stopped state when the process's state
changes from active to stopped. Note that a thread may simultaneously be in any
one ofthe first three states and the fourth state.

704 Chap. 12 • CaseStudies
The following system calls affect process and thread states:
• actorCreate is usedtocreate anew process. The process type (kernel, system, or
user) and the initial state of the process can be specified as parameters. The
process's capability is returned to the caller.
• actorStopisusedtochangethestateofaprocessfromactivetostopped.Thestate
of all the threads of this process is changed to stopped.
• actorStartis used to change a process's state from stopped to active. All the
threads of this process resume their original states from the stopped state.
• threadCreate is used to create a thread. The initial state, priority, and so on, are
specified as parameters.
• threadSuspend is used to suspend a thread.
• threadkesume is used to restart a suspended thread.
Threads Synchronization
Sinceallthethreadsofaprocess shareacommon address space, theexecution ofcritical
regions by the threads mustbemutually exclusive intime.The two mechanisms used in
Chorus for this purpose are the mutex variable technique and the counting-semaphore
technique. The mutexvariabletechnique (described inChapter 8)is usedonly formutual
exclusion. Ithastheadvantagethatoperations thatdonotcause thecaller to blockcanbe
carried out entirely in the caller's space without the need for a kernel call. On the other
hand,thecounting-semaphore technique isusedinChorus forgeneral synchronization of
threads. In this technique, system calls are provided for performing UP and DOWN
operations on the counting semaphore to increment or decrement its value. The system
calls provided for threads synchronization in Chorus are as follows:
• mutexlnitis used to initialize a mutex variable.
• mutexGetis used to perform a lock operation on a mutex variable.
• mutexRelis used to perform an unlock operation on a mutex variable.
• semlnitis used to initialize a counting semaphore.
• semPis used to perform a DOWN operation on a counting semaphore.
• semVis used to perform an UP operation on a counting semaphore.
Threads Scheduling
In Chorus, threads are scheduled by the kernel according to individual kernel priorities
that can bedynamically changed. Each process is assigned a priority and each thread is
alsoassigned arelative priority within itsprocess.Theabsolute priorityofathreadisthe
sum of itsown priority andthepriority of itsprocess.A separatequeue of active threads
ismaintained foreach absolute priority level.When aCPU becomesfree, the firstthread
of the highest priority nonempty queue is allocated to it for execution.

Sec.12.5 • Chorus 70S
We have seen that Chorus provides support for real-time applications. For this, a
threshold priority level is used to divide the active threads into two groups, and different
scheduling algorithms are used for scheduling of threads in these two groups. That is,
threads that belong to a queue having lower priority than the threshold priority level are
timesliced and consume CPU time on a quantum basis in a round-robin mode. On the
other hand, a thread that belongs to a queue having higher priority than the threshold
priority level, once run, will continue to rununtil eitherit voluntarily releases its CPUor
an even higherpriority thread becomes active to run. This mechanismisconsideredto be
good enough for handling most real-time applications.
The system calls for dynamically changing the priorities are as follows:
• actorPriority is used to either read or change the priority value of a process.
• threadPriority is used to either read or change the relative priority value of a
thread.
Exception Handling
In Chorus, there is a single exception handler kernel thread, and each process may also
have itsown exceptionhandlerthread. Whenanexceptionoccurs,anattemptisfirst made
to handleit by using the kernel exceptionhandlerthread. Ifthis fails, the kernel suspends
the thread that causedthe exception and sends a message to the exception handlerthread
of the same process. The faulting thread is killed by the kernel if this method also
fails.
12.5.5 M8mory Manag_ment
Regions and Segments Management
Ofthe key abstractions described in Section 12.5.3, the two basic abstractions used for
memory management are region and segment. They have already been described in
Section 12.5.3, and hence only thesystemcalls supportedinChorusfortheirmanagement
and use are presented here. Only the important ones (not all) are given below.
• rgnAliocate is used to allocate a region in a process's address space and set its
properties. The process'8 capability, starting address of the region, size of the
region, and various initialization options are specified as parameters.
• rgnlnitisused toallocatearegion and toinitializeitwith thecontentsofasegment
whose capability isspecifiedas aparameter. Severalothercalls that are similarto
rgnlnit but fill the allocated region in different ways are also provided.
• rgnSetProtect is used to change the protection status (read, write, execute bits)
associated with a region. It is also used to make aregion accessible only to the
kernel.
• rgnStat is used to get the size and other information about a region.
• rgnFree is used to free the space allocatedto aregion, which can then be used for
allocation to new regions.

706 Chap. 12 • Case Studies
• sgReadis used to read data from a segment. The portion ofthe data to be read is
specified by an offset and number ofbytes as parameters. The buffer address to
which data is to be copied is also specified as a parameter.
• sgWrite is used to write data to a segment. The portion to be written is specified
by an offset and number ofbytes as parameters. The buffer address from which
data is to be copied is also specified as a parameter.
Mappers
Mappers ofChorus are very similar to external memory managers ofMach. For every
segmentthat is mapped to a region, there is a mapperto control it.A singlemappermay
control multiple segments. The kernel acts as acache managerfor the mappedsegments.
That is,the virtual-memorycomponentofeach kernel maintainsapage cacheofmemory
resident pages and keeps track of which page belongs to which segment. It also keeps
track ofwhich ofthecachedpages are dirty and which are clean. When apage isselected
for replacement tocreate space for a new page in the cache, the kernel simply discards it
if it is clean; otherwise it is sent to the appropriate mapper to be written back to the
corresponding segment. On the other hand, when a page fault occurs, the kernel first
checks if the page is present in the cache. If not, it makes a request to the appropriate
mapperto send the page and suspends the faulting thread until the page is received. The
mapperthen fetches thepage from thesegment'sstorage locationifthepage isnotalready
present in its address space. The mapper notifies the kernel when the page becomes
available with it.The kernel then accepts the page from the mapper, updates the memory
management unit (MMU) page tables, and resumes the faulting thread.
Some mapper-related system calls provided in Chorus are as follows:
• MpCreate is used by the kernel or a program to request a mapper to swap out a
segment and to allocate disk space for it. In response to this call, the mapper
allocates a new segment on disk and returns a capability for itto the caller.
• MpRelease is used to request a mapper to release a previously created segment.
• MpPullln isused bythekernel tomake arequesttothemapperforsomedata from
a segment that it controls.
• MpPushOutisused by the kernel tosend some data ofasegmenttoamapperthat
controlsthat segment.This isneeded when thekernel wants toreplace adirtypage
ofthe segment from its cache.
Distributed Shared Memory
The distributed shared-memory mechanism supported in Chorus may be characterized as
follows (see Chapter5 for details ofthese characteristics):
1. It uses page size as block size. To facilitate this, segments are split up into
fragments ofone or more pages.
2. It uses the replicated, migrating blocks (RMB) strategy.

Sec.12.5 • Chorus 707
3. To simplify the memory coherence problem, it classifies fragments into two
types-read only and read-write. Replication of read-write fragments is not
performed. Therefore, only read-only fragments are replicated and read-write
fragments have only one copy in the entire system.
4. It uses the dynamic distributed-server algorithm for data locating.
12.5.6 Interprocess Communication
Ofthe key abstractions described in Section 12.5.3, the two basic abstractions used for
interprocesscommunicationare messageand port. Messagesaresent toandreceivedfrom
ports, and ports are created and deleted dynamically by processes. The system calls for
creating and deleting ports are as follows:
• portCreate is used for creating a new port. The capability ofthe port is returned
tothecaller, which can be sent to otherprocesses toallow them tosend messages
to the port.
• portDelete is used to delete a port.
Sending and Receiving of Messages
Chorus provides the following communication operations to allow threads to send and
receive messages in a flexible manner:
1. Asynchronous send. The system call ipcSend is used to asynchronously send a
message to a port. There is no guarantee of successful message delivery and no
notification is made to the sender in case of communication failure. This type of
communicationfacility can beused byusers tobuild arbitrarycommunicationpatternson
top of the basic communication facility of Chorus.
2. Receive.The systemcall ipckeceiveisused toreceive amessage. Twooptionsare
available in this case. In the first one, a thread can specify which port it wants to receive
on. In the second one, a process can specify that it wants to receive from anyone of the
ports that itowns. The receive operationofthe secondoption can be made more selective
by using the following mechanisms:
(a) By disabling some of the ports by using portDisable system call, in which case
only enabled ports are eligible to satisfy the request. A disabled port may be
enabled by using the portEnable system call. Ports can be enabledand disabled
dynamically.
(b) By assigning prioritiesto ports, in which case if more than one enabledport has
a message, the enabled port having the highestpriority is selectedto receive the
message. Priorities of ports can be changed dynamically.
Ifnomessage isavailablewhen the ipcReceivesystemcall ismade, thecallingthread
is suspended until a message arrives or a user-specified timer expires.

708 Chap.12 • CaseStudies
Chorus uses thecopy-on-write mechanism of Mach forefficiency.Therefore, when
amessageisreceived,itsbodyiscopiedtothereceiver process'saddress spaceonlyifthe
sender and receiver processes are on different machines. If the two are on the same
machine, themessagebodyissimplymapped toaregionofthereceiver process'saddress
space, and a page of the message body is copied only when the receiver modifies it.
3. RPC. The third type of communication operation supported in Chorus is RPC,
which allows request/reply type interactions. The system call for initiating an RPC is
ipcCali.Theprocessmaking thiscallisautomatically blockeduntileither thereplycomes
in or the RPC timer expires, at which time the sender is unblocked. At-most-once
semantics is supported for RPCs.
Group Communication
Group communication facility is supported in Chorus by allowing multiple ports to be
grouped together to form aportgroup.The system calls provided for this purpose areas
follows:
• grpAllocisusedtocreateanemptyportgroup.Thecapability ofthenewlycreated
port group is returned to the caller. Using this capability, the caller or any other
process that subsequently acquires thecapability can add ordelete ports fromthe
port group.
• grpPortlnsert is used to add a new port to an existing port group.
• grpPortRemove is used to delete a port from a port group.
A sender sending a message to a port group has the flexibility to select one of the
following addressing modes:
1. Broadcast mode. In this case, the message is sent to all ports in the group. This
mode of communication is useful in such cases as sending a file update to all servers
having areplicaofthefileor forrequesting anoperation uponaresource that ismanaged
bysome member of agroup of servers, butit is notknown which member.Inthe former
example, allmembers ofthegroup takeaction onreceiving themessage, andinthelatter
example, only themember that manages the resource takes action andtheother members
simply discard the message.
Note that the broadcast mode ofcommunication does notprovide message-ordering
guarantees. If this is required, it must be implemented by the user.
2. Functional mode. In this case, the message is sent to only one port in the group
that is selected by the system. This mode of communication is useful in such cases as
whereaservice isperformed identically byanumber ofservers andaclientdoes notcare
which server provides the service and may not want to be bothered with the knowledge
of the identity of the particular one assigned. In such cases, the client simply sends a
request to the group of servers providing the service, and the system selects the most
suitable server at that time for servicing the client's request.

Sec. 12.5 • Chorus 709
An interesting application of the functional mode of communication is to provide
reconfigurable services in a system. For instance, all servers providing some service can
be grouped together to form a single port group. Now clients send their requests to the
group in the functional mode without having to know which servers are available at that
time to provide the service. This transparency feature allows new upgraded servers to be
added to the group and removal of old servers from the group without disrupting the
services and without the clients even being aware that the system has been
reconfigured.
3. Selectivefunctional mode. In this case also the message is sent to only one port
in the group, but theport is selected by the caller and not by the system. Apractical use
ofthismodeofcommunicationisinloadbalancing. Anoverloadednodemayfirstusethe
broadcast communication mode to get the current loads of all other nodes. From the
replies received, it selects the most lightly loaded node. It then uses the selective
functional mode of communication to transfer some of its load to the selected node.
The threemodesofgroupcommunicationaresummarizedinFigure 12.14.Notethat
in all three modes messages are sent to port groups by using the asynchronous send and
hence the communication is unreliable.
Communication Message Member
mode sentto selectedby
Broadcast Allmembers -
Functional Onemember System
Fig. 12.14 Group communication Selective Onemember Caller
functional
modes inChorus.
Port Migration
In Chorus, a system call named portMigrate maybe used to remove a port from one
process and to move it to another process. When a port is moved, all the messages
currently initaremovedalongwithit.Asone wouldexpect, portsremainmembersofport
groups when they migrate.
The port migration facility allows the responsibility of providing some service to be
dynamically transferred from one server to another without being noticed by the clients
whoarecurrently interacting withtheserver forthat service.Aninteresting application of
this facility is for supporting maintenance transparency. For example, a server on a
machine that is going down for maintenance can migrate its ports to another server on a
different machinethatprovides thesameservices. Client requestscontinue tobeprocessed
consistently without any disruption. In this way, server machines can go down for
maintenance and come up later without being noticed by their clients.

710 Chap. 12 • Case Studies
Networkwide IPe
Fornetworkwide IPC,Chorus uses networkmanagersthat aresimilar toMach's network
message servers.There isa network manager process ateach node of the system, and all
thenetwork managers work together toextend thecommunicationfacilities of thekernel
transparently across a network.
When a thread sends a message to a port, the kernel of that machine looks up the
port'sVIinalistoflocal ports.Ifitisnotfoundinthelist,thekernel forwards theport's
VI to the local network manager. The local network manager then communicates with
other network managers to get the location of the port.Once the port has been located,
messages senttotheportthereafteraredelivereddirectly toaportofthenetworkmanager
ofthe nodeonwhichthemessage portislocated.The network manager ofthatnodethen
forwards the message to the local port to which the message was sent.
12.5.7 UNIX Emulation andExt&nslons
UNIX Emulation Approach
Chorus designers have built a subsystem called ChoruslMiX (Modular UNIX) for
providing binary compatibility with UNIX System V. The basic approach used for
designing this UNIXemulation subsystem is briefly described below.Further detailscan
be found in [Gien and Grob 1992,Armand et al. 1989].
The ChoruslMiX.subsystem primarily consists of the following processes (see Fig.
12.15):
I
UNIX
process Userprocesses
O
Object
manager
Process
manager
3
Streams UNIXemulation
4 manager subsystem
5
O
fPC
manager
Microkernel
'--------------------*--
1Traptothekernel
2Processmanagergetscontrol
3APetoasuitaolemanager
4Systemcallperformed
5Replyreturned
6BlockedUNIXprocessrestarted
Fig.12.15 UNIXemulation inChorus.

Sec.12.5 • Chorus 711
1. Process manager. This process is the main component of the emulation
subsystem. Itcatches allsystemcallinterrupts and,ifnecessary,communicateswithother
processes of the subsystem tohandle system calls. Inaddition, itperforms jobsrelated to
process management (including creation and termination of processes), signal handling,
and resource naming.
2. Object manager. This process mainly performs file management activities and
may also contain the disk driver. It also acts as a mapper for the files itcontrols.
3. Streams manager. This process manages pipes, sockets and networking, and
stream devices such as keyboard, display, mouse, tape devices, and so on.
4. Interprocess communication manager. This process handles system calls related
to System V messages, semaphores, and shared memory.
Of these four processes, the process manager contains a large amount of newly
written code, while others mostly contain UNIX code itself. Moreover, only the process
manager isneededatallnodeshaving the UNIXemulation subsystem; theotherprocesses
are optional and are loaded only where needed. For example, the object manager is not
loaded on diskless nodes.
The four processes may be run either in the kernel mode or in the user mode.
However, for performance and security reasons, currently they all are run in the kernel
mode.
The process manager has multiple threads. Other managers initially start with one
thread and additional threads are created as requests come in.
When the system is booted, it inspects its environment and accordingly loads only
thoseoptionalprocessesthatareneeded.Aspartoftheirinitialization,eachoftheseoptional
processes send amessage totheprocess managerannouncing their ports and telling what
systemcallstheycanhandle.Ontheotherhand,theprocessmanagerinformsthekernelthat
it wants to handle trap numbers that UNIX uses for making system calls. The kernel
maintains atablecontainingtheaddressofaroutine foreachemulated systemcall.
With this setup, when a UNIX process makes a system call and traps to the kernel,
the thread in the process that handles the corresponding system call automatically gets
back control from the kernel. If the system call can be completely handled locally, the
process manager performs the requested system call itself. Otherwise, depending on the
system call, the process manager does an RPC to either the object manager, streams
manager, or interprocess communication manager. The contacted manager process
performs the requested systemcall and sends a reply back tothe process manager, which
then sets up the proper return value and restarts the blocked UNIX process. The entire
process issummarizedinFigure 12.15.Intheexample shown inthefigure,thesystemcall
is handled by the streams manager.
Comparison with Mach's UNIX EmulationApproach
Therelative advantages anddisadvantagesoftheapproachestakeninMachandinChorus
for supporting UNIXemulation are as follows [Coulouris et al. 1994]:

712 Chap. 12 • CaseStudies
1. Modularity.Chorus design provides better modularity than Mach design because
the emulation subsystem uses different server processes to provide different UNIX
facilities, as opposed toMach in which a single UNIX server process is used to provide
all UNIX facilities. Since thefour manager processes ofChorus do notshare any variable
or other memory and they communicate exclusively by RPC, any of them can be
reimplemented independently of the others, provided the interfaces are not changed.
2. System state management. Since Mach uses a single UNIX server process as
compared to multiple server processes in Chorus, it is easier to manage the system state
in Mach. This is because inChorus the system state relevant to a single emulated UNIX
process is distributed across several server processes that may even be on different
machines. If this state is replicated for performance, a mechanism and extra
communication overhead will be needed to keep this state consistent. On the other hand,
ifthestate isdistributedbutnotreplicated, functionality can beaffected because different
servers may fail independently of each other. These problems are not associated with
Mach'sapproach because aprocess'sstate isconfined only totheemulationlibrary'sdata
and the single UNIX server process.
3. Protection.InMach, whenaprocess executes aUNIXsystem calland trapstothe
kernel,controlispassedbacktotheemulation librarythatresidesintheaddressspaceofthe
process. Therefore, if the process has buggy code that interferes with the data in the
emulationlibrary,thismaygiverisetononstandard failuremodescausing trouble.Sincethe
approach usedinChoruscompletely isolates system datastructures from userprocesses, it
avoids suchproblems andprovides betterprotection tothesystemthanMach'sapproach.
Further details ofthedifferences between theapproaches taken byMach andChorus
for UNIX emulation can be found in [Dean and Armand 1992].
Extensions to UNIX
Inaddition to UNIX emulation, Chorus also provides many extensions to UNIX toallow
UNIX processes touse standard Chorus properties that are not available in UNIX. Some
of the most important ones are as follows:
1. Threads.Thisextension allows UNIXprocesses tocreate anddestroy newthreads
using the Chorus threads package.
2. Remote process creation. This extension allows a UNIX process to create a new
process on a remote node. Inthe system call for process creation, itcan be specified that
the new process isto becreated, not on the local node, but on the specifiedremote node.
The newprocess startsonthelocal nodewhenforked off,butwhenitdoes anexecsystem
call, it is started on the specified remote node.
3. Groupcommunication. The same group communicationfacility asthat ofChorus
can also be enjoyed by UNIX processes. This is because user processes using the UNIX
subsystem can create ports and port groups and send and receive messages in the same
manner as Chorus processes.

Sec. 12.5 • Chorus 713
4. Memory model. UNIX processes can also enjoy the flexibility of the memory
model supported inChorus.Thatis,userprocesses usingthe UNIXsubsystem cancreate
regions and map segments on to them in the same manner as Chorus processes.
12.5.8 Th. COOL Subsyst.m
In addition to the ChoruslMiX subsystem, another subsystem developed for Chorus is
Chorus Object-OrientedLayer(COOL). Its main goal isto provide system-level support
forfine-grained object-oriented languagesandapplications inChorus.Thebasicapproach
used for designing this subsystem is briefly described below. Its further details can be
found in [Lea et al. 1991, 1993].
The COOL (COOL-2, the second version) subsystem that sits on top of the Chorus
microkernel is structured into the following layers (see Fig. 12.16):
Userprogram
Language
COOLuserprograms runtimesystem
Generic
T .....................runtimesystem
Languageruntimelayer
,....... ...
~
......................................................................................
SCUObOSLYI
Genericruntimelayer(objects)
...... ......................................................
COOL-baselayer(cluster)
Microkernel
Fig. 12.16 The implementation structure and layers ofCOOL subsystem ofChorus.
1. COOL-base layer. This layer provides a system can interface that presents the
illusion of a new object-oriented microkernel to user applications. It deals with
abstractions called clusters, which are places where objects exist. A cluster holds a
group of related objects, such as objects belonging to the same class. The decision
regarding which objects belong to which cluster is made by the upper layers of
software.
From the memory model viewpoint, clusters are collections of Chorus regions
backed by segments. Therefore,acluster can be simultaneously mapped intothe address
spaces of multiple processes, possibly on different machines. However, clusters are not
replicated and there is only one physical copy of each cluster at any time. Requests
corresponding to remote clusters are serviced either by forwarding the request to the
machine that currently holds thecluster forremote invocation or bymigrating thecluster
to the requesting machine for local invocation. In this manner, the COOL base layer
manages clusters, mapping them into multiple address spaces to produce distributed

714 Chap. 12 • Case Studies
cluster spaces that are visible to all COOL processes without regard to where they are
running.
2. Genericruntime(GRDlayer.Thislayerprovides supportforfinergrained objects
within clusters. In particular, it provides necessary operations for creating and deleting
objects, formappingthemintoandoutofaddress spaces, andforinvoking their methods.
It also provides support for a single-level persistent object store, interobject communica
tions based on Chorus RPC, and protection of objects during application execution.
3. Languageruntimelayer.Programmers mayusedifferentobject-orientedprogram
ming languages, suchasC++,Smalltalk,andEiffel,todefine theirobjects. The language
specificruntimelayermapstheobjectmodelsofseveralsuchprogramminglanguagesonto
theGRT's abstractions. Itusespreprocessors togenerate anup-call table forevery typeof
object created at the GRT level. This mechanism is used to build a standard interface
between thegenericruntime layerandthelanguage-dependentruntime layer.The generic
runtime layer uses this interface to make calls to the language runtime system to obtain
language-specific,information about the semantics of certain operations. For example. it
couldfindouthowtoconvertin-memoryobjectpointerstopersistentpointersforstorageor
howtohandlemethoddispatch.Thismechanismenables COOLtosupport manydifferent
object-orientedprogramming languages withreasonableefficiency.
Ofthe three layers, the COOL-base layer is implemented as a Chorus process that
runs in the subsystems layer of Chorus. On the other hand, the software in the generic
runtime layer is linked with every COOL program, and the appropriate software of the
language-specificruntimelayer(depending onthelanguage inwhichaCOOL program is
written) is linked with the COOL program. Therefore, the language runtime and the
generic runtime systems reside in the user program's address space.
11.6 ACOMPARISON OF AMOESA, V-SYSTEM, MACH,
AND CHORUS
Each of the four distributed operating systems (Amoeba, V-System, Mach, and Chorus)
described above have their own strengths and weaknesses that is mainly due to the goals
setby their designers and their evolution histories.Amoeba andV-Systemwere designed
from scratch as distributed operating systems for loosely coupled distributed memory
multiprocessors. From the beginning till now, they have remained university research
projects. On the other hand, Mach started as a university research project to build an
operating system for tightly coupled shared-memory multiprocessors and was later
extended forlooselycoupleddistributed-memorymultiprocessors. Itwaslaterselectedfor
commercialization. Finally,Chorus alsostartedasaresearch project tobuild adistributed
operating system for loosely coupled distributed-memory systems, but its design goals
were changed as it evolved from a research project to a commercial system. The
consequences of the design goals and evolution histories of these systems are visible in
their strengths and weaknesses. To make these differences clearer, let us look at the
important aspects of all these four systems together.

Sec. 12.6 • AComparison ofAmoeba, V-System, Mach, andChorus 715
11.6.1 System Model
Amoeba is based on the processor-pool model whereas V-System, Mach, and Chorus are
based on the workstation-server model.Therefore inAmoeba, auser does not logon to a
specific machine but to the system as a whole and the selection of CPU (or CPUs) for
running a user'sjob is done automatically by the operating system. In general, a user is
not aware of on which CPU his or herjob is being processed.
On the other hand, in V-System, Mach, and Chorus, a user logs on to a specific
machine (called his or her home machine) on which most of his or her jobs are run by
default. Users can request forremote execution oftheirjobs in these systems. Moreover,
V-Systemalso provides automatic load-balancing facility due to which auser'sjobs may
get processed remotely without his or her knowledge. But in any case, all three systems
have the concept of local and remote processing of jobs. This concept does not exist in
Amoeba.
12.6.2 Kernel
Allfour systems are based onthe microkernel model, in which thekernel isminimal and
other functionality and features are supported by user-level servers. However, the
complexity ofthe microkernel andthekernel interface ofthe foursystems differbased on
theflexibility offered bythem.Themicrokernels ofAmoeba and V-Systemaresimpleand
have very few system calls. Mach's microkernel is very complex with too many system
calls. This ismainly because itattempts to provide greater flexibility in thesense that the
same thing can be done in two or three different ways, and the users have the option to
select the most convenient or efficient way for a particular circumstance. Chorus's
microkernel has fewer system calls than Mach's but more than Amoeba's or V-System's.
Therefore, its complexity is moderate.
12.6.3 Process Management
Abstractions corresponding to a process in Amoeba, V-System, Mach, and Chorus are
process, team, task, and actor, respectively. All four systems support multithreaded
processes and in all four threads are managed and scheduled by the kernel.
Mach and Chorus have the flexibility to run the threads of the same process in
parallel on different CPUs. This flexibility is not available in Amoeba and V-System.
Hence, in these systems, a CPU is time shared by the threads ofa process and run in
pseudo-parallel.
Amoeba does not provide any type of user control over thread scheduling.
V-System and Chorus allow processes to set priorities of their threads. Mach provides
maximum flexibility in this case because, in addition to allowing threads priorities to
be dynamically changed, it also provides the facility of handoffscheduling. This facility
may be used by a thread to hand off the CPU to another thread of its choice when it
has finished using the CPU.
Amoeba and V-System support automatic load-balancing facility to spread the
workload uniformly over all the machines in the system. Mach and Chorus do not have

716 Chap. 12 • CaseStudies
this facility and bydefault a user'sjobs are executed on his or her home machine. Only
on explicit request can a user'sjob berun remotely.
V-System also has process migration facility.The other three systems do not have
this facility.
11.6.4 Int.rproc.ss Communication
Amoeba, V-System, Mach, and Chorus all provide RPC facility for interprocess
communication. Mach and Chorus also provide the facility to send messages asynch
ronously. Mach packages all forms of message passing in a single system call, whereas
Chorus provides alternativecalls. The attempt of Mach to package all forms of message
passing ina single systemcall makes itscommunicationinterface verycomplex because
of the large number of parameters and options used in the system call to handle all
possible different cases.
Chorus makes useof lightweight RPC facility for efficient communication between
local kernel processes.The other three systems do not use lightweight RPC.
In Amoeba and V-System,messages consist of a fixed-size header and an optional
out-of-line block of data of variable size. In Chorus, messages consist of a fixed-size
header,anoptionalfixed-sizein-linedataof64bytes,andanoptional variable-size in-line
data of a maximum 64 kilobytes. Mach provides maximum flexibility in this case by
allowing multiple variable-size out-of-line blocks of data in a single message.
Mach's messagescanbeeither simpleorcomplex.The contents of simple messages
arecontiguous sequenceofbytes,whereascomplexmessagescontaintypeddata.Complex
messages maybeusedtotransmitcapabilities. Ontheotherhand,Amoeba,V-System,and
Chorus usesimpleuntypedmessagesthatcontainacontiguoussequenceofbytes.
Messages are addressed to ports in Mach and Chorus, to processes inAmoeba, and
tomanager IDsinV-System.Amanager IDinV-Systemmayeitherbeaprocessoraport
identifier.All four systems use the mechanism of hint cache plus broadcasting to locate
ports or processes.
In localIPC,MachandChorus usethecopy-on-write mechanism topassout-of-line
data. Amoeba and V-Systemdo not use this mechanism. Therefore, local IPC is faster in
Mach and Chorus than inAmoeba or V-System.
Amoeba, V-System, and Chorus support group communication, but Mach does not.
The group communication facility of Amoeba provides reliable, ordered broadcast
facility; that of Chorus provides unreliable, unordered broadcast facility; and that of
V...System provides unordered broadcast facility with the flexibility given to the users to
choose the degree of reliability.
In Amoeba and V-System,networkwide IPC is handled by the kernel. On the other
hand, in Mach and Chorus, user-level network servers are used for networkwide fPC.
Therefore, internode IPC is faster inAmoeba and V-Systemthan in Mach or Chorus.
On the network, all four systems provide support for conventional TCPII~ In
addition, Amoeba supports FLIP, and V-System supports VMTP. These protocols,
although not standard and widely used, have been specifically designed for the needs of
distributed operating systems and are faster than conventional network protocols for
typical RPC usage.

Sec. 12.6 • AComparison ofAmoeba, V-System, Mach, andChorus 717
Amoeba has a very simple memory management scheme without support for the virtual
memory(demandpaging)mechanism.Therefore, whenanAmoebaprocessruns,itsentire
address space is present in memory. This scheme is simple to implement and has the
advantage of high performance. However, it requires that the machines have extremely
largememories. Aprocess whoseaddressspaceislargerthanthe memoryofthemachine
having the largest memory size among all machines inthe system cannot be run on that
system. In contrast, V-System, Mach, and Chorus provide support for a paged virtual
memory mechanism. Therefore, any process can be run on these systems no matter how
large its address space is.
The virtual-memory management schemes of Mach and Chorus are very powerful
andflexible because theyallow pagestobeshared between multiple processesin various
ways. For example, the copy-on-write sharing mechanism allows efficient sharing of
pages among multiple processes of a single node, and the external pager mechanism
allows virtual memory to be shared even among processes that run on different
machines.
Amoeba, V-System, Mach, and Chorus all provide support for distributed
shared memory. Amoeba supports an object-based distributed shared memory in
which variable-size objects are shared by replicating them on all machines using
them. Read operations on an object are performed locally, while write operations on
it are performed using the reliable broadcast protocol of Amoeba. On the other
hand, V-System, Mach, and Chorus support page-based distributed shared memory.
Like Amoeba, V-System allows both read-only and writable pages to be replicated.
However, V-System uses problem-oriented approach for solving the memory coher
ence problem. On the other hand, to solve the memory coherence problem, Mach
and Chorus allow only read-only pages to be replicated and there is only one copy
of writable pages in the entire system. Due to replication of writable objects and
the use of write-all protocol for memory coherence, updates in Amoeba's scheme
are more expensive than in V-System's, Mach's, or Chorus's schemes. However,
unlike Amoeba's scheme, Mach's and Chorus's schemes suffer from thrashing
problem. Thrashing can occur if a writable page is heavily accessed from two
machines.
Amoeba's file management scheme is verydifferent from those of V-System,Mach, and
Chorus. Amoeba uses the immutable file model, whereas V-System,Mach, and Chorus
usethemutablefilemodel.Therefore,afilecannot beoverwritten inAmoebaandupdates
are performed by creating new file versions. In V-System, Mach, and Chorus, a file is
modified by overwriting the same file.
InAmoeba, anentirefile isalwaysstoredcontiguously both inmemoryandondisk.
This provides efficient accesses to files but leads to the external fragmentation problem.
In V-System,Mach, and Chorus, a file need not be stored contiguously. Storage space is
allocated in units of fixed-size blocks or variable-size segments.

718 Chap. 12 • Case Studies
Allfoursystemsusethedata-cachingmodelforremotefileaccessing.However,in
Amoebatheunitofcachingisafile,whereasintheotherthreesystemstheunitofcaching
is a block.
Since Amoeba uses the immutable file model, it supports immutable shared file
semantics.Ontheotherhand,thefile-sharingsemanticsofV-System, Mach,andChorus
depend on the semanticssupportedbythe user-levelfileserver being used.
Amoebaalsosupportsautomaticreplicationoffiles.Theotherthree systemsdonot
have this facility.
11.6.7 SfaCurity
InAmoebaandChorus,resourcesarenamedandprotectedbycapabilities.Therefore,the
capability-basedaccesscontrolmechanismisusedtocontrolaccesstoresourcesinthese
two systems. In Mach, the access control mechanism is based on port rights (port
capabilities).Thatis,Machserversgenerallymanagemanyports,oneforeveryresource,
andonlyprocessesthatownappropriateportrightsforaportcanaccessthecorresponding
resource.Finally,inV-System, theaccesscontrolmechanismisbasedonACLsmanaged
by resource managers.
CapabilitiesinAmoebaandChorusaremanagedinuserspace,butportcapabilities
inMacharemanagedbythekernel.Amoebacapabilitiesareprotectedbyusingone-way
encryptionfunctions.Chorusalsoprovidesprotectionidentifiersthatmaybeusedtoknow
the actual senderofa message.
Chorus and V-System providesupportfor real-timeapplications.Amoebaand Mach do
not provideany specialfacilityfor real-timeapplications.
All four systemsprovide support for UNIXemulation. However,while V-System,
Mach, and Chorus support binary compatibility, Amoeba supports source code
compatibility withUNIX.
Chorusalsoprovidessystem-levelsupportforfine-grainedobject-orientedlanguages
and applications.Theother threesystemsdo notcurrentlyprovidethis facility.
Figure 12.17presentsa summaryof the comparisonof the four systems presented
above.
11.7 SUMMARY
Thischapterhaspresentedadescriptionoffourdistributedoperatingsystemstorelatethe
variousconceptsdescribedinthe precedingchaptersofthis booktoreal systems.These
systems are Amoeba, V-System, Mach, and Chorus. The design goals, system
architectures, and the most important and noteworthy aspects of these systems were
covered in sufficient detail to consolidate the reader's understanding of the design
principles of distributedoperating systems.A comparisonof the four systems was also
presented.

~ .~
surohC
hcaM
metsvS-V
abeomA
erutaeF
~
revres-noitatskroW
revres-noitatskroW
revres-noitatskroW
looprossecorP
ledom
metsyS
..N.; I-'
lenrekorciM
lemekorciM
lenrekorciM
lenrekorciM
ledomlenreK
> o
xelpmocylriaF
oot(
xelpmocyreV
metsyswef(elpmiS
metsyswef(elpmiS
fosmretniytixelpmoclenreK
0 3
)sllacmetsysynam(
)sllacmetsysynam
)sllac
)sllac
ecafretnillac
metsys
0"" .l~::
rotcA
ksaT
maeT
ssecorP
noitcartsba
ssecorP
= )'0IC :
seY
seY
seY
seY
anisdaerhtelpitlumstroppuS
,.. 0 .
?ssecorpelgnis
> 3
seY
seY
oN
oN
nonurssecorpafosdaerhtnaC
8
?sUPCtnereffid
"$
lenreK
lenreK
lenreK
lenreK
dnadeganamerasdaerhT
~
ybdeludehcs
n.c e-
-ngissaytiroirp(seY
-nqlssaytiroirp(seY
-ngissaytiroirp(seY
oN
revolortnocresustroppuS
)0'IC
)tnem
ffodnahdnatnem
)tnem
gniludehcs
sdaerht
~
)anfudehcs
~ ~
oN
oN
seY
seY
?ytilicafgnicnalabdaolcitamotuA
"?
oN
oN
seY
oN
?ytilicafnoitargim
ssecorP
. = . ~ c : Q
-orhcnysadnaePR
-orhcnysadnaCPR
ePR
CPR
smsinahcemCPIcisaB
0
dnessuon
dnessuon
2 n:
-acinummocrof,seY
oN
oN
oN
?CPAthgiewthgilsesU
Q
lacol
neewtebnoit
sessecorplemek
.=g:
·dexiflanoitponA
ezis-elbairavelpitlUM
ezis-elbairavelgniS
ezis-elbairavelgniS
ydobegasseM
c
dnaatadenil-niezis
toskcolbenil-to..tuo
fokcolbenil-fo-tuo
fokcolbenil-fo-tuo
. ~ 0
-elbairavlanoitpona
atad
atad
atad
=0:
foatadenil-niezis
=:
setybK46mumixam
~ P'( 't' ~ n '-'
J..,...... C\

a.... ~
~ ."..
surohC
hcaM
metsyS-V
abeomA
erutaeF
... ~
depytnU
depytnu/depytebyaM
depytnU
depytnU
?segassemdepytnu/depyT
......
stroP
stroP
aebyam(sDI-reganaM
sessecorP
otdesserddasegasseM
>
)01troparossecorp
)e'(
-daorbdnaehcactniH
-daorbdnaehcactniH
-daorbdnaehcactniH
-daorbdnaehc8CtniH
gnitacolrofdesumsinahceM
g.
gnitsac
gnitsac
gnitsac
gnitsac
sessecorprostrop
.§
seY
seY
oN
oN
msinahcemetirw-no-ypocsesU
8)ft
?CPIlacolrof
deredronu
,elbailernU
enoN
tsacdaorbderedronU
deredro,elbaileR
ytilicafnoitacinummocpuorG
~
ytilicaftsacdaorb
ytilibixelfhtiwytilicaf
ytilicaftsacdaorb
J
foeergedehtesoohcot
ytilibailer
krowtenlevel-resU
krowtenlevel-resU
lenreK
lemeK
ybdetroppusCPfediw-krowteN
~
srevres
srevres
ne e< ne
PI/PCT
PIIPCT
PTMVdnaPIIPCT
PILFdnaPIIPCT
slocotorpkrowtendetroppuS
Dp<
seY
seY
seY
oN
yromemlautrivstroppuS
c:
?msinahcem
~
seY
seY
oN
oN
rofgnirahsetirw-no-ypocstroppuS
F
ehtnosessecorpybderahsstcejbo
~
?edonemas
..Q ~
seY
seY
oN
oN
?msinahcemregaplanretxesesU
2
)desab-egap(seY
)desab-egap(seY
)desab-egap(seY
)desab-tcejbo(seY
detubirtsidroftroppussedivorP
~
?yromemderahs
Q I:: .C i
noitacilperylno-daeR
noitacilperylno-daeR
cificeps-melborP
lla-etirW
rofdesulocotorpecnerehoC
setadpu
yromem
derahsdetubirtsid
e I::
selifelbatuM
selifelbatuM
selifelbatuM
selifelbatummI
ledomeliF
~
oN
oN
oN
seY
derotsebotsaheliferitnenA
?ylsuougitnoc
0 ~ " QJ( ~

~ -Qic .. ..N; ......
surohC
hcaM
metsvS-V
abeomA
erutaeF
->
ledomgnihcac-ataD
ledomgnihcac-ataD
ledomgnihcac-ataD
ledomgnihcac-ataD
ledomgnissecca-elifetomeR
3 Jo"( 0"
kcotB
kcolB
kcolB
eliF
refsnartatadfotinU
-1~: ;~:
-resue
r h e t v n re o s s e d l n if e l p ev e e D
l
-resue
r h e t v n re o s s e d l n if e le p v e e D l
-resue
r h e t v n re o s s e d
l n if e le p v e e D l
selif-derahse
s l c b i a tn tu a m m m es
I
scitnamesgnirahs-eliF
.e- >
oN
oN
oN
seY
?noitacilperelifcitamotuA
3 ~
se
-n it
e ili
d b i a n p o a i c tc e e c to ru rp os d e n R a
trop( )s s e t i h ti g fi i b r a t p ro a P c
sr
y e b ga
de n g a a m na e m cru
s o L s C e A r
gn
s i e s i u til
d ib
e a t p p
a y
c rc
s n e e c e ru ra os ta e
h R t
msinahcem
lortnocsseccA
" ~ 0
sreifit
snoitcnufyaw-eno
n ~ e e-
seY
oN
seY
oN
?Snoitacilppaemit-laerroftroppuS
-Snt F
ytilibitapmocyraniB
ytilibitapmocyraniB
ytilibitapmocyraniB
-apmocedocecruoS
noitalume
XINU
vtilibit
:3 ~o
seY
oN
oN
oN
detneiro-tcejboroftroppuS
"?
?Snoitacilppadnasegaugnal
s~:: ..0 o r:: o 2 n:
a;.... N ~

722 Chap. 12 • Case Studies
From the description of these systems, it can be concluded that microkemel-based
distributed operating systems with open-system architecture are going to dominate in
future. For commercial success, it is important for any newly developed distributed
operating system to provide binary compatibility with UNIX. Features such as process
migration and load balancing are still restricted to research projects. This is mainly
because, with the current state of the art, these features are expensive to implement and
use and have not yet proved to give promising results. Therefore, more research work
needs to be carried out in these areas.
EXERCISES
12.1. What isan"open distributed system"? What isthemostimportant factorinthedesign ofa
distributed operating system that influences thischaracteristic of the system?
12.2. Based onthedescription ofAmoeba, V-System,Mach, andChorus, whattrend are modern
distributed operating systems following for kernel design? What are the main reasons for
using this approach?Are there any disadvantages in usingthisapproach?
12.3. Name the main hardware and software components of Amoeba and describe their
functions.
12.4. Answer the following questions for the capability-based object-naming mechanism of
Amoeba:
(a) Howdoes asubjectgetacapability foranobject?Consider boththecase in which the
subject istheowneroftheobject and thecase in whichthesubject isnottheowner of
theobject.
(b) How are capabilities protected against unauthorized access?
(c) How arecapabilities madedifficult to forge?
(d) When a subject accesses an object, how is the validation check made if the subject is
allowed to access theobject in the requested mode?
(e) Howcantheownerofanobjectwhowantstosharetheobjectwithanothersubjectallow
restricted access rights to the subject for itsobject?
12.5. In acapability-based system, anobject whose capability is lostcan never be accessed but
will remain forever inthe system. How isthis situation prevented inAmoeba?
12.6. Describe the process model ofAmoeba.
12.7. What is aglocal variable inAmoeba? Why is itused?
12.8. Inthe processor-pool modelofAmoeba, explain howaprocessor isselected from the pool
for executing a new process.
12.9. Describe the policy used inAmoeba for object replication.
12.10. Amoeba usesthe immutablefile model. However,inafile system thatusesthe immutable
file model, every time a file is modified, a new version of the file is created and the old
version isdeleted.Thismakesfile modifications slowandcomplicated, especially forafile
that is frequently modified. Explain howAmoeba solves this problem.
12.11. Describe the group communication facility ofAmoeba.
12.12. Explain how the following areensured inAmoeba:
(a) Only genuine clients can request services from a server.
(b) Only genuine servers can receive client requests.

Chap. 12 • Exercises 723
12.13. Name the main hardware and software components of V-System and describe their
functions.
12.14. V-Systemintegratesobject-namingandobjectmanagementmechanisms.Whatadvantages
doesthisapproach haveinthedesignofadistributedoperatingsystem?
12.15. Describe the mechanisms used in V-System to minimize the kernel's job in process
managementactivities.
12.16. Describe the UIO interface of V-System. Explain how this facility simplifies device
managementactivities.
12.17. DescribethemulticastcommunicationfacilityofV-System.Mention someofthepractical
applicationsof this facilityinthedesignof V-System.
12.18. InV-System,thesenderactionsofsendingarequestandreceivingaresponsearecombined
intoasingleSend primitive.Whatadvaritagesdoesthisapproachprovide?Woulditnothave
beenbetter and simpler to have twocalls, Send request and Get_reply, one forsendinga
requestandone forreceivinga response?
12.19. Explainhowthecopy-on-writemechanismofMachprovidesthepotentialtoeliminatemuch
data movementinthe system.
12.20. InMach,manykernelactivitiesaremovedoutofthekerneldirectlyintotheclient'saddress
space by being placed in a transparent shared library.What advantages does this design
provides?
12.21. To keep track of the state (suspended/running) of a thread (or process), Mach uses a
"suspended" counter that isassociatedwitheachthread(orprocess).Sincethereareonly
twopossiblestates(suspendedandrunning),asinglebitwouldhavebeensufficientforthis
purpose.Explain thereasonfortheuseof acounterinsteadofa bitforthispurpose.
12.22. Describehow threadsof aprocessaresynchronizedinMach.
12.23. Answerthefollowingquestions forthethreads-schedulingschemeof Mach:
(a) What isthe useof a "hint" variable?
(b) Whatisthe useof a"count" variable?
(c) What is the need to have two run queues instead of a single run queue for each
CPU?
(d) What happens if there are no threads waiting to be executed on either of the two
queues?
(c) Howisitensuredthatthesystemwillprovidehighefficiencywhenlightlyloaded,while
providinggood responsetoshortrequestseven whenheavily.loaded?
(f) How isthe monopolizationof aCPU bya high-prioritythreadprevented?
(g) How isfair CPt] utilizationensured ina situationin which the systemloadsuddenly
increases?
(h) Whyisaglobalrunqueuelockedbeforebeingsearchedforaready-to-runthreadbuta
localrunqueue is notlockedbeforebeingsearched?
12.24. Acommonlyusedapproachtomaintainthevirtualaddressspaceofeachprocessistokeep
a linear page table from zero to the highest used page in the kernel. Explain why this
approach is not used.in Mach. Describe the approach used in Mach for maintaining the
virtualaddressspaceofeach process.
12.25. WhatisanexternalmemorymanagerinMach?flowisitrealized?Describehowanexternal
memorymanagerandthekernelinteractwitheachothertomanageamemoryobjectandto
satisfyuseraccess requestsfordata inthememoryobject.

724 Chap. 12 • Case Studies
12.26. Describehow theexternalmemory managerconceptofMachcan beusedtoimplementthe
distributed shared memory facility.
12.27. Describe the copy-on-write and controlled inheritance mechanisms used in Mach for
memory sharing. Give an example to illustrate the practical use of each of these
mechanisms.
12.28.Answerthe following questions for the port-based IPC mechanism of Mach:
(a) Why are ports kept track of on a per-process basis rather than on aper-thread basis?
(b) Anapplicationrequiresthatasenderbeallowedtosendmessages toaportonlyntimes.
How can this be realized?
(c) What happens when a process having a port capability with receive right sends that
capability in a message toanotherprocess?
(d) How are ports protected?
(e) What happens when a process holding a port capability with receive right exits or is
killed?
(f) What happens if a sender sends a message to a port whose receiverprocess has been
killed?
(g) What happens if a message arrives at a port whose queue is full?
(h) What is a port-set? Give a practical use of this facility.
(i) Why is message sending to aport-setnot allowed?
(j) Why can a port that belongs to a port-set not be used directly to receive messages?
12.29. Differentiatebetweenin-line dataandout-of-linedata inthemessage-passingmechanismof
Mach. Describe how out-of-linedata are transferred from asender to a receiver.
12.30. InMach, explainhow messagesareexchangedtransparentlybetween twoprocessesthatare
locatedon differentnodes.
12.31. What is the role of MIG in Mach? Explain how it simplifies the job of application
programmers.
12.32. In Mach's UNIX emulation, system calls related to file VO have been implemented
differently than the basic approach described in this chapter. Find out how file 110 system
calls are implemented and the reason for using a different approach for implementing
them.
12.33. Describe how Chorus provides support for real-time applications.
12.34. Chorusallowsservers(subsystemprocesses)toresideeitherinashared kerneladdressspace
or in private user addressspaces. Discussthe relativeadvantagesand disadvantagesof this
feature ofChorus.
12.35. ExplainhowChorusprovidestheflexibilitytodynamicallyconfigurethesystem softwareto
match the hardwarecomponentsof a particularnode of the system.
12.36.Whatare the varioussecurity mechanismsprovided inChorus?What type of securitydoes
each ofthese mechanisms provide?
12.37.Explain why threetypes of processes are used in Chorus.
12.38. Describe the threads-scheduling scheme of Chorus. What are its advantages and
disadvantages?
12.39. Differentiatebetweenbroadcast,functional, and selectivefunctional modes of communica-
tion in Chorus. Give a practical use of each of these modes of communication.
12.40. Explain the port migrationfacility of Chorus. How does ithelp?
12.41. What is a port group in Chorus?Give two practical uses of this facility.
12.42. Explain how UNIX processescan be created on a remote node inChorus.

Chap. 12 • Bibliography 725
12.43. Discuss therelative advantages anddisadvantages oftheapproaches taken byMachand
Chorus forUNIX emulation.
12.44. Explain howobject-oriented facility issupported inChorus. Canthisfacility be provided
alongwithUNIXemulationfacilityonthesamemachine?Givereasonsforyouranswer.
818llOGRAPHY
[Abrossimovetal, 1992]Abrossimov,A.,Armand,F.,andOrtega,M.,"ADistributedConsistency
Serverfor theCHORUSSystem," In: Proceedings ofthe USENIX SEDMS III Symposium on
Experience withDistributedand Multiprocessor Systems, USENIXAssociation,Berkeley,CA,
pp. 129-148 (1992).
[Accetta et al, 1986]Accetta,M.,Baron,R.,Golub,{J.,Rashid,R.,Tevanian,A.,andYoung, M.,
"Mach:ANewKernelFoundationforUNIXDevelopment,"In:ProceedingsoftheSummer1986
USENIX TechnicalConference, USENIXAssociation,Berkeley,CA,pp.93-112 (July 1986).
[Almesetal,1985]Almes,G.T.,Black,A.~,Lazowska,E.D.,andNoe,1.D.,uTheEdenSystem:
ATechnicalReview,"IEEETransactionsonSoftwareEngineering,Vol.SE-ll,No.1,pp.43-59
(January1985).
[Andrewset al, 1987]Andrews,G.R.,Schlichting,R.D.,Hayes,R.,andPurdin,T.D.M.,"The
Design of the Saguaro Distributed Operating System," IEEE Transactions on Software
Engineering, Vol. SE-I3, No. I, pp. 104-118(1987).
[Armand et al, 1986] Armand, F., Gien, M., GuilJemont, M., and Leonard, P., "Towards a
Distributed UNIXSystem-The CHORUSApproach,"In: Proceedings ofthe Autumn 1986
EUUG Conference, USENIXAssociation,Berkeley, CA,pp.4]3-431 (September1986).
[Armand et al, 1989]Armand, F.,Gien, M.,Herrman, E, and Rozier,M.,"DistributingUNIX
BringsIt Backto Its OriginalVirtues,"In:Proceedings ofthe Workshopon Experiences with
Building Distributed and MultiprocessorSystems, pp. 153-174 (October1989).
[Babaoglu 1990] Babaoglu, 0., "Fault-TolerantComputingBasedon Mach,"Operating Systems
Review,Vol. 24,No.1, pp.27-39 (1990).
[Balet al, 1992]Bal, H.E., Kaashoek,F.,and Tanenbaum,A.S.,"Orca:ALanguageforParallel
ProgrammingofDistributedSystems,"IEE'ETransactionsonSoftwareEngineering, Vol. SE-18,
No.3, pp. 190-205 (1992).
[BaninoandFabre1982]Banino,1.S.,andFabre,1.C.,"DistributedCoupledActors:ACHORUS
Proposalfor Reliability,"In: Proceedings ofthe 3rd International Conference on Distributed
Computing Systems, IEEEPress,NewYork, NY, p.7 (October1982).
[Baninoetal,1985JBanino,1.S.,Fabre,1.C.,Guillemont,M.,Morisset,G.,andZimmermann,H.,
"SomeFaultTolerantAspectsoftheCHORUSDistributeSystem,"In:Proceedings ofthe 5th
InternationalConference on DistributedComputing Systems, IEEEPress,NewYork,NY(May
1985).
[Baron et al, 1985]Baron,R.,Rashid,R.,Siegel,E.,Tevanian,A.)andYoung, M.,"Mach-I: An
OperatingEnvironmentforLarge-ScaleMultiprocessorApplications,"IEEESoftware,Vol.2,pp.
65-67 (1985).
[Batlivalaetal, 1992]Batlivala,N.,Gleeson,B.,Hamrick,1.,Lumdal,S.,Price,D.,Soddy,1.,and
Abrossimov, V., "Experience with SVR4 over CHORUS," In: Proceedings of the USENIX
WorkshoponMicrokernels andOtherKernelArchitectures,USENIXAssociation,Berkeley,CA,
pp.223-241 (1992).

726 Chap. 12 • CaseStudies
[Berglund 1986]Berglund,E.J., "An Introduction to the V-System," IEEEMICRO, pp. 35-52
(August]986).
[Beveret ale1993]Bever,M.,Geihs,K.,Heuser,L.,Muhlhauser,M.,andSchill,A.,"Distributed
oce-ts«
Systems,OSFDCE,andBeyond,"In:A.Schill(Ed.), OSF DistributedComputing
Environment, Springer-Verlag, Berlin,pp. 1-20 (1993).
[Black 1990] Black, D. L., "SchedulingSupportfor Concurrencyand Parallelism in the Mach
OperatingSystem,"IEEE Computer, Vol. 23,No.5, pp.35-43 (1990).
[Blacket al, 1992]Black,D.L.,Golub,D.B.,Julin,D.P.,Rashid,R.F.,Draves,R.P., Dean,R.
W., Forin,A" Barrera,J., Tokuda,H.,Malan,G., and Bohman, D.,"Microkernel Operating
SystemArchitectureandMach,"I~: Proceedingsofthe USENIX WorkshoponMicrokernels and
Other Kernel Architectures, VSENIXAssociation,Berkeley,CA,pp. 11-30(1992).
[Boykinetal.1993]Boykin,J.,Kirschen,D.,Langerman,A.,andLoVerso,S.,Programmingunder
Mach, Addison-Wesley, Reading,MA(1993).
[Boykinand Langerman 1990)Boykin,J., andLangerman,A.,"Mach/4.3BSD: AConservative
ApproachtoParallelization,"Computing Systems Journal, Vol. 3,pp.69-99 (1990).
[Champine et al. 1990] Champine,G.A.,Geer,Jr.,D.E.~ andRuh, W.N.,"ProjectAthenaasa
DistributedComputerSystem,"IEEE Computer, pp.40-50 (September]990).
-[Cheriton1984]Cheriton,D.R.,"TheVkernel:ASoftwareBaseforDistributedSystems,"IEEE
Software, Vol. 1,No.2, pp. 19-42(April1984).
[Cheriton 1987]Cheriton,D.R.,"UIO:AUniformI/OInterfaceforDistributed Systems,"ACM
Transactions on Computer Systems, Vol. 5, No. I, pp. 12-46(1987).
[Cheriton1988]Cheriton,D.R.,"TheVDistributedSystem,"Communications oftheACM, Vol.
31,No.3, pp.314-333 (1988).© ACM,Inc.,1988.
[Cheriton and Mann 1989]Cheriton,D.R.,andMann,T.P.,"Decentralizing aGlobalNaming
Service for Improved Performance and Fault Tolerance,"ACM Transactions on Computer
Systems, Vol. 7, No.2, pp. 147-183(1989).
[Cheriton etal. 1979]Cheriton,D.R.,Malcolm,M.A.,Melen,L.S.,andSager,G.R.,"Thoth,a
Portable Real-Time Operating System," Communications of the ACM, Vol. 22, No.2, pp.
105-115 (February1979).
[Cheriton et al. 1990]Cheriton,D.,Whitehead,G., andSznyter,E., "BinaryEmulationofUNIX
UsingtheVKernel,"In:ProceedingsoftheSummer USENIXConference,USENIXAssociation,
Berkeley,CA,pp.73-85 (1990).
[Coulouris et al. 1994]Coulouris,G. F.,Dollimore,J., and Kindberg,T.,Distributed Systems
Concepts and Design, 2nded.,Addison-Wesley, Reading,MA(1994).
[Dasgupta et al, 1991]Dasgupta, P, LeBlanc,R.J.,Ahamad,M.,andRamachandran, V., "The
CloudsDistributedOperatingSystem," IEEE Computer, Vol. 24,No. 11,pp.34-44(1991).
[DeanandArmand1992)Dean,R.,andArmand,F.,"DataMovementinKemelizedSystems,"In:
Proceedings ofthe USENIX Workshop on Microkemels, USENIXAssociation, Berkeley, CA
(1992).
(Dougliset al, 1991]Douglis,F.,Ousterhout,J. K.,Kaashoek,M.F.,andTanenbaum, A.S.,"A
ComparisonofTwoDistributedSystems:AmoebaandSprite,"Computing SystemsJournal,Vol.
4, pp.353-384(1991).
[Draves 1990]Draves,R. P.,"A RevisedfPCInterface,"In: Proceedings ofthe USENIX Mach
Workshop, VSENIXAssociation,Berkeley, CA,pp. 101-121(October1990).
[Dravesetat.1989]Draves,R.P.,Jones,M.B.,andThompson,M.R.,HMIG-TheMachInterface
Generator,"TechnicalReport,Departmentof ComputerScience,CarnegieMellonUniversity
(1989).

Chap. 12 • Bibliography 727
[DutY1990]Duff,T.,"Rc-AShellforPlan9andUNIXSystems,"In:ProceedingsoftheSummer
1990UKUUG Conference, USENIXAssociation, Berkeley, CA,pp.21-33 (July 1990).
[Finkeletal. 1989]Finkel,R.,Scott,M.L.,Kalsow,W.K.,Artsy,Y.,andChang,H.Y.,"Experience
withCharlotte:SimplicityandFunctioninaDistributedOperatingSystem,"IEEETransactions
onSoftware Engineering, Vol. SE-15,No.6, pp.676-685 (1989).
[Fitzgerald and Rashid 1986] Fitzgerald, R., and Rashid, R. F., "The Integration of Virtual
Memory Management and Interprocess Communication in Accent," ACM Transactions on
ComputerSystems, Vol. 4, No.2, pp. ]47-177 (1986).
[Gienand Grob 1992] Gien,M.,andGrob, L.,"MicrokernelBasedOperatingSystems:Moving
UNIXon toModernSystemArchitectures,"In: Proceedings ofthe UniForum'92 Conference,
USENIXAssociation,Berkeley,CA,pp.43-55 (1992).
[Golub et al, 1990]Golub, D., Dean, R., Forin,A., and Rashid,R., "UNIX as anApplication
Program," In: Proceedings ofthe Summer 1990 USENIX Conference, USENIXAssociation,
Berkeley,CA,pp.87-95 (June 1990).
[Guillemont 1982] Guillemont, M., "The CHORUSDistributedComputingSystem:Design and
Implementation,"In:ProceedingsoftheInternationalSymposiumonLocalComputerNetworks,
pp.207-223 (April 1982).
[Jones and Rashid 1986] Jones, M. B.,and Rashid,R.F, "Machand Matchmaker:Kerneland
LanguageSupportforObject-OrientedDistributedSystems,"In: Proceedings ofOOPSLA'86,
AssociationforComputingMachinery,NewYork,NY, pp.67-77 (September1986).
[KaashoekandTanenbaum 1991] Kaashoek,M.F.,andTanenbaum,A.S., "GroupCommunica
tion in theAmoebaDistributedOperating System," In:Proceedings ofthe 11thInternational
Conference onDistributedComputing Systems,IEEEPress,NewYork, NY, pp.222-230(May
1991).
[Keefeeet al. 1985] Keefee,D.,Tomlinson,G.M.,Wand, I.C.,andWeilings,A.1.,PULSE: An
Ada-BasedDistributedOperating System, Academic Press, San Diego, CA (1985).
[Lea et al, 1991] Lea.R., Amara),P.,and Jacquemot,C., "COOL-2:AnObject-OrientedSupport
PlatformBuiltAbovetheChorusMicrokernel,"In:ProceedingsoftheInternationalWorkshopon
Object-OrientedSystems,pp.51-55 (1991).
[Lea et ale 1993] Lea, R., Jacquemot, C., and Pillevesse, E., "COOL: System Support for
DistributedProgramming,"CommunicationsoftheACM,Vol.36,No.9, pp.37-46(September
1993).
[Levine 1987] Levine,P. H.,"The DOMAINSystem," In:Proceedings ofthe2ndACMSIGOPS
Workshop onMakingDistributedSystemsWork, Operating SystemsReview, Vol. 21,No.1, pp.
49-84 (1987).
[Miller et al. 1987J Miller, B. P., Presotto, D. L., and Powell, M. L., "DEMOSIMP: The
DevelopmentofaDistributedOperatingSystem,"Software-i-PracticeandExperience, Vol. 17,
No.4, pp.277-290 (1987).
[Milojcic 1994] Milojcic, D. S., Load Distribution, Implementation for the MachMicrokernel,
Verlag Vieweg, Wiesbaden(1994).
[Morris et al, 1986] Morris,J.H.,Satyanarayanan,M.,Conner,M.H.,Howard,J.H.,Rosenthal,
D. S. H., and Smith, F. D., "Andrew: A Distributed Personal Computing Environment,"
Communications oftheACM,Vol. 29, No.3, pp. ]84-201 (]986).

728 Chap. 12 • CaseStudies
[Mullender and Thnenbaum 1986]Mullender, S. L, andTanenbaum,A. S., "The Design of a
Capability-BasedDistributedOperatingSystem," The Computer Journal, Vol. 29, No.4, pp.
289-299(1986).
[Mullenderetal,1990]Mullender,S.1.,VanRossum,G.,Tanenbaum,A.S.,VanRenesse,R.,and
VanStaverene,H.,"Amoeba:ADistributedOperatingSystemforthe 19908,"IEEE Computer,
Vol. 23,No.5, pp.44-53 (1990).
[Needhamand Herbert 1982]Needham,R.M.,andHerbert,A.J., The Cambridge Distributed
Computing System,Addison..Wes)ey, Reading,MA(1982).
[NelsonandLeach1984]Nelson,D.L,andLeach,P.J.,"TheArchitectureandApplicationsofthe
ApolloDOMAIN," IEEEComputerGraphicsandApplications(April1984).
[Orman etal, 1993]Orman,H.,Menze,E.,O'Malley,S.,andPeterson,L.,"A FastandGeneral
ImplementationofMachIPCinaNetwork,"In:Proceedingsofthe3rdUSENIXMach Workshop,
USENIXAssociation,Berkeley,CA(April1993).
[Ousterhout et al. 1988]Ousterhout, J. K.,Cherenson,A. R.,Douglis,F.,Nelson,M.N.,and
Welch,B. B., "The Sprite NetworkOperatingSystem," IEEE Computer, Vol. 21, No.2, pp.
23-36(1988).
[Pikeetal.1990]Pike,R.,Presotto,D.,Thompson,K.,andTrickey,H.,"Plan9fromBeJl Labs,"
In:Proceedings ofthe Summer 1990 UKUUG (UK Unix Users Group) Conference, USENIX
Association,Berkeley,CA,pp. 1-9(July 1990).
[Popekand Walker1985]Popek,G.,andWalker,B.,TheLOCUSDistributedSystemArchitecture,
MITPress,Cambridge,MA(1985).
[Pountain 1994]Pountain,D.,"TheChorusMicrokernel,"BYTE, pp. 131-136(January1994).
[Presottoetal.1991]Presotto,D.,Pike,R.,Thompson,K.,andTrickey,H.,"Plan9,ADistributed
System,"In:ProceedingsoftheSpring 1991EurOpenConference,EurOpen,Hertfordshire,UK,
pp.43-50(May 1991).
[Rashid1986]Rashid,R.F.,"FromRIGtoAccenttoMach:TheEvolutionofaNetworkOperating
System," In: Proceedings of the Fall Joint Computer Conference, AFIPS, pp. 1128-1137
(November1986).
[Rashid1987]Rashid,R.F.,"Mach:ANewFoundationforMultiprocessorSystemsDevelopment,"
In:ProceedingsofCOMPCON'87-DigestofPapers, IEEEPress,NewYork,NY,pp.192-193
(1987).
[Rashid and Robertson 1981]Rashid, R. F.,and Robertson,G., "Accent: A Communication
OrientedNetworkOperatingSystemKernel,"In: Proceedings ofthe 8th ACM Symposium on
OperatingSystemsPrinciples,AssociationforComputingMachinery,NewYork,NY,pp.64-75
(December1981).
[Rashidetale1988]Rashid,R.,Tevanian,A.,Young,M.,Golub,D.,Baron,R.,Black,D.,Bolosky,
W.J.,andChew,1.,"Machine-IndependentVirtualMemoryManagementforPagedUniprocessor
and MultiprocessorArchitecture," IEEE Transactions on Computers, Vol. C-37, No.8, pp.
869-908 (1988).
[Rozier and Legatheaux 1986] Rozier,M., and Legatheaux, J. M., "The Chorus Distributed
Operating System: Some Design Issues," In: Y. Parker et at (Eds.), Distributed Operating
Systems: Theory and Practice,NATOASISeries,Vol.F28,Springer-Verlag,NewYork,NY,pp.
261-289 (1986).
[Rozieretal. 1988]Rozier,M.,Abrossimov, V.,Armand,F.,Boule,I.,Gien,M.,Guillernont, M.,
Herrmann,F., Kaiser,C., Leonard,P.,Langlois,S., and Neuhauser,W.,"ChorusDistributed
OperatingSystem,"Computing Systems Journal, Vol. 1,No.4, pp.305-379(1988).

Chap.12 • Bibliography 729
[Satyanarayananetal,1990]Satyanarayanan,M.,Kistler,J.1.,Kumar,P.,Okasaki,M.E.,Siegel,
E.H.,andSteere,D.C.,"Coda:AHighlyAvailableFileSystemforaDistributedWorkstation
Environment,"IEEE Transactions on Computers, Vol. 39,No.4, pp.447-459 (1990).
[Schantzetal, 1986]Schantz,R.E.,Thomas,R.H.,andBono,G.,"TheArchitectureoftheCronus
Distributed Operating System," In: Proceedings of the 6th International Conference on
DistributedComputing Systems, IEEEPress,NewYork, NY, pp.250-259 (1986).
[Schroederet al. 1984]Schroeder, M.D.,Birrell,A.D.,andNeedham, R.M.,"Experiencewith
Grapevine:TheGrowthofaDistributedSystem,"ACMTransactionsonComputerSystems, Vol.
2,No.1, pp.3-23 (1984).
[Shrivastavaetal.1991]Shrivastava,S.,Dixon,G.N.,andParrington,G.D.,"AnOverviewofthe
ArjunaDistributedProgrammingSystem,"IEEE Software, pp.66-73 (January1991).
[Silberschatzand Galvin 1994]Silberschatz,A.,andGalvin,P.B.,Operating Systems Concepts,
4thed., Addison-Wesley. Reading,MA(1994).
[Sinha etal. 1991]Sinha,PK.,Maekawa,M.,Shimizu,K.,Jia,X.,Ashihara,H.,Utsunomiya,N.,
Park,K.S.,andNakano,H.,"TheGalaxyDistributedOperatingSystem,"IEEE'Computer, Vol.
24,No.8, pp.34-41 (1991).
[Sinha etal, 1994]Sinha,P K.,Maekawa,M.,Shimizu.K,Jia,X.,Ashihara,H.,Utsunomiya,N.,
Park,K.S.,andNakano,H.,"TheArchitecturalOverviewoftheGalaxyDistributedOperating
System,"In:T.L.CasavantandM.Singhal(Eds.),ReadingsinDistributed Computing Systems,
IEEEComputerSocietyPress,LosAlamitos,CA,pp.327-345 (1994).
[Swinehart et at. 1986] Swinehart, D., et al., "A Structural View of the Cedar Programming
Environment,"ACMTransactions on Programming Languagesand Systems, Vol. 8,No.4, pp.
419-490 (1986).
[Tanenbaum 1995]Tanenbaum, A. S., Distributed Operating Systems, Prentice-Hall, Englewood
Cliffs,NJ(1995).
(Tanenbaum and Van Renesse 1985] Tanenbaum, A. S., and Van Renesse, R., "Distributed
OperatingSystems,"ACM Computing Surveys, Vol. 17,No.4, pp. 419-470 (1985).
[Tanenbaum et al. 1990] Tanenbaum, A. S., VanRenesse,R., Staveren, H. Van, Sharp,G. 1.,
Mullender,S.1., Jansen,1., and Van Rossum,G., "ExperienceswiththeAmoebaDistributed
OperatingSystem."Communications oftheACM, Vol. 33,pp.46-63 (1990).
[Theimer et al. 1985]Theimer, M. M.,Lantz,K.A.,andCheriton,D.R.,"Preernptable Remote
Execution Facilities for the V-System," In: Proceedings of the JOih ACM Symposium on
OperatingSystems Principles, AssociationforComputingMachinery, NewYork, NY, pp.2-12
(December1985).
[Tokuda et al, 1990] Tokuda, H., Nakajima, T., and Rao, P., "Real-Time Mach: Towards a
Predictable Real-Time System," In: Proceedings of the USENIX Mach Workshop, USENIX
Association, Berkeley, CA, pp.73-82 (October1990).
[Tripathi1989]Tripathi,A.R.,"AnOverviewoftheNexusDistributedOperatingSystemDesign,"
IEE'ETransactions on Software Engineering, Vol. SE-15,No.6, pp.686-695 (1989).
[Wilkes and Needham 1980] Wilkes, M. V., and Needham, R. M., "The Cambridge Model
DistributedSystem,"Operating Systems Review, Vol. 14,No.1, pp.21-29 (1980).
[Zimmermann et al, 1981]Zimmermann,H., Banino,J.S., Caristan, A., Guillemont,M., and
Morisset,G.,"BasicConceptsfortheSupportofDistributedSystems:TheCHORUSApproach,"
In:Proceedings ofthe 2nd International Conference on DistributedComputing Systems, IEEE
Press,NewYork, NY, pp.60-66 (1981).

730 Chap. 12 • CaseStudies
POINTERS TO818UOGRAPHIES ONTHE INTERNET
Bibliographies containing references onAmoeba can be found at:
http:www.cs.vu.nVvakgroepen/cs/amoeba...:papers.html
ftp:ftp.cs.umanitoba.calpublbibliographieslDistributedlamoeba.html
I could not find a bibliography dedicated only to the V-System. However, the following
bibliographies contain references on this system:
http:www-dsg.stanford.edulPublications.html
ftp:ftp.cs.umanitoba.calpublbibliographies/OsIIMMD_IV.html
ftp:ftp.cs.umanitoba.calpublbibliographies/Os/os.html
Bibliographies containing references on Mach can be found at:
ftp:ftp.cs.umanitoba.calpublbibliographies/DistributedlMach.html
http:www.cs.cmu.edu/afs/cs/project/mach/public/www/doc/publications.html
A list of books dealing with Mach can be found at:
http:www.cs.cmu.edu/afs/cs/project/mach/public/www/doclbooks.html
Bibliographies containing references on Real Time Mach can be found at:
http:www.cs.cmu.edu/afs/cs/project/mach/public/www/doc/rtmach.html
http:www.cs.cmu.edu/afs/cs/project/art-6/www/publications.html
Bibliography containing references on Mach-US (an operating system developed as part
of theeMU Mach project) can be found at:
http:www.cs.cmu.edu/afs/cs.cmu.edu/project/machlpublic/www/projects/mach_us.html
Bibliography containing references on Chorus can be found at:
ftp:ftp.cs.umanitoba.calpublbibliographieslDistributedlchorus.html

Index
A
Abbreviation, 505-06 Affinity scheduling, 406
ABCAST protocol, 150-151 Aggregate, 476
Absolute ordering semantics, 148-49 Agora, 268
Accent, 389 Alewife, 266
Access control, 475, 566, 607-23 Alias, 505-06
Access Control List (ACL), 475, 545, All-or-nothing property, 453
616-17, 629 Alto, 4
Access matrix, 610-15 Amoeba, 11,390-91,429,512,515,
Access rights, 608, 626 643-59,714-21
Access synchronization, 234 Andrew File System (AFS), 429, 439-40,
ACID properties, 453 474, 476
Acknowledgement message, 26 Apollo Domain File System, 430
Actor, 696, 700 Application layer, 69
Address Resolution Protocol (ARP), 76 protocols, 78-79
Address space, 387 Archie, 88
transfer mechanisms, 387-90 ARPANET, 4, 88
Advanced Peer-to-Peer Networking (APPN), ASN.l notation, 549
73 Assignment edge, 308
731

732 Index
Asynchronous transfer mode (ATM),4-5, Best-effort delivery semantics, 77
48,91-104 Binding, 193,218
At-least-once semantics, 135, 186 changing bindings, 196-97
ATM-MAC layer, 101-02 multiple simultaneous bindings, 197
Atomic transactions, 25, 453-74, 483 Binding agent, 194-95
Atomicity property, 453 Binding time, 195-96
concurrency atomicity, 453 Biometric system, 587
failure atomicity, 453 Blast protocol, 139
Attack Block service, 422
active attack, 567, 569-73 Blocks, 233-34
authenticity attack, 573 Bridge, 84-85
chosen-plaintext attack, 576 Broadband Integrated Services Digital
ciphertext-only attack, 576 Network (B-ISDN), 48
delay attack, 573 Broadcast
denial attack, 573 broadcast address, 142-43
integrity attack, 572 broadcast mode, 708
known-plaintext attack, 576 broadcast-when-idle policy, 370
passive attack, 567-68 broadcasting, 246-47,253, 512-15
replay attack, 573 Brouter, 85
Trojan horse attack, 568 Browsing, 567
Attacker, 567 4.3BSD UNIX IPC mechanism, 153-57
Authentication, 586-607, 628-29 Buffering, 122-25
authentication server, 594, 596, 600 finite-bound buffer, 124-25
Kerberos authentication system, 589, 591, multiple-message buffer, 124-25
600-607 null buffer, 122-23
one-way authentication of communicating single-message buffer, 124
entities, 592-97 unbounded-capacity buffer,·124
two-way authentication of communicating Bulletin-board semantics, 143-44
entities, 597-99 Byzantine failures, 23-24
user authentication, 566
user logins authentication, 588-92
Authenticity, 566 (
Authority attributes, 502 Cache consistency problem, 428
Authorization, 607 Cache inheritance, 543
in DCE, 629 Cache location, 433-35
Automatic job sequencing, 3 Cache manager, 477
Autonomy, 17-18 Cache validation, 438-40
Availability, 15,422,424-25,441,447 Call buffering approach, 208-10
Call-by-move, 184
Call-by-object-reference, 184
8 Call-by-reference, 183-84
Back-offalgorithm, 56, 202 Call-by-value, 183
Bandwidth management, 102-03 Call-by-visit, 184
Barrier, 242, 260 Call semantics, 184-87, 218
Batch processing, 3 Callback policy,440

Index 733
Cambridge File Server, 430, 432 Clock synchronization, 283-92
Capability, 544-45, 618-22, 646-47, 702 Clock tick, 284
owner capability, 647 Cloning, 476, 481-82
Capability list, 618 Cluster, 713
Carrier, 52 Clustering
Carrier Sense Multiple Access with algorithmic clustering, 526-28
Collision Detection (CSMAlCD), attribute clustering, 528-30
52-54 syntactic clustering, 528
Cascaded aborting, 467 Clustering condition, 526
Causal ordering, 292 Collision detection, 52
semantics, 151-53 Collision interval, 52
CBCAST protocol, 152-53 Collision window, 52
Cedar File System (CFS), 427 Common Management Information Protocol
Cell Directory Service (CDS), 546, (CMIP), 87-88
550-55 Communication
Cell loss priority (CLP) field, 97 asynchronous, 121
Cell switching, 91, 93 broadcast, 141
Cells, 91, 93, 95 flow-controlled, 124-25
Central processing unit (CPU) utilization, group, 139-53
360 local, 116
Centralized algorithm, 31 many-to-many, 147-53
Centralized entity, 30-31 many-to-one, 140, 147
Centralized-server algorithm, 247, 254 multicast, 141, 671-72
Centralized system, 12 one-to-many, 140-47
Challenge-response method, 587 one-to-one, 139
Chandy-Misra-Hass (CMH) algorithm, point-to-point, 139
329-30 remote, 116
Charlotte, 392 synchronous, 12)
Chorus, 515, 696-721 unicast, 139
Chorus/MiX (Modular UNIX), 697, 710 unsuccessful, 124
Chorus Object-Oriented Layer (COOL), Communication domain, 153
700,713-14 Communication properties, 154
Cipher, one-way cipher, 589 Communication protocols, 64-83
Ciphertext, 575 for RPCs, 187-91
Clearinghouse, 502, 551 Computer network, 46-113
Cleartext, 575 Computer-supported cooperative working
Client-server-based communication, 79 (CSCW), 13
Client-server binding see binding Concurrency control, 464-71
Client-server model, 9-10 optimistic concurrency control, 469-70
Clocks Concurrent events, 292
drifting ofclocks, 284-85 Condition variable, 405
externally synchronized clocks, 285 Confidentiality, 625
internally synchronized clocks, 285 Confinement problem, 568, 573-75
logical clocks, 293-97 Connection-oriented protocol, 67
Clock skew, 286 Connectionless protocol, 67

734 Index
Consistency models, 238-62 D
causal consistency, 239-40 Dash, 266
lazy-release consistency, 243 Data caching, 233
pipelined random-access memory Data Encryption Standard (DES), 578
(PRAM) consistency, 240 Data integrity,425
processor consistency, 240-41 Data-link layer,67
release consistency, 242-43, 259-62 protocols, 74-76
sequential consistency, 238-39 Data transfer models, 429-30
protocols for implementing, 244-58 block-level transfer model, 429-30
strict consistency, 238 byte-level transfer model, 430
weak consistency, 241 file-level transfer model, 429
Consistency property, 453 page-level transfer model, 429
Consistent ordering semantics, 149-51 record-level transfer model, 430
Constant bit rate (CBR), 91-93 Datagram, 76, 125
Context, 504, 526-28 Deadlock, 305-32,468-69
current context, 506 avoidance, 313-16
current working context, 506 callback deadlock, 20I
metacontext, 538 communication deadlock, 312
Context binding, 531 detection, 320-30
Context distribution, 532-40 handling, 312-32
Controlled retransmission mechanism, 53 modeling, 307-11
Controller, 325 necessary conditions for, 307, 310-11
Coordinated Universal Time (UTC), 285 phantom deadlock, 321, 323
Coordinator, 332 prevention, 316-19
Coprocesses, 392-93 recovery from, 330-32
Copy-on-write mechanism, 692 resource deadlock, 312
Copy-on-write sharing, 688-89 sufficient conditions for, 310-11
Copy sharing, 114 Decentralized algorithm, 31
Covert channel, 574 Decipher, 575
Critical section/region, 297, 404 Decoding of message data, ]26-27
Cronus, 514 Decryption, 575
Cryptanalyst, 576 Delayed-write scheme, 437-38
Cryptography, 33, 573, 575-86 DEMOSIMP,388, 391-92
Cryptosystem Deserializing, 393
asymmetriccryptosystem,576-78,583-86 Digest function, 623
DES cryptosystem, 578 Digital signature, 623-25
private-key cryptosystem, 577 Direct demonstration method, 587
public-key cryptosystem, 577 Directed graph, 307
RSA cryptosystem, 578 Directory, 531
shared..key cryptosystem, 577 Directory Information Base (DIB), 549
symmetric cryptosystem, 576-83 Directory Information Tree (DIT), 549
Customer Information Control System Directory service, 423
(CICS),483 Disinfection utility,570
Cutset, 352, 354 Disk service, 422
Cycle, 307 Distinguished name (ON), 549

Index 735
Distributed Computing Environment (DeE), Domain name, 79
34-38,475 Domain Name/Naming Service/System
cells, 37-38 (DNS), 78-79, 502-04, 533, 547,
components, 36-37 549-50
creation of, 35-36 DOMAIN system, 515
definition of, 35 Duplicate request, 136
Directory Service, 546-56 Dynamic distributed-server algorithm, 249,
Distributed File System (DFS), 475-84 256
Distribute Time Service (DTS), Dynamic scheduling algorithms, centralized
290-92 versus distributed, 357-58
Remote Procedure Call (RPC), 221-22
Security Service, 627-30 E
technologies, 34 Eager object movement mechanism, 261
threads, 410-14 Early reply approach, 208
Distributed computing systems. 1-3 Election algorithms, 332-36
evolution, 3-5 bully algorithm, 333-36
models, 5-12 ring algorithm, 335-36
Distributed control, 25 E-mail (electronic mail), 89
Distributed Double-Loop Computer Encina, 483
Network (DOLeN), 533 Encipher, 575
Distributed dynamic scheduling algorithms, Encoding of message data, 126-27
cooperative versus noncooperative, Encryption, 575
358 Endpoint, 222
Distributed file service (DFS), 36, 475-84 Episode, 477
Distributed file system, 421-95 Ethernet, 53, 56-7
Distributed Management Environment Event ordering, 292-97
(DME), 87-88 Exactly-oncesemantics, 136-38, 186-87
Distributed operating system, 16-19 Exception handling, 198,219, 683-84, 705
design issues, 19-34 Expanding ring broadcast (ERB), 513
Distributed Shared Memory (DSM), Extensibility, 15
231-81, 706-07 eXternal Data Representation (XDR), 69, 393
advantages, 270-72 External memory manager, 685-88
block size selection, 235-·37
design and implementation issues, F
234-35 Fail-safe defaults, 626
general architecture, 233-34 Fail-stop failure, 23
heterogeneous, 267-70 Failure handling, 130-39
structure of shared memory space, False sharing, 236, 467-68, 480
237-38 Fast Local Internet Protocol (FLIP), 80,
Distributed Shared Virtual Memory 82-83
(DSVM),232 Fault, 23
Distributed system, 2, 19 Fault avoidance, 24
Distributed time service COTS), 290-92 Fault detection and recovery, 25-26
Distributed transaction service, 471-73 Fault tolerance, 24-25, 350,447-53, 482,
Domain, 79, 205, 501, 533, 608 498

736 Index
Fault tolerance capability, 18 Global Directory Agent (GOA), 554-55
FiberDistributedDataInterface(FOOl), 86 Global Directory Service (GDS), 546
File, 421 Global Name Service (GNS)t 520-22
File accessing models, 427-30, 477 Glocal variable, 651
data-caching model, 428 Gopher,88
remote service model, 428 Granularity, 235-37t467-68
File attributes, 426-27 Grapevine, 502
File caching, 433-40, 481 Group
File exporter, 477 closed group, 141
File group, 533 open group, 141
File models, 426-27 Group addressing, 142
immutable file, 427 Group communication, 79-80, 146-47,
mutable file, 427 657-59t708-09, 712
structured filet 426-27 Group management,141
unstructured file, 426-27 Group naming, 498
File replication, 440-47, 481 Group server,141
File service, 423, 454 Groupwaret 13
true file service, 423 Guarded command, 147
File sharing semantics, 430-33, 478-81
immutable shared-files semantics, 432 H
session semantics, 432 Handoff scheduling, 406
transaction-like semantics, 432-33 Happened-before relation, 151,292-93
UNIX semantics, 430-31 Header error control (HEe) field, 97
File system, 421 Heterogeneity, 32, 83-84, 203-04t235,
File transfer, 17 267-70,393-96,425
File transfer protocol (fTP), 69, 78 High-low policy, 361
File versions approach, 460-63 Hint, 535-36
Fileset, 476 Hint cache, 514-15
Fileset location server,478 Home machine, 6, 10-11
Fileset server,478 Host, 46
Fine-grain parallelism, 30 Host number, 74
Fixed distributed..server algorithm, 247, 254 Hydra, 620-21
Flavors, 219
Flexibility, 15-16, 26-29 I
Forward location pointer, 514 Idempotency, 136-39
Fragment, 82 Identification, 586
Frame, 56, 67 Identifier, 499, 509
Freezing time, 383 protection identifier, 702
Functional mode, 708 IEEE 802 LAN reference model, 71-73
IEEEToken Ring, 57-58
G Incremental growth, 15
Galaxy,526, 537, 542-43 Inferencing, 568
Gateway,86 InformationManagementSystem(IMS),483
Generic flow control (GFC) field, 97 Informationsharingamongdistributedusers,13
Generic runtime (GRT) layer,714 Inherently distributed application, 13

Index 737
Integrity, 566 Layered protocols, 64
Interactivejob, 3 Leaking, 568
Interconnection technologies, 84-86 Legitimate channel, 574
Interface Definition Language (IDL), 174, Linda, 237,267
212-16,221 Link, 391-92
Interface name, 193-94 Load balancing, 348, 355-67
Internet, 88-91 Load estimation policies, 359-60, 368
Internet address, 74 Load leveling, 355
Internet Control Message Protocol (ICMP), Load sharing, 348, 367-71
76-77 Local area network (LAN), 4, 47-58
Internet Protocol (IP), 67, 73-79 emulation overATM, 100-102
Internetworking, 83-91 Locating, 245-49, 253-58, 49~ 512-15
Interoperability, 100-102 Location independency, 497
Interprocess communication (IPC), 114, Location policies, 362-64, 368-69
118-19,656-59,690-94,707-10, Locking, 465-70
716 granularity oflocking, 467-68
Interrupt, 120 intention-to-write locks, 465-67
Intruder, 567 two-phase locking protocol, 467
IP address, 74 type-specific locking, 465
IP overATM, 100, 102 LOCUS, 388, 393, 430, 526, 533, 542-43
ISOIOSI reference model, 65-71 Logic bombs, 572
l'I'C File System, 437 Logical host, 392
IVY, 237, 244, 266 Long haul network, 47
Loosely-coupled system, 1-2
J
Jacket routine, 409-10, 412 M
Jamming signal, 53 ~ach, 410, 515,674-96, 714-21
Mach Interface Generator (MIG), 695-96
K Machine, 3, 46
k-fault tolerant, 24 Mailbox, 125
Kerberization, 607 Mapper, 706
Kerberos,589, 591, 600-607 Marshaling, 177-78, 218
Kernel, 18, 27, 715 Masquerading, 568
microkernel, 27-29 Maximum transfer unit (MTU), 76, 125
monolithic kernel, 27-29 May-be call semantics, 185
Kernel-mode, 610 Medium access control protocols, 51-55
Key distribution center (KDC), 578-83 Memory block, 233-34
Key distribution problem, 578-86 Memory coherence, 234
Knot, 307 Memorymanagement,684-89,705-07,717
Memory object, 685
L Memory sharing, 688-89
Language runtime layer, 714 Mercury communication system, 209
Last-of-many call semantics, 186 Mermaid, 267-69
Last-one semantics, 136, 185-86 Message, 115, 136--39,575, 690, 701
Latency/bandwidth tradeoff, 103-04 Message forwarding mechanisms, 390-92

738 Index
Message integrity, 623--25, 629 Mutex variable, 404-05
Message passing, 114-66, 691-92 fast mutex variable, 411
Message structure, 118-19 nonrecursive mutex variable, 411
Melber, 237 recursive mutex variable, 411
MetropolitanArea Network (~1AN), 48 Mutual exclusion, 297-305
Midway, 237, 267
Migration limiting policies, 367 N
Minicomputer, 4 Name, 499-500
Minicomputer model, 5 absolute name, 506-08
Mirage, 265-66 attribute-based name, 508-09
Modification propagation, 435-38 compound name, 501
Mosaic, 88 descriptive name, 508-09
Mount protocol, 523-25 flat name, 501
auto mounting, 523-24 generic name, 508
manual mounting, 523 group name, 508
static mounting, 523 hierarchical name, 502
Multiaccess branching bus network, 49 high-level name, 500
Multiaccess bus network, 48 human-oriented name, 500, 515-40
Multicache consistency, 435-40, 543-44 low-level name, 500
Multicast multicast name, 508
atomic multicast, 145-46 nickname, 506
buffered multicast, 143 primitive name, 501
unbuffered multicast, 143 qualified name, 504
Multicast address, 142 relative name, 506-08
Multicast Backbone (MBooe), 74 simple name, 501
Multicast communication, 141,671-72 source-routing name, 509
all-reliable, 145 system-oriented name, 500, 509-12
m-out-of-n-reliable, 145 Name agent, 503-04
l-reliable, 144 Name cache, 541-44
O-reliable, 144 directory cache, 542
Multicopy update protocols full-name cache, 542
available-copies protocol, 444 prefix cache, 542
majority-consensus protocol, 446 Name prefix, 534-35
primary-copy protocol, 444 Name resolution, 505, 532-40, 553
quorum-based protocol, 445-46 Name server, 130,502-03
read-all-write-any protocol, 446 authoritative name server, 502
read-any-write-all protocol, 443-44, 446 Name service, 423
read-only replication, 443 Name space, 500-502
weighted-voting protocol, 446-47 flat name space, 501
Multics, 610, 626 partitioned name space, 501-02
Multidatagram message, 125, 139 single global name space, 526
Multihomed host, 76 Naming,496-564, 663-65
Multimedia application, 5 and security, 544-46
Multiprogramming, 3 group naming, 498
Munin, 237, 259-62, 265, 267 uniform naming convention, 497

Index 739
Need-to-know principle, 566,626 Parallel processing system, 2
Negative right, 617 Password, 588-92
NetBIOS, 73 one-time password, 591
Net number, 74 Password generator, 591
Network File System (NFS), 428, 430, 451, Path, 307
475, 478, 481, 522, 524, 526 Payload type identifier (PTI) field, 97
Network layer, 67 Performance, 29-30, 424, 498
protocols, 76-77 Permanence property, 453
Network management, 80, 86-88 Physical layer, 66-7, 94-96
Network manager, 710 protocols, 74
Network message server, 692 Plaintext, 575
Network operating system, 16-18 Poll-when-idle policy, 371
Network system, 18 Polling, 120, 365-66
Network Time Protocol (NTP), 290 Port, 125, 690, 701
Network weight, 82 control port, 687
Networkwide IPC, 692-94, 710 name port, 687
NEXUS system, 515 network port, 692
Node, 3,46 object port, 687
home node, 391, 393 process port, 679
process node, 308 thread port, 679
resource node, 308 Port group, 708
Nonce, 573 Port migration, 709
Port number, 77
o Port set, 691
Portmapper, 218, 220
Object accessing, 512
POSIX, 410, 413
Object locating, 512-15
Presentation layer, 68-69
Octet, 74
Prctransferring (precopying), 388-89
Off-line processing, 3
Principal, 627
On-line shopping, 90
Principle of least privilege, 566, 626
On-line transaction processing (OLTP), 483
Priority assignment policies, 366
On-use consistency control, 541, 544
Privacy, 566
One-copy/single-copy semantics, 239
Privacy Enhanced Mail (PEM), 624-25
Open distributed system, 15
Privilege attributes, 628
Operating system, 16
Probable owner, 249
Optical Carrier level n (OC-n), 95-96
Process, 114,650,678-79
Orca, 267, 643
heavyweight process, 399
Ordered message delivery, J47-52
lightweight process, 399
Original sharing, 114
local process, 358
Orphan call, 186
remote process, 358
Orphan extermination, 186
Process addressing, 127-30
explicit addressing, 127
p
functional addressing, 127
Packet, 52, 125 implicit addressing, 127
Packet switching exchange (PSE), 59 link-based addressing, 128-29

740 Index
Process descriptor, 650 Remote procedure call (RPC), 36, 167-230
Process group, 671 asynchronous RPC, 188,220
Process management, 381-420,649-53, authenticated RPC, 629
666-69, 702-5, 715-16 batch-mode RPC, 203,220
Process migration, 272,381-98 broadcast RPC, 202,220
flow of execution in, 382 callback RPC, 199-201,220
heterogeneous systems in, 393-96 communication protocols for RPCs,
mechanisms, 384-93 187-91
nonpreemptive process migration, 370, complicated RPC, 191-92
382 DCE RPC, 221-22
preemptive process migration, 369,382 heterogeneous RPC (HRPC), 203-4
Process state, 387 lightweight RPC (LRPC), 204-8
Process transfer policies, 360-62, 368 protocol specification, 212
Processor allocation, 381 RPC Language (RPCL), 212-13
Processor-pool model, 10-11, 645 RPC messages, 174-77
Progress property, 321 call messages, 175
Promise data type, 209 reply messages, 175-77
Proofby knowledge, 586-87 simple RPC, 189
Proofby possession, 587 special types of RPC, 199-203, 220
Proofby property, 587 Sun RPC, 212-21
Protection domain, 608-10 transparency of RPC, 170-1
Protection rule, 608 Transport IndependentRPC(TI-RPC), 220
Protection state, 608, 611,613 Remote process creation, 712
Protocol, 64 Repeater, 49
Protocol family, 65 Replacement strategy, 235, 262-64
Protocol stack, 65 Replica, 422,440,498-99
Protocol suite, 65 first-class replica, 441
Public key manager (PKM), 584 naming, 442
Pup name service, 533 second-class replica, 441
Replication control, 442-43.
R explicit replication, 443
Reachable set, 307 implicitllazy replication, 443
Read-ahead mechanism, 478 Replication server, 478, 481
Read quorum, 445 Request edge, 308
Receiver Request (R) protocol, 188-89
nonselective, 147 Request/reply (RR) protocol, 189
selective, 147 Request/reply/acknowledge-reply(RRA)
Receiver-initiative policy, 369-70 protocol, 190-91
Recovery techniques, 459-64 Request-response protocol, 9
Redundancy techniques, 24 Research Storage System (RSS), 430
Redundant pages, 389 Resource
Region, 684,701,705 local, 3
Relative distinguished name (RDN), 549 nonpreemptable, 306
Reliability, 14-15, 23-6, 425, 441 preemptable, 318
Remote File Server (RFS), 433 remote, 3

Index 741
Resource allocation graph, 308-10 Selective retransmission mechanism, 81
Resource management, 347-80 Semantic transparency, 170
Resource sharing, 13 Send-to-all semantics, 143-44
Response time, 14 Sender-initiative policy, 369-70
Retransmission slot time, 53 Sequencer, 149
Reverse Address Resolution Protocol Serial Line Internet Protocol (SLIP), 74
(RARP),76 Serializability conflict, 461
Ring network, 50-51 Serializability property, 453
Rivesi-Shamir-Adlernan (RSA) Serializing, 393
cryptosystem, 578, 625 Serially equivalent, 453, 458
rlogin, 69 Server, 8
Robustness, 447 Servercreation semantics, 181-83
Router, 85 instance-per-call server, 181-82
Routing techniques, 61-64 instance-per-session server, 182
adaptive routing, 63 persistent server, 182-83
deterministic routing, 63 Server implementation, 178-81
dynamic routing, 63-64 Server locating, 194-95
hop-by-hop routing, 62 Server management, 178-83
hybrid routing, 63 Server naming, 193
source routing, 62 Service, 9
static routing, 63 Service paradigm, 449-53
rpcd, 222 Session, 432, 449, 622
rpcgen, 214 Session layer, 68
RPCRuntime, 171, 173 Shadow blocks technique, 462-63
RS232-C,67 Signal handling, 407, 413
Run server, 10, 652 Simple.and Efficient Adaptation Layer
(SEAL),99
Simple Mail Transfer Protocol (SMTP), 78
5 Simple Network Management Protocol
Safe sequence, 313 (SNMP), 87-88
Safe state, 313 Single-datagram messages, 125
Safety property, 321 Single sign-on, 591
Scalability, 30-3I, 350, 424, 441, 497 Site, 3,46
Schedule, 458 monitor site, 50
SDD-l database system, 471 origin site, 391, 393
Secrecy, 566 Skulking, 551
Security, 33,80, 198-99,219,425,544-46, Slotted ring protocol, 55
565-641,718 Smart card, 591
communication security, 566--67 Socket, 154
external security, 566 Spoofing, 568
internal security, 566 Sprite, 388, 391,430,437,439, 526, 533,
Security bit, 82 542-43
Segment, 650-51, 701, 705 Stable storage, 448-49
Selective functional mode, 709 Start-up misses, 543
Selective repeat technique, 139 Starvation, 298, 319, 332, 469

742 Index
State information exchange policies, Threads, 381,398-414,651,678-83,700,712
364-66, 370-71 creation, 403
Stateful server, 26, 178-79, 449-50 models for organizing, 401-3
Stateless server, 25-26, 180-81, 450-51 motivations for using, 399-401
Stop-and-wait protocol, 139 scheduling, 406, 412-13, 681-83, 704-5
Storage channel, 574 synchronization, 404-5, 411-12, 704
Storage service, 422 termination, 403-4
Store-and-forward communication, 61 Threads package, 36, 403
Stub, 171 CThreads package, 410, 680-1
client stub, 171-72 DeE Threads, 410-14
server stub, 171, 173 design, 403-7
Stub generation, 174, 212-16 GNU Threads, 410
Subnet number, 74 implementing, 407-10
SunOS, 410 lightweight processes package, 410
Superuser, 610 P-Threads, 410
Supervisor, 699 Threshold policy, 360
Switching techniques, 60-61 Throughput, 14
circuit switching, 60-61 Tightly-coupled system, 1
packet switching, 61 Time provider, 285
Symbolic link, 505 Time server, 287
Synchronization, 120-22, 282-346 active time server, 287-88
blocking type, 120 passive time server, 287
clock synchronization, 283-92 Timeout-based retransmission, 26, 131, 211
nonblocking type, 120 Timesharing system, 3
Synchronization variable, 241 Timestamp, 470-71, 511
Synchronous Digital Hierarchy (5DH),96 Token, 54, 303
SynchronousOpticalNETwork(SONET),95 Token manager, 477-78
SynchronousTransport Signal level n Token-passing approach, 303-5
(STS-n),95 Token ring protocol, 54-55
Synonym, 506 Total ordering of events, 297
Syntactic transparency, 170 Trampoline mechanism, 694
System failures, 23 Transaction, 25, 332, 432, 453-74
inconsistency due to, 454-55 nested transaction, 473-74
System image, 16-17 subtransaction, 473-74
System Network Architecture (SNA), 73, 84 Transparency, 17, 19,79,383,698
access transparency, 20, 424
T concurrency transparency, 22
TI signal, 95 failure transparency, 21
T3 signal, 95 forms oftransparency, 19-23
Tagged representation, 126 location transparency, 20, 497
Task assignment, 348, 351-55 migration transparency, 22
Team, 666 naming transparency, 20, 424
TELNET protocol, 78 performance transparency, 23
Thrashing, 235-36, 264-65 replication transparency, 21, 424, 442,
processor thrashing, 349-50 498

Index 743
scaling transparency, 23 Virtual path identifier (VPI) field, 97
structure transparency, 423 Virtual uniprocessor, 17,20
transparency of RPC, 170-71 Viruses, 569-70, 572
Transparent Computing Facility (TCF), 391 Volatilestorage, 448
Transport Control Protocol (TCP), 68, 77 V-system, 10,389-93~ 515, 526, 542-43,
Transport laye~ 67-68 659-74, 714-21
protocols, 77~78
Trivial FileTransfer Protocol (T~lP), 78 W
Trojan horse program, 568 Wait-die scheme, 319
Tuple space, 237 Wait-for-graph (WFG), 311, 320-29
Twin page, 261 Wait-wound scheme, 319
Two-phase multi-server commit protocol, WideAreaInformationServers(WAIS),
472-73 88-89
Wide area network (WAN),4, 47, 59-64
U Workstation, 4
Unique identifier, 500, 509, 511-12 diskful, 8
Universal Directory Service (UDS), 502 diskless, 8
Universally Unique Identifier (VUID), 221 Workstation model, 6-8
UNIX, 27,32,33,570-71,712 Workstation-server model, 8-10
UNIX emulation, 644, 661, 694-95, 697, World Wide Web(WWW), 88-89
710-12 Worms, 570-72
UNIX United, 520 Write-ahead log approach, 463-64
Untagged representation, 126 Write-invalidate protocol, 250-51
User Datagram Protocol (UDP), 68, 77 Write quorum, 445
User mobility, 20, 422, 424, 497 Write-through scheme, 437
User-mode, 610 Write-update protocol, 251-52
DUCP name space, 509
X
V X.SOO protocol, 69, 546, 548-9
Variablebit rate (VBR), 91, 92 X.400 protocol, 69
Verification, 586 X.25 protocol, 67
VersatileMessage Transfer Protocol Xerox Networking System (XNS), 73
(VMTP), 80-82 X/Open Directory Server (XDS), 556
Victim, 332 X/Open Object Management (XOM), 556
Video conferencing, 91
Virtualchannel identifier (Vel) field, 97 Z
Virtual-memory management, 684 Zone, 533-36

